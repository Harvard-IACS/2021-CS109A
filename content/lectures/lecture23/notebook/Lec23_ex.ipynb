{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5,
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import re\n",
                "from sklearn.model_selection import train_test_split\n",
                "np.random.seed(0)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Movie Review Classifier üçøüìΩÔ∏è"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In this exercise we'll be training a model to classify movie reviews as 'good' or 'bad.'\\\n",
                "The data consists of 50,000 real move reviews from IMBD.\\\n",
                "**Obligatory Disclaimer:** This is real-world data and so it's possible that it contains language or topics that some may find offensive. üôà"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We'll load the data which is hosted on the course Github repo as a zipped csv (it's too big to upload to Ed).\\\n",
                "Notice that `pd.read_csv()` can take a URL as the path argument and that we can read in a compressed file without first expanding it if we specify the `compression` format!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_url = 'https://github.com/Harvard-IACS/2021-CS109A/raw/master/content/lectures/lecture23/data/movie_reviews.zip'\n",
                "df = pd.read_csv(data_url, compression='zip')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.label.unique()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We see that the dataset consists of text reviews and binary labels. Intuitively, the positive class is \"good\" while the negative is \"bad.\""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Here are two examples from the dataset:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "labels = {0: 'bad', 1: 'good'}\n",
                "seen = {'bad': False, 'good': False}\n",
                "for i in range(df.shape[0]):\n",
                "    label = df.loc[i,'label']\n",
                "    if not seen[labels[label]]:\n",
                "        # display/print combination used to appease Ed's strange output behavior\n",
                "        display(df.loc[i, 'text'])\n",
                "        print()\n",
                "        display(f\"label: {labels[label]}\")\n",
                "        print()\n",
                "        seen[labels[label]] = True\n",
                "    if all(val == True for val in seen.values()):\n",
                "        break"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Some Preprocessing**"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In the 2nd example, we can see some html tags inside the review text.\n",
                "\n",
                "Complete the `remove_br()` function by providing its call to `re.sub()` with a regex that removes those pesky \"\\\u003cbr /\u003e\" tags from an input string, `x`.\\\n",
                "Speciffically, we should replace 2 consecutive occurances of \"\\\u003cbr /\u003e\" with a single space (can you see why?).\n",
                "\n",
                "**Hint:** It is good practice to use 'raw' string when writing regular expressions to ensure that special characters are treated correctly. Raw strings are appended with an 'r' like this: `r'this is a raw string'`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_remove_br) ###\n",
                "# fill in the regular expression\n",
                "remove_br = lambda x: re.sub(___, ' ', x)\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Use the dataframe's `apply()` method to apply `remove_br` to each review in both train and test."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "df['text'] = df.text.apply(___)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "And we can see that the tags have been removed!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.loc[4,'text']"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Don't worry about any newline characters or backslashes you may see before apostrophes in the examples above. This is just a quirk of how Jupyter displays strings by default.\\\n",
                "We don't see that these characters if we explicitly `print` the string."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "example_str = df.loc[4,'text']\n",
                "print(example_str)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We'll continue our preprocessing by next **removing punctuation**.\\\n",
                "But first, let's keep a copy of the data *with* punctuation. This will be useful at the end of the notebook when we want to display the original text of specific observations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "# store copy of data with punctuation\n",
                "df_raw = df.copy()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The next regex we need is a bit more involved.\\\n",
                "**This should match any non-whitespace, any non-alphanumeric characters, and underscores** (strangly, underscores are not covered by the first 2 conditions).\n",
                "\n",
                "**Hints:**\n",
                "- `\\w` matches alphanumeric characters\n",
                "- `\\s` matches whitespace\n",
                "- `[]` can be used to denote a set of characters. ex: `r'[ab]'` will match on 'a' *or* 'b'\n",
                "- `^` at the beginning of a character set denotes *negation*. ex: `r'[^0-9]'` will matching any non-integer\n",
                "- `|` is the *logical or* operator. ex: `r'cat|dog'` will match the strings 'cat' *or* 'dog' \n",
                "- There are many helpful sites for testing regexes. [Here's a nice one](https://www.regextester.com/)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_punc_regex) ###\n",
                "# create a regex that will match the characters described above \n",
                "punc_regex = ___"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Here we'll use an alternative to the `apply` approach we saw above.\\\n",
                "Pandas has its own set of built-in string methods which includes a version of `replace`. But unlike Python's `str.replace()` this can actually use regexes!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "df['text'] = df.text.str.replace(punc_regex, '', regex=True) # remove punctuation"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "If all went well we can see that punctuation has been removed from our dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "example_str = df.loc[4,'text']\n",
                "print(example_str)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Train/Test Split**\n",
                "\n",
                "Rather than splitting the data directly with `train_test_split` we'll instead use it to generate indices for the train and test data.\\\n",
                "This may seem strange, but there is a good reason for it. These indices will later allow us to recover the original, unprocessed text from `df_raw` for any given training and test observations. \n",
                "\n",
                "Notice too that we are stratifying on the label. This will help ensure that good and bad reviews appear in the same proportions in both train and test."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "# generate indices to designate train and test observations\n",
                "train_idx, test_idx = train_test_split(range(df.shape[0]), test_size=0.2, random_state=0, stratify=df['label'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Separate the predictor from the response\n",
                "x = df.text.values\n",
                "y = df.label.values"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create train and test sets using the generated indices\n",
                "x_train = x[train_idx]\n",
                "y_train = y[train_idx]\n",
                "x_test = x[test_idx]\n",
                "y_test = y[test_idx]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Building the Classifier Pipeline**\\\n",
                "**Step 1: Vectorizor**\n",
                "\n",
                "It's true that there are still several preprocessing steps to be done such as converting to lowercase and tokenizing the reviews, but these can be done for using sklearn's [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.feature_extraction.text import TfidfVectorizer"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Instantiate a `TfidfVectorizer` with parameters such that it will:\n",
                "- set all reviews to lowercase\n",
                "- remove english stopwords\n",
                "- exclude words that occur in less than 1 review in 10,000\n",
                "- exclude words that occur in more than 90% of reviews\n",
                "\n",
                "**Hint:** Reading the documentation, you'll see the arguments you need are `lowercase`, `stop_words`, `min_df`, and `max_df`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_tfidf) ###\n",
                "vec = TfidfVectorizer(___)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Step 2: Classifier**\n",
                "\n",
                "We'll use logistic regression with l2 regularization as our classifier model. The [LogisticRegressionCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html?highlight=logisticregressioncv#sklearn.linear_model.LogisticRegressionCV) object allows us to easily tune for the best regularization parameter."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.linear_model import LogisticRegressionCV"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "With 40,000 training observations and each word in the vectorizer's vocabulary counting acting as a predictor training could be slow.\\\n",
                "This issue is exacerbated when using cross validation as we need fit the model multiple times!\\\n",
                "We'll set our classifier CV parameters so as to help keep the training time down to around 30 seconds or so.\\\n",
                "- l2 penalty (e.g., Ridge)\n",
                "- 10 iterations per fit (remember, logistic regression has no closed form solution for the betas!)\n",
                "- 5-fold CV\n",
                "- random state of 0 (the fitting can be stochastic)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_clf) ###\n",
                "# Instantiate our Classifier\n",
                "clf = LogisticRegressionCV(___)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Step 3: Pipeline**\n",
                "\n",
                "Any text data going into our classifier will have to first be converted to numerical data by our vectorizer.\\\n",
                "One way to do this would be to:\n",
                "1. fit the vectorizor on the training data\n",
                "2. transform a dataset with the fitted vectorizer\n",
                "3. pass the transformed data to the classifier\n",
                "\n",
                "(1) only needs to be done once, but (2) \u0026 (3) would need to be done manually for train, test, and any other data we want to give them model.\\\n",
                "This would be tedious! Luckily, sklearn's [Pipline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html?highlight=pipeline#sklearn.pipeline.Pipeline) object allow use to connect one more 'transformers' (such as a scaler or vectorizer) with a model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.pipeline import make_pipeline"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Use [make_pipeline()](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=make_pipeline#sklearn.pipeline.make_pipeline) to connect the vectorizor, `vec`, and our classifier, `clf`, into a single pipeline.\n",
                "\n",
                "**Hint:** You can set `verbose=True` to see the individual steps during the fit process later."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_pipeline) ###\n",
                "# Construct the pipeline\n",
                "pipe = make_pipeline(___)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Step 4: Fitting**\n",
                "\n",
                "When it comes to fitting, we can treat the pipeline object as if it were the classifier object itself, and simply call `fit` on the pipeline."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [],
            "source": [
                "# For the sake of time, we are fitting quickly and we may not converge\n",
                "# We'll supress those pesky warnings\n",
                "from warnings import simplefilter\n",
                "from sklearn.exceptions import ConvergenceWarning\n",
                "# We also ignore FutureWarnings due to version issues on Ed\n",
                "simplefilter(\"ignore\", category=(ConvergenceWarning, FutureWarning))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_fit) ###\n",
                "# Fit the model via the pipeline\n",
                "pipe.___(___,___)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can inspect the steps of the pipeline."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [],
            "source": [
                "pipe.get_params()['steps']"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "By default they are named using the all lowercase class name of each object.\\\n",
                "We can use these names to access the fitted objects inside. Here we see the size of our vectorizer's vocabulary."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "features = pipe.get_params()['tfidfvectorizer'].get_feature_names()\n",
                "print('# of features:', len(features))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "There are too many to print, but we can peek at a random sample."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [],
            "source": [
                "sample_size = 40\n",
                "feature_sample_idx = np.random.choice(len(features), size=sample_size, replace=False)\n",
                "print(np.array(features)[feature_sample_idx])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Similarly, we can access the fitted logistic model and see what regularization parameter was used."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "best_C = pipe.get_params()['logisticregressioncv'].C_[0]\n",
                "print(f'Best C from cross-validation: {best_C:.4f}')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Step 5: Prediction**\n",
                "\n",
                "Just like we did when fitting, we can treat the pipeline object as the classifier when making predictions.\\\n",
                "Predict on the test data to get:\n",
                "1. class labels\n",
                "2. probabilities of being the positive class (i.e., 'good' reviews)\n",
                "3. test accuracy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_pred) ###\n",
                "# Predict class labels on test data\n",
                "y_pred = pipe.___(___)\n",
                "\n",
                "# Predict probabilities of the positive on the test data\n",
                "y_pred_proba = pipe.___(___)[___,___]\n",
                "\n",
                "# Calculate test accuracy (there are several ways to do this)\n",
                "test_acc = ___\n",
                "print(f\"test accuracy: {test_acc:0.3f}\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Can you get better than 0.896 by tweaking the preprocessing, or vetorizer and classifier parameters? Perhaps inspecting how our model makes its predictions may help us decide how we might improve the model in the future."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Step 6: Interpretation**\n",
                "\n",
                "Below we'll use the `eli5` library we saw in Model Interpretation Lab (#11) to have some fun interpreting what is driving our model's predictions on specific test observations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [],
            "source": [
                "# For interpretation\n",
                "import eli5\n",
                "# for parsing/formating eli5's HTML output\n",
                "from bs4 import BeautifulSoup\n",
                "# for displaying formatted HTML output\n",
                "from IPython.display import HTML"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Here are the words driving positive class predictions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [],
            "source": [
                "eli5.show_weights(clf, vec=vec, top=25)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Hmm, those digits like 710, 810, and 410 driving predictions seems strange. What might they represent?\\"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We'll use the 'raw' data with punctuation when inspecting the data (See! It is coming in handy!)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [],
            "source": [
                "x_train_raw = df_raw.text[train_idx].values\n",
                "x_test_raw = df_raw.text[test_idx].values"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_raw[df.text.str.contains(' 710 ')].iloc[0].text"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "These are actually numerical ratings embedded in the reviews! Looking at the text without the punctuation made it hard for us to see this at first."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Here's a helper function used to remove some extraneous things from `eli5`'s output. We just want to see the highlighted text.\\\n",
                "You don't need to read through the function but it is here as a nice resource/example. ü§ì"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [],
            "source": [
                "def eli5_html(clf, vec, observation):\n",
                "    \"\"\"\n",
                "    helper function for nicely formatting and displaying eli5 output\n",
                "    \"\"\"\n",
                "    # Get info on is driving a given observation's predictions\n",
                "    eli5_results = eli5.show_prediction(estimator=clf, doc=observation, vec=vec, targets=[True], target_names=['bad', 'good'])\n",
                "    # Convert eli5's HTML data to BS object for parsing/formatting\n",
                "    soup = BeautifulSoup(eli5_results.data, 'html.parser')\n",
                "    # Remove a table we don't want\n",
                "    soup.table.decompose()\n",
                "    # Remove the first \u003cp\u003e tag with unwanted text\n",
                "    soup.p.decompose()\n",
                "    # Display the newly formatted HTML!\n",
                "    display(HTML(str(soup)))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now all you need to do is find the specific observations requested.\\\n",
                "You'll need your `y_pred_proba` values for this section to find which elements from `x_test_raw` to select.\n",
                "\n",
                "**Hint:** [np.argsort()](https://numpy.org/doc/stable/reference/generated/numpy.argsort.html), [np.flip()](https://numpy.org/doc/stable/reference/generated/numpy.flip.html?highlight=flip#numpy.flip), and [np.abs()](https://numpy.org/doc/stable/reference/generated/numpy.absolute.html) may be useful here. "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### What are the **5 worst** movie reviews in the test set according to your model? üçÖ"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find indices of 5 worst reviews\n",
                "worst5 = x_test_raw[___]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {},
            "outputs": [],
            "source": [
                "for i, review in enumerate(worst5):\n",
                "    style = 'background-color:black;color:white;font-weight:bold;padding:4px'\n",
                "    display(HTML(f\"\u003cp style={style}\u003eBad Movie #{i+1} üçÖ\u003c/p\u003e\"))\n",
                "    eli5_html(clf, vec, review)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### What are the **5 best** movie review in the test set according to your model? üèÜ"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find indices of 5 best reviews\n",
                "best5 = x_test_raw[___]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [],
            "source": [
                "for i, review in enumerate(best5):\n",
                "    display(HTML(f\"\u003cp style={style}\u003eGood Movie #{i+1} üèÜ\u003c/p\u003e\"))\n",
                "    eli5_html(clf, vec, review)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "What are the **5 most 'meh'** movie review in the test set according to your model? üòê\\\n",
                "That is, which reviews are the most neutral according to your model?\\\n",
                "Upon reading some of these reviews you may find their sentiment to actually *not* be very ambiguous. What might be confusing our model?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find indices of the 5 most neutral reviews\n",
                "meh5 = x_test_raw[___]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {},
            "outputs": [],
            "source": [
                "for i, review in enumerate(meh5):\n",
                "    display(HTML(f\"\u003cp style={style}\u003e'Meh' Movie #{i+1} üòê\u003c/p\u003e\"))\n",
                "    eli5_html(clf, vec, review)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Despite some difficulties with a few of the 'meh' movies, our model is actually pretty good! In fact, it works so well you can actually use it to find _mistakes_ in the manually labeled data!\\\n",
                "This can be done by inspecting which training observation predictions differ the most from the provided labels.\\\n",
                "**(But if you do decide to explore this, just remember the disclaimer at the top of the notebook!)**"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Write your own review**\n",
                "\n",
                "Finally, you can try writing a review of your own and see what your model does with it!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "metadata": {},
            "outputs": [],
            "source": [
                "my_review = \"\"\"\n",
                "            your review here\n",
                "            \"\"\"\n",
                "\n",
                "# Remove punctuation using your regex from earlier\n",
                "my_review = re.sub(punc_regex, '', my_review)\n",
                "# Remove leading \u0026 trailing whitespace\n",
                "# and put into a numpy array (which the model expects)\n",
                "my_review = np.array([my_review.strip()])\n",
                "my_review"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "metadata": {},
            "outputs": [],
            "source": [
                "my_review_proba = pipe.predict_proba(my_review)[:,1][0]\n",
                "my_review_label = pipe.predict(my_review)[0]\n",
                "print('predicted class:', my_review_label)\n",
                "print('predicted probability:', my_review_proba)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "metadata": {},
            "outputs": [],
            "source": [
                "display(HTML(f\"\u003cp style={style}\u003eMy Review üçø\u003c/p\u003e\"))\n",
                "eli5_html(clf, vec, my_review[0])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "metadata": {},
            "outputs": [],
            "source": [
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "metadata": {},
            "outputs": [],
            "source": [
                ""
            ]
        }
    ]
}
