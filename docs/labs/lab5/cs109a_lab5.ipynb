{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109A Introduction to Data Science "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 5: Regularization and Model Selection\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2021**<br/>\n",
    "**Instructors**: Pavlos Protopapas and Natesh Pillai<br/>\n",
    "**Lab Team**: Marios Mattheakis, Hayden Joy, Chris Gumb, and Eleni Kaxiras<br/>\n",
    "**Authors**: Hayden Joy, Marios Mattheakis, Abhimanyu Vasishth\n",
    "<hr style='height:2px'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"http://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section, our goal is to get you familiarized with Regularization in Multiple Linear Regression and to start thinking about Model and Hyper-Parameter Selection. \n",
    "\n",
    "Specifically, we will:\n",
    "\n",
    "- Load in the King County House Price Dataset\n",
    "- Perform some basic EDA\n",
    "- Split the data up into a training, **validation**, and test set\n",
    "- Scale the variables (by standardizing or normalizing them) and seeing why we need to do this\n",
    "- Learn what **regularization** is and how it can help\n",
    "- Understand **ridge** and **lasso** regression\n",
    "- Get an introduction to **cross-validation** using RidgeCV and LassoCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig/meme.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data and Stats packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('max_columns', 200)\n",
    "\n",
    "# Visualization packages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "#from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA: House Prices Data From Kaggle\n",
    "\n",
    "For our dataset, we'll be using the house price dataset from [King County, WA](https://en.wikipedia.org/wiki/King_County,_Washington). The dataset is from [Kaggle](https://www.kaggle.com/harlfoxem/housesalesprediction). \n",
    "\n",
    "The task is to build a regression model to **predict the price**, based on different attributes. First, let's do some EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset \n",
    "house_df = pd.read_csv('./data/kc_house_data.csv')\n",
    "len(house_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_df = house_df.sample(frac=1, random_state=42)[0:4000]\n",
    "print(house_df.shape)\n",
    "print(house_df.dtypes)\n",
    "house_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check for null values and look at the datatypes within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's choose a subset of columns here. **NOTE**: The way I'm selecting columns here is not principled and is just for convenience. In your homework assignments (and in real life), we expect you to choose columns more rigorously.\n",
    "\n",
    "1. `bedrooms`\n",
    "2. `bathrooms`\n",
    "3. `sqft_living`\n",
    "4. `sqft_lot`\n",
    "5. `floors`\n",
    "6. `sqft_above`\n",
    "7. `sqft_basement`\n",
    "8. `lat`\n",
    "9. `long`\n",
    "10. **`price`**: Our response variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_of_interest = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'sqft_above', 'sqft_basement',\n",
    "                    'lat', 'long', 'price']\n",
    "house_df = house_df[cols_of_interest]\n",
    "\n",
    "# Convert house price to 1000s of dollars\n",
    "house_df['price'] = house_df['price']/1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the response variable (`price`) is distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "ax.hist(house_df['price'], bins=100)\n",
    "ax.set_title('Histogram of house price (in 1000s of dollars)', fontsize = 20);\n",
    "ax.set_xlabel(\"price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_df['log_price'] = np.log10(house_df['price'])\n",
    "house_df['log_price'].describe()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "ax.hist(house_df['log_price'], bins=100)\n",
    "ax.set_title('Histogram of log house price', fontsize = 20);\n",
    "ax.set_xlabel(\"price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes a bit of time but is worth it!!\n",
    "sns.pairplot(house_df[['bedrooms', 'sqft_living', 'sqft_above', 'sqft_basement']], \n",
    "             kind='reg', diag_kind='hist');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Validation-Test Split\n",
    "\n",
    "Up until this point, we have only had a train-test split. Why are we introducing a validation set? What's the point?\n",
    "\n",
    "This is the general idea: \n",
    "\n",
    "1. **Training Set**: Data you have seen. You train different types of models with various different hyper-parameters and regularization parameters on this data. \n",
    "\n",
    "\n",
    "2. **Validation Set**: Used to compare different models. We use this step to tune our hyper-parameters i.e. find the optimal set of hyper-parameters (such as $k$ for k-NN or our $\\beta_i$ values or number of degrees of our polynomial for linear regression). Pick your best model here. \n",
    "\n",
    "\n",
    "\n",
    "3. **Test Set**: Using the best model from the previous step, simply report the score e.g. $R^2$ score, MSE or any metric that you care about, of that model on your test set. **DON'T TUNE YOUR PARAMETERS HERE!**. Why, I hear you ask? Because we want to know how our model might do on data it hasn't seen before. We don't have access to this data (because it may not exist yet) but the test set, which we haven't seen or touched so far, is a good way to mimic this new data. \n",
    "\n",
    "Let's do 60% train, 20% validation, 20% test for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# first split the data into a train-test split and don't touch the test set yet\n",
    "train_val_df, test_df = train_test_split(house_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# next, split the training set into a train-validation split\n",
    "# the test-size is 0.25 since we are splitting 80% of the data into 20% and 60% overall\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.25, random_state=42)\n",
    "\n",
    "print('Train Set: {0:0.2f}%'.format(100*train_df.size/house_df.size))\n",
    "print('Validation Set: {0:0.2f}%'.format(100*val_df.size/house_df.size))\n",
    "print('Test Set: {0:0.2f}%'.format(100*test_df.size/house_df.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "In the [last section](https://github.com/Harvard-IACS/2019-CS109A/tree/master/content/sections/section3), we went over the mechanics of Multiple Linear Regression and created models that had interaction terms and polynomial terms. Specifically, we dealt with the following sorts of models. \n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_M x_M + \\epsilon\n",
    "$$\n",
    "\n",
    "Let's adopt a similar process here and get a few different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Design Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our model setup in the equation in the previous section, we obtain the following: \n",
    "\n",
    "$$\n",
    "Y = \\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_n\n",
    "\\end{bmatrix}, \\quad X = \\begin{bmatrix}\n",
    "x_{1} \\\\\n",
    "x_{2} \\\\\n",
    "\\vdots \\\\\n",
    "x_{n} \\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "x_{1,1} & x_{1,2} & \\dots & x_{1,M} \\\\\n",
    "x_{2,1} & x_{2,2} & \\dots & x_{2,M} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{n,1} & x_{n,2} & \\dots & x_{n,M} \\\\\n",
    "\\end{bmatrix}, \\quad \\beta = \\begin{bmatrix}\n",
    "\\beta_1 \\\\\n",
    "\\beta_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_M\n",
    "\\end{bmatrix}, \\quad \\epsilon = \\begin{bmatrix}\n",
    "\\epsilon_1 \\\\\n",
    "\\epsilon_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\epsilon_n\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "$$Y = X\\beta + \\epsilon$$\n",
    "\n",
    "$X$ is an n$\\times$M matrix: this is our **design matrix** (and ${x_1}$ is a vector consisting of the values of the predictors for the first data point), $\\beta$ is an M-dimensional vector (an M$\\times$1 matrix), and $Y$ is an n-dimensional vector (an n$\\times$1 matrix). In addition, we know that $\\epsilon$ is an n-dimensional vector (an n$\\times$1 matrix).\n",
    "\n",
    "We have $M$ predictors and $n$ data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df[cols_of_interest]\n",
    "y = train_df[['price']]\n",
    "print(\"X.shape:\", X.shape)\n",
    "print(\"y.shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling our Design Matrix\n",
    "\n",
    "### Warm-Up Exercise\n",
    "<img src=\"fig/apt_example.png\" width=\"400\">\n",
    "$$\\text{Euclidean Distance} : \\sqrt{ (X_{1A}-X_{1B})^2 + (X_{2A} - X_{2B})^2 }$$\n",
    "\n",
    "\n",
    "Warm-Up Exercise: for which of the following do the units of the predictors matter (e.g., trip length in minutes vs seconds; temperature in F or C)? A similar question would be: for which of these models do the magnitudes of values taken by different predictors matter? \n",
    "\n",
    "(We will go over Ridge and Lasso Regression in greater detail later)\n",
    "\n",
    "- k-NN (Nearest Neighbors regression)\n",
    "- Linear regression\n",
    "- Lasso regression\n",
    "- Ridge regression\n",
    "\n",
    "**Solutions**\n",
    "\n",
    "- kNN: **yes**. Scaling affects distance metric, which determines what \"neighbor\" means\n",
    "- Linear regression: **no**. Multiply predictor by $c$ -> divide coef by $c$.\n",
    "- Lasso: **yes**: If we divided coef by $c$, then corresponding penalty term is also divided by $c$.\n",
    "- Ridge: **yes**: Same as Lasso, except penalty divided by $c^2$.\n",
    "\n",
    "### Standard Scaler (Standardization)\n",
    " \n",
    "[Here's](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) the scikit-learn implementation of the standard scaler. What is it doing though? Hint: you may have seen this in STAT 110 or another statistics course multiple times.\n",
    "\n",
    "$$\n",
    "z = \\frac{x-\\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "In the above setup: \n",
    "\n",
    "- $z$ is the standardized variable\n",
    "- $x$ is the variable before standardization\n",
    "- $\\mu$ is the mean of the variable before standardization\n",
    "- $\\sigma$ is the standard deviation of the variable before standardization\n",
    "\n",
    "Let's see an example of how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "x = house_df[['sqft_living']]\n",
    "mu = x.mean()\n",
    "sigma = x.std()\n",
    "z = (x-mu)/sigma\n",
    "\n",
    "#declare sklearn class instance:\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# reshaping x to be a n by 1 matrix since that's how scikit learn likes data for standardization\n",
    "z_sklearn = scaler.fit_transform(x)\n",
    "\n",
    "# Plotting the histogram of the variable before standardization\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(24,5))\n",
    "ax = ax.ravel()\n",
    "\n",
    "ax[0].hist(x, bins=100, color = \"red\")\n",
    "ax[0].set_title('Histogram of sqft_living before standardization', fontsize = 20)\n",
    "\n",
    "ax[1].hist(z, bins=100)\n",
    "ax[1].set_title('Manually standardizing sqft_living', fontsize = 20)\n",
    "\n",
    "ax[2].hist(z_sklearn, bins=100)\n",
    "ax[2].set_title('Standardizing sqft_living using scikit learn', fontsize = 20);\n",
    "print(z['sqft_living'].values)\n",
    "# making things a dataframe to check if they work\n",
    "pd.DataFrame({'x': x['sqft_living'], 'z_manual': z['sqft_living'], 'z_sklearn': z_sklearn.flatten()}).describe().iloc[[1,2],:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min-Max Scaler (Normalization)\n",
    "\n",
    "[Here's](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) the scikit-learn implementation of the min-max scaler. What is it doing though? \n",
    "\n",
    "$$\n",
    "x_{new} = \\frac{x-x_{min}}{x_{max}-x_{min}}\n",
    "$$\n",
    "\n",
    "In the above setup: \n",
    "\n",
    "- $x_{new}$ is the normalized variable\n",
    "- $x$ is the variable before normalized\n",
    "- $x_{max}$ is the max value of the variable before normalization\n",
    "- $x_{min}$ is the min value of the variable before normalization\n",
    "\n",
    "Let's see an example of how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "x = house_df['sqft_living']\n",
    "x_new = (x-x.min())/(x.max()-x.min())\n",
    "\n",
    "# reshaping x to be a n by 1 matrix since that's how scikit learn likes data for normalization\n",
    "x_reshaped = np.array(x).reshape(-1,1)\n",
    "x_new_sklearn = MinMaxScaler().fit_transform(x_reshaped)\n",
    "\n",
    "# Plotting the histogram of the variable before normalization\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(24,5))\n",
    "ax = ax.ravel()\n",
    "\n",
    "titles = ['Histogram of sqft_living before normalization',\n",
    "         'Manually normalizing sqft_living',\n",
    "         'Normalizing sqft_living using scikit learn']\n",
    "\n",
    "for i, design_matrix in enumerate([x, x_new, x_new_sklearn]):\n",
    "    ax[i].hist(design_matrix, bins=100)\n",
    "    ax[i].set_title(titles[i], fontsize = 25)\n",
    "    ax[i].tick_params(axis='both', which='major', labelsize=20)\n",
    "plt.tight_layout()\n",
    "\n",
    "# making things a dataframe to check if they work\n",
    "pd.DataFrame({'x': x, 'x_new_manual': x_new, 'x_new_sklearn': x_new_sklearn.flatten()}).describe().iloc[[1,2,3,7],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(16,5))\n",
    "ax.hist((x - np.mean(x))/np.std(x), bins=100, alpha = 0.8, label = \"sq feet standardized\", color = \"tab:blue\", edgecolor = \"none\")\n",
    "ax.hist(x_new, bins=100, label = \"sq feet normalized\", color = \"darkorange\", alpha = 1, edgecolor = \"none\")\n",
    "\n",
    "ax.set_title('Histogram of sqft_living Comparing normalization and standardization')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The million dollar question**\n",
    "\n",
    "Should I standardize or normalize my data? [This](https://medium.com/@rrfd/standardize-or-normalize-examples-in-python-e3f174b65dfc), [this](https://medium.com/@swethalakshmanan14/how-when-and-why-should-you-normalize-standardize-rescale-your-data-3f083def38ff) and [this](https://stackoverflow.com/questions/32108179/linear-regression-normalization-vs-standardization) are useful resources that I highly recommend. But in a nutshell, what they say is the following: \n",
    "\n",
    "**Pros of Normalization**\n",
    "\n",
    "1. Normalization (which makes your data go from 0-1) is widely used in image processing and computer vision, where pixel intensities are non-negative and are typically scaled from a 0-255 scale to a 0-1 range for a lot of different algorithms. \n",
    "2. Normalization is also very useful in neural networks (which we will see later in the course) as it leads to the algorithms converging faster. #next semester also we will see\n",
    "3. Normalization is useful when your data does not have a discernible distribution and you are not making assumptions about your data's distribution.\n",
    "\n",
    "**Pros of Standardization**\n",
    "\n",
    "1. Standardization maintains outliers (do you see why?) whereas normalization makes outliers less obvious. In applications where outliers are useful, standardization should be done.\n",
    "2. Standardization is useful when you assume your data comes from a Gaussian distribution (or something that is approximately Gaussian). \n",
    "\n",
    "**Some General Advice**\n",
    "\n",
    "1. We use the data to calculate the parameters for standardization ($\\mu$ and $\\sigma$) and for normalization ($x_{min}$ and $x_{max}$). Make sure these parameters are learned on the training set i.e use the training set parameters even when normalizing/standardizing the test set. In sklearn terms, fit your scaler on the training set and use the scaler to transform your test set and validation set (**don't re-fit your scaler on test set data!**).\n",
    "2. The point of standardization and normalization is to make your variables take on a more manageable scale. You should ideally standardize or normalize all your variables at the same time. \n",
    "3. Standardization and normalization is not always needed and is not an automatic thing you have to do on any data science homework!! Do so sparingly and try to justify why this is needed.\n",
    "\n",
    "**Interpreting Coefficients**\n",
    "\n",
    "A great quote from [here](https://stats.stackexchange.com/questions/29781/when-conducting-multiple-regression-when-should-you-center-your-predictor-varia)\n",
    "\n",
    "> [Standardization] makes it so the intercept term is interpreted as the expected value of 𝑌𝑖 when the predictor values are set to their means. Otherwise, the intercept is interpreted as the expected value of 𝑌𝑖 when the predictors are set to 0, which may not be a realistic or interpretable situation (e.g. what if the predictors were height and weight?)\n",
    "\n",
    "### Standardizing our Design Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'sqft_above', 'sqft_basement',\n",
    "            'lat', 'long']\n",
    "\n",
    "X_train = train_df[features]\n",
    "X_val = val_df[features]\n",
    "X_test = test_df[features]\n",
    "\n",
    "#declare specific scaler instance from StandardScaler class. Note that this is not a function.\n",
    "scaler = StandardScaler()\n",
    "\n",
    "vars(scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The vars function can be a useful way to check class attributes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now fit the scaler\n",
    "scaler.fit(X_train)\n",
    "vars(scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notice how the scaler class instance is storing pesky details —like the mean and standard deviations of the individual features of the data for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This converts our matrices into numpy matrices\n",
    "X_train_t, X_val_t, X_test_t = [scaler.transform(df) for df in [X_train, X_val, X_test]]\n",
    "\n",
    "# Making the numpy matrices pandas dataframes\n",
    "dfs = [ pd.DataFrame(np_array, columns=features) for np_array in [X_train_t, X_val_t, X_test_t]]\n",
    "\n",
    "X_train, X_val, X_test = dfs\n",
    "\n",
    "[display(df.describe().iloc[[1,2],:]) for df in dfs];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same code without list comprehension:\n",
    "\n",
    "# X_train_t = scaler.transform(X_train)\n",
    "# X_val_t = scaler.transform(X_val)\n",
    "# X_test_t =  scaler.transform(X_test)\n",
    "\n",
    "# X_train_df = pd.DataFrame(X_train_t, columns=features)\n",
    "# X_val_df = pd.DataFrame(X_val_t, columns=features)\n",
    "# X_test_df = pd.DataFrame(X_test_t, columns=features)\n",
    "\n",
    "# display(X_train_df.describe())\n",
    "# display(X_val_df.describe())\n",
    "# display(X_test_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df[['log_price']]\n",
    "y_val = val_df[['log_price']]\n",
    "y_test = test_df[['log_price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare scaler class instance from StandardScaler class\n",
    "y_scaler = StandardScaler()\n",
    "\n",
    "#call the .fit_transform method (a method is a class function) to return the transformed data\n",
    "y_train = scaler.fit_transform(y_train)\n",
    "\n",
    "#use this to transform the validation and test sets\n",
    "y_val = scaler.transform(y_val)\n",
    "y_test = scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $PolynomialFeatures$\n",
    "A common problem that students have when using this class is that they forget that $PolynomialFeatures$ adds a constant column. This can come back to bite us, especially if we aren't aware of it when we use PolynomialFeatures.\n",
    "\n",
    "#### To simply add a bias column using $PolynomialFeatures$ set the degree equal to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "#declare polynomial features object instance\n",
    "poly_object = PolynomialFeatures(degree = 1, include_bias = True)\n",
    "\n",
    "#now we can call the fit_transform method on the dataframe, := also assigns X_train_df to the reference X.\n",
    "X_np = poly_object.fit_transform(X := X_train[['sqft_living', 'bedrooms']])\n",
    "cols = list(X.columns)\n",
    "\n",
    "if poly_object.include_bias:\n",
    "    cols = ['const'] + cols\n",
    "\n",
    "X_pd = pd.DataFrame(X_np, columns = cols)\n",
    "X_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we simply want to add a bias column an easier way is to use use stats_model's `add_constant` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "X_train_bias = sm.add_constant(X_train)\n",
    "X_train_bias.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $PolynomialFeatures$ is more powerful though, as it can add squared columns to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_object = PolynomialFeatures(degree = 2, include_bias = True, interaction_only = False)\n",
    "\n",
    "X_np = poly_object.fit_transform(X)\n",
    "pd.DataFrame(X_np).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### But we lost our column names! No problem. If you saved your polynomial_features $object$ there should be no problem. We can call the $instance's$ `get_feature_names` method to retrieve them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col_names = poly_object.get_feature_names(X.columns)\n",
    "new_col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pd = pd.DataFrame(X_np, columns = new_col_names)\n",
    "X_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do have a column of bias in the dataframe you are using to fit, be sure not to fit an intercept! Otherwise sklearn will add a redundant column of ones which can lead to unstable beta0 estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is bootstrapping?\n",
    "\n",
    "Bootstrapping is a procedure for resampling a dataset with replacement to produce an **empirical distribution** of the value of interest.\n",
    "\n",
    "#### Key points:\n",
    "\n",
    "1) Ideally we need to preserve key statistics about the original distribution to come up with a good empirical distribution.\n",
    "\n",
    "For example, in order to preserve the standard error of the original data we need to sample the same number of points as were in the original data.\n",
    "\n",
    "$$SE = \\frac{\\sigma}{\\sqrt{n}}$$\n",
    "\n",
    "2) We must sample with replacement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of bootstraps\n",
    "\n",
    "boot_linreg_models = []\n",
    "boot_betas = []\n",
    "numboot = 200\n",
    "\n",
    "col = [\"sqft_above\"]\n",
    "X_tr_df = X_train[col]\n",
    "\n",
    "for i in range(numboot):\n",
    "    \n",
    "    #randomly sample indices\n",
    "    boostrap_i_indices= np.random.choice(X_tr_df.index, replace = True, size = len(X_tr_df.index))\n",
    "    \n",
    "    #get bootstrapped dfs\n",
    "    Xtr_boot = X_tr_df.iloc[boostrap_i_indices,:]\n",
    "    ytr_boot = y_train[boostrap_i_indices]\n",
    "    \n",
    "    bootstrap_lr = LinearRegression(fit_intercept = True).fit(Xtr_boot, ytr_boot)\n",
    "    \n",
    "    boot_linreg_models.append(bootstrap_lr)\n",
    "    coefs = (bootstrap_lr.intercept_[0], bootstrap_lr.coef_[0][0])\n",
    "    boot_betas.append(coefs)\n",
    "\n",
    "#linreg_models = np.array(linreg_models)\n",
    "boot_betas = pd.DataFrame(boot_betas, columns = [\"beta0\", \"beta1\"])\n",
    "boot_betas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot the bootstrap model predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bootstrap_predictions(boot_linreg_models):\n",
    "    plt.figure(figsize=(16, 4))\n",
    "    mses = []\n",
    "    plt.scatter(X_val[col], y_val, color = \"blue\", alpha = 0.2, label = \"training set\")\n",
    "    for i, model in enumerate(boot_linreg_models[:30]):\n",
    "        pred = model.predict(X_val[col])\n",
    "        if i != 0:\n",
    "            plt.plot(X_val[col], pred, color = \"red\", alpha = 0.1)\n",
    "        else:\n",
    "            label = \"prediction\"\n",
    "            plt.plot(X_val[col], pred, color = \"red\", alpha = 0.5, label = label)\n",
    "\n",
    "        mses.append(r2_score(y_val, pred))\n",
    "    plt.xlabel(col[0], fontsize = 16)\n",
    "    plt.ylabel(\"log price\", fontsize = 16)\n",
    "    plt.legend();\n",
    "plot_bootstrap_predictions(boot_linreg_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now lets plot the confidence intervals for beta0 and beta1:\n",
    "\n",
    "\n",
    "#### What is a confidence interval?\n",
    "\n",
    "A confidence interval is a range of values that is likely to include a parameter of interest with some degree of certainty or \"confidence.\"\n",
    "\n",
    "#### How do we interpret 95% confidence intervals?\n",
    "\n",
    "If we were to compute 95% confidence intervals for each of K repeated samples, we would expect 0.95\\*K of those confidence intervals to contain the true parameter of interest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_colors = [\"tab:blue\", \"teal\"]\n",
    "fig, ax = plt.subplots(1,2, figsize = (10,5))\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i in range(2):\n",
    "    betavals = boot_betas.iloc[ : , i]\n",
    "    betavals.values.sort()\n",
    "    \n",
    "    x1 = np.percentile(betavals, 5)\n",
    "    x2 = np.percentile(betavals, 95)\n",
    "    \n",
    "    x = np.linspace( x1, x2, 500)\n",
    "    counts, bins = np.histogram( betavals )\n",
    "    y = counts.max() \n",
    "    plt.sca( ax[i] )\n",
    "    plt.fill_between(x, y+100, color = 'red',alpha=0.3)\n",
    "    plt.hist(betavals, \n",
    "             bins = bins, \n",
    "             color=hist_colors[i],\n",
    "             alpha=1,\n",
    "             edgecolor='black', \n",
    "             linewidth=1)\n",
    "    plt.ylim(0,y+ 4)\n",
    "    plt.ylabel(f'Distribution of beta {i}', fontsize=18)\n",
    "    plt.xlabel(f'Value of beta {i}', fontsize=18)\n",
    "    plt.axvline(x = np.mean(betavals), color='w', linewidth = 3, linestyle = \"--\")\n",
    "\n",
    "#fig.delaxes(ax[3])\n",
    "fig.suptitle(f'90 % confidence interval of Model Coefficients', fontsize = 20)\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <strong>🏋🏻‍♂️ TEAM ACTIVITY 1:</strong> The bootstrap loop below is broken. How can we fix it? </div>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: focus on the line with this pandas dataframe method, use tab inside the parenthesis to see the documentation.\n",
    "\n",
    "\n",
    "`pd.DataFrame.sample()`\n",
    "\n",
    "If you have extra time, standardize each Xtr_boot and ytr_boot within the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of bootstraps\n",
    "\n",
    "boot_linreg_models = []\n",
    "boot_betas = []\n",
    "numboot = 200\n",
    "\n",
    "col = [\"sqft_above\"]\n",
    "X_tr_df = X_train[col]\n",
    "\n",
    "for i in range(numboot):\n",
    "    \n",
    "    #randomly sample indices\n",
    "    Xtr_boot = X_tr_df.sample(10) #############################<--- edit this line\n",
    "    \n",
    "    #get bootstrapped dfs\n",
    "    boostrap_i_indices = list(Xtr_boot.index)\n",
    "    ytr_boot = y_train[boostrap_i_indices]\n",
    "    \n",
    "    #as a bonus you can standardize the bootstrapped design matrix and response variable with StandardScaler\n",
    "    #Xtr_boot = ...\n",
    "    #ytr_boot = ...\n",
    "    \n",
    "    bootstrap_lr = LinearRegression(fit_intercept = True).fit(Xtr_boot, ytr_boot)\n",
    "    \n",
    "    boot_linreg_models.append(bootstrap_lr)\n",
    "    coefs = (bootstrap_lr.intercept_[0], bootstrap_lr.coef_[0][0])\n",
    "    boot_betas.append(coefs)\n",
    "\n",
    "#linreg_models = np.array(linreg_models)\n",
    "boot_betas = pd.DataFrame(boot_betas, columns = [\"beta0\", \"beta1\"])\n",
    "boot_betas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bootstrap_predictions(boot_linreg_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "## What is Regularization and why should I care?\n",
    "\n",
    "When we have a lot of predictors, we need to worry about overfitting. Let's check this out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from statsmodels.regression.linear_model import OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 6\n",
    "degrees = [1,2,3,N]\n",
    "r2_train = []\n",
    "r2_val = []\n",
    "mse_train = []\n",
    "mse_val = []\n",
    "\n",
    "for i, d in enumerate(degrees):\n",
    "    poly = PolynomialFeatures(d, include_bias = True)\n",
    "    X_tr = poly.fit_transform(X_train)\n",
    "    Xval = poly.fit_transform(X_val)\n",
    "    \n",
    "    model_N = LinearRegression(fit_intercept = False).fit(X_tr, y_train)\n",
    "    y_pred_tra = model_N.predict(X_tr)\n",
    "    y_pred_val = model_N.predict(Xval)\n",
    "    \n",
    "    r2_train.append(r2_score(y_train, y_pred_tra))\n",
    "    r2_val.append(r2_score(y_val, y_pred_val))\n",
    "    \n",
    "    mse_train.append(mean_squared_error(y_train, y_pred_tra))\n",
    "    mse_val.append(mean_squared_error(y_val, y_pred_val))\n",
    "\n",
    "best_least_squares_r2 = np.max(r2_val)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,4))\n",
    "plt.sca(ax[0])\n",
    "plt.plot(degrees, mse_train, 'o-', label=r'Training $MSE$')\n",
    "plt.plot(degrees, mse_val, 'o-', label=r'Validation $MSE$')\n",
    "plt.xlabel('Number of degree of polynomial')\n",
    "plt.ylabel(r'$MSE$')\n",
    "plt.title(r'$MSE$ vs polynomial degree')\n",
    "plt.yscale('log')\n",
    "plt.legend();\n",
    "\n",
    "plt.sca(ax[1])\n",
    "plt.plot(degrees, r2_train, 'o-', label=r'Training $R^2$')\n",
    "plt.plot(degrees, r2_val, 'o-', label=r'Validation $R^2$')\n",
    "plt.xlabel('Number of degree of polynomial')\n",
    "plt.ylabel(r'$R^2$')\n",
    "plt.title(r'$R^2$ vs polynomial degree')\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(model_N.coef_.reshape(-1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice a big difference between training and validation R^2 scores: seems like we are overfitting. **Introducing: regularization.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about Multicollinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's seemingly a lot of multicollinearity in the data. Here is a standard collinearity warning issued by statsmodels:\n",
    "\n",
    "<img src=\"fig/warning.png\" width=400>\n",
    "\n",
    "What is [multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity)? Why do we have it in our dataset? Why is this a problem? \n",
    "\n",
    "Does regularization help solve the issue of multicollinearity? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does Regularization help with?\n",
    "\n",
    "We have some pretty large and extreme coefficient values in our most recent models. These coefficient values also have very high variance. We can also clearly see some overfitting to the training set. In order to reduce the coefficients of our parameters, we can introduce a penalty term that penalizes some of these extreme coefficient values. Specifically, regularization helps us: \n",
    "\n",
    "1. Avoid overfitting. Reduce features that have weak predictive power.\n",
    "2. Discourage the use of a model that is too complex\n",
    "\n",
    "<img src=\"./fig/overfit.png\" width=\"600\">\n",
    "\n",
    "### Big Idea: Reduce Variance by Increasing Bias\n",
    "\n",
    "Image Source: [here](https://www.cse.wustl.edu/~m.neumann/sp2016/cse517/lecturenotes/lecturenote12.html)\n",
    "\n",
    "<img src=\"fig/bias_variance.png\" width=\"600\">\n",
    "\n",
    "## Ridge Regression\n",
    "\n",
    "Ridge Regression is one such form of regularization. In practice, the ridge estimator reduces the complexity of the model by shrinking the coefficients, but it doesn’t nullify them. We control the amount of regularization using a parameter $\\lambda$. **NOTE**: sklearn's [ridge regression package](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) represents this $\\lambda$ using a parameter alpha. In Ridge Regression, the penalty term is proportional to the L2-norm of the coefficients. \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"fig/ridge.png\" width=\"400\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Lasso Regression\n",
    "\n",
    "Lasso Regression is another form of regularization. Again, we control the amount of regularization using a parameter $\\lambda$. **NOTE**: sklearn's [lasso regression package](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) represents this $\\lambda$ using a parameter alpha. In Lasso Regression, the penalty term is proportional to the L1-norm of the coefficients. \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"fig/lasso.png\" width=\"400\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Some Differences between Ridge and Lasso Regression\n",
    "\n",
    "1. Since Lasso regression tend to produce zero estimates for a number of model parameters - we say that Lasso solutions are **sparse** - we consider to be a method for variable selection.\n",
    "2. In Ridge Regression, the penalty term is proportional to the L2-norm of the coefficients whereas in Lasso Regression, the penalty term is proportional to the L1-norm of the coefficients.\n",
    "3. Ridge Regression has a closed form solution! Lasso Regression does not. We often have to solve this iteratively. In the sklearn package for Lasso regression, there is a parameter called `max_iter` that determines how many iterations we perform. \n",
    "\n",
    "<img src=\"fig/3d_ridge_lasso.png\" width=\"800\">\n",
    "<img src=\"fig/Lasso_formula.png\" width=\"400\">\n",
    "<img src=\"fig/Ridge_formula.png\" width=\"400\">\n",
    "\n",
    "<img src=\"fig/ridge_geom_breakdown.png\" width=\"800\">\n",
    "<img src=\"fig/ridge_geometric.png\" width=\"400\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"fig/lasso_geometric.png\" width=\"400\">\n",
    "\n",
    "\n",
    "\n",
    "### Why Standardizing Variables was not a waste of time\n",
    "\n",
    "Lasso regression puts constraints on the size of the coefficients associated to each variable. However, this value will depend on the magnitude of each variable. It is therefore necessary to standardize the variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's use Ridge and Lasso to regularize our degree N polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Play around with different values of alpha. Notice the new $R^2$ value and also the range of values that the predictors take in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 4\n",
    "poly = PolynomialFeatures(N)\n",
    "X_train_df_N = poly.fit_transform(X_train)\n",
    "X_val_df_N = poly.fit_transform(X_val)\n",
    "\n",
    "n_scaler = StandardScaler()\n",
    "X_train_df_N = n_scaler.fit_transform(X_train_df_N)\n",
    "X_val_df_N = n_scaler.transform(X_val_df_N )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# some values you can try out: 0.01, 0.1, 0.5, 1, 5, 10, 20, 40, 100, 200, 500, 1000, 10000\n",
    "alpha =0.01\n",
    "\n",
    "ridge_model = Ridge(alpha=alpha).fit(X_train_df_N, y_train)\n",
    "\n",
    "print('R squared score for our original OLS model: {}'.format(r2_val[-1]))\n",
    "print('R squared score for Ridge with alpha={}: {}'.format(alpha, ridge_model.score(X_val_df_N,y_val)))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18,8), ncols=2)\n",
    "ax = ax.ravel()\n",
    "ax[0].hist(model_N.coef_.flatten(), bins=10, alpha=0.5)\n",
    "ax[0].set_title('Histogram of predictor values for Original model with N: {}'.format(N))\n",
    "ax[0].set_xlabel('Beta values')\n",
    "ax[0].set_ylabel('Frequency')\n",
    "\n",
    "ax[1].hist(ridge_model.coef_.flatten(), bins=20, alpha=0.5)\n",
    "ax[1].set_title('Histogram of predictor values for Ridge Model with alpha: {}'.format(alpha))\n",
    "ax[1].set_xlabel('Beta values')\n",
    "ax[1].set_ylabel('Frequency');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# some values you can try out: 0.00001, 0.0001, 0.001, 0.01, 0.05, 0.1, 0.5, 1, 2, 5, 10, 20\n",
    "#alpha = 0.0001\n",
    "lasso_model = Lasso(alpha=alpha, max_iter = 1000).fit(X_train_df_N, y_train)\n",
    "\n",
    "print('R squared score for our original model: {}'.format(r2_val[-1]))\n",
    "print('R squared score for Lasso with alpha={}: {}'.format(alpha, lasso_model.score(X_val_df_N,y_val)))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18,8), ncols=2)\n",
    "ax = ax.ravel()\n",
    "ax[0].hist(model_N.coef_.flatten(), bins=10, alpha=0.5)\n",
    "ax[0].set_title('Histogram of predictor values for Original model with N: {}'.format(N), fontsize = 20)\n",
    "ax[0].set_xlabel('Predictor values')\n",
    "ax[0].set_ylabel('Frequency')\n",
    "\n",
    "ax[1].hist(lasso_model.coef_.flatten(), bins=20, alpha=0.5)\n",
    "ax[1].set_title('Histogram of predictor values for Lasso Model with alpha: {}'.format(alpha), fontsize = 20)\n",
    "ax[1].set_xlabel('Predictor values')\n",
    "ax[1].set_ylabel('Frequency');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection and Cross-Validation\n",
    "\n",
    "Here's our current setup so far: \n",
    "\n",
    "<img src=\"fig/cross_val.png\" width=\"400\">\n",
    "\n",
    "\n",
    "So we try out 10,000 different models on our validation set and pick the one that's the best? No! **Since we could also be overfitting the validation set!** \n",
    "\n",
    "One solution to the problems raised by using a single validation set is to evaluate each model on multiple validation sets and average the validation performance. This is the essence of cross-validation!\n",
    "\n",
    "<img src=\"fig/cross_val.png\" width=\"700\">\n",
    "\n",
    "Image source: [here](https://medium.com/@sebastiannorena/some-model-tuning-methods-bfef3e6544f0)\n",
    "\n",
    "Let's give this a try using [RidgeCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html) and [LassoCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "alphas = (0.001, 0.01, 0.1, 10, 100, 1000, 10000)\n",
    "\n",
    "# Let us do k-fold cross validation \n",
    "k = 4\n",
    "fitted_ridge = RidgeCV(alphas=alphas, cv = k).fit(X_train_df_N, y_train)\n",
    "fitted_lasso = LassoCV(alphas=alphas, cv = k).fit(X_train_df_N, y_train)\n",
    "\n",
    "print('R^2 score for our original least squares model: {}\\n'.format(r2_val[-1]))\n",
    "\n",
    "ridge_a = fitted_ridge.alpha_\n",
    "print('Best alpha for ridge: {}'.format(ridge_a))\n",
    "print('R^2 score for Ridge with alpha={}: {}\\n'.format(ridge_a, fitted_ridge.score(X_val_df_N,y_val)))\n",
    "\n",
    "lasso_a = fitted_lasso.alpha_\n",
    "print('Best alpha for lasso: {}'.format(lasso_a))\n",
    "print('R squared score for Lasso with alpha={}: {}'.format(lasso_a, fitted_lasso.score(X_val_df_N,y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the coefficients of our CV models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Step:** report the score on the test set for the model you have chosen to be the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn's <a href = 'https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html' > cross_validate</a> function:\n",
    "\n",
    "You may find this useful on the homework. Say you have fit a model on the training set. You will be asked to perform cross validation and score models on the homework based on this function. Below is an example of how to do this. You will also want to choose an appropriate <a href = 'https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter'> scoring method</a>. Please read the documentation for this function. I've included some psuedo-code below:\n",
    "\n",
    "loss_score = cross_validate(model, X, y, cv=3, scoring = 'r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df_N.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "#scale the design matrix\n",
    "X_train_df_N = StandardScaler().fit_transform(X_train_df_N)\n",
    "y_train= StandardScaler().fit_transform(y_train)\n",
    "\n",
    "#alpha values to optimize\n",
    "alphas = [10**i for i in np.linspace(0, 5, 5)]\n",
    "sklearn_models = []\n",
    "\n",
    "#fit our various models\n",
    "for i, alpha in enumerate(alphas):\n",
    "    model = Ridge(alpha=alpha)\n",
    "    sklearn_models.append(model)\n",
    "\n",
    "#get the cross validation scores\n",
    "for i, model in enumerate(sklearn_models):\n",
    "    #if i == 0 initialize our lists\n",
    "    if not i:\n",
    "        r2_train = []\n",
    "        r2_val = []\n",
    "\n",
    "    r2_cv_scores = cross_validate(model, X_train_df_N, y_train, cv=4, scoring = 'r2')\n",
    "    r2_avg_score = np.mean(r2_cv_scores[\"test_score\"])\n",
    "    r2_train.append(r2_avg_score)\n",
    "    \n",
    "#plot\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.plot(np.log10(alphas), r2_train, 'o-', label=r'CV $R^2$ score')\n",
    "ax.set_xlabel('log10(alpha)')\n",
    "ax.set_ylabel(r'Cross Validated $R^2$ score')\n",
    "ax.set_title(r'Ridge Regression $R^2$ score vs log10(alpha)')\n",
    "ax.legend()\n",
    "\n",
    "print(\"Best R^2 Score: {}: \".format(np.max(r2_train)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <strong>🏋🏻‍♂️ TEAM ACTIVITY 2:</strong> Fixing Ridge and Lasso CV </div>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Finish the `preprocess` helper function <br>\n",
    "2) Finish the `fit_ridge_and_lasso_cv` helper function <br>\n",
    "3) Play with the arguments of the `preprocess` and `fit_ridge_and_lasso_cv` helper functions to try and improve the performance on the test set. <br>\n",
    "4) Talk with your break out room members about the questions posed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run make_plots/make_plots.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"./data/team_exercise2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Fill out the standardization lines in this helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, standardize = False):\n",
    "    \"\"\"Splits the data into training and validation sets.\n",
    "    arguments:\n",
    "        df: the dataframe of training and test data you want to split.\n",
    "        standardize: if True returns standardized data.\n",
    "    \"\"\"\n",
    "    #split the data\n",
    "    train, test = train_test_split(df, train_size=0.8, random_state = 42)\n",
    "    \n",
    "    #sort the data\n",
    "    train = train.sort_values(by = [\"x1\"])\n",
    "    test = test.sort_values(by = [\"x1\"])\n",
    "\n",
    "    train.describe()\n",
    "\n",
    "    X_train, y_train = train[[\"x1\"]], train[\"y\"]\n",
    "    X_test, y_test = test[[\"x1\"]], test[\"y\"]\n",
    "\n",
    "    X_train_N = add_higher_order_polynomial_terms(X_train, N=15)\n",
    "    X_test_N = add_higher_order_polynomial_terms(X_test, N=15)\n",
    "    \n",
    "    if standardize:\n",
    "        scaler = ...\n",
    "        X_train_N = ...\n",
    "        X_test_N = ...\n",
    "    \n",
    "    #\"X_val\" : X_val_N, \"y_val\" : y_val,\n",
    "    datasets = {\"X_train\": X_train_N, \"y_train\":  y_train,  \"X_test\" : X_test_N, \"y_test\": y_test}\n",
    "    return(datasets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Fill out the missing lines in this helper function in order to perform cross validated ridge and lasso regression on the synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ridge_and_lasso_cv(X_train, y_train,  X_test, y_test,  k = None, alphas = [10**7],\n",
    "                           best_OLS_r2 = best_least_squares_r2): #X_val, y_val,\n",
    "    \"\"\" takes in train and validation test sets and reports the best selected model using ridge and lasso regression.\n",
    "    Arguments:\n",
    "        X_train: the train design matrix\n",
    "        y_train: the reponse variable for the training set\n",
    "        X_val: the validation design matrix\n",
    "        y_train: the reponse variable for the validation set\n",
    "        k: the number of k-fold cross validation sections to be fed to Ridge and Lasso Regularization.\n",
    "    \"\"\"\n",
    "    #declare the ridge and lasso cv objects: (change the arguments to include alphas and k).\n",
    "    # Let us do k-fold cross validation.\n",
    "    ridgecv_instance = RidgeCV(...)\n",
    "    lassocv_instance = LassoCV(...)\n",
    "    \n",
    "    # now fit the cv_object instances.\n",
    "    ridgecv_instance.fit(...)\n",
    "    lassocv_instance.fit(...)\n",
    "    \n",
    "    print('R^2 score for our original OLS model: {}\\n'.format(best_OLS_r2))\n",
    "\n",
    "    ridge_a = ridge_instance.alpha_\n",
    "    ridge_score = ridge_instance.score(X_test, y_test)\n",
    "    print('Best alpha for ridge: {}'.format(ridge_a))\n",
    "    print('R^2 score for Ridge with alpha={}: {}\\n'.format(ridge_a, ridge_score))\n",
    "\n",
    "    lasso_a = lasso_instance.alpha_\n",
    "    lasso_score = lasso_instance.score(X_test, y_test)\n",
    "    print('Best alpha for lasso: {}'.format(lasso_a))\n",
    "    \n",
    "    print('R^2 score for Lasso with alpha={}: {}'.format(lasso_a, lasso_score))\n",
    "    \n",
    "    r2_df = pd.DataFrame({\"OLS\": best_OLS_r2, \"Lasso\" : lasso_score, \"Ridge\" : ridge_score}, index = [0])\n",
    "    r2_df = r2_df.melt()\n",
    "    r2_df.columns = [\"model\", \"r2_Score\"]\n",
    "    plt.title(\"Validation set\")\n",
    "    sns.barplot(x = \"model\", y = \"r2_Score\", data = r2_df)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are on your local machine you may view the solutions by running the following magic command. We will release a notebook with the solutions later on ed as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"./solutions/solution1.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 1) Run the cell below to see the performance of the OLS, Lasso, and Ridge models on the test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = preprocess(df1)\n",
    "#Here we input arguments to a function from a dictionary using the ** syntax option.\n",
    "# X_train, y_train, X_val, y_val,\n",
    "\n",
    "fit_ridge_and_lasso_cv(**datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) How can we improve our results using ridge and lasso (ie boost the $R^2$ score?). Try to play with the arguments of the helper functions to increase the $R^2$ scores and discuss with your group why this works. Don't split the data or try to write your own functions here.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TODO (hint copy and paste the previous cell and then change arguments)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"./solutions/solution2.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***3) Would we in effect be cheating if we optimize our model over k at this stage? Why?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "### End of Standard Section\n",
    "---------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
