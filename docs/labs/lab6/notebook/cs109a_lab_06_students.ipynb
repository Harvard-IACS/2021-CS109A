{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109A Introduction to Data Science "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 6: Principal Components Analysis (PCA)\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2021**<br/>\n",
    "**Instructors**: Pavlos Protopapas and Natesh Pillai<br/>\n",
    "**Lab Team**: Marios Mattheakis, Hayden Joy, Chris Gumb, and Eleni Kaxiras<br/>\n",
    "**Authors**: Marios Mattheakis, Hayden Joy \n",
    "<hr style='height:2px'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"http://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this Lab, our goal is to get you familiarized with Dimensionality Reduction using Principal Components Analysis (PCA). This [medium article](https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60) was referenced extensively while creating this notebook.\n",
    "\n",
    "Specifically, we will:\n",
    "\n",
    "- Define the terms **big data** and **high-dimensionality** \n",
    "- Learn what PCA is. What is the motivation to use PCA. \n",
    "- Learn about the sklearn PCA library and [its nuances](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
    "- Get familiar with the Linear Algebra of PCA\n",
    "- Use PCA to **visualize** high-dimensional data in  2-dimensions\n",
    "- Meet the MNIST handwritten digit dataset (and hopefully stay friends for a while)\n",
    "- Use PCA in order to **improve model training time** and understand the **speed-accuracy trade-off**\n",
    "- Discuss the assumptions behind PCA to understand when to use PCA and when not to use it\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../fig/meme.png\" width=\"400\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data and stats packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization packages\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "#\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# NEW PACKAGES\n",
    "from sklearn.decomposition import PCA\n",
    "from time import time\n",
    "\n",
    "\n",
    "# Other packages\n",
    "pd.set_option('display.max_columns', 50)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Components Analysis helps us deal with high-dimensionality in big-data. \n",
    "\n",
    "But first...\n",
    "\n",
    "<img src=\"../fig/bigdata.png\" width=\"600\">\n",
    "\n",
    "**High-dimensionality** is the case when p is large i.e. there are a lot of predictors. This is sometimes a problem because:\n",
    "\n",
    "1. Our models may be overfit\n",
    "2. There may be multi-collinearity\n",
    "3. Matrices may not be invertible (in the case of OLS)\n",
    "\n",
    "**Our challenge** is to represent the p dimensions by a smaller number (m) of dimensions without losing much information. Then, we can fit a model using the m predictors, which addresses the three problems listed above. Here's where **PCA** comes into play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Principal Components Analysis (PCA)?\n",
    "\n",
    "## A Framework For Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider that the data are described by a Linear Regression Model:\n",
    "$$\n",
    "Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p + \\epsilon,\n",
    "$$\n",
    "where  $p$ is very large, i.e. **high dimensionality**.\n",
    "\n",
    "\n",
    "**LASSO for dimensional reduction:**\n",
    "\n",
    "We can use LASSO: which will drop some of the predictors by forcing $\\beta_j=0$. That effectively reduces the dimensions.  Are we happy with that? \n",
    "No! Because we totally lose the information kept by the $X_j$ predictors\n",
    "\n",
    "**PCA for dimensional reduction:**\n",
    "\n",
    "Considering a new system of coordinates, namely a new set of predictors, denoted by\n",
    " $Z_1$, $Z_2$,$\\dots$, $Z_m$, where $m \\leq p$ and where each $Z_i$ is a linear combination of the original $p$ predictors, $X_1, \\dots~ X_p$, thus:\n",
    "\n",
    "$$\n",
    "Z_i = \\sum_{j=1}^{p} c_{ij} X_i\n",
    "$$\n",
    "\n",
    "for some fixed coefficients $c_{ij}$  (PCA will determine them).  For  example:\n",
    "$$\n",
    "Z_1 = 3.3 X_1 + 4 X_2 + 0 X_3 + \\dots + 1.2 X_p.\n",
    "$$\n",
    "\n",
    "Then we can build a linear regression model using the new predictors as follows:\n",
    "$$\n",
    "Y = \\theta_0 + \\theta_1 Z_1 + \\theta_2 Z_2 + \\dots + \\theta_m Z_m + \\epsilon\n",
    "$$\n",
    "\n",
    "**Notice:** For $m=p$ the number of predictors is the same, hence, we have not lost any information. We just  transform the coordinate systems of the *data-space*. \n",
    "\n",
    "**Determine the PCA coefficients $c_{ij}$:**\n",
    "PCA  identifies a new set of predictors, as linear combinations of the original ones, that captures the *maximum amount* of variance  in the observed data. In other words, PCA determines the $c_{ij}$ such as the data varies most along the new axis $Z_i$, which are called **principal components**. PCA  sorts the axis such as the largest variance goes along $Z_1$, the second largest variance goes to $Z_2$ and so on.\n",
    "\n",
    "**Comments:**\n",
    "- The  basic assumption in PCA is that higher variance indicates more importance\n",
    "- The principal components consist an $m$-dimensional **orthonormal** system of coordinates\n",
    "\n",
    "\n",
    "<img src=\"../fig/pca.png\" width=\"400\">\n",
    "\n",
    "We see that the \"best line\" is the one where there is maximal variance along the line. Source [here](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues/140579).\n",
    "\n",
    "<img src=\"../fig/pca.gif\" width=\"1000\">\n",
    "\n",
    "In principle, we could explore all the rotations, that is, rotating the original coordinate system under all the angles, and find which rotation yields the maximum variance. However, when the dimensionality (p) is large this is an extremely  time consuming and inefficient technique. In that case we may use PCA which is systematic way to find the best rotation or the best coordinate system. \n",
    "PCA is a mathematical method based on linear algebra, for more details and rigorous formulation check the advanced section for PCA.\n",
    "\n",
    "\n",
    "**Have we reduced the dimensions yet?**\n",
    "No yet... So far we have defined new predictors that linearly depend on the original predictors. Hence, we still have all the information stored in the data. \n",
    "\n",
    "**Reducing the dimensions:** Since the $Z_j$ are sorted with respect how much information the *carry*, the larger the $j$ the less important the $Z_j$. Hence, we can keep just a few of the principal components and drop the rest. For instance, keeping only the first two components $(m=2)$, we obtain two predictors that contain information from all the original $p$ predictors. \n",
    "\n",
    "How can we choose the m? PCA takes care of that too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of PCA\n",
    "\n",
    "One major application of PCA is to address the issue of high-dimensionality (reduce the number of predictors).\n",
    "\n",
    "Another major application of PCA is in **visualization**. Specifically, if we have an N-dimensional dataset (N>3), how do we visualize it? \n",
    "\n",
    "**One option**: \n",
    "\n",
    "<img src=\"../fig/matrix.png\" width=\"400\">\n",
    "\n",
    "**A more practical option**: use PCA to get the top 2-3 principal components and plot these components on 2D or 3D plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA for Visualization\n",
    "\n",
    "Data Source: [MTCars Dataset](https://gist.github.com/seankross/a412dfbd88b3db70b74b)\n",
    "\n",
    "Here are a few resources that use this dataset and apply PCA for visualization. This notebook references [this PCA tutorial in R](https://www.datacamp.com/community/tutorials/pca-analysis-r), [these lecture notes from CMU](http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf), this [blog](https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/), and [this blog](http://setosa.io/ev/principal-component-analysis/) which has some nice visualizations of PCA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in The Cars Dataset and carry out EDA\n",
    "\n",
    "This dataset consists of data on **32 models** of car, taken from an American motoring magazine (1974 Motor Trend magazine). For each car, you have **11 features**, expressed in varying units (US units), They are as follows ([source](https://www.datacamp.com/community/tutorials/pca-analysis-r)):\n",
    "\n",
    "- `mpg`: Fuel consumption (Miles per (US) gallon): more powerful and heavier cars tend to consume more fuel.\n",
    "- `cyl`: Number of cylinders: more powerful cars often have more cylinders\n",
    "- `disp`: Displacement (cu.in.): the combined volume of the engine's cylinders\n",
    "- `hp`: Gross horsepower: this is a measure of the power generated by the car\n",
    "- `drat`: Rear axle ratio: this describes how a turn of the drive shaft corresponds to a turn of the wheels. Higher values will decrease fuel efficiency.\n",
    "- `wt`: Weight (1000 lbs): pretty self-explanatory!\n",
    "- `qsec`: 1/4 mile time: the car's speed and acceleration\n",
    "- `vs`: Engine block: this denotes whether the vehicle's engine is shaped like a \"V\", or is a more common straight shape.\n",
    "- `am`: Transmission: this denotes whether the car's transmission is automatic (0) or manual (1).\n",
    "- `gear`: Number of forward gears: sports cars tend to have more gears.\n",
    "- `carb`: Number of carburetors: associated with more powerful engines\n",
    "\n",
    "Note that the units used vary and occupy different scales. \n",
    "\n",
    "**Dropping the categorical variables `vs` and `am` and only keeping in the continuous predictors**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_df = pd.read_csv('../data/mtcars.csv')\n",
    "cars_df = cars_df[cars_df.columns.difference(['am', 'vs'])]\n",
    "cars_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our task** is to try to visualize this data in a meaningful way. Obviously we can't make a 9-dimensional plot, but we can try to make several different plots using the `pairplot` function from seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(cars_df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there are numerous variables and numerous more relationships between these variables. We can do better through PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Visualization by using PCA\n",
    "\n",
    "### Standardizing Variables\n",
    "Standardization is a necessary  step in PCA formulation (more details in advanced section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating the quantitative predictors from the model of the car (a string)\n",
    "model = cars_df['model']\n",
    "quant_df = cars_df[cars_df.columns.difference(['model'])]\n",
    "\n",
    "# Standardization\n",
    "quant_scaled = StandardScaler().fit_transform(quant_df)\n",
    "cars_df_scaled = pd.DataFrame(quant_scaled, columns=quant_df.columns)\n",
    "\n",
    "# We can bring back the model variable, although we do not need it\n",
    "cars_df_scaled['model'] = cars_df['model']\n",
    "cars_df_scaled.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_df_scaled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carrying out PCA\n",
    "\n",
    "[Sklearn PCA documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop again the model predictor\n",
    "quant_df = cars_df_scaled[cars_df_scaled.columns.difference(['model'])]\n",
    "\n",
    "# fitting the PCA object onto our dataframe (excluding the model name column)\n",
    "pca = PCA().fit(quant_df)\n",
    "\n",
    "# transforming the dataframe\n",
    "quant_df_pca = pca.transform(quant_df)\n",
    "\n",
    "print('Original dimensions:', quant_df.shape)\n",
    "print('PCA dimensions:     ', quant_df_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us examine some of the attributes we obtain from PCA.\n",
    "\n",
    "1. `explained_variance_`: The amount of variance explained by each of the selected principal components.\n",
    "2. `explained_variance_ratio_`: Percentage of variance explained by each of the selected principal components. By default, unless `n_components` is  specified  all components will be stored and the sum of the ratios will be  1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(20,6))\n",
    "ax1, ax2 = ax.ravel()\n",
    "\n",
    "ratio = pca.explained_variance_ratio_\n",
    "ax1.bar(range(len(ratio)), ratio, color='purple', alpha=0.8)\n",
    "ax1.set_title('Explained Variance Ratio PCA', fontsize=20)\n",
    "ax1.set_xticks(range(len(ratio)))\n",
    "ax1.set_xticklabels(['PC {}'.format(i+1) for i in range(len(ratio))])\n",
    "ax1.set_ylabel('Explained Variance Ratio')\n",
    "\n",
    "# ratio[0]=0\n",
    "ratio = pca.explained_variance_ratio_\n",
    "ax2.plot(np.cumsum(ratio), 'o-')\n",
    "\n",
    "ax2.set_title('Cumulative Sum of Explained Variance Ratio PCA', fontsize=20)\n",
    "\n",
    "ax2.set_ylim(0,1.1)\n",
    "ax2.set_xticks(range(len(ratio)))\n",
    "ax2.set_xticklabels(['PC {}'.format(i+1) for i in range(len(ratio))])\n",
    "ax2.set_ylabel('Cumulative Sum of Explained Variance Ratio');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that over 85% of the variance is explained by the first 2 principal components! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. `components_`: This represents the principal components i.e. directions of maximum variance in the data. The components are sorted by `explained_variance_`. \n",
    "\n",
    "Let us write the equation for all the principal components using our formulation of the principal components above:\n",
    "\n",
    "$$\n",
    "Z_i = \\sum_{j=1}^{p} w_{ij} X_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, comp in enumerate(pca.components_):\n",
    "    expression = 'Z_{} = '.format(i+1)\n",
    "    for c, x in zip(comp, quant_df.columns):\n",
    "        if c < 0:\n",
    "            expression += str(np.round(c,2)) + '*' + x + ' '\n",
    "        else:\n",
    "            expression += '+' + str(np.round(c,2)) + '*' + x + ' '\n",
    "    print(expression + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the printed equations above, we can create vectors showing where each feature has a high value. Let us do this for the **first 2 principal components** (using $v$ to denote a vector): \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{carb} = \\begin{pmatrix}-0.24 \\\\ 0.48 \\end{pmatrix}, \\;\n",
    "v_{cyl} = \\begin{pmatrix}-0.4 \\\\ 0.02 \\end{pmatrix}, \\; \n",
    "v_{disp} = \\begin{pmatrix}-0.4 \\\\ -0.09 \\end{pmatrix}, \\\\\n",
    "v_{drat} = \\begin{pmatrix}0.31 \\\\ 0.34 \\end{pmatrix}, \\;\n",
    "v_{gear} = \\begin{pmatrix}0.21 \\\\ 0.55 \\end{pmatrix}, \\;\n",
    "v_{hp} = \\begin{pmatrix}-0.37 \\\\ 0.27 \\end{pmatrix}, \\\\\n",
    "v_{mpg} = \\begin{pmatrix}0.39 \\\\ 0.03 \\end{pmatrix}, \\;\n",
    "v_{qsec} = \\begin{pmatrix}0.22 \\\\ -0.48 \\end{pmatrix}, \\;\n",
    "v_{wt} = \\begin{pmatrix}-0.37 \\\\ -0.17 \\end{pmatrix}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if our vectors are orthonormal\n",
    "Orthonormal vectors are the vectors which are orthogonal (zero dot product) with length equal to one (unit vectors).\n",
    "\n",
    "#### Orthogonal:\n",
    "We use the dot product between two vectors to check if the vectors are orthogonal or not. If the dot product is 0, that means that the two vectors are orthogonal. The dot product between two vectors is (geometrically): \n",
    "\n",
    "$$\n",
    "\\textbf{a} \\cdot \\textbf{b} = ||\\textbf{a}|| ||\\textbf{b}|| \\cos(\\theta)\n",
    "$$\n",
    "\n",
    "Where $\\theta$ is the angle between the two vectors and $||\\cdot||$ denotes the norm of the vector. Since we assume that the norm of a vector is non-zero, the only way the dot product of two vectors to be zero is when the angle between them is 90 degrees ($\\cos(90) = 0$). Thus, the dot product is a way to check if two vectors are perpendicular.\n",
    "\n",
    "#### Unit vectors\n",
    "In order to calculate the length $||\\textbf{a}||$ of a vector we can take the dot product of a vector with itself, namely\n",
    "$$ \n",
    "||\\textbf{a}|| =  \\textbf{a}\\cdot \\textbf{a}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec1 = pca.components_[0]\n",
    "vec2 = pca.components_[1]\n",
    "\n",
    "# print(np.dot(vec1.T, vec2))\n",
    "print('The dot product between the first two principal components is ',np.round(np.dot(vec1, vec2),5))\n",
    "print('The length of the first  principal component is ',np.round(np.dot(vec1, vec1),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the first two principal components are orthogonal and the first principal component is also a unit vector. You can check other pairs of principal components in order to convince yourself that all principal components are always pairwise orthogonal unit vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing PCA results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to plot vectors from the center\n",
    "vecs = pca.components_[0:10].T *2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "ax.plot(quant_df_pca[:,0], quant_df_pca[:,1], 'ok', markersize=4)\n",
    "ax.set_xlabel('Principal Component 1')\n",
    "ax.set_ylabel('Principal Component 2')\n",
    "ax.set_title('Cars Dataset plotted using first 2 Principal Components', fontsize=20)\n",
    "\n",
    "# plotting arrowheads of the original axes projected on the 2D PCA space\n",
    "for i, vec in enumerate(vecs):\n",
    "    ax.arrow(0,0,vec[0],vec[1], color='brown', head_width=0.1)\n",
    "    s = 1.3\n",
    "    ax.annotate(quant_df.columns[i], (s*vec[0], s*vec[1]), color='brown')\n",
    "\n",
    "# annotating text\n",
    "for i, txt in enumerate(cars_df_scaled['model']):\n",
    "    ax.annotate(txt, (quant_df_pca[:,0][i], quant_df_pca[:,1][i]), size=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Any patterns of interest?** Let us examine the geography more closely. Source: [this blog](https://www.datacamp.com/community/tutorials/pca-analysis-r). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = [\"Japan\", \"US\", \"EU\", \"US\", \"EU\", \"Japan\", \"US\", \"EU\", \"US\", \"EU\"]\n",
    "times = [3, 4, 7, 3, 1, 3, 4, 3, 1, 3]\n",
    "country_list = np.array(sum(([x]*y for x,y in zip(country, times)),[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "\n",
    "# main plot\n",
    "ax.plot(quant_df_pca[:,0], quant_df_pca[:,1], 'ok', markersize=4)\n",
    "ax.set_xlabel('Principal Component 1')\n",
    "ax.set_ylabel('Principal Component 2')\n",
    "ax.set_title('Cars Dataset plotted using first 2 Principal Components', fontsize=20)\n",
    "\n",
    "# plotting arrowheads\n",
    "for i, vec in enumerate(vecs):\n",
    "    ax.arrow(0,0,vec[0],vec[1], color='brown', head_width=0.05)\n",
    "    s = 1.3\n",
    "    ax.annotate(quant_df.columns[i], (s*vec[0], s*vec[1]), color='brown')\n",
    "\n",
    "# plotting names\n",
    "cs = [sns.xkcd_rgb[\"magenta\"], sns.xkcd_rgb[\"denim blue\"], sns.xkcd_rgb[\"medium green\"]]\n",
    "colors = {\"Japan\": cs[0], \"US\": cs[1], \"EU\": cs[2]}\n",
    "\n",
    "# dummy plots to show up in the legend\n",
    "ax.plot(0,0, color=cs[0], label='Japan')\n",
    "ax.plot(0,0, color=cs[1], label='US')\n",
    "ax.plot(0,0, color=cs[2], label='EU')\n",
    "\n",
    "# plotting text with color\n",
    "for i, txt in enumerate(cars_df_scaled['model']):\n",
    "    country = country_list[i]\n",
    "    ax.annotate(txt, (quant_df_pca[:,0][i], quant_df_pca[:,1][i]), color=colors[country], size=12)\n",
    "    \n",
    "ax.legend(fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What patterns do you see now?\n",
    "\n",
    "For example, looking at the axes, we  see that the American cars are characterized by high values for number of *cyl*, *disp*, and  *wt*. Thus they are more powerful and heavier cars. Japanese cars, on the other hand, are characterized by higher fuel efficieny (*mpg*). European cars are somewhat in the middle and less tightly clustered than either group, but are average more efficient than American cars\n",
    "\n",
    "**We can draw conclusions visually. Not any modeling so far.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing `n_components`\n",
    "\n",
    "PCA takes in 1 parameter: `n_components`. This defines the number of principal components that PCA will  use. By default, the number of the used principal components is the minimum of the number of rows and the number of columns in the dataset `min(p, n) `. This is the maximum number of dimensions that PCA transform is allowed to use; For more details check the advanced section. \n",
    "\n",
    "**Note**: Setting the default parameter for `n_components` and taking the top k principal components is equivalent to setting `n_components=k`. The former is computationally more expensive though. Let's check this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_components = pca.components_[0:2]\n",
    "\n",
    "# doing pca with 2 components\n",
    "pca2 = PCA(n_components=2).fit(quant_df)\n",
    "\n",
    "new_components = pca2.components_\n",
    "\n",
    "# checking equivalence\n",
    "print(new_components.all() == old_components.all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <strong>🏋🏻‍♂️ TEAM ACTIVITY 1:</strong> Visualizing high-dimensional data in a 2D plot </div>  \n",
    "\n",
    "\n",
    "\n",
    "- Perform PCA\n",
    "- Explore the \"Explained Variance Ratio\" \n",
    "- Keep the first two principal components and visualize the data in this low-dimensional 2D space.\n",
    "- Explore the \"cumulative explained variance ratio\"\n",
    "\n",
    "<img src=\"../fig/iris.png\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_iris()\n",
    "print('classes   ', dataset.target_names)\n",
    "print('predictors', dataset.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(dataset.data).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_scaled = StandardScaler().fit_transform(dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(iris_scaled).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# perform a PCA transform on your dataset\n",
    "pca_iris = \n",
    "iris_scaled_pca = \n",
    "\n",
    "# compare the dimensions of the original and PCA-transformed datasets\n",
    "print('Original dimensions:', iris_scaled.shape)\n",
    "print('PCA dimensions:     ', iris_scaled_pca.shape)\n",
    "\n",
    "## Plot the explained and accumulated explained variance ratios\n",
    "\n",
    "\n",
    "#Visualize the pca-transformed data in a 2D space\n",
    "# Use different color for each label (flower type)\n",
    "\n",
    "# plotting arrowheads to project the original dimensions/predictors\n",
    "vecs_iris = pca_iris.components_[0:4].T\n",
    "for i, vec in enumerate(vecs_iris):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA vs manual or Lasso 2D representations:**  Outline the data by using two predictors after applying LASSO or manual selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose two of the predictors to make a 2D plot\n",
    "# For this dataset the first two predictors are the most important\n",
    "# print('predictors', dataset.feature_names)\n",
    "\n",
    "pred_1 = 0\n",
    "pred_2 = 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "\n",
    "for i in range(dataset.target.shape[0]):\n",
    "    if dataset.target[i]==0:\n",
    "        c='b'\n",
    "    elif dataset.target[i]==1:\n",
    "        c='r'\n",
    "    else:\n",
    "        c='g'\n",
    "        \n",
    "    ax.plot(iris_scaled[i,pred_1], iris_scaled[i,pred_2], 'o', \n",
    "            markersize=8, color=c, label = flower_species[dataset.target[i]])\n",
    "    \n",
    "ax.set_xlabel(dataset.feature_names[pred_1])\n",
    "ax.set_ylabel(dataset.feature_names[pred_2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which visualization is more informative? Think about the difference between LASSO and PCA as dimensional reduction methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA to speed up classification of Handwritten Digits\n",
    "\n",
    "This example, using the [MNIST dataset](http://yann.lecun.com/exdb/mnist/), was borrowed from this [Towards Data Science blog post](https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60). In this example, we will be classifying hand-written digits.\n",
    "\n",
    "## Data Loading and EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading from Keras datasets\n",
    "\n",
    "That requires to have installed TensorFlow 2 and Keras. You can verify the installation by displaying the package information as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 show keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll use keras a lot more in the last few weeks of the course\n",
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternatively, just  load the MNIST dataset from the Lab data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip the file\n",
    "! unzip ../data/mnist.zip \n",
    "\n",
    "#the data are saved in a dictionary with pickle. You can import the data as follows:\n",
    "import pickle\n",
    "\n",
    "with open('mnist.pickle', 'rb') as handle:\n",
    "    mnist_data = pickle.load(handle)\n",
    "(x_train, y_train), (x_test, y_test) = mnist_data[\"data\"]\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training set (`x_train`) contains 60000 images of size 28 by 28. Our training labels (`y_train`) are numbers from 0 to 9. Let's examine one of these values below.\n",
    "\n",
    "The point of this example **is not** to learn modeling with logistic regression, you will learn about it in the following lectures. Here, we want to demonstrate how much PCA accelarates the training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train[0])\n",
    "fig, ax = plt.subplots()\n",
    "ax.grid(None)\n",
    "ax.imshow(x_train[0], cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our task** is to classify the test set digits as accurately as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the training set is $6000 \\times 28 \\times 28$.  We have not dealt with 3D arrays before. We will deal with images in greater detail (and not only!!!) in the follow-up course, `CS109b`, if you are interested in doing more of this kind of stuff you should take this course. For now, we will **reshape** the array into a 2D array of shape $6000\\times 784$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], 784)\n",
    "x_test = x_test.reshape(x_test.shape[0], 784)\n",
    "\n",
    "# check if the shapes are ok\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing Data\n",
    "\n",
    "Image data is usually between 0 and 255 (0 indicates a black pixel and 255 indicates a white pixel). We can normalize these values by dividing by 255.\n",
    "\n",
    "Why do we prefer normalization instead of standardization in this case? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the min and max of x_train and x_test\n",
    "print(x_train.min(), x_train.max(), x_test.min(), x_test.max())\n",
    "\n",
    "x_train = (x_train - x_train.min())/(x_train.max() - x_train.min()) \n",
    "x_test = (x_test - x_train.min())/(x_train.max() - x_train.min()) \n",
    "\n",
    "print(x_train.min(), x_train.max(), x_test.min(), x_test.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling using Logistic Regression\n",
    "\n",
    "**Again:** The point of this example **is not** to learn modeling with logistic regression, you will learn about it in the following lectures. Here, we want to demonstrate how much PCA accelarates the training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "#‘lbfgs’ solver handles multinomial loss in multiclass problems \n",
    "logreg_model = LogisticRegression(solver='lbfgs').fit(x_train, y_train)\n",
    "end = time()\n",
    "\n",
    "full_logreg_time = end-start\n",
    "print('Time to fit: {}s'.format(full_logreg_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_train = logreg_model.predict(x_train)\n",
    "y_preds_test = logreg_model.predict(x_test)\n",
    "\n",
    "full_logreg_score_train = accuracy_score(y_train, y_preds_train)\n",
    "full_logreg_score_test = accuracy_score(y_test, y_preds_test)\n",
    "\n",
    "# Evaluation\n",
    "print('Training Set Score: {}'.format(full_logreg_score_train))\n",
    "print('Test Set Score: {}'.format(full_logreg_score_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get performance by class \n",
    "pd.crosstab(y_test, y_preds_test, margins=True, rownames=['Actual'], colnames=['Predicted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a high training and test set score but it takes a relatively long time to fit a model. Let us see if we can speed things up when using PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model after PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do PCA onto our training set and inspect\n",
    "pca = PCA(n_components=100).fit(x_train)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(20,6))\n",
    "ax1, ax2 = ax.ravel()\n",
    "\n",
    "ratio = pca.explained_variance_ratio_\n",
    "ax1.plot(range(1,len(ratio)+1), ratio, 'o-')\n",
    "ax1.set_title('Explained Variance Ratio PCA', fontsize=20)\n",
    "ax1.set_ylabel('Explained Variance Ratio')\n",
    "\n",
    "ratio = pca.explained_variance_ratio_\n",
    "ax2.plot(range(1,len(ratio)+1), np.cumsum(ratio), 'o-')\n",
    "ax2.set_title('Cumulative Sum of Explained Variance Ratio PCA', fontsize=20)\n",
    "ax2.set_ylabel('Cumulative Sum of Explained Variance Ratio');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the first 100 principal components hold over 90% of the variance and the first 50 principal components hold over 80% of the variance! We have significantly reduced the dimensionality of our problem! Let us use PCA to find the first 100 principal components of our dataset and transform our `x_train` and `x_test` accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pca = pca.transform(x_train)\n",
    "x_test_pca = pca.transform(x_test)\n",
    "\n",
    "print(x_train_pca.shape, x_test_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "logreg_model_pca = LogisticRegression(solver='lbfgs').fit(x_train_pca, y_train)\n",
    "end = time()\n",
    "\n",
    "print('Time to fit model (100 PCs): {}s'.format(end-start))\n",
    "print('Time to fit model (full dataset): {}s'.format(full_logreg_time))\n",
    "print('So to fit the model with the full dataset is about', np.round(full_logreg_time/(end-start),0), ' times slower than using PCA')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.bar(0, full_logreg_time, width=0.5)\n",
    "ax.bar(1, end-start, width=0.5)\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_xticks([0,1])\n",
    "ax.set_xticklabels(['Full Dataset', '100 PCs'])\n",
    "ax.set_ylabel('Time to Fit Model (s)')\n",
    "ax.set_title('Time taken to fit different models (s)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The time taken to fit our model is considerably smaller! Now let us check our accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_train_pca = logreg_model_pca.predict(x_train_pca)\n",
    "y_preds_test_pca = logreg_model_pca.predict(x_test_pca)\n",
    "\n",
    "# Evaluation\n",
    "print('Training Set Score (100 PCs): {}'.format(accuracy_score(y_train, y_preds_train_pca)))\n",
    "print('Test Set Score (100 PCs): {}\\n'.format(accuracy_score(y_test, y_preds_test_pca)))\n",
    "\n",
    "print('Training Set Score (full dataset): {}'.format(full_logreg_score_train))\n",
    "print('Test Set Score (full dataset): {}'.format(full_logreg_score_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get performance by class \n",
    "pd.crosstab(y_test, y_preds_test_pca, margins=True, rownames=['Actual'], colnames=['Predicted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a significant drop in accuracy!! But, since we are losing information by not accounting for all the variance, we are faced with a speed accuracy tradeoff. \n",
    "You should explore the case of keeping less principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting PCA\n",
    "\n",
    "### Plotting the Reconstructed Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))\n",
    "ax1, ax2 = ax.ravel()\n",
    "\n",
    "ax1.imshow(x_train[0].reshape(28,28), cmap='gray')\n",
    "ax1.grid(None)\n",
    "ax1.set_title('Original Image with 784 components')\n",
    "\n",
    "ax2.imshow(x_train_pca[1].reshape(10,10), cmap='gray')\n",
    "ax2.grid(None)\n",
    "ax2.set_title('Image after PCA with 100 components')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uhhh... this is terrible. But we can use PCA to carry out an inverse transform in order to get a reconstructed image. Let's try again, using `pca.inverse_transform()`! Source: [this github repo](https://github.com/mGalarnyk/Python_Tutorials/blob/master/Sklearn/PCA/PCA_Image_Reconstruction_and_such.ipynb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_reconstructed = pca.inverse_transform(x_train_pca[0])\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))\n",
    "ax1, ax2 = ax.ravel()\n",
    "\n",
    "ax1.imshow(x_train[0].reshape(28,28), cmap='gray')\n",
    "ax1.grid(None)\n",
    "ax1.set_title('Original Image with 784 components')\n",
    "\n",
    "ax2.imshow(img_reconstructed.reshape(28,28), cmap='gray')\n",
    "ax2.grid(None)\n",
    "ax2.set_title('Reconstructed Image after PCA with 100 components')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting all our points on a 2-dimensional plot given by the first 2 principal components of PCA\n",
    "\n",
    "This [towards data science article](https://towardsdatascience.com/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b) has a few similar plots that are pretty cool!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_2 = PCA(n_components=2).fit(x_train)\n",
    "x_train_2 = pca_2.transform(x_train)\n",
    "print(x_train_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "for i in range(10):\n",
    "    indices = np.where(y_train == i)[0]\n",
    "    data = x_train_2[indices]\n",
    "    ax.plot(data[:,0], data[:,1], 'ob', label='{}'.format(i), alpha=0.5)\n",
    "ax.set_title('First 2 Principal Components of MNIST Data', fontsize=20)\n",
    "ax.set_xlabel('Principal Component 1')\n",
    "ax.set_ylabel('Principal Component 2')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add some colors: Different color for each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "for i in range(10):\n",
    "    indices = np.where(y_train == i)[0]\n",
    "    data = x_train_2[indices]\n",
    "    ax.plot(data[:,0], data[:,1], 'o', label='{}'.format(i), alpha=0.5)\n",
    "ax.set_title('First 2 Principal Components of MNIST Data', fontsize=20)\n",
    "ax.set_xlabel('Principal Component 1')\n",
    "ax.set_ylabel('Principal Component 2')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any patterns of interest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Should I always use PCA?\n",
    "\n",
    "\n",
    "PCA is **great** for:\n",
    "1. Speeding up the training of a model without significantly decreasing the predictive ability relative to a model with all p predictors.\n",
    "2. Visualizing how predictive your features can be of your response, especially in the case of classification.\n",
    "3. Reducing multicollinearity, and thus potentially improving the computational time required to fit models.\n",
    "4. Reducing dimensionality in very high dimensional settings.\n",
    "\n",
    "PCA is **not so good** in certain situations because:\n",
    "1. Interpretation of coefficients in PCA is completely lost. So do not do PCA if interpretation is important.\n",
    "3. When the predictors' distribution deviates significantly from a multivariable Normal distribution.\n",
    "4. When the high variance does not indicate high importance.\n",
    "5. When the hidden dimensions are not orthonormal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assumptions of PCA\n",
    "\n",
    "- **Linear change of basis:** PCA is a linear transformation from a Euclidean basis (defined by the original predictors) to an abstract orthonormal basis. Hence, PCA assumes that such a linear change of basis is sufficient for identifying degrees of freedom and conducting dimensionality reduction\n",
    "\n",
    "\n",
    "- **Mean/variance are sufficient (data are approximately multi-variate gaussian):** \n",
    "In applying  PCA to our data, we are only using the means (for standardizing) and the covariance matrix that are associated with our predictors. Thus, PCA  assumes that such statistics are sufficient for describing the distributions of the predictor variables. This is true  only if the predictors are drawn jointly from a multivariable Normal distribution. When the predictor distributions heavily violate this assumption,  PCA components may not be as informative.\n",
    "\n",
    "\n",
    "- **High variance indicates high importance:** This fundamental assumption is intuitively reasonable, since components corresponding to low variability likely say little about the data, but this is not always true.\n",
    "\n",
    "\n",
    "- **Principal components are orthogonal:** PCA explicitly assumes that *intrinsic (abstract) dimensions* are orthogonal, which may  not  be true. However, this allowes us to use techniques from linear algebra such as the spectral decomposition and thereby, simplify our calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <strong>🏋🏻‍♂️ TEAM ACTIVITY 2:</strong> Perform classification by using reduced dimensional data-space. </div>  \n",
    "\n",
    "\n",
    "Specifically compare the classification score when we use: (a) all the predictors; (b) 2 of the original predictors; (c) 2 principal components as predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use all the 4 predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_iris, y_test_iris = train_test_split(iris_scaled, dataset.target , test_size=0.4, random_state=42)\n",
    "print('Shapes for X and y training sets:', X_train.shape, y_train_iris.shape)\n",
    "print('Shapes for X and y testing sets:', X_test.shape, y_test_iris.shape)\n",
    "\n",
    "#Training\n",
    "model_logistic = LogisticRegression(C=100).fit(X_train, y_train_iris)\n",
    "\n",
    "#Predict\n",
    "y_pred_train = model_logistic.predict(X_train)\n",
    "y_pred_test = model_logistic.predict(X_test)\n",
    "\n",
    "#Performance Evaluation\n",
    "train_score = accuracy_score(y_train_iris, y_pred_train)*100\n",
    "test_score = accuracy_score(y_test_iris, y_pred_test)*100\n",
    "\n",
    "print(\"Training Set Accuracy:\",str(train_score)+'%')\n",
    "print(\"Testing Set Accuracy:\",str(test_score)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the 2 predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the first two predictors (sepal length and sepal width)\n",
    "iris_2predictors = iris_scaled[:,0:2]\n",
    "X_train, X_test, y_train_iris, y_test_iris = train_test_split(iris_2predictors, dataset.target , test_size=0.4, random_state=42)\n",
    "print('Shapes for X and y training sets:', X_train.shape, y_train_iris.shape)\n",
    "print('Shapes for X and y testing sets:', X_test.shape, y_test_iris.shape)\n",
    "\n",
    "#Training\n",
    "model_logistic = LogisticRegression(C=100).fit(X_train, y_train_iris)\n",
    "\n",
    "#Predict\n",
    "y_pred_train = model_logistic.predict(X_train)\n",
    "y_pred_test = model_logistic.predict(X_test)\n",
    "\n",
    "#Perfromance Evaluation\n",
    "train_score = accuracy_score(y_train_iris, y_pred_train)*100\n",
    "test_score = accuracy_score(y_test_iris, y_pred_test)*100\n",
    "\n",
    "print(\"Training Set Accuracy:\",str(train_score)+'%')\n",
    "print(\"Testing Set Accuracy:\",str(test_score)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the first two  principal componets as predictors\n",
    "\n",
    "- Perform a PCA transform with 2 principal components\n",
    "- Use the 2 principal components for a logistic regression\n",
    "- Print the score for the training and testing sets\n",
    "- Compare with the previous two cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing PCA and asking for 2 principal components (use the scaled data)\n",
    "pca_iris_2 = \n",
    "iris_scaled_pca_2 = \n",
    "\n",
    "\n",
    "# Split the dataset using test_size = 0.4\n",
    "X_train, X_test, y_train_iris, y_test_iris = \n",
    "\n",
    "#Training a logistic regression model \n",
    "model_logistic = LogisticRegression(C=100).fit( # , #)\n",
    "\n",
    "#Predict\n",
    "y_pred_train = model_logistic.predict(X_train)\n",
    "y_pred_test = model_logistic.predict(X_test)\n",
    "\n",
    "#Perfromance Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "### End of  the Lab\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
