{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109A Introduction to Data Science "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8: Logistic Regression, Multiple Logistic Regression, and kNN Classification\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2021**<br/>\n",
    "**Instructors**: Pavlos Protopapas and Natesh Pillai<br/>\n",
    "**Lab Team**: Marios Mattheakis, Hayden Joy, Chris Gumb, and Eleni Kaxiras<br/>\n",
    "**Authors**: Mike Sedelmeyer, Hayden Joy, and Marios Mattheakis\n",
    "<hr style='height:2px'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\n",
    "    \"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\"\n",
    ").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "## Contents\n",
    "    \n",
    "### In this lab we will be covering Linear Regression, Logistic Regression, and kNN Classification using the Iris dataset.\n",
    "\n",
    "----\n",
    "    \n",
    "\n",
    "**Specifically, we will:**\n",
    " \n",
    "0. [Review Multiple Logistic Regression](#multiple)\n",
    "\n",
    "1. [Import and explore data](#data)\n",
    "\n",
    "2. [Fit a LINEAR regression model for classification, understand drawbacks, and interpret results](#linear)\n",
    "\n",
    "3. [Fit a simple LOGISTIC regression model for classification, compare performance, and interpret results](#logistic)\n",
    "\n",
    "4. [Visualize predictions and decision boundaries](#plot)\n",
    "\n",
    "5. [Fit a higher order polynomial logistic regression model for classification, compare performance, plot decision boundaries, and interpret results](#poly)\n",
    "\n",
    "6. [Consider the impact of different regularization parameters on the above polynomial model](#regularize)\n",
    "\n",
    "7. [Fit and investigate a kNN CLASSIFIER](#knn)\n",
    "\n",
    "8. [Introduce `sklearn` pipelines](#pipeline)\n",
    "\n",
    "**For our team exercises, we will:**\n",
    "\n",
    "1. [Standardize our data and fit a simple logistic regression model](#ex1)\n",
    "\n",
    "2. [Plot and examine decision boundaries](#ex2)\n",
    "\n",
    "3. [Add an interaction term and use `LogisticRegressionCV` to cross-validate a regularization parameter](#ex3)\n",
    "\n",
    "By the end, we hope you will be comfortable building your own classification models.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig/multiple1.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig/multiple2.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig/multiple3.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig/multiple4.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another type of multiple logistic regression <a href = \"https://en.wikipedia.org/wiki/Multinomial_logistic_regression\">(multinomial logistic regression)</a> which strictly enforces that the probabilities will sum to one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab we will be using the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data and Stats packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# NEW packages\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Visualization packages\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import and explore data\n",
    "\n",
    "[▲ Return to contents](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For Lab 8 we will be using datasets available directly within `sklearn`.**\n",
    "\n",
    "- For lab examples, we will be using Fisher's famous \"Iris\" dataset\n",
    "- For our team exercises, we will be using the \"Wine\" dataset\n",
    "\n",
    "To learn more about `sklearn` datasets, please see the documentation: https://scikit-learn.org/stable/datasets/toy_dataset.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_iris()\n",
    "print(dataset.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are our the target and features? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.target_names)\n",
    "print(dataset.feature_names)\n",
    "\n",
    "# Uncomment if you want to see much, much more info on this dataset\n",
    "#print(dataset['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig/iris_yong_cui_towarddatascience.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig/fischer.jpg\" width=\"150px\" style=\"float:right; margin: 30px\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**As we can see, the response variable (y) is the flower type, it has 3 classes:**\n",
    "\n",
    "- setosa\n",
    "- versicolor \n",
    "- virginica\n",
    "\n",
    "These are three different species of Iris flowers.\n",
    "\n",
    "**The 4 predictor variables are flower characteristics (x):**\n",
    "\n",
    "- sepal length (cm)\n",
    "- sepal width (cm)\n",
    "- petal length (cm)\n",
    "- petal width (cm)\n",
    "\n",
    "\n",
    "Although this is a toy dataset, in real datasets it will be important to understand the underlying features. Below is a comparison of the three species and their sepals/petals, as well as the overall flower.\n",
    "\n",
    "<img src=\"fig/Aa_irises_at_gauntlet.jpg\" width=\"150px\" style=\"float:right; margin: 30px\">\n",
    "\n",
    "![Iris features](fig/iris_yong_cui_towarddatascience.png \"Image src: https://towardsdatascience.com/the-iris-dataset-a-little-bit-of-history-and-biology-fb4812f5a7b5\")\n",
    "\n",
    "\n",
    "*Images not loading? See them online:*\n",
    "\n",
    "- [*Left: Labelled pictures of iris species from dataset*](https://commons.wikimedia.org/w/index.php?curid=5971251 \"By Anthony Appleyard at English Wikipedia - Transferred from en.wikipedia to Commons., Public Domain\") \n",
    "\n",
    "- [*Right: Yellow Iris flowers*](https://miro.medium.com/max/700/1*f6KbPXwksAliMIsibFyGJw.png \"The Iris Dataset — A Little Bit of History and Biology\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create design matrix\n",
    "X = pd.DataFrame(data=dataset.data, columns=dataset.feature_names)\n",
    "print(X.shape)\n",
    "display(X.head())\n",
    "display(X.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create response variable\n",
    "y = pd.DataFrame(data=dataset.target, columns=['species'])\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(y.head())\n",
    "display(y.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Data \n",
    "\n",
    "Check which variables have high correlations and distinctive patterns with the response. \n",
    "\n",
    "Any patterns worth mentioning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Are the features sepal/petal length and width uniformally distributed or do you observe some clusters of data points?**\n",
    "\n",
    "What do you expect? Let's add color according to our response variable with `hue='species'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.concat([X,y], axis=1)\n",
    "sns.pairplot(full_df, hue='species', palette = sns.color_palette(\"tab10\")[:3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some features like 'petal length' and 'petal width' do have very high correlations and distinctive patterns with the response variable 'flower species'. When we would use these features for predicting the flower species, the classification wouldn't be very difficult. Certain ranges of 'petal length' and 'petal width' are very much correlated with a specific flower species and they are almost seperating our classes perfectly.\n",
    "\n",
    "**Just for illustration purposes we will continue to use only `'sepal width (cm)'` and `'sepal length (cm)'`. We are making the problem harder for ourselves by only using 'weaker' or less-discriminative features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[['sepal width (cm)', 'sepal length (cm)']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train.shape', X_train.shape)\n",
    "print('y_train.shape', y_train.shape)\n",
    "\n",
    "print('X_test.shape', X_test.shape)\n",
    "print('y_test.shape',y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='linear'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fit a LINEAR regression model for classification, understand drawbacks, and interpret results\n",
    "\n",
    "[▲ Return to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_linear_sklearn = LinearRegression()\n",
    "\n",
    "#Training\n",
    "model_linear_sklearn.fit(X_train, y_train)\n",
    "\n",
    "#Predict\n",
    "y_pred_train = model_linear_sklearn.predict(X_train)\n",
    "y_pred_test = model_linear_sklearn.predict(X_test)\n",
    "\n",
    "#Perfromance Evaluation\n",
    "train_score = accuracy_score(y_train, y_pred_train)*100\n",
    "test_score = accuracy_score(y_test, y_pred_test)*100\n",
    "\n",
    "print(\"Training Set Accuracy:\",str(train_score)+'%')\n",
    "print(\"Testing Set Accuracy:\",str(test_score)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>  \n",
    " \n",
    "**QUESTION: Can anyone explain why we are getting an error? What is wrong with `y_train` and `y_pred_train`?**\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train[:5], y_train.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The fact that our linear regression is outputting continuous predictions is one of the major drawbacks of linear regression for classification. We can solve this in two manners:**\n",
    "\n",
    "- Simply rounding our prediction by using `np.round()` and converting it to an int data type with `.astype(int)`\n",
    "\n",
    "\n",
    "- Or, use a modified algorithm that has bounded outputs (more about Logistic Regression later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(y_pred_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(y_pred_train[:5]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_linear_sklearn = LinearRegression()\n",
    "\n",
    "#Training\n",
    "model_linear_sklearn.fit(X_train, y_train)\n",
    "\n",
    "#Predict\n",
    "y_pred_train = np.round(model_linear_sklearn.predict(X_train)).astype(int)\n",
    "y_pred_test = np.round(model_linear_sklearn.predict(X_test)).astype(int)\n",
    "\n",
    "#Perfromance Evaluation\n",
    "train_score = accuracy_score(y_train, y_pred_train)*100\n",
    "test_score = accuracy_score(y_test, y_pred_test)*100\n",
    "\n",
    "print(\"Training Set Accuracy:\",str(train_score)+'%')\n",
    "print(\"Testing Set Accuracy:\",str(test_score)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Performance by Class (Lookup Confusion Matrix)\n",
    "\n",
    "- Each row of the matrix represents the instances in an actual class\n",
    "- Each column represents the instances in a predicted class (or vice versa)\n",
    "- The name stems from the fact that it makes it easy to see if the system is confusing two classes (i.e. commonly mislabeling one as another)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_linear = pd.crosstab(\n",
    "    y_test.values.flatten(), \n",
    "    y_pred_test.flatten(), \n",
    "    rownames=['Actual Class'], \n",
    "    colnames=['Predicted Class'],\n",
    ")\n",
    "display(confusion_matrix_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<img src=\"fig/ghost_dog.jpg\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>  \n",
    "    \n",
    "**QUESTION: How many classes do we have in our y_test and in our y_pred_test? Why do we have 4 different predicted classes?**\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Can we use Linear Regression for classification?**\n",
    "\n",
    "**Four Assumptions of Linear Regression:**\n",
    "1. Linearity: Our dependent variable Y is a linear combination of the  explanatory variables X (and the error terms)\n",
    "2. Observations are independent of one another\n",
    "3. Constant variance of the error terms (i.e. homoscedasticity)\n",
    "4. **The error terms are normally distributed ~ $N(0,\\sigma^2)$**\n",
    "\n",
    "**Suppose we have a binary outcome variable. Can we use Linear Regression?** \n",
    "\n",
    "Then we will have the following problems:\n",
    "1. The error terms are heteroskedastic\n",
    "2. $\\epsilon$ is not normally distributed because Y takes on only two values\n",
    "3. The predicted probabilities can be greater than 1 or less than 0\n",
    "\n",
    "**Datasets where linear regression is problematic:** \n",
    "1. Binary response data where there are only two outcomes (yes/no, 0/1, etc.)\n",
    "2. Categorical or ordinal data of any type, where the outcome is one of a number of discrete (possibly ordered) classes\n",
    "3. Count data in which the outcome is restricted to non-negative integers.\n",
    "4. Continuous data in which the noise is not normally distributed\n",
    "\n",
    "**This is where Generalized Linear Models (GLMs), of which logistic regression is a specific type, come to the rescue!**\n",
    "\n",
    "- Logistic regression is most useful for predicting binary or multi-class responses.\n",
    " \n",
    "![Linear versus Logistic](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1534281070/linear_vs_logistic_regression_edxw03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='logistic'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fit a simple LOGISTIC regression model for classification, compare performance, and interpret results\n",
    "\n",
    " [▲ Return to contents](#Contents)\n",
    "\n",
    "The logistic regression formula: \n",
    "\n",
    "$$\\hat{p}= \\dfrac{e^{w^T x}}{1+e^{w^T x}}$$\n",
    "\n",
    "This is equivalent to: \n",
    "\n",
    "$$\\hat{p}= \\dfrac{1}{1+e^{-w^T x}}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<img src=\"https://miro.medium.com/max/3000/1*RqXFpiNGwdiKBWyLJc_E7g.png\" alt=\"Sigmoid Function\" style=\"width: 600px;\"/>\n",
    "</center>\n",
    "\n",
    "[Medium Article: Detailed overview of Logistic Regression](https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "model_logistic = LogisticRegression(C=100, multi_class = \"ovr\").fit(X_train, y_train)\n",
    "\n",
    "#Predict\n",
    "y_pred_train = model_logistic.predict(X_train)\n",
    "y_pred_test = model_logistic.predict(X_test)\n",
    "\n",
    "#Perfromance Evaluation\n",
    "train_score = accuracy_score(y_train, y_pred_train)*100\n",
    "test_score = accuracy_score(y_test, y_pred_test)*100\n",
    "\n",
    "print(\"Training Set Accuracy:\",str(train_score)+'%')\n",
    "print(\"Testing Set Accuracy:\",str(test_score)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logistic.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_probabilities = model_logistic.predict_proba(X_test)\n",
    "predicted_probabilities.argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's compare logistic regression against linear regression predictions\n",
    "\n",
    "**To do this, we will hold one of our 2 predictors constant so we can easily visualize and compare our prediction curves.**\n",
    "\n",
    "- We fix `X_train['sepal width (cm)']` to its mean value. \n",
    "\n",
    "  - `x_1 = X_train['sepal width (cm)']`\n",
    "\n",
    "  - `x_1_range  = np.ones_like(x_2_range)*x_1.mean()`\n",
    "    \n",
    "\n",
    "- We vary `X_train['sepal length (cm)']` from its minimum to its maximum and look how the predicted class evolves.\n",
    "  \n",
    "  - `x_2 = X_train['sepal length (cm)']`\n",
    "  \n",
    "  - `x_2_min, x_2_max = x_2.min(), x_2.max()+0.3`\n",
    "  \n",
    "  - `x_2_range  = np.arange(x_2_min, x_2_max, 0.003)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vary_x2(X_train):\n",
    "    # Making our input features (x_2 varying, x_1 constat = mean of x_1)\n",
    "    x_1 = X_train['sepal width (cm)']\n",
    "    x_2 = X_train['sepal length (cm)']\n",
    "    \n",
    "    # get min and max values cushioned by some value so that points don't lie on boundary of plot\n",
    "    x_2_min, x_2_max = x_2.min()-0.1, x_2.max()+0.3\n",
    "    \n",
    "    #make x2 vary\n",
    "    x_2_range  = np.arange(x_2_min, x_2_max, 0.003)\n",
    "    \n",
    "    #set every value in x1 to be ones, then broadcast multipy by the mean of x1\n",
    "    x_1_range  = np.ones_like(x_2_range)*x_1.mean()\n",
    "\n",
    "    # Construct our input features\n",
    "    X_with_varying_x_2 = pd.DataFrame({'sepal width (cm)':  x_1_range,\n",
    "                                      'sepal length (cm)' : x_2_range}\n",
    "        \n",
    "    )\n",
    "    return x_2_range, X_with_varying_x_2\n",
    "vary_x2(X_train)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison_plot(X_train, y_train, X_test,  y_test, label_shift = None, plot_confusion_matrix = False):\n",
    "    \"\"\"\n",
    "    args:\n",
    "    label_shift : int\n",
    "        changes the labels by label_shift\n",
    "    plot_confusion_matrix: bool\n",
    "        if true plots confusion matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    def change_labels(y, label_shift):\n",
    "        return (y + label_shift) % 3\n",
    "    \n",
    "    if label_shift is not None:\n",
    "        y_train = change_labels(y_train, label_shift)\n",
    "        y_test = change_labels(y_test, label_shift)\n",
    "        \n",
    "    #fit our linear regression model\n",
    "    model_linear_sklearn = LinearRegression().fit(X_train, y_train)\n",
    "    \n",
    "    #fit our logistic regression model\n",
    "    model_logistic = LogisticRegression(C=100).fit(X_train, y_train)\n",
    "    \n",
    "    #get varying data\n",
    "    x_2_range, X_with_varying_x_2 = vary_x2(X_train)\n",
    "\n",
    "    # Make linear Predictions\n",
    "    prediction_linear = model_linear_sklearn.predict(X_with_varying_x_2)\n",
    "\n",
    "    # Make logistic Predictions\n",
    "    prediction_proba = model_logistic.predict_proba(X_with_varying_x_2)\n",
    "    prediction_thresholded = model_logistic.predict(X_with_varying_x_2)\n",
    "\n",
    "    f, axes = plt.subplots(1,2, figsize=(14,6))\n",
    "\n",
    "    # Plot Linear Predictions\n",
    "    axes[0].plot(\n",
    "        x_2_range, prediction_linear,\n",
    "        'k--',\n",
    "        label= 'Linear Output (raw = continuous)'\n",
    "    )\n",
    "    axes[0].plot(\n",
    "        x_2_range, np.round(prediction_linear),\n",
    "        'k-',\n",
    "        alpha=1,\n",
    "        label= 'Linear Predicted Class (rounded = integer)',\n",
    "    )\n",
    "    axes[0].plot(\n",
    "        x_2_range, np.round(prediction_thresholded),\n",
    "        'r-',\n",
    "        alpha=0.5,\n",
    "        label= 'Logistic Predicted Class (as shown on right)',\n",
    "    )\n",
    "    axes[0].legend()\n",
    "    axes[0].set_title(\n",
    "        'LINEAR Regression:\\nRaw output and rounded output',\n",
    "        fontsize=14,\n",
    "    )\n",
    "    axes[0].set_yticks([0, 1, 2, 3])\n",
    "    axes[0].set_ylabel('Predicted Class', fontsize=12)\n",
    "\n",
    "    # Plot Logistic Predictions\n",
    "    for i in sorted(set(prediction_thresholded)):\n",
    "        axes[1].plot(\n",
    "            x_2_range, prediction_proba[:,i],\n",
    "            linewidth=2,\n",
    "            label= '$\\hat{{P}}\\:(Y={})$'.format(i)\n",
    "        )\n",
    "        axes[1].fill_between(\n",
    "            x_2_range[prediction_thresholded==i], 1, 0,\n",
    "            alpha=0.2,\n",
    "            edgecolor='gray',\n",
    "        )\n",
    "    axes[1].legend()\n",
    "    axes[1].set_title(\n",
    "        \"LOGISTIC Regression: predicted probability\\nper class and the predicted class\",\n",
    "        fontsize=14,\n",
    "    )\n",
    "    axes[1].set_ylabel('Predicted Probability', fontsize=12)\n",
    "\n",
    "    for ax in axes.flat:\n",
    "        ax.tick_params(labelsize=12)\n",
    "        ax.set_xlabel('sepal width (cm)', fontsize=12)\n",
    "        ax.legend(fontsize=11, framealpha=1, edgecolor='k')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    y_pred_test_logistic = model_logistic.predict(X_test)\n",
    "\n",
    "    #Perfromance Evaluation\n",
    "    test_score_logistic = accuracy_score(y_test, y_pred_test_logistic)*100\n",
    "\n",
    "    print(\" Logistic Testing Set Accuracy:\",str(test_score)+'%')\n",
    "    y_pred_test_linear = np.round(model_linear_sklearn.predict(X_test)).astype(int)\n",
    "\n",
    "    #Perfromance Evaluation\n",
    "    linear_test_score = accuracy_score(y_test, y_pred_test_linear)*100\n",
    "    print(\"Linear Testing Set Accuracy:\",str(linear_test_score)+'%')\n",
    "    \n",
    "    if plot_confusion_matrix == True:\n",
    "        confusion_matrix_linear = pd.crosstab(\n",
    "            y_test.values.flatten(), \n",
    "            y_pred_test_linear.flatten(), \n",
    "            \n",
    "            rownames=['Actual Class'], \n",
    "            colnames=['Predicted Class'],\n",
    "        )\n",
    "        confusion_matrix_logistic = pd.crosstab(\n",
    "            y_test.values.flatten(), \n",
    "            y_pred_test_logistic.flatten(), \n",
    "            rownames=['Actual Class'], \n",
    "            colnames=['Predicted Class'],\n",
    "        )\n",
    "        print('======================================')\n",
    "        print('Confusion Matrix Linear Regression:')\n",
    "        display(confusion_matrix_linear)\n",
    "\n",
    "        print('\\n======================================')\n",
    "        print('Confusion Matrix Logistic Regression:')\n",
    "        display(confusion_matrix_logistic)\n",
    "        print('======================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "comparison_plot(X_train, y_train, X_test, y_test, label_shift = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does our Logistic Regression come up with mutiple class predictions?\n",
    "\n",
    "\n",
    "- Each class $y_i$ has a sigmoid function that tries to predict the probability of the tested input belonging to that specific class $y_i$.\n",
    "  - In our case when we have 3 classes, thus we have 3 sigmoid functions (the blue, orange and green line in the right figure).\n",
    "\n",
    "\n",
    "- ```LogisticRegression().predict_proba(...)``` : returns probability estimates $P(y_i|x)$ for each $y_i$. In our case ```.predict_proba(...)``` returns 3 values (one for each class). In the figure we observe that : \n",
    "  - we have a **high probability of predicting Class 0** in regions with **low 'sepal width' values (left)**.\n",
    "  - we have a **high probability of predicting Class 1** in regions with **medium 'sepal width' regions (middle)**.\n",
    "  - have a **high probability of predicting Class 2** in regions with **high 'sepal width' regions (right)**.\n",
    "  \n",
    "  \n",
    "- ```LogisticRegression().predict(...)```: returns 1 value: the predicted class label. The class with the highest probability given by ```.predict_proba(...)``` is exactly the predicted class output of ```.predict(...)```\n",
    "  - In the figure our final prediction is the **red line**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig/bad_time.jpg\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ex1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <strong>🏋🏻‍♂️ TEAM ACTIVITY 1:</strong> Fit and examine a multiclass logistic regression model </div>  \n",
    "\n",
    " [▲ Return to contents](#Contents)\n",
    "\n",
    "\n",
    "\n",
    "- Load and get to know a new dataset\n",
    "- Select our predictors and response variable\n",
    "- Generate a train-test-split\n",
    "- Scale our data\n",
    "- Perform logistic regression and investigate our results\n",
    "\n",
    "![wine](fig/wine.jpg)\n",
    "\n",
    "*Image source: https://en.wikipedia.org/wiki/Wine*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset_wine = datasets.load_wine()\n",
    "# print(dataset_wine.keys())\n",
    "# print()\n",
    "#print(dataset_wine['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_wine = pd.DataFrame(data=dataset_wine.data, columns=dataset_wine.feature_names)\n",
    "y_wine = pd.DataFrame(data=dataset_wine.target, columns=['class'])\n",
    "print(X_wine.shape)\n",
    "print(y_wine.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_wine = pd.concat([X_wine, y_wine], axis=1)\n",
    "full_df_wine.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment and run the pairplot in the cell below if you wish to see a scatter-matrix of all variables contained in the wine dataset.\n",
    "\n",
    "**Please note:** This plotting code will take a minute or so to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.pairplot(full_df_wine, hue='class')\n",
    "# plt.suptitle('Wine dataset features, colored by cultivator class', fontsize=30, y=1)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The wine dataset provides many variables to use in our model.\n",
    "\n",
    "- **Because we have limited time for this exercise, we will pretend we have already conducted EDA on our dataset and have chosen to focus solely on the predictors `alcohol` and `flavanoids`.**\n",
    "\n",
    "- Run the code below to subset our full wine dataframe and to perform our train-test-split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = ['alcohol', 'flavanoids']\n",
    "\n",
    "full_df_wine_train, full_df_wine_test, = train_test_split(\n",
    "    full_df_wine[predictors+['class']],\n",
    "    test_size=0.2,\n",
    "    random_state=109,\n",
    "    shuffle=True,\n",
    "    stratify=full_df_wine['class'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now let's take a quick look at our training data to get a sense for how `alcohol` and `flavanoids` relate to one another, as well as to our cultivator `class` response variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(\n",
    "    data=full_df_wine_train,\n",
    "    x='alcohol', y='flavanoids',\n",
    "    hue='class', palette = sns.color_palette(\"tab10\")[:3])\n",
    "plt.title(\n",
    "    'Wine training observations,\\ncolored by cultivator class',\n",
    "    fontsize=14\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do you notice about the scale of our two predictor variables?**\n",
    "\n",
    "- It looks like the mean is shifted, but the total range of values are similar.\n",
    "- Let's standardize our predictors to center our points and to ensure that both variables are commonly scaled.\n",
    "- **IMPORTANT:** Remember to fit your scaler ONLY on your training data and to transform both train and test from that training fit. NEVER fit your scaler on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "## SUBSET OUR X AND y DATAFRAMES ##\n",
    "###################################\n",
    "\n",
    "X_wine_train, y_wine_train = full_df_wine_train[predictors], full_df_wine_train['class'] \n",
    "X_wine_test, y_wine_test = full_df_wine_test[predictors], full_df_wine_test['class']\n",
    "\n",
    "# print(\n",
    "#     \"Summary statistics for our training predictors BEFORE standardizing:\\n\\n\"\n",
    "#     \"{}\\n\".format(X_wine_train.describe())\n",
    "# )\n",
    "\n",
    "##########################\n",
    "## SCALE THE PREDICTORS ##\n",
    "##########################\n",
    "\n",
    "# Be certain to ONLY EVER fit your scaler on X train (NEVER fit it on test)\n",
    "scaler = StandardScaler().fit(X_wine_train[predictors])\n",
    "\n",
    "# Use your train-fitted scaler to transform both X train and X test\n",
    "X_wine_train[predictors] = scaler.transform(X_wine_train[predictors])\n",
    "X_wine_test[predictors] = scaler.transform(X_wine_test[predictors])\n",
    "\n",
    "# print(\n",
    "#     \"Summary statistics for our training predictors AFTER standardizing:\\n\\n\"\n",
    "#     \"{}\".format(X_wine_train.describe())\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>  \n",
    "\n",
    "### Student Exercise:\n",
    "\n",
    "1. Fit a logistic regression model (name it `model1_wine`) without regularization (i.e. `penalty='none'`) to predict the `class` of each wine using our scaled predictors `alcohol` and `flavanoids`\n",
    "2. Report on the training and test accuracy of our fitted model\n",
    "3. Show that for each training prediction, the probability of each predicted class sums to one. (**HINT:** You can use the `predict_proba` method to generate your predicted probabilities.)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give it a shot! \n",
    "\n",
    "model1_wine = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>  \n",
    "     \n",
    "#### Solution\n",
    "    \n",
    "After you've attempted the above exercise, uncomment and run the code below.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load 'solutions/exercise_1_sol.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='plot'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Predictions and Decision boundaries\n",
    " [▲ Return to contents](#Contents)\n",
    "\n",
    "**What are decision boundaries:**\n",
    " - In general, a pattern classifier carves up (or tesselates or partitions) the feature space into volumes called decision regions. \n",
    " - All feature vectors in a decision region are assigned to the same category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_points(ax, X_train = X_train):\n",
    "    for i, y_class in enumerate(set(y_train.values.flatten())):\n",
    "        index = (y_train == y_class).values\n",
    "        ax.scatter(\n",
    "            X_train[index]['sepal width (cm)'],\n",
    "            X_train[index]['sepal length (cm)'],\n",
    "            c=colors[i],\n",
    "            marker=markers[i],\n",
    "            s=65, \n",
    "            edgecolor='w',\n",
    "            label=names[i],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(7, 6))\n",
    "\n",
    "colors = [\"tab:blue\", \"tab:orange\",\"tab:green\"]\n",
    "markers = [\"s\", \"o\", \"D\"]\n",
    "names = dataset.target_names\n",
    "\n",
    "plot_points(ax)\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\n",
    "ax.set_title('Classes of Flowers')\n",
    "ax.set_ylabel('sepal length (cm)')\n",
    "ax.set_xlabel('sepal width (cm)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the decision boundary by using the following functions:\n",
    "\n",
    "- **`np.meshgrid()`**\n",
    "\n",
    "  - `meshgrid` returns coordinate matrices from coordinate vectors, effectively constructing a grid.\n",
    "  - The documentation: https://numpy.org/doc/stable/reference/generated/numpy.meshgrid.html\n",
    "\n",
    "\n",
    "- **`plt.contourf()`**\n",
    "\n",
    "  - `contourf` draws filled contours.\n",
    "  - The documentation: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.contourf.html#matplotlib.pyplot.contourf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is meshgrid? A quick numpy dive...\n",
    "\n",
    "How does `meshgrid` work?\n",
    "\n",
    "Creates two, 2D arrays of all the points in the grid: one for x coordinates, one for y coordinates\n",
    "\n",
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20190326203020/mesh04.png\" alt=\"meshgrid example from geeksforgeeks\">\n",
    "\n",
    "We will go step-by-step to create a 3-by-4 grid below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_vector = np.linspace(-6, 6, 2)  # start, stop, num pts\n",
    "y_vector = np.linspace(-2, 1, 4) \n",
    "\n",
    "print(f\"x_vector: {x_vector} \\t shape: {x_vector.shape}\")\n",
    "print(f\"y_vector: {y_vector} \\t shape: {y_vector.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return 2D x,y points of a grid\n",
    "x_grid, y_grid = np.meshgrid(x_vector, y_vector)\n",
    "\n",
    "print(f\"x_grid, shape {x_grid.shape}:\\n\\n{x_grid}\\n\")\n",
    "print(f\"y_grid, shape {y_grid.shape}:\\n\\n{y_grid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Additional `numpy` methods you will notice used in our decision boundary plots:**\n",
    "\n",
    "- `np.ravel()`, which returns a contiguous flattened array.\n",
    "- `np.stack()`, which concatenates a sequence of arrays along a new axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_grid_ravel = x_grid.ravel()\n",
    "y_grid_ravel = y_grid.ravel()\n",
    "\n",
    "print(f\"x_grid.ravel(), shape {x_grid_ravel.shape}:\\n\\n{x_grid_ravel}\\n\")\n",
    "print(f\"y_grid.ravel(), shape {y_grid_ravel.shape}:\\n\\n{y_grid_ravel}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xy_grid_stacked = np.stack(\n",
    "    (x_grid.ravel(), y_grid.ravel()),\n",
    "    axis=1\n",
    ") \n",
    "\n",
    "print(f\"x and y stacked as columns in output:\\n\\n{xy_grid_stacked}\\n\")\n",
    "\n",
    "xy_grid_stacked = np.stack(\n",
    "    (x_grid.ravel(), y_grid.ravel()),\n",
    "    axis=0\n",
    ")\n",
    "\n",
    "print(f\"x and y stacked as rows in output:\\n\\n{xy_grid_stacked}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, with our `numpy` review over, let's get back to plotting our boundaries! \n",
    "\n",
    "- First, we will create a very fine (spacing of 0.003!) `np.meshgrid` of points to evaluate and color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = X_train['sepal width (cm)']\n",
    "x_2 = X_train['sepal length (cm)']\n",
    "\n",
    "# Just for illustration purposes we use a margin of 0.2 to the\n",
    "# left, right, top and bottum of our minimal and maximal points.\n",
    "# This way our minimal and maximal points won't lie exactly \n",
    "# on the axis. \n",
    "\n",
    "x_1_min, x_1_max = x_1.min() - 0.2, x_1.max() + 0.2\n",
    "x_2_min, x_2_max = x_2.min() - 0.2, x_2.max() + 0.2\n",
    "\n",
    "xx_1, xx_2 = np.meshgrid(\n",
    "    np.arange(x_1_min, x_1_max, 0.003),\n",
    "    np.arange(x_2_min, x_2_max, 0.003),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Then, we will generate predictions for each point in our grid and use `plt.contourf` to plot those results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting decision regions\n",
    "f, ax = plt.subplots(1, 1, figsize=(7, 6))\n",
    "\n",
    "X_mesh = np.stack((xx_1.ravel(), xx_2.ravel()),axis=1)\n",
    "\n",
    "Z = model_logistic.predict(X_mesh)\n",
    "Z = Z.reshape(xx_1.shape)\n",
    "\n",
    "# contourf(): Takes in x,y, and z values -- all 2D arrays of same size\n",
    "ax.contourf(xx_1, xx_2, Z, alpha=0.3, colors=colors, levels=2)\n",
    "\n",
    "plot_points(ax)\n",
    "    \n",
    "ax.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\n",
    "ax.set_title('Classes of Flowers')\n",
    "ax.set_ylabel('sepal length (cm)')\n",
    "ax.set_xlabel('sepal width (cm)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why are the decision boundaries of this Logistic Regression linear?\n",
    "\n",
    "Imagine the simple case where we have only a 2 class classification problem: \n",
    "\n",
    "The logistic regression formula can be written as: \n",
    "$$\n",
    "\\hat{p} = \\cfrac{e^{w^T x}}{1+e^{w^T x}}\n",
    "$$\n",
    "\n",
    "Dividing through by the numerator, This is equivalent to\n",
    "$$\n",
    "\\hat{p} = \\cfrac{1}{1+e^{-w^T x}}\n",
    "$$\n",
    "\n",
    "Expanding $w^T x$, we have $x_1$ (sepal width), $x_2$ (sepal length), and our intercept $x_0$ (constant = 1):\n",
    "\n",
    "$$\n",
    "w^T x = \n",
    "\\begin{bmatrix}\n",
    "w_0 & w_1 & w_2 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_0 \\\\ x_1 \\\\ x_2 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Which makes our formula for $\\hat{p}$ equivalent to:\n",
    "\n",
    "$$\n",
    "\\hat{p} = \\cfrac{1}{1 +e^{\\displaystyle -(w_0 \\cdot 1 + w_1  \\cdot x_1 +  w_2 \\cdot x_2)}}\n",
    "$$\n",
    "\n",
    "Since we don't use multiple higher order polynomial features like $x_1^2, x_2^2$, our logistic model only depends on the first order simple features $x_1$ and $x_2$. \n",
    "\n",
    "**What do we have to do to find the the decision boundary?**\n",
    "\n",
    "The decision boundaries are exactly at the position where our algorithm \"hesitates\" when predicting which class to classify. The output probability of our sigmoid (or softmax) is exactly 0.5. Solving our sigmoid function for $p=0.5$:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "0.5 &= \\cfrac{1}{1 +e^{\\displaystyle -(w_0 \\cdot 1 + w_1 x_1 +  w_2 x_2)}} \\\\\n",
    "0.5 &= \\cfrac{1}{1 + 1} \\\\\n",
    "e^{\\displaystyle -(w_0 \\cdot 1 + w_1  x_1 +  w_2 x_2)} &= 1 \\\\\n",
    "& \\\\\n",
    "-(w_0 \\cdot 1 + w_1  x_1 +  w_2 x_2 ) &= 0\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "When we only use two predictor features this constraint of $p=0.5$ results in a linear system; thus we observe a **linear decision boundary.**\n",
    "\n",
    "In our case when we have three classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='poly'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fit a higher order polynomial logistic regression model for classification, compare performance, plot decision boundaries, and interpret results\n",
    "\n",
    " [▲ Return to contents](#Contents)\n",
    "\n",
    "**In this lab we will create degree-2 polynomial features for both of our predictors and re-fit our model and examine the results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_poly = X_train.copy()\n",
    "X_train_poly['sepal width (cm)^2'] = X_train['sepal width (cm)']**2\n",
    "X_train_poly['sepal length (cm)^2'] = X_train['sepal length (cm)']**2\n",
    "\n",
    "X_test_poly = X_test.copy()\n",
    "X_test_poly['sepal width (cm)^2'] = X_test_poly['sepal width (cm)']**2\n",
    "X_test_poly['sepal length (cm)^2'] = X_test_poly['sepal length (cm)']**2\n",
    "X_test_poly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "model_logistic_poly = LogisticRegression(penalty=\"none\").fit(X_train_poly, y_train)\n",
    "\n",
    "#Predict\n",
    "y_pred_train = model_logistic_poly.predict(X_train_poly)\n",
    "y_pred_test = model_logistic_poly.predict(X_test_poly)\n",
    "\n",
    "#Perfromance Evaluation\n",
    "train_score = accuracy_score(y_train, y_pred_train)*100\n",
    "test_score = accuracy_score(y_test, y_pred_test)*100\n",
    "\n",
    "print(\"Training Set Accuracy:\",str(train_score)+'%')\n",
    "print(\"Testing Set Accuracy:\",str(test_score)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>  \n",
    "\n",
    "**Our test performance is decreasing, what might be happening?**\n",
    "\n",
    "</div>\n",
    "\n",
    " - How would you test if this is happening? \n",
    " - How would you inhibit this phenomenon from happening?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting decision regions\n",
    "f, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "\n",
    "X_mesh_poly = np.stack(\n",
    "    (xx_1.ravel(), xx_2.ravel(), xx_1.ravel()**2,xx_2.ravel()**2),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "Z = model_logistic_poly.predict(X_mesh_poly)\n",
    "Z = Z.reshape(xx_1.shape)\n",
    "\n",
    "ax.contourf(xx_1, xx_2, Z, alpha=0.3, colors=colors, levels=2)\n",
    "\n",
    "plot_points(ax, X_train = X_train_poly)\n",
    "    \n",
    "ax.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\n",
    "ax.set_title('Classes of Flowers')\n",
    "ax.set_ylabel('sepal length (cm)')\n",
    "ax.set_xlabel('sepal width (cm)')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>  \n",
    "\n",
    "**What do you observe regarding the form of the decision boundaries? Does this make sense?**\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision boundaries for higher order logistic regression \n",
    "\n",
    "Let's return to the mathematical representation of our logistic regression model:\n",
    "\n",
    "$$\\hat{p}= \\dfrac{e^{w^T x}}{1+e^{w^T x}}$$\n",
    "\n",
    "Which is equivalent to: \n",
    "\n",
    "$$\\hat{p}= \\dfrac{1}{1+e^{-w^T x}}$$\n",
    "\n",
    "Now we use $x_1$ (sepal width), $x_2$ (sepal length), an intercept $x_0$ (constant =1), PLUS two higher order terms while making predictions: \n",
    "\n",
    "- $x_1^2 = [\\text{sepal width}]^2$\n",
    "\n",
    "- $x_2^2 = [\\text{sepal length}]^2$\n",
    "\n",
    "$$\\hat{p}= \\cfrac{1}{1+e^{\\displaystyle -(w_0 \\cdot 1 + w_1 x_1 +  w_2 x_2 + w_3 x_1^2 +  w_4 x_2^2)}}$$\n",
    "\n",
    "Now solving for $p=0.5$ results in an equation also dependent on $x_1^2$ and $x_2^2$: thus we observe **non-linear decision boundaries**:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "0.5 &= \\cfrac{1}{1 +e^{\\displaystyle -(w_0 \\cdot 1 + w_1 x_1 +  w_2 x_2 + w_3 x_1^2 +  w_4 x_2^2)}} \\\\\n",
    "0.5 &= \\cfrac{1}{1 + 1} \\\\\n",
    "e^{\\displaystyle -(w_0 \\cdot 1 + w_1 x_1 +  w_2 x_2 + w_3 x_1^2 +  w_4 x_2^2)} &= 1 \\\\\n",
    "& \\\\\n",
    "-(w_0 \\cdot 1 + w_1 x_1 +  w_2 x_2 + w_3 x_1^2 +  w_4 x_2^2) &= 0\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ex2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[▲ Return to contents](#Contents)\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <strong>🏋🏻‍♂️ TEAM ACTIVITY 2:</strong> Plot boundaries for our prior team exercise model </div>  \n",
    "\n",
    "\n",
    "- In this session, we will plot decision boundaries for the `model1_wine` model we fit during Team Exercise #1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>  \n",
    " \n",
    "### Student Exercise:\n",
    "\n",
    "Using the `plot_wine_2d_boundaries` function provided below, you are going to plot the TRAINING data decision boundaries for `model1_wine` from team exercise 1 above.\n",
    "\n",
    "**To do this, you will need to:**\n",
    "\n",
    "1. Define an appropriate `xx_1_wine` and `xx_2_wine` for input into this function using `np.meshgrid`\n",
    "2. And call `plot_wine_2d_boundaries` specifying the appropriate input parameters\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We give you the plotting function here\n",
    "def plot_wine_2d_boundaries(X_data, y_data, predictors, model, xx_1, xx_2):\n",
    "    \"\"\"Plots 2-dimensional decision boundaries for a fitted sklearn model\n",
    "    \n",
    "    :param X_data: pd.DataFrame object containing your predictor variables\n",
    "    :param y_data: pd.Series object containing your response variable\n",
    "    :param predictors: list of predictor names corresponding with X_data columns\n",
    "    :param model: sklearn fitted model object\n",
    "    :param xx_1: np.array object of 2-dimensions, generated using np.meshgrid\n",
    "    :param xx_2: np.array object of 2-dimensions, generated using np.meshgrid\n",
    "    \"\"\"\n",
    "\n",
    "    def plot_points(ax):\n",
    "        for i, y_class in enumerate(set(y_data.values.flatten())):\n",
    "            index = (y_data == y_class).values\n",
    "            ax.scatter(\n",
    "                X_data[index][predictors[0]],\n",
    "                X_data[index][predictors[1]],\n",
    "                c=colors[i],\n",
    "                marker=markers[i],\n",
    "                s=65, \n",
    "                edgecolor='w',\n",
    "                label=\"class {}\".format(i),\n",
    "            )\n",
    "\n",
    "    # Plotting decision regions\n",
    "    f, ax = plt.subplots(1, 1, figsize=(7, 6))\n",
    "\n",
    "    X_mesh = np.stack((xx_1.ravel(), xx_2.ravel()), axis=1)\n",
    "\n",
    "    Z = model.predict(X_mesh)\n",
    "    Z = Z.reshape(xx_1.shape)\n",
    "\n",
    "    ax.contourf(xx_1, xx_2, Z, alpha=0.3, colors=colors, levels=2)\n",
    "\n",
    "    plot_points(ax)\n",
    "\n",
    "    ax.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\n",
    "    ax.set_title(\n",
    "        'Wine cultivator class prediction\\ndecision boundaries',\n",
    "        fontsize=16\n",
    "    )\n",
    "    ax.set_xlabel(predictors[0], fontsize=12)\n",
    "    ax.set_ylabel(predictors[1], fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>  \n",
    " \n",
    "#### Enter your code below\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n",
    "\n",
    "# Give exercise 2 a try! Just adapt the iris code to the wine dataset\n",
    "\n",
    "# Create your meshgrid arrays similar to xx_1 and xx_2\n",
    "# Be certain to use reasonable min and max bounds for your data\n",
    "\n",
    "xx_1_wine, xx_2_wine = ..., ...\n",
    "\n",
    "# Then plot your boundaries\n",
    "\n",
    "plot_wine_2d_boundaries( ... )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>  \n",
    "    \n",
    "#### Solution\n",
    "    \n",
    "After you've attempted the above exercise, uncomment and run the code below.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load 'solutions/exercise_2_sol.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='regularize'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Fit regularized polynomial logistic regression models and examine the results\n",
    "\n",
    " [▲ Return to contents](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>  \n",
    "    \n",
    "**What do you expect to happen to our decision boundaries if we apply regularization to our polynomial regression model?**\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "f, ax = plt.subplots(1, 3, sharey=True, figsize=(6*3, 6))\n",
    "\n",
    "model_logistics =[]\n",
    "model_logistics_test_accs_scores =[]\n",
    "model_logistics_train_accs_scores =[]\n",
    "\n",
    "for test, C in enumerate([10000, 100, 1]):\n",
    "    model_logistics.append(LogisticRegression(C=C).fit(X_train_poly, y_train))\n",
    "    \n",
    "    y_pred_train = model_logistics[test].predict(X_train_poly)\n",
    "    y_pred_test = model_logistics[test].predict(X_test_poly)\n",
    "    model_logistics_train_accs_scores.append(accuracy_score(y_train, y_pred_train)*100)\n",
    "    model_logistics_test_accs_scores.append(accuracy_score(y_test, y_pred_test)*100)\n",
    "\n",
    "    Z = model_logistics[test].predict(X_mesh_poly)\n",
    "    Z = Z.reshape(xx_1.shape)\n",
    "\n",
    "    ax[test].contourf(xx_1, xx_2, Z, alpha=0.3, colors=colors, levels=2)\n",
    "    plot_points(ax[test])\n",
    "\n",
    "    ax[test].set_title('Classes of Flowers, with C = '+ str(C), fontsize=16)\n",
    "    ax[test].set_xlabel('sepal width (cm)', fontsize=12)\n",
    "    \n",
    "    if test==0:\n",
    "        ax[test].legend(loc='upper left', ncol=1, fontsize=12)\n",
    "        ax[test].set_ylabel('sepal length (cm)', fontsize=12)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do you observe?**\n",
    "\n",
    " - How are the decision boundaries looking?\n",
    " - What happens when the regularization term `C` changes? \n",
    " - You may want to look at the documentation of `sklearn.linear.LogisticRegression()` to see how the `C` argument works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the documentation uncomment and run the following command:\n",
    "\n",
    "# LogisticRegression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do expect regarding the evolution of the norm of the coefficients of our models when the regularization term `C` changes?**\n",
    "\n",
    "Our list contains all 3 models with different values for `C` (**take a look at the first parameter within brackets**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test, model in enumerate(model_logistics):\n",
    "    print('\\nRegularization parameter : \\tC = {}'.format(model.C))\n",
    "    print(\"Training Set Accuracy : \\t{}\".format(model_logistics_train_accs_scores[test])+'%')\n",
    "    print(\"Testing Set Accuracy : \\t\\t{}\".format(model_logistics_test_accs_scores[test])+'%')\n",
    "    print('Mean absolute coeficient : \\t{:0.2f}'.format(np.mean(np.abs(model.coef_))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Interpretation of Results: What happens when our Regularization Parameter decreases?**\n",
    "\n",
    "The amount of regularization increases, and this results in:\n",
    "\n",
    "- The training set accuracy decreasing a little bit (not much of a problem)\n",
    "\n",
    "\n",
    "- The TEST Accuracy INCREASING a little bit (better generalization!)\n",
    "\n",
    "\n",
    "- The size of our coefficents DECREASES on average.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ex3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <strong>🏋🏻‍♂️ TEAM ACTIVITY 3:</strong> Tune and fit a Lasso regularized model using cross-validation. </div>  \n",
    "\n",
    " [▲ Return to contents](#Contents)\n",
    "\n",
    "- Perform Lasso regularized logistic regression and choose an appropriate `C` by using cross-validation\n",
    "\n",
    "\n",
    "- Confirm whether any predictors are identified as unimportant (i.e. $w_i=0$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**There are a number of different tools built into `sklearn` that help you to perform cross-validation.**\n",
    "\n",
    "- A lot of examples in this class so far have used `sklearn.model_selection.cross_validate` as the primary means for peforming cross-validation.\n",
    "\n",
    "\n",
    "- **BUT WAIT!!!** As it turns out, `sklearn` provides a very useful tool for performing logistic regression cross-validation across a range of regularization hyperparameters.\n",
    "\n",
    "**For this problem, you should use the `sklearn.linear_model.LogisticRegressionCV`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>  \n",
    "\n",
    "### Student Exercise:\n",
    "\n",
    "1. Add a new predictor to your `X` data measuring the interaction between `alcohol` and `flavanoid` (make certain to do this for both your training and TEST `X` dataframes).\n",
    "\n",
    "    \n",
    "2. Please review the documentation to learn how to fit and use the `LogisticRegressionCV` model object: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html\n",
    "\n",
    "    \n",
    "3. Using `LogisticRegressionCV` to fit your model, perfrom cross-validation with `3` k-folds, Lasso-like regularization, and the following list of regularization parameters `[1e-4,1e-3,1e-2,1e-1,1e0,1e1,1e2,1e3,1e4]`.\n",
    "\n",
    "    \n",
    "4. Print (1) the regularization parameter chosen by the model, (2) your train and test accuracies, and (3) your model coefficients (including the intercept).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>  \n",
    "    \n",
    "#### Solution\n",
    "    \n",
    "After you've attempted the above exercise, uncomment and run the code below.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load 'solutions/exercise_3_sol.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:powderblue;\">\n",
    "    \n",
    "## What did we learn today?\n",
    "\n",
    "- Linear regression does not work well with discrete features\n",
    "\n",
    "    \n",
    "- Logistic regression is the way to go: outputs are clipped from 0 to 1\n",
    "\n",
    "\n",
    "- Multiclass? Can still use logistic regression! Separate probabilities for each class - total P is 1\n",
    "\n",
    "    \n",
    "- If just have two features, put them on x and y axis and plot data, and can easily draw decision boundary\n",
    "\n",
    "    \n",
    "- How to use meshgrid and contourf\n",
    "\n",
    "    \n",
    "- What happens with polyomial features in logistic regression: nonlinear decision boundaries\n",
    "\n",
    "    \n",
    "- Impact of regularization argument for logistic regession\n",
    "    \n",
    "-------\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "    \n",
    "## END OF STANDARD LAB\n",
    " \n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='knn'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. kNN Classification\n",
    "\n",
    " [▲ Return to contents](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this lab, we will fit a kNN-classification model, plot our decision boundaries, and interpret the results.**\n",
    "\n",
    "You can read the `KNeighborsClassifier` documentation here: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "model_KNN_classifier = KNeighborsClassifier(n_neighbors=1).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict\n",
    "y_pred_train = model_KNN_classifier.predict(X_train)\n",
    "y_pred_test = model_KNN_classifier.predict(X_test)\n",
    "\n",
    "#Perfromance Evaluation\n",
    "train_score = accuracy_score(y_train, y_pred_train)*100\n",
    "test_score = accuracy_score(y_test, y_pred_test)*100\n",
    "\n",
    "print(\"Training Set Accuracy:\",str(train_score)+'%')\n",
    "print(\"Testing Set Accuracy:\",str(test_score)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact we have a big gap of performance between the test and training set means we are overfitting. This can be explained by our choice to use `n_neighbors=1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>  \n",
    "\n",
    "- **Based on your knowledge of kNN-regression can you guess how the decision boundary of the kNN-classification will look when using \"n_neighbors=1\"?**\n",
    "  \n",
    "\n",
    "- **How about if we were to increase that number to \"n_neighbors=50\" or \"n_neighbors=100\" (our total number of training observations)?**\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please note:** *The below code cell will take several minutes to run, primarily because of the large number of values in our meshgrid against which each kNN model must generate predictions.*\n",
    "\n",
    "**What does this tell us about the efficiency of the kNN algorithm vs. regularized logistic regression?**\n",
    "\n",
    "- Notice that the comparable set of 3 subplots generated using logistic regression only took about 1 second to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Similar to what we did above for varying values C\n",
    "# let's explore the effect that k in kNN classification has on our decision boundaries\n",
    "ks = [1, 50, 100]\n",
    "\n",
    "X_mesh = np.stack((xx_1.ravel(), xx_2.ravel()),axis=1)\n",
    "Z = model_logistic.predict(X_mesh)\n",
    "Z = Z.reshape(xx_1.shape)\n",
    "\n",
    "f, ax = plt.subplots(1, 3, sharey=True, figsize=(6*3, 6))\n",
    "\n",
    "model_ks = []\n",
    "model_ks_test_accs_scores = []\n",
    "model_ks_train_accs_scores = []\n",
    "\n",
    "for test, k in enumerate(ks):\n",
    "    model_ks.append(KNeighborsClassifier(n_neighbors=k).fit(X_train, y_train))\n",
    "    \n",
    "    y_pred_train = model_ks[test].predict(X_train)\n",
    "    y_pred_test = model_ks[test].predict(X_test)\n",
    "    model_ks_train_accs_scores.append(accuracy_score(y_train, y_pred_train)*100)\n",
    "    model_ks_test_accs_scores.append(accuracy_score(y_test, y_pred_test)*100)\n",
    "\n",
    "    Z = model_ks[test].predict(X_mesh)\n",
    "    Z = Z.reshape(xx_1.shape)\n",
    "\n",
    "    ax[test].contourf(xx_1, xx_2, Z, alpha=0.3, colors=colors, levels=2)\n",
    "    plot_points(ax[test])\n",
    "\n",
    "    ax[test].set_title('Classes of Flowers, with k = '+ str(k), fontsize=16)\n",
    "    ax[test].set_xlabel('sepal width (cm)', fontsize=12)\n",
    "    \n",
    "    if test==0:\n",
    "        ax[test].legend(loc='upper left', ncol=1, fontsize=12)\n",
    "        ax[test].set_ylabel('sepal length (cm)', fontsize=12)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>  \n",
    "\n",
    "- **What do you notice in the plots above in regards to our decision boundaries as $k$ increases?**\n",
    "- **What implications might this have for our Train vs. Test accuracies at each value $k$?**\n",
    "- **If we wanted to tune our kNN classification model to find the best value $k$ given our data, what approach should we take?** (You better say \"cross-validation\" with many values $k$!)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pipeline'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Introducing `sklearn` pipelines (optional)\n",
    "\n",
    " [▲ Return to contents](#Contents)\n",
    "\n",
    "**Pipelines can be used to to sequentially apply a list of transforms (e.g. scaling, polynomial feature creation) and a final estimator.**\n",
    "\n",
    "Instead of manually building our polynomial features which might take a lot of lines of code we can use a **pipeline** to sequentially create polynomials before fitting our logistic regression. Scaling can also be done inside the ```make_pipeline```.\n",
    "\n",
    "The documentation: https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html\n",
    "\n",
    "**Previously we did:**\n",
    "\n",
    "```python\n",
    "X_train_poly_cst=X_train_cst.copy()\n",
    "X_train_poly_cst['sepal width (cm)^2'] = X_train_cst['sepal width (cm)']**2\n",
    "X_train_poly_cst['sepal length (cm)^2'] = X_train_cst['sepal length (cm)']**2\n",
    "\n",
    "X_test_poly_cst=X_test_cst.copy()\n",
    "X_test_poly_cst['sepal width (cm)^2'] = X_test_poly_cst['sepal width (cm)']**2\n",
    "X_test_poly_cst['sepal length (cm)^2'] = X_test_poly_cst['sepal length (cm)']**2 \n",
    "```\n",
    "\n",
    "**Now it is a one-liner:**\n",
    "\n",
    "```python\n",
    "make_pipeline(PolynomialFeatures(degree=2, include_bias=False), LogisticRegression())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "polynomial_logreg_estimator = make_pipeline(\n",
    "    PolynomialFeatures(degree=2,include_bias=False),\n",
    "    LogisticRegression(),\n",
    ")\n",
    "\n",
    "#Training\n",
    "polynomial_logreg_estimator.fit(X_train_poly, y_train)\n",
    "\n",
    "#Predict\n",
    "y_pred_train = polynomial_logreg_estimator.predict(X_train_poly)\n",
    "y_pred_test = polynomial_logreg_estimator.predict(X_test_poly)\n",
    "\n",
    "#Perfromance Evaluation\n",
    "train_score = accuracy_score(y_train, y_pred_train)*100\n",
    "test_score = accuracy_score(y_test, y_pred_test)*100\n",
    "\n",
    "print(\"Training Set Accuracy:\",str(train_score)+'%')\n",
    "print(\"Testing Set Accuracy:\",str(test_score)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
