{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109A Introduction to Data Science "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 10: Random Forest and Boosting\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2021**<br/>\n",
    "**Instructors**: Pavlos Protopapas and Natesh Pillai<br/>\n",
    "**Lab Team**: Marios Mattheakis, Hayden Joy, Chris Gumb, and Eleni Kaxiras<br/>\n",
    "**Authors**: Hayden Joy, and Marios Mattheakis\n",
    "<hr style='height:2px'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will work with a spam email dataset again. Our ultimate goal is to be able to build models so that we can predict whether an email is spam or not spam based on word characteristics within each email. We will review Decision Trees and Bagging methods, and introduce Random Forest and Boosting: Ada Boost and XGBoost.\n",
    "\n",
    "Specifically, we will: \n",
    "  \n",
    "1. *Quick review of last week*  \n",
    "2. What is a Random Forest model?\n",
    "3. Build the Decision Tree model, Bagging model, Random Forest Model for comparison with Boosting. \n",
    "4. *Theory:* What is Boosting?\n",
    "5. Use the Adaboost on the Spam Dataset.\n",
    "6. *Theory:* What is Gradient Boosting and XGBoost?\n",
    "7. Use XGBoost on the Spam Dataset: Extreme Gradient Boosting\n",
    "\n",
    "Optional: Example to better understand Bias vs Variance tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "## 1. *Quick review of last week*  \n",
    "\n",
    "#### The Idea: Decision Trees are just flowcharts and interpretable!\n",
    "\n",
    "It turns out that simple flow charts can be formulated as mathematical models for classification and these models have the properties we desire;\n",
    " - interpretable by humans \n",
    " - have sufficiently complex decision boundaries \n",
    " - the decision boundaries are locally linear, each component of the decision boundary is simple to describe mathematically. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "\n",
    "#### How to build Decision Trees (the Learning Algorithm in words): \n",
    "To learn a decision tree model, we take a greedy approach: \n",
    " 1. Start with an empty decision tree (undivided feature space) \n",
    " 2. Choose the ‘optimal’ predictor on which to split and choose the ‘optimal’ threshold value for splitting by applying a **splitting criterion (1)**\n",
    " 3. Recurse on on each new node until **stopping condition (2)** is met"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So we need a (1) splitting criterion and a (2) stopping condition:\n",
    "\n",
    "  #### (1) Splitting criterion \n",
    "\n",
    "<img src=\"data/split2_adj.png\" alt=\"split2\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Stopping condition\n",
    "\n",
    "**Not stopping while building a deeper and deeper tree = 100% training accuracy; Yet we will overfit! \n",
    "\n",
    "To prevent the **overfitting** from happening, we should have stopping condition.\n",
    "\n",
    "-------------\n",
    "\n",
    "#### How do we go from Classification to Regression?\n",
    "\n",
    "- For classification, we return the majority class in the points of each leaf node. \n",
    "- For regression we return the average of the outputs for the points in each leaf node. \n",
    "\n",
    "-------------\n",
    "\n",
    "### ensemble: a group of items viewed as a whole rather than individually\n",
    "\n",
    "#### What is bagging?\n",
    "  \n",
    "One way to adjust for the high variance of the output of an experiment is to perform the experiment multiple times and then average the results. \n",
    "\n",
    " 1. **Bootstrap:** we generate multiple samples of training data, via bootstrapping. We train a full decision tree on each sample of data. \n",
    " 2. **AGgregatiING** for a given input, we output the averaged outputs of all the models for that input. \n",
    " \n",
    "This method is called **Bagging: B** ootstrap + **AGG**regat**ING**. \n",
    "\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-------------\n",
    "\n",
    "## 2. Building the tree models of last week \n",
    "\n",
    "### Rebuild the Decision Tree model and Bagging model for comparison with Boosting methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import sklearn.metrics as metrics\n",
    "import time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.width', 1500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "from sklearn.model_selection import learning_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Dataframe and Set Column Names\n",
    "spam_df = pd.read_csv('data/spam.csv', header=None)\n",
    "columns = [\"Column_\"+str(i+1) for i in range(spam_df.shape[1]-1)] + ['Spam']\n",
    "spam_df.columns = columns\n",
    "display(spam_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us split the dataset into a 70-30 split by using the following:\n",
    "#Split data into train and test\n",
    "np.random.seed(42)\n",
    "msk = np.random.rand(len(spam_df)) < 0.7\n",
    "data_train = spam_df[msk]\n",
    "data_test = spam_df[~msk]\n",
    "\n",
    "#Split predictor and response columns\n",
    "x_train, y_train = data_train.drop(['Spam'], axis=1), data_train['Spam']\n",
    "x_test , y_test  = data_test.drop(['Spam'] , axis=1), data_test['Spam']\n",
    "\n",
    "print(\"Shape of Training Set :\",data_train.shape)\n",
    "print(\"Shape of Testing Set :\" ,data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Percentage of Spam in Train and Test Set\n",
    "percentage_spam_training = 100*y_train.sum()/len(y_train)\n",
    "percentage_spam_testing  = 100*y_test.sum()/len(y_test)\n",
    "                                                  \n",
    "print(\"Percentage of Spam in Training Set \\t : {:0.2f}%.\".format(percentage_spam_training))\n",
    "print(\"Percentage of Spam in Testing Set \\t : {:0.2f}%.\".format(percentage_spam_testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "\n",
    "### Fitting an Optimal Single Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Best depth for single decision trees of last week\n",
    "best_depth = 7\n",
    "print(\"The best depth was found to be:\", best_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evalaute the performance at the best depth\n",
    "model_tree = DecisionTreeClassifier(max_depth=best_depth)\n",
    "model_tree.fit(x_train, y_train)\n",
    "\n",
    "#Check Accuracy of Spam Detection in Train and Test Set\n",
    "acc_trees_training = accuracy_score(y_train, model_tree.predict(x_train))\n",
    "acc_trees_testing  = accuracy_score(y_test,  model_tree.predict(x_test))\n",
    "\n",
    "print(\"Simple Decision Trees: Accuracy, Training Set \\t : {:.2%}\".format(acc_trees_training))\n",
    "print(\"Simple Decision Trees: Accuracy, Testing Set \\t : {:.2%}\".format(acc_trees_testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "--------\n",
    "\n",
    "### Fitting 100 Single Decision Trees while Bagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trees = 100 # we tried a variety of numbers here\n",
    "\n",
    "#Creating model\n",
    "np.random.seed(0)\n",
    "model = DecisionTreeClassifier(max_depth=best_depth+5)\n",
    "\n",
    "#Initializing variables\n",
    "predictions_train = np.zeros((data_train.shape[0], n_trees))\n",
    "predictions_test = np.zeros((data_test.shape[0], n_trees))\n",
    "\n",
    "#Conduct bootstraping iterations\n",
    "for i in range(n_trees):\n",
    "    temp = data_train.sample(frac=1, replace=True)\n",
    "    response_variable = temp['Spam']\n",
    "    temp = temp.drop(['Spam'], axis=1)\n",
    "    \n",
    "    model.fit(temp, response_variable)  \n",
    "    predictions_train[:,i] = model.predict(x_train)   \n",
    "    predictions_test[:,i] = model.predict(x_test)\n",
    "    \n",
    "#Make Predictions Dataframe\n",
    "columns = [\"Bootstrap-Model_\"+str(i+1) for i in range(n_trees)]\n",
    "predictions_train = pd.DataFrame(predictions_train, columns=columns)\n",
    "predictions_test = pd.DataFrame(predictions_test, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to ensemble the prediction of each bagged decision tree model\n",
    "def get_prediction(df, count=-1):\n",
    "    count = df.shape[1] if count==-1 else count\n",
    "    temp = df.iloc[:,0:count]\n",
    "    return np.mean(temp, axis=1)>0.5\n",
    "\n",
    "#Check Accuracy of Spam Detection in Train and Test Set\n",
    "acc_bagging_training = 100*accuracy_score(y_train, get_prediction(predictions_train, count=-1))\n",
    "acc_bagging_testing  = 100*accuracy_score(y_test, get_prediction(predictions_test, count=-1))\n",
    "\n",
    "print(\"Bagging: \\tAccuracy, Training Set \\t: {:0.2f}%\".format(acc_bagging_training))\n",
    "print(\"Bagging: \\tAccuracy, Testing Set \\t: {:0.2f}%\".format( acc_bagging_testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weaknesses of Bagging\n",
    "\n",
    "Bagging is a greedy algorithm. What does this mean?\n",
    "    We always choose the feature with the most impact: ie the most information gain.\n",
    "    \n",
    "In what scenarios is this likely to be a problem?\n",
    "\n",
    "    \n",
    "<img src=\"data/dep_predictors.png\" alt=\"split2\" width=\"40%\"/>\n",
    "\n",
    "Imagine that this is the true underlying data generative process. Here predictors $x_2$ and $x_3$ influence $x_1$.\n",
    "\n",
    "$\\bullet$ Which predictor do you think bagging is likely to select as the root node?\n",
    "\n",
    "$\\bullet$ Why is this likely to be an issue?\n",
    "\n",
    "$\\bullet$ Because of their greedy nature, bagging ensembles are very likely to be correlated, especially in the shallower nodes of the individual decision trees.\n",
    "\n",
    "\n",
    "### Why are decision trees greedy?\n",
    "\n",
    "$\\bullet$ Decision trees are NP-complete, there is no way to find the global minima ie. the best tree unless we use brute force and try all possible combinations. In practice this is infeasible.\n",
    "\n",
    "$\\bullet$ Thus decision trees are **hueristic algorithms**. Hueristic algorithms are designed to solve problems in a faster more efficient method by sacrificing optimality, accuracy or precision in favor of speed. Hueristic algorithms are often used to solve NP-complete problems.\n",
    "\n",
    "\n",
    "Helpful analogies: playing chess, class tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 : Random Forest vs Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Random Forest? \n",
    "\n",
    "- **Many trees** make a **forest**.\n",
    "- **Many random trees** make a **random forest**.\n",
    "\n",
    "\n",
    "Random Forest is a modified form of bagging that creates ensembles of independent decision trees. \n",
    "To *de-correlate the trees*, we: \n",
    "1. train each tree on a separate bootstrap **random sample** of the full training set (same as in bagging) \n",
    "2. for each tree, at each split, we **randomly select a set of 𝐽′ predictors from the full set of predictors.** (not done in bagging)\n",
    "3. From amongst the 𝐽′  predictors, we select the optimal predictor and the optimal corresponding threshold for the split. \n",
    "\n",
    "*Question:* Why would this second step help (only considering random sub-group of features)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will fit an ensemble method, the Random Forest technique, which is different from the decision tree. Refer to the lectures slides for a full treatment on how they are different. Let's use ```n_estimators = predictor_count/2``` and ```max_depth = best_depth```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit a Random Forest Model\n",
    "best_depth = 7\n",
    "#Training\n",
    "model = RandomForestClassifier(n_estimators=int(x_train.shape[1]/2), max_depth=best_depth)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "#Predict\n",
    "y_pred_train = model.predict(x_train)\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "#Performance Evaluation\n",
    "acc_random_forest_training = accuracy_score(y_train, y_pred_train)*100\n",
    "acc_random_forest_testing = accuracy_score(y_test, y_pred_test)*100\n",
    "\n",
    "print(\"Random Forest: Accuracy, Training Set : {:0.2f}%\".format(acc_random_forest_training))\n",
    "print(\"Random Forest: Accuracy, Testing Set :  {:0.2f}%\".format(acc_random_forest_testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <strong>🏋🏻‍♂️ TEAM ACTIVITY 1:</strong> Random Forests </div>  \n",
    "\n",
    "*Let's try to improve our accuracy scores on the cancer dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import tree_pd\n",
    "get_tree_scores = tree_pd.get_tree_scores\n",
    "cancer_scaled, target = tree_pd.load_cancer_dataset(10, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### Train Test split\n",
    "np.random.seed(40)\n",
    "\n",
    "#test_proportion\n",
    "test_prop = 0.2\n",
    "msk = np.random.uniform(0, 1, len(cancer_scaled)) > test_prop\n",
    "\n",
    "#Split predictor and response columns\n",
    "ex1_x_train, ex1_y_train =  cancer_scaled[msk], target[msk]\n",
    "ex1_x_test , ex1_y_test  = cancer_scaled[~msk], target[~msk]\n",
    "\n",
    "print(\"Shape of Training Set :\", ex1_x_train.shape)\n",
    "print(\"Shape of Testing Set :\" , ex1_x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your tasks:\n",
    "1) Use the `get_tree_scores` function to assign a dataframe `rf_val_acc` using a class instance of `RandomForestClassifier`. As a reminder this function takes four arguments (x_train, y_train, model, tree_depth_range). This time don't feed a random state.\n",
    "\n",
    "2) Use pandas groupby function to to get the mean cross-validation accuracy for specific depths. Assign to a new dataframe `rf_mean_acc`.\n",
    "\n",
    "3) Visualize the mean cross validation accuracy scores by running the cell provided. Answer the subsequent questions.\n",
    "\n",
    "4) Plot the feature importance of the best random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load 'solutions/sol2.py'\n",
    "tree_depth_range = range(1, 40, 2)\n",
    "rf_val_acc = get_tree_scores(ex1_x_train, \n",
    "                             ex1_y_train, \n",
    "                             RandomForestClassifier(), \n",
    "                             tree_depth_range)\n",
    "#rf_mean_acc = pd.DataFrame(rf_val_acc)\n",
    "rf_mean_acc  = rf_val_acc.groupby(\"depth\").mean()\n",
    "rf_mean_acc[\"depth\"] = list(tree_depth_range)\n",
    "rf_mean_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run this code when you are finished with the first exercise to compare random forests and simple decision trees. More questions and one final task lie below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### add a decision tree classifier for comparison.\n",
    "tree_val_acc = get_tree_scores(ex1_x_train, \n",
    "                               ex1_y_train, \n",
    "                               DecisionTreeClassifier(), \n",
    "                               tree_depth_range)\n",
    "\n",
    "tree_mean_acc  = tree_val_acc.groupby(\"depth\").mean()\n",
    "tree_mean_acc[\"depth\"] = list(tree_depth_range)\n",
    "\n",
    "### Make the plot\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.title('Variation of Accuracy on Validation set with Depth - Simple Decision Tree')\n",
    "sns.lineplot(x = \"depth\", y = \"cv_acc_score\", data = rf_val_acc,  \n",
    "             label = \"random forest\");\n",
    "sns.lineplot(x = \"depth\", y = \"cv_acc_score\", data = tree_val_acc, \n",
    "             label = \"simple decision tree\");\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"validation set accuracy score\")\n",
    "\n",
    "max_idx = tree_mean_acc[\"cv_acc_score\"].idxmax()\n",
    "best_depth_tree = tree_mean_acc[\"depth\"][max_idx]\n",
    "\n",
    "best_tree_model = DecisionTreeClassifier(max_depth=best_depth)\n",
    "best_tree_model.fit(ex1_x_train, ex1_y_train)\n",
    "tree_test_accuracy = best_tree_model.score(ex1_x_test, ex1_y_test.reshape(-1,))\n",
    "\n",
    "max_idx = rf_mean_acc[\"cv_acc_score\"].idxmax()\n",
    "best_depth_rf = rf_mean_acc[\"depth\"][max_idx]\n",
    "\n",
    "\n",
    "best_rf_model = RandomForestClassifier(max_depth=best_depth_rf, random_state = 42)\n",
    "best_rf_model.fit(ex1_x_train, ex1_y_train.reshape(-1,))\n",
    "tree_rf_accuracy = best_rf_model.score(ex1_x_test, ex1_y_test.reshape(-1,))\n",
    "\n",
    "print(\"Decision Tree best depth {:}\".format(best_depth))\n",
    "print(\"Random Forest best depth {:}\".format(best_depth_rf))\n",
    "\n",
    "print(\"Best Decision Tree test set accuracy:  {:0.2f}%\".format(tree_test_accuracy*100))\n",
    "print(\"Best Random Forest test set accuracy:  {:0.2f}%\".format(tree_rf_accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\bullet$ Why doesn't the random forest accuracy score deteriorate in the same way that the decision tree does for deeper trees?\n",
    "\n",
    "\n",
    "$\\bullet$ What are the two kinds of stochasticity that lead to the robustness of random forests?\n",
    "\n",
    "$\\bullet$ How do random forests differ from Bagging?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "#### Lets plot the feature importance of the best random forest model:\n",
    "Random Forest gives the above values as ```feature_importance``` where it normalizes the impact of a predictor to the number of times it is useful and thus gives overvall significance for free. Explore the attributes of the Random Forest model object for the best nodes.\n",
    "\n",
    "Feature importance is calculated as the decrease in node impurity **weighted by the probability of reaching that node. The node probability can be calculated by the number of samples that reach the node**, divided by the total number of samples. The higher the value the more important the feature.\n",
    "\n",
    "source: https://towardsdatascience.com/the-mathematics-of-decision-trees-random-forest-and-feature-importance-in-scikit-learn-and-spark-f2861df67e3#:~:text=Feature%20importance%20is%20calculated%20as,the%20more%20important%20the%20feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <strong>🏋🏻‍♂️ TEAM ACTIVITY 2:</strong> Feature importance </div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) extract the `.feature_importances_` attribute from your `best_rf_model`. Assign this to a variable called `feature_importance`.\n",
    "\n",
    "2) Rescale the feature importances such that the most important feature is has an importance of 100.\n",
    "\n",
    "3) use `np.argsort` to return the indices of the sorted features.\n",
    "\n",
    "4) finally pass the sorted index to `plt.barh` and plot the feature importances!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "#help(best_rf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = ...\n",
    "feature_importance = ...\n",
    "sorted_idx = ...\n",
    "\n",
    "\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "plt.figure(figsize=(10,12))\n",
    "plt.barh(pos, ..., align='center')\n",
    "plt.yticks(pos, ex1_x_train.columns[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load 'solutions/importance.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's compare the performance of our 3 models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Decision Trees:\\tAccuracy, Training Set \\t: {:.2%}\".format(acc_trees_training))\n",
    "print(\"Decision Trees:\\tAccuracy, Testing Set \\t: {:.2%}\".format(acc_trees_testing))\n",
    "\n",
    "print(\"\\nBagging: \\tAccuracy, Training Set \\t: {:0.2f}%\".format(acc_bagging_training))\n",
    "print(\"Bagging: \\tAccuracy, Testing Set \\t: {:0.2f}%\".format( acc_bagging_testing))\n",
    "\n",
    "print(\"\\nRandom Forest: \\tAccuracy, Training Set \\t: {:0.2f}%\".format(acc_random_forest_training))\n",
    "print(\"Random Forest: \\tAccuracy, Testing Set \\t: {:0.2f}%\".format(acc_random_forest_testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we see above, the performance of both Bagging and Random Forest was similar, so what is the difference? Do both overfit the data just as much?\n",
    "\n",
    "Hints :\n",
    "\n",
    "- What is the only extra parameter we declared when defining a Random Forest Model vs Bagging? Does it have an impact on overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit a Random Forest Model\n",
    "\n",
    "new_depth = best_depth + 20 \n",
    "\n",
    "#Training\n",
    "model = RandomForestClassifier(n_estimators=int(x_train.shape[1]/2), max_depth=new_depth)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "#Predict\n",
    "y_pred_train = model.predict(x_train)\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "#Perfromance Evaluation\n",
    "acc_random_forest_deeper_training = accuracy_score(y_train, y_pred_train)*100\n",
    "acc_random_forest_deeper_testing = accuracy_score(y_test, y_pred_test)*100\n",
    "\n",
    "print(\"Random Forest: Accuracy, Training Set (Deeper): {:0.2f}%\".format(acc_random_forest_deeper_training))\n",
    "print(\"Random Forest: Accuracy, Testing Set (Deeper):  {:0.2f}%\".format(acc_random_forest_deeper_testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training accuracies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Accuracies:\")\n",
    "print(\"Decision Trees:\\tAccuracy, Training Set \\t: {:.2%}\".format(acc_trees_training))\n",
    "print(\"Bagging: \\tAccuracy, Training Set \\t: {:0.2f}%\".format(acc_bagging_training))\n",
    "print(\"Random Forest: \\tAccuracy, Training Set \\t: {:0.2f}%\".format(acc_random_forest_training))\n",
    "print(\"RF Deeper: \\tAccuracy, Training Set \\t: {:0.2f}%\".format(acc_random_forest_deeper_training))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing accuracies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing Accuracies:\")\n",
    "print(\"Decision Trees:\\tAccuracy, Testing Set \\t: {:.2%}\".format(acc_trees_testing))\n",
    "print(\"Bagging: \\tAccuracy, Testing Set \\t: {:0.2f}%\".format( acc_bagging_testing))\n",
    "print(\"Random Forest: \\tAccuracy, Testing Set \\t: {:0.2f}%\".format(acc_random_forest_testing))\n",
    "print(\"RF Deeper:  \\tAccuracy, Testing Set \\t: {:0.2f}%\".format(acc_random_forest_deeper_testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vars(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(model.estimators_[0].tree_.feature))\n",
    "print(len(model.estimators_[0].tree_.threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <strong>🏋🏻‍♂️ TEAM ACTIVITY 2:</strong> Exploring RandomForestClassifier class instances. </div>  \n",
    "\n",
    "For more resources on python classes (we're relying on them all the time via sklearn!) see <a href = \"https://docs.python.org/3/tutorial/classes.html#a-first-look-at-classes\">this link.</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import tree_pd\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.width', 1500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "get_tree_pd = tree_pd.get_tree_scores\n",
    "cancer_scaled, target = tree_pd.load_cancer_dataset(10, 4)\n",
    "\n",
    "################################### Train Test split\n",
    "np.random.seed(40)\n",
    "\n",
    "#test_proportion\n",
    "test_prop = 0.2\n",
    "msk = np.random.uniform(0, 1, len(cancer_scaled)) > test_prop\n",
    "\n",
    "#Split predictor and response columns\n",
    "X_train, y_train =  cancer_scaled[msk], target[msk]\n",
    "X_test , y_test  = cancer_scaled[~msk], target[~msk]\n",
    "\n",
    "print(\"Shape of Training Set :\", X_train.shape)\n",
    "print(\"Shape of Testing Set :\" , X_test.shape)\n",
    "\n",
    "################################### Train a bagging and random forest model\n",
    "\n",
    "depth = 13\n",
    "n_estimators = 100\n",
    "best_rf_model = RandomForestClassifier(max_depth=depth, random_state = 42, n_estimators= n_estimators)\n",
    "best_rf_model.fit(X_train, y_train.reshape(-1,))\n",
    "tree_rf_accuracy = best_rf_model.score(X_test, y_test.reshape(-1,))\n",
    "\n",
    "\n",
    "bagging_model = BaggingRegressor(DecisionTreeClassifier(max_depth=depth), \n",
    "                                 n_estimators = 100,\n",
    "                                 random_state = 42).fit(X_train, y_train.reshape(-1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directions \n",
    "Run the cell below and look at the output. The .estimators_ attribute of a RandomForestClassifier class instance is a list of the individual DecisionTreeClassifier class instance estimators that make up the ensemble model. Calling .tree_ on the DecisionTreeClassifier will give you the individual tree estimator. \n",
    "1. Complete the function by extracting the impurity and feature attributes for each decision tree estimator at a specific decision node.\n",
    "2. Fix the creation of the dictionary at the bottom of the function and return a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(best_rf_model.estimators_[0].tree_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(best_rf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(best_rf_model.estimators_[0].tree_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"exercises/exercise1.py\"\n",
    "def get_impurity_pd(model, n = 0):\n",
    "    \"\"\"\n",
    "    This function returns a pandas dataframe with all of the nth nodes feature impurities.\n",
    "    \"\"\"\n",
    "    rf_estimators = model.estimators_.copy()\n",
    "    features = np.array(X_train.columns)\n",
    "    \n",
    "    node_impurities, node_features = [], []\n",
    "\n",
    "    for i, estimator in enumerate(rf_estimators):\n",
    "        estimator_impurity = #TODO 0 \n",
    "        estimator_feature  = #TODO 1\n",
    "        \n",
    "        node_impurities.append(estimator_impurity)\n",
    "        node_features.append(estimator_feature)\n",
    "    node_impurity_dict = {\"feature\": #TODO \n",
    "                        \"impurity\": #TODO\n",
    "    df = #TODO\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"solutions/impurity.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_node = 0\n",
    "rf_df = get_impurity_pd(best_rf_model, tree_node)\n",
    "bagging_df = get_impurity_pd(bagging_model, tree_node)\n",
    "\n",
    "#plot\n",
    "fig, ax = plt.subplots(1,2, figsize = (20, 5))\n",
    "ax.ravel()\n",
    "\n",
    "sns.swarmplot(x = \"feature\", y = \"impurity\", data = rf_df, ax = ax[0])\n",
    "#sns.swarmplot(x = \"feature\", y = \"impurities\", data = rf_df, ax = ax[0])\n",
    "ax[0].tick_params(labelrotation=45)\n",
    "ax[0].set_title(\"Random Forest: Node 0 impurities after split\")\n",
    "\n",
    "sns.swarmplot(x = \"feature\", y = \"impurity\", data = bagging_df, ax = ax[1])\n",
    "ax[1].set_title(\"Bagging: Node 0 impurities after split\")\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________\n",
    "\n",
    "## The limitations of random forest\n",
    "\n",
    "#### When can Random Forest overfit? \n",
    "- Increasing the number of trees in RF generally doesn't increase the risk of overfitting, BUT if the number of trees in the ensemble is too large then the trees in the ensemble may become correlated, and therefore increase the variance.\n",
    "\n",
    "#### When can Random Forest fail? \n",
    "\n",
    "- **When we have a lot of predictors that are completely independent of the response and one overwhelmingly influential predictor**.\n",
    "\n",
    "#### Why aren't random forests and bagging interpretable?  How about a very deep decision tree?\n",
    "\n",
    "____________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging and random forest vs. Boosting\n",
    "\n",
    "- **Bagging and Random Forest:**\n",
    "  - complex and deep trees **overfit**\n",
    "  - thus **let's perform variance reduction on complex trees!**\n",
    "- **Boosting:** \n",
    "  - simple and shallow trees **underfit** \n",
    "  - thus **let's perform bias reduction of simple trees!**\n",
    "  - make the simple trees more expressive!\n",
    "  \n",
    "**Boosting** attempts to improve the predictive flexibility of simple models.\n",
    " - It trains a **large number of “weak” learners in sequence**.\n",
    " - A weak learner is a constrained model (limit the max depth of each decision tree).\n",
    " - Each one in the sequence focuses on **learning from the mistakes** of the one before it.\n",
    " - By more heavily weighting in the mistakes in the next tree, our next tree will learn from the mistakes.\n",
    " - A combining all the weak learners into a single strong learner = **a boosted tree**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/gradient_boosting1.png?\" alt=\"tree_adj\" width=\"70%\"/>\n",
    "\n",
    "----------\n",
    "\n",
    "### Illustrative example (from [source](https://towardsdatascience.com/underfitting-and-overfitting-in-machine-learning-and-how-to-deal-with-it-6fe4a8a49dbf))\n",
    "\n",
    "<img src=\"data/boosting.png\" alt=\"tree_adj\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We built multiple trees consecutively: Tree 1 -> Tree 2 -> Tree 3 - > ....\n",
    "\n",
    "**The size of the plus or minus signs indicates the weights of a data points for every Tree**. How do we determine these weights?\n",
    "\n",
    "For each consecutive tree and iteration we do the following:\n",
    " - The **wrongly classified data points (\"mistakes\" = red circles)** are identified and **more heavily weighted in the next tree (green arrow)**. \n",
    " - Thus the size of the plus or minus changes in the next tree\n",
    " - This change in weights will influence and change the next simple decision tree\n",
    " - The **correct predictions are** identified and **less heavily weighted in the next tree**.\n",
    "\n",
    "We iterate this process for a certain number of times, stop and construct our final model: \n",
    "- The ensemble (**\"Final: Combination\"**) is a linear combination of the simple trees, and is more expressive!\n",
    "- The ensemble (**\"Final: Combination\"**) has indeed not just one simple decision boundary line, and fits the data better.\n",
    " \n",
    " \n",
    "<img src=\"data/boosting_2.png?\" alt=\"tree_adj\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Ada Boost?\n",
    "\n",
    "- Ada Boost = Adaptive Boosting.\n",
    "- AdaBoost is adaptive in the sense that subsequent weak learners are tweaked in favor of those instances misclassified by previous classifiers\n",
    "\n",
    "<img src=\"data/AdaBoost1.png\" alt=\"tree_adj\" width=\"70%\"/>\n",
    "<img src=\"data/AdaBoost2.png\" alt=\"tree_adj\" width=\"70%\"/>\n",
    "\n",
    "For an individual training point the loss can be defined as:\n",
    "$$\\text{ExpLoss_i} = \\begin{cases}\n",
    "      e^{\\hat{y}}, & \\text{if}\\ y=-1 \\\\\n",
    "      e^{-\\hat{y}}, & \\text{} y=1\n",
    "    \\end{cases}\n",
    "$$\n",
    "<img src=\"data/AdaBoost3.png\" alt=\"tree_adj\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustrative Example\n",
    "------\n",
    "**Step1. Start with equal distribition initially**\n",
    "<img src=\"data/ADA2.png\" alt=\"tree_adj\" width=\"40%\">\n",
    "\n",
    "------\n",
    "**Step2. Fit a simple classifier**\n",
    "<img src=\"data/ADA3.png\" alt=\"tree_adj\" width=\"40%\"/>\n",
    "\n",
    "------\n",
    "**Step3. Update the weights**\n",
    "<img src=\"data/ADA4.png\" alt=\"tree_adj\" width=\"40%\"/>\n",
    "\n",
    "**Step4. Update the classifier:** First time trivial (we have no model yet.)\n",
    "\n",
    "------\n",
    "**Step2. Fit a simple classifier**\n",
    "<img src=\"data/ADA5.png\" alt=\"tree_adj\" width=\"40%\"/>\n",
    "\n",
    "**Step3. Update the weights:** not shown.\n",
    "\n",
    "------\n",
    "**Step4. Update the classifier:**\n",
    "<img src=\"data/ADA6.png\" alt=\"tree_adj\" width=\"40%\">\n",
    "\n",
    "\n",
    "-------------\n",
    "\n",
    "### Let's talk about  random forest and boosting in the context of bias and variance.\n",
    "<img src=\"data/bias_variance.png\" alt=\"split2\" width=\"40%\"/>\n",
    "<img src=\"data/fitting.png\" alt=\"split2\" width=\"40%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Use the Adaboost method to visualize Bias-Variance tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try Boosting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit an Adaboost Model\n",
    "\n",
    "x_train, y_train = data_train.drop(['Spam'], axis=1), data_train['Spam']\n",
    "x_test , y_test  = data_test.drop(['Spam'] , axis=1), data_test['Spam']\n",
    "\n",
    "#Training\n",
    "model = AdaBoostClassifier(base_estimator= DecisionTreeClassifier(max_depth=3), \n",
    "                           n_estimators=200, \n",
    "                           learning_rate=0.05)\n",
    "model.fit(x_train.values, y_train.values)\n",
    "\n",
    "#Predict\n",
    "y_pred_train = model.predict(x_train.values)\n",
    "y_pred_test = model.predict(x_test.values)\n",
    "\n",
    "#Performance Evaluation\n",
    "acc_boosting_training = accuracy_score(y_train, y_pred_train)*100\n",
    "acc_boosting_test = accuracy_score(y_test, y_pred_test)*100\n",
    "\n",
    "print(\"Ada Boost:\\tAccuracy, Training Set \\t: {:0.2f}%\".format(acc_boosting_training))\n",
    "print(\"Ada Boost:\\tAccuracy, Testing Set \\t: {:0.2f}%\".format(acc_boosting_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does the test and training accuracy evolve with every iteration (tree)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Iteration based score\n",
    "train_scores = list(model.staged_score(x_train.values,y_train))\n",
    "test_scores = list(model.staged_score(x_test.values, y_test))\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(train_scores,label='train')\n",
    "plt.plot(test_scores,label='test')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Variation of Accuracy with Iterations - ADA Boost\")\n",
    "plt.legend();\n",
    "print('best number of iterations', np.array(test_scores).argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Decision Trees:\\tAccuracy, Testing Set \\t: {:.3%}\".format(acc_trees_testing))\n",
    "print(\"Bagging: \\tAccuracy, Testing Set \\t: {:0.3f}%\".format( acc_bagging_testing))\n",
    "print(\"Random Forest: \\tAccuracy, Testing Set \\t: {:0.3f}%\".format(acc_random_forest_testing))\n",
    "print(\"Ada Boost:\\tAccuracy, Testing Set \\t: {:0.3f}%\".format(acc_boosting_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost seems to be performing better than Simple Decision Trees and has a similar Test Set Accuracy performance compared to Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random tip:** If a \"for\"-loop takes some time and you want to know the progress while running the loop, use: **tqdm()** ([link](https://github.com/tqdm/tqdm)). No need for 1000's of ```print(i)``` outputs.\n",
    "\n",
    "\n",
    "Usage: ```for i in tqdm( range(start,finish) ):```\n",
    "\n",
    " - tqdm means *\"progress\"* in Arabic (taqadum, تقدّم) and \n",
    " - tqdm is an abbreviation for *\"I love you so much\"* in Spanish (te quiero demasiado)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if we change the depth of our AdaBoost trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Timer\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "#Find Optimal Depth of trees for Boosting\n",
    "score_train, score_test = {}, {}\n",
    "depth_start, depth_end  =  2, 30\n",
    "for i in tqdm(range(depth_start, depth_end, 2)):\n",
    "    model = AdaBoostClassifier(\n",
    "        base_estimator=DecisionTreeClassifier(max_depth=i),\n",
    "        n_estimators=200, learning_rate=0.05)\n",
    "    model.fit(x_train, y_train)\n",
    "    score_train[i] = accuracy_score(y_train, model.predict(x_train))\n",
    "    score_test[i] = accuracy_score(y_test, model.predict(x_test))\n",
    "    \n",
    "# Stop Timer\n",
    "end = time.time()\n",
    "elapsed_adaboost = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot\n",
    "lists1 = sorted(score_train.items())\n",
    "lists2 = sorted(score_test.items())\n",
    "x1, y1 = zip(*lists1) \n",
    "x2, y2 = zip(*lists2) \n",
    "plt.figure(figsize=(10,7))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Depth\")\n",
    "plt.title('Variation of Accuracy with Depth - ADA Boost Classifier')\n",
    "plt.plot(x1, y1, 'b-', label='Train')\n",
    "plt.plot(x2, y2, 'g-', label='Test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaboost complexity depends on both the number of estimators and the base estimator. \n",
    "- In the beginning as our model complexity increases (depth 2-3), we first observe a small increase in accuracy.\n",
    "- But as we go further to the right of the graph (**deeper trees**), our model **will overfit the data.**\n",
    "- **REMINDER and validation: Boosting relies on simple trees!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <strong>🏋🏻‍♂️ TEAM ACTIVITY 3:</strong> Exploring learning rate and TQDM </div>  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore how changing the learning rate changes the training and testing accuracy. Use the Te Quero Demasiado (TQDM) wrap around your range as above. (Hint you will probably want to explore a range from $e^{-6}$ to $e^{-1}$\n",
    "\n",
    "1) copy the tqdm loop code and vary the learning rate as suggested.\n",
    "\n",
    "2) plot the staged score to visualize how the adaboost model learns differently depending on the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"solutions/bo2.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is this exercise useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Food for Thought :**\n",
    "- Are **boosted models independent of one another?** Do they need to wait for the previous model's residuals?\n",
    "- Are **bagging or random forest models independent of each other**, can they be trained in a parallel fashion?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Standard Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. *Theory:* What is Gradient Boosting and XGBoost?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Gradient Boosting?\n",
    "\n",
    "To improve its predictions, **gradient boosting looks at the difference between its current approximation, and the known correct target vector, which is called the residual**.\n",
    "\n",
    "The mathematics:\n",
    "\n",
    "- It may be assumed that there is some imperfect model $F_{m}$ \n",
    "- The gradient boosting algorithm improves on $F_{m}$ constructing a new model that adds an estimator $h$ to provide a better model: \n",
    "$$F_{m+1}(x)=F_{m}(x)+h(x)$$\n",
    "\n",
    "- To find $h$, the gradient boosting solution starts with the observation that a perfect **h** would imply\n",
    "\n",
    "$$F_{m+1}(x)=F_{m}(x)+h(x)=y$$\n",
    "\n",
    "- or, equivalently solving for h,\n",
    "\n",
    "$$h(x)=y-F_{m}(x)$$\n",
    "\n",
    "- Therefore, gradient boosting will fit h to the residual $y-F_{m}(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/gradient_boosting2.png\" alt=\"tree_adj\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-------\n",
    "\n",
    "### XGBoost: [\"Long May She Reign!\"](https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d)\n",
    "\n",
    "<img src=\"data/kaggle.png\" alt=\"tree_adj\" width=\"100%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----------\n",
    "\n",
    "### What is XGBoost and why is it so good!?\n",
    " - Based on Gradient Boosting\n",
    " - XGBoost = **eXtreme Gradient Boosting**; refers to the engineering goal to push the limit of computations resources for boosted tree algorithm\n",
    " \n",
    "**Accuracy:**\n",
    " - XGBoost however uses a **more regularized model formalizaiton to control overfitting** (=better performance) by both L1 and L2 regularization.\n",
    " - Tree Pruning methods: more shallow tree will also prevent overfitting\n",
    " - Improved convergence techniques (like early stopping when no improvement is made for X number of iterations)\n",
    " - Built-in Cross-Validaiton\n",
    " \n",
    "**Computing Speed:**\n",
    " - Special Vector and matrix type data structures for faster results.\n",
    " - Parallelized tree building: using all of your CPU cores during training.\n",
    " - Distributed Computing: for training very large models using a cluster of machines.\n",
    " - Cache Optimization of data structures and algorithm: to make best use of hardware.\n",
    "\n",
    "**XGBoost is building boosted trees in parallel? What? How?**\n",
    "- No: Xgboost doesn't run multiple trees in parallel, you need predictions after each tree to update gradients.\n",
    "- Rather it does the parallelization WITHIN a single tree my using openMP to create branches independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Use XGBoost: Extreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's install XGBoost\n",
    "#! pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Create the training and test data\n",
    "dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "dtest = xgb.DMatrix(x_test, label=y_test)\n",
    "\n",
    "# Parameters\n",
    "param = {\n",
    "    'max_depth': best_depth,  # the maximum depth of each tree\n",
    "    'eta': 0.3,               # the training step for each iteration\n",
    "    'silent': 1,              # logging mode - quiet\n",
    "    'objective': 'multi:softprob',  # error evaluation for multiclass training\n",
    "    'num_class': 2}           # the number of classes that exist in this datset\n",
    "\n",
    "# Number of training iterations\n",
    "num_round = 200  \n",
    "\n",
    "# Start timer\n",
    "start = time.time()\n",
    "\n",
    "# Train XGBoost\n",
    "bst = xgb.train(param, \n",
    "                dtrain, \n",
    "                num_round, \n",
    "                evals= [(dtrain, 'train')], \n",
    "                early_stopping_rounds=20, # early stopping\n",
    "                verbose_eval=20)\n",
    "\n",
    "\n",
    "# Make prediction training set\n",
    "preds_train = bst.predict(dtrain)\n",
    "best_preds_train = np.asarray([np.argmax(line) for line in preds_train])\n",
    "\n",
    "# Make prediction test set\n",
    "preds_test = bst.predict(dtest)\n",
    "best_preds_test = np.asarray([np.argmax(line) for line in preds_test])\n",
    "\n",
    "# Performance Evaluation \n",
    "acc_XGBoost_training = accuracy_score(y_train, best_preds_train)*100\n",
    "acc_XGBoost_test = accuracy_score(y_test, best_preds_test)*100\n",
    "\n",
    "# Stop Timer\n",
    "end = time.time()\n",
    "elapsed_xgboost = end - start\n",
    "\n",
    "print(\"XGBoost:\\tAccuracy, Training Set \\t: {:0.2f}%\".format(acc_XGBoost_training))\n",
    "print(\"XGBoost:\\tAccuracy, Testing Set \\t: {:0.2f}%\".format(acc_XGBoost_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about the accuracy performance: AdaBoost versus XGBoost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ada Boost:\\tAccuracy, Testing Set \\t: {:0.2f}%\".format(acc_boosting_test))\n",
    "print(\"XGBoost:\\tAccuracy, Testing Set \\t: {:0.2f}%\".format(acc_XGBoost_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about the computing performance: AdaBoost versus XGBoost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AdaBoost elapsed time: \\t{:0.2f}s\".format(elapsed_adaboost))\n",
    "print(\"XGBoost elapsed time: \\t{:0.2f}s\".format(elapsed_xgboost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if we change the depth of our XGBoost trees and compare to Ada Boost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_xgboost(best_depth):\n",
    "    param = {\n",
    "    'max_depth': best_depth,  # the maximum depth of each tree\n",
    "    'eta': 0.3,  # the training step for each iteration\n",
    "    'verbosity': 0,  # logging mode - quiet\n",
    "    'objective': 'multi:softprob',  # error evaluation for multiclass training\n",
    "    'num_class': 2}  # the number of classes that exist in this datset\n",
    "\n",
    "    # the number of training iterations\n",
    "    num_round = 200  \n",
    "\n",
    "    bst = xgb.train(param, \n",
    "                    dtrain, \n",
    "                    num_round, \n",
    "                    evals= [(dtrain, 'train')], \n",
    "                    early_stopping_rounds=20,\n",
    "                    verbose_eval=False)\n",
    "\n",
    "    preds_train = bst.predict(dtrain)\n",
    "    best_preds_train = np.asarray([np.argmax(line) for line in preds_train])\n",
    "    preds_test = bst.predict(dtest)\n",
    "    best_preds_test = np.asarray([np.argmax(line) for line in preds_test])\n",
    "\n",
    "    #Performance Evaluation\n",
    "    XGBoost_training = accuracy_score(y_train, best_preds_train)\n",
    "    XGBoost_test = accuracy_score(y_test, best_preds_test)\n",
    "    \n",
    "    return XGBoost_training, XGBoost_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find Optimal Depth of trees for Boosting\n",
    "score_train_xgb, score_test_xgb = {}, {}\n",
    "depth_start, depth_end = 2, 30\n",
    "for i in trange(depth_start, depth_end, 2):\n",
    "    XGBoost_training, XGBoost_test = model_xgboost(i)\n",
    "    score_train_xgb[i] = XGBoost_training\n",
    "    score_test_xgb[i] = XGBoost_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot\n",
    "lists1 = sorted(score_train_xgb.items())\n",
    "lists2 = sorted(score_test_xgb.items())\n",
    "x3, y3 = zip(*lists1) \n",
    "x4, y4 = zip(*lists2) \n",
    "plt.figure(figsize=(10,7))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Depth\")\n",
    "plt.title('Variation of Accuracy with Depth - Adaboost & XGBoost Classifier')\n",
    "plt.plot(x1, y1, label='Train Accuracy Ada Boost')\n",
    "plt.plot(x2, y2, label='Test Accuracy Ada Boost')\n",
    "plt.plot(x3, y3, label='Train Accuracy XGBoost')\n",
    "plt.plot(x4, y4, label='Test Accuracy XGBoost')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interesting**: \n",
    "- No real optimal depth of the simple tree for XGBoost, probably a lot of regularization, pruning, or early stopping when using a deep tree at the start.\n",
    "- XGBoost does not seem to overfit when the depth of the tree increases, as opposed to Ada Boost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All the accuracy performances:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Decision Trees:\\tAccuracy, Testing Set \\t: {:.2%}\".format(acc_trees_testing))\n",
    "print(\"Bagging: \\tAccuracy, Testing Set \\t: {:0.2f}%\".format( acc_bagging_testing))\n",
    "print(\"Random Forest: \\tAccuracy, Testing Set \\t: {:0.2f}%\".format(acc_random_forest_testing))\n",
    "print(\"Ada Boost:\\tAccuracy, Testing Set \\t: {:0.2f}%\".format(acc_boosting_test))\n",
    "print(\"XGBoost:\\tAccuracy, Testing Set \\t: {:0.2f}%\".format(acc_XGBoost_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----------\n",
    "\n",
    "**Overview of all the tree algorithms:** [Source](https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d)\n",
    "\n",
    "<img src=\"data/trees.png\" alt=\"tree_adj\" width=\"100%\"/>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Example to better understand Bias vs Variance tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A central notion underlying what we've been learning in lectures and sections so far is the trade-off between overfitting and underfitting. If you remember back to Homework 3, we had a model that seemed to represent our data accurately. However, we saw that as we made it more and more accurate on the training set, it did not generalize well to unobserved data.\n",
    "\n",
    "As a different example, in face recognition algorithms, such as that on the iPhone X, a too-accurate model would be unable to identity someone who styled their hair differently that day. The reason is that our model may learn irrelevant features in the training data. On the contrary, an insufficiently trained model would not generalize well either. For example, it was recently reported that a face mask could sufficiently fool the iPhone X.\n",
    "\n",
    "A widely used solution in statistics to reduce overfitting consists of adding structure to the model, with something like regularization. This method favors simpler models during training.\n",
    "\n",
    "The bias-variance dilemma is closely related. \n",
    "- The **bias** of a model quantifies how precise a model is across training sets. \n",
    "- The **variance** quantifies how sensitive the model is to small changes in the training set. \n",
    "- A **robust** model is not overly sensitive to small changes. \n",
    "- **The dilemma involves minimizing both bias and variance**; we want a precise and robust model. Simpler models tend to be less accurate but more robust. Complex models tend to be more accurate but less robust.\n",
    "\n",
    "**How to reduce bias:**\n",
    " - **Use more complex models, more features, less regularization,** ...\n",
    " - **Boosting:** attempts to improve the predictive flexibility of simple models. Boosting uses simple base models and tries to “boost” their aggregate complexity.\n",
    " \n",
    "**How to reduce variance:**\n",
    " - **Early Stopping:** Its rules provide us with guidance as to how many iterations can be run before the learner begins to over-fit.\n",
    " - **Pruning:** Pruning is extensively used while building related models. It simply removes the nodes which add little predictive power for the problem in hand.\n",
    " - **Regularization:** It introduces a cost term for bringing in more features with the objective function. Hence it tries to push the coefficients for many variables to zero and hence reduce cost term.\n",
    " - **Train with more data:** It won’t work every time, but training with more data can help algorithms detect the signal better.\n",
    " - **Ensembling:** Ensembles are machine learning methods for combining predictions from multiple separate models. For example:\n",
    "   - **Bagging** attempts to reduce the chance of overfitting complex models: Bagging uses complex base models and tries to “smooth out” their predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "\n",
    "#### Interesting Piazza post: why randomness in simple decision tree?\n",
    " \n",
    " ```\"Hi there. I notice that there is a parameter called \"random_state\" in decision tree function and I wonder why we need randomness in simple decision tree. If we add randomness in such case, isn't it the same as random forest?\"```\n",
    " \n",
    "  - The problem of learning an optimal decision tree is known to be **NP-complete** under several aspects of optimality and even for simple concepts. \n",
    "  - Consequently, practical decision-tree learning algorithms are based on **heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node**. \n",
    "  - Such algorithms **cannot guarantee to return the globally optimal decision tree**. \n",
    "  - This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement (Bagging).\n",
    "  \n",
    "For example: **What is the defaulth DecisionTreeClassifier behaviour when there are 2 or more best features for a certain split (a tie among \"splitters\")?** (after a deep dive and internet search [link](https://github.com/scikit-learn/scikit-learn/issues/12259 ) ):\n",
    "\n",
    "  - The current default behaviour when splitter=\"best\" is to shuffle the features at each step and take the best feature to split. \n",
    "  - In case there is a tie, we take a random one."
   ]
  }
 ],
 "metadata": {
  "jupytext": {},
  "kernelspec": {
   "display_name": "arm",
   "language": "python",
   "name": "arm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
