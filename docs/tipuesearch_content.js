var tipuesearch = {"pages":[{"title":"FAQ","text":"General I'm unable to attend lectures in person. Can I take the class asynchronously? Lecture attendance is required. Non-DCE students should only register if they can attend in person. Does the individual HW mean I have to submit on my own but can I still work with my HW partner? An individual CS109A HW means you are supposed to work on your own, without any human intervention, assistance, or input. You are not allowed to work with partner. You are allowed to use OHs to ask for clarification questions, but we may not be able to answer all of your questions or help with your coding. You are allowed to use all of your materials from the course up to the HW, look at problem resolution online, and look at libraries documentation. Where should I send my questions? Use Ed for anying related to the course content or assignments. All other concerns should be sent to the course helpline: cs109a2021@gmail.com Auditing Can I audit this course? What are the rules for auditing? Yes, you are welcome to audit this course. Send an email to cs109a2021@gmail.com to request full Canvas access. All auditors must agree to abide by the following rules: All auditors are held to the same standard of academic honesty as our registered students. Please do not share homeworks or solutions with anyone. Violations will be reported to the Harvard Administrative Board. Auditors are not permitted to take the course for credit in the future. Auditors may not attend lectures or section during the live stream. Students are randomly assigned to small groups for coding exercises and mixing auditors and students in this way is not ideal. We are investigating methods that would allow auditors to join, but this is the current policy. Audiors should not submit HWs or participate in projects. Auditors should refrain from using any course and TF resources that are designed for our registered students like Ed, Jupyter Hub, and office hours. Auditors must have an active HUID number. Quizzes & Exercises When are quizzes and exercises due? Quizzes and exercises are due before the next lecture. I missed a quiz/exercise. Can I make it up? No. We will drop your lowest 25% of quizzes and your lowest 25% of exercises. This policy is to reduce stress and is in place so that missing a quiz or exercise on occasion should not affect your final grade.","tags":"pages","url":"pages/faq.html"},{"title":"Preparation","text":"In order to get the most out of CS109A, knowledge of multivariate calculus, probability theory, statistics, and some basic linear algebra (e.g., matrix operations, eigenvectors, etc.) is suggested. Below are some resources for self-assessment and review: Multivariate Calculus : multiple exams /w solutions Linear Algebra: multiple exams /w solutions 1 , 2 Probability: exams with solutions and problem sets with solutions Statistics: multiple pairs of exam questions and answers Q1 , A1 , Q2 , A2 , Q3 , A3 Here is a useful textbook for reviewing many of the above topics: Mathematics for Machine Learning Note: you can be successful in the course (assignments, quizzes, etc.) with the listed pre-requisites, but some of the material presented in lectures may be more easily understood with more background. This course dives right into core Machine Learning topics. For this, students are expected to be fluent in Python programming. You can familiarize yourself with Python by completing online tutorials. Additionally, there are free Basics Python courses that cover all necessary python topics. You are also expected to be able to manipulate data using pandas DataFrames, perform basic operations with numpy arrays, and make use of basic plotting functions (e.g. line chart, histogram, scatter plot, bar chart) from matplotlib. Python basics Throughout this course, we will be using Python as our primary programming language for exercises, labs, and homework. Thus, you must have basic Python programming knowledge. The following are the topics you need to cross off your checklist before the course begins: Variables, Datatypes, strings, file operations, Data structures such as lists, dictionaries, tuples and classes. Pandas Basics Most of the exercises you will encounter in this course exploit various datasets. Pandas is an open-source data analysis and manipulation tool, built on top of Python. In this course, we have provided the necessary support material and resources to work with Pandas. However, it is highly recommended that you get yourselves familiar with basic data manipulation using Pandas to ensure a smooth learning experience. Numpy Basics NumPy is a library for the Python programming language that provides support for large, multi-dimensional arrays and matrices and a large collection of high-level mathematical functions to operate on these data structures. Because of the extensive exercises provided in this course, it is important to use Numpy for efficient problem-solving to get identical results. Though this course aims to support individuals with no prior Numpy knowledge, you must go through the basics of this library to avoid any possible hiccups. Matplotlib Basics A large portion of this course uses different graphs and charts to explain topics and validate results. Matplotlib is a plotting library for Python. This library has been used to create all the graphs you will see throughout the course. Additionally, the exercises and homeworks are structured in a manner that integrates this library. Henceforth, it is highly recommended to get yourselves acquainted with Matplotlib Basics. For this course, we will be using Jupyter Notebooks. You can familiarize yourself with Jupyter notebooks by reading the following tutorials: A Beginner's Tutorial to Jupyter Notebooks Finally, we assume that students have a strong foundation in calculus, linear algebra, statistics, and probability. You should review these concepts before the course begins. Here is one useful resource: Mathematics for Machine Learning","tags":"pages","url":"pages/preparation.html"},{"title":"Schedule","text":"Date (Mon) Lecture (Mon) Lecture (Wed) Lab (Fri) Advanced Section (Wed) Assignment (R:Released Wed - D:Due Wed) 30-Aug No Lecture Lecture 1: Introduction to CS109A Lab 1: Data - formats| sources| & scraping 6-Sep No Lecture (Labor Day) Lecture 2: Introduction to PANDAS and EDA Lab 2: Pandas & EDA 2 R:HW1 - D:HW0 13-Sep Lecture 3: Introduction to Regression kNN and Linear Regression Lecture 4: Multi-linear and Polynomial Regression Lab 3: R:HW2 - D:HW1 20-Sep Lecture 5: Model Selection and Cross Validation Lecture 6: Regularization Ridge and Lasso Regression Lab 4: 27-Sep Lecture 7: Probability Lecture 8: Inference in Regression and Hypothesis Testing Lab 5: Estimation of Regulariztion Coeffs /w CV Advanced Section 1: Linear Algebra Primer R:HW3 - D:HW2 4-Oct Lecture 9: Missing Data & Imputation Lecture 10: PCA Lab 6: Advanced Section 2: Hypothesis Testing 11-Oct No Lecture (Indigenous Peoples' Day) Lecture 11: Case Study Midterm Advanced Section 3: Math Foundations of PCA D: HW3 18-Oct Lecture 12: Visualization Lecture 13: Ethics Lab 7: R:HW4 25-Oct Lecture 14: Logistic Regression 1 Lecture 15: Logistic Regression 2 Lab 8: Advanced Section 4: GLM R:HW5 - D:HW4 1-Nov Lecture 16: Decision Tree Lecture 17: Bagging Lab 9: 8-Nov Lecture 18: Random Forest Lecture 19: Boosting Lab 10: Advanced Section 5: Stacking & Mixture of Experts R:HW6 - D:HW5 15-Nov Lecture 20: Model Interpretability Lecture 21: Experimental Design Lab 11: Advanced Section 6: Bandits (tentative) 22-Nov Lecture 22: NLP 1 No Lecture No Lab R:HW7 - D:HW6 29-Nov Lecture 23: NLP 2 Lecture 24: Final Review D:HW7 6-Dec Project Submission Deadline Reading Period 13-Dec Finals Week 20-Dec Projects: Final Showcase","tags":"pages","url":"pages/schedule.html"},{"title":"Syllabus","text":"TENTATIVE SYLLABUS SUBJECT TO CHANGE Introduction to Data Science (Fall 2021) CS 109a, AC 209a, Stat 121a, or CSCI E-109a Course Heads Pavlos Protopapas (SEAS) and Natesh Pillai (Statistics) Lectures: Mon & Wed 9:45am-11am - SEC Room 1.321 Labs: Friday 9:45am-11am - 114 Western Ave., Allston Room 2.111 Advanced Sections: Wed 12:45-2pm [starts 9/29] - SEC LL2.229 (see schedule for dates) Office Hours: Current Times With More To Come Prerequisites: You are expected to have programming experience at the level of CS 50 or above, and statistics knowledge at the level of Stat 100 or above (Stat 110 recommended). HW #0 is designed to test your knowledge on the prerequisites. Successful completion of this assignment will show that this course is suitable for you. HW #0 will not be graded but you are required to submit. Welcome to CS109a/STAT121a/AC209a, also offered by the DCE as CSCI E-109a, Introduction to Data Science. This course is the first half of a one‐year course in data science. The course focuses on the analysis of messy, real-life data to perform predictions using statistical and machine learning methods. Throughout the semester, our content continuously centers around five key facets: 1. data collection ‐ data wrangling, cleaning, and sampling to get a suitable data set; 2. data management ‐ accessing data quickly and reliably; 3. exploratory data analysis – generating hypotheses and building intuition; 4. prediction or statistical learning; and 5. communication – summarizing results through visualization, stories, and interpretable summaries. Only one of CS109a, AC209a, or STAT121a can be taken for credit. Students who have previously taken CS109, AC209, or STAT121 cannot take CS109A, AC 209A, or STAT121A for credit. Course Components The lectures, labs, and advanced sections will be recorded and accessed through the Zoom section on Canvas for Extension School students. Attendance is required for on campus students. Lectures The class meets for lectures twice a week for lectures (M & W). Attending and participating in lectures is a crucial component of learning the material presented in this course. What to expect A lecture will have the following pedagogy layout which will be repeated: Asynchronous pre-class exercises of approxmately 30 min. This will include, reading from the textbooks or other sources, watching videos to prepare you for the class. Approx. 10 minutes of Q&A regarding the pre-class exercises and/or review of homework and quiz questions. Live online instruction followed by a short Q/A session Hands-on exercises, on the ED platform. Sessions will help students develop the intuition for the core concepts, provide the necessary mathematical background, and provide guidance on technical details. Sessions will be accompanied by relevant examples to clarify key concepts and techniques. Labs Lab will be held every Friday at the same time and place as lectures. Labs guided hands-on coding challenges which will prepare students for successfully completing the homework assignments. Quizzes At the end of each lecture, there will be a short, graded quiz that will cover the pre-class and in-class material; there will be no AC209a content in the quizzes. The quizzes will be available until the next lecture. 25% of the quizzes will be dropped from your grade. Exercises Lectures will include one or more coding exercises focused on the newly introduced material; there will be no AC209a content in the exercises. The exercises are short enough to be completed during the time allotted in lecture but they will remain available until the beginning of the following lecture to accomodate those who cannot attend in real time. Your final grade will be calculated twice: one including exercise grades and one without. You will be given the higher of the two. In this way, exercises can only help your grade. Advanced Sections The course will include advanced sections for 209a students and will cover a different topic per week. These are 75-min lectures and will cover advanced topics like the mathematical underpinnings of the methods seen in lecture and hands-on exercises, along with extensions of those methods. The material covered in the advanced sections is required for all AC209a students. But all students are welcome and encouraged to attendadvanced sections. Note: Advanced sections are not held every week. Consult the course schedule for exact dates. Exams There will be a midterm exam on October 15th. Projects Students will work in groups of 2-4 to complete a final group project, due during the Exams period. See schedule for specific dates. Homework Assignments There will be 7 graded homework assignments. Some of them will be due one week after being assigned, and some will be due two weeks after being assigned. You have the option to work and submit in pairs for all the assignments except HW3 and HW6, which you will do individually. You will be working in Jupyter Notebooks, which you can run in your own environment or in the SEAS JupyterHub cloud. [Instructions for Setting up Your Environment] (coming soon) [Instructions for Using JupyterHub] (coming soon) On weeks with new assignments, the assignments will be released by Wednesday 3pm. Standard assignments are graded out of 5 points. AC209a students will have additional homework content for most assignments worth 1 point. Instructor Office Hours Natesh : (TBD) Pavlos : Monday 6:30-7:30 PM [IACS Office]; 7:30-8 PM [Online] Participation Students are expected to be actively engaged with the course. This includes: Attending and participating in lectures Making use of office hours Participating in the Ed discussion forum — both through asking thoughtful questions and by answering the questions of others Recommended Textbook An Introduction to Statistical Learning by James, Witten, Hastie, Tibshirani. The book is available here: Free electronic version : http://www-bcf.usc.edu/~gareth/ISL/ (Links to an external site). HOLLIS : http://link.springer.com.ezp-prod1.hul.harvard.edu/book/10.1007%2F978-1-4614-7138-7 Amazon: https://www.amazon.com/Introduction-Statistical-Learning-Applications-Statistics/dp/1461471370 (Links to an external site) . Course Policies Getting Help For questions about homework, course content, package installation, JupyterHub, and after you have tried to troubleshoot yourselves, the process to get help is: 1. Post the question in Ed and get a response from your peers. Note that in Ed questions are visible to everyone. The teaching staff monitors the posts. 2. Go to Office Hours ; this is the best way to get direct help. 3. For private matters send an email to the Helpline: cs109a2021@gmail.com . The Helpline is monitored by the teaching staff. 4. For personal and confidential matters send an email to the instructors . Collaboration Policy We expect you to adhere to the Harvard Honor Code at all times. Failure to adhere to the honor code and our policies may result in serious penalties, up to and including automatic failure in the course and reference to the ad board. If you work with a partner on an assignment make sure both parties solve all the problems. Do not divide and conquer. You are expected to be intellectually honest and give credit where credit is due. In particular: if you work with a fellow student but decide to submit individual assignments, include the name of each other in the designated area of the submission. if you work with a fellow student and want to submit the same assignment, you need to form a group prior to the submission. Details in the assignment. Remember, not all assignments will permit group submissions. you need to write your solutions entirely on your own or with your collaborator (e.g., not entirely from Google search results) you are welcome to take ideas from code presented in lecture or section, but you need to change it, adapt it to your style, and ultimately write your own. We do not want to see code copied verbatim from the above sources. if you use code found on the internet, books, or other sources you need to cite those sources. you should not view any written materials or code created by other students for the same assignment; you may not provide or make available solutions to individuals who take or may take this course in the future. if the assignment allows it you may use third-party libraries and example code, so long as the material is available to all students in the class and you give proper attribution. Do not remove any original copyright notices and headers. Late or Wrongly Submitted Assignments Each student is allowed up to 3 late days over the semester with at most 1 day applied to any single homework. Outside of these allotted late days, late homework will not be accepted unless there is a medical (if accompanied by a doctor's note) or other official University-excused reasons. There is no need to ask before using one of your late days. If you forgot to join a Group with your peer and are asking for the same grade we will accept this with no penalty up to HW3. For homeworks beyond that we feel that you should be familiar with the process of joining groups. After that there will be a penalty of -1 point for both members of the group provided the submission was on time. Grading Guidelines Homework will be graded based on: 1. How correct your code is (the Notebook cells should run, we are not troubleshooting code) 2. How you have interpreted the results — we want text not just code. It should be a report. 3. How well you present the results. The scale is 0 to 5 for each assignment. Re-grade Requests Our graders and instructors make every effort in grading accurately and in giving you a lot of feedback. If you discover that your answer to a homework problem was correct but it was marked as incorrect, send an email to the Helpline with a description of the error. Please do not submit regrade requests based on what you perceive is overly harsh grading , The points we take off are based on a grading rubric that is being applied uniformly to all submissions. If you decide to send a regrade request , send an email to the Helpline with subject line \"Regrade HW1: Grader=johnsmith\" replacing 'HW1' with the current assignment and 'johnsmith' with the name of the grader within 48 hours of the grade release . Communication from Staff to Students Class announcements will be through Ed . All homework and will be posted and submitted through Canvas . Quizzes are completed on Ed as well as all feedback forms. NOTE: make sure you adjust your account settings so you can receive emails from Canvas. Submitting an assignment Please consult [Homework Policies & Submission Instructions] (coming soon) Course Grade Your final score for the course will be computed using the following weights: Assignment Final Grade Weight Homework 0 1% Paired Homework (5) 35% (5% per HW) Individual Homework (2) 16% (8% per HW) Midterm 10% Quizzes 6% Exercises 6% Project 26% Total 100% Software We will be using Jupyter Notebooks, Python 3, and various python modules. You can access the notebook viewer either on your own machine by installing the Anaconda platform (Links to an external site) which includes Jupyter/IPython as well all packages that will be required for the course, or by using the SEAS JupyterHub from Canvas. Details in class. Auditing the Class If you would like to audit the class, please send an email to the Helpline indicating who you are and why you want to audit the class. You need a HUID to be included to Canvas. Please note that auditors may not submit assignments for grading or make use of other limited student resources such as office hours. Academic Integrity Ethical behavior is an important trait of a Data Scientist, from ethically handling data to attribution of code and work of others. Thus, in CS109A we give a strong emphasis to Academic Honesty. As a student your best guidelines are to be reasonable and fair. We encourage teamwork for problem sets, but you should not split the homework and you should work on all the problems together. For more detailed expectations, please refer to the Collaborations section above. Accommodations for Students with Disabilities Students needing academic adjustments or accommodations because of a documented disability must present their Faculty Letter from the Accessible Education Office (AEO) and speak with the professor by the end of the second week of the term, (fill in specific date). Failure to do so may result in the Course Head's inability to respond in a timely manner. All discussions will remain confidential, although Faculty are invited to contact AEO to discuss appropriate implementation. Diversity and Inclusion Statement Data Science and Computer Science have historically been representative of only a small sliver of the population. This is despite the contributions of a diverse group of early pioneers - see Ada Lovelace, Dorothy Vaughan, and Grace Hopper for just a few examples. As educators, we aim to build a diverse, inclusive, and representative community offering opportunities in data science to those who have been historically marginalized. We will encourage learning that advances ethical data science, exposes bias in the way data science is used, and advances research into fair and responsible data science. We need your help to create a learning environment that supports a diversity of thoughts, perspectives, and experiences, and honors your identities (including but not limited to race, gender, class, sexuality, religion, ability, etc.) To help accomplish this: If you have a name and/or set of pronouns that differ from those in your official Harvard records, please let us know! If you feel like your performance in the class is being impacted by your experiences outside of class, please do not hesitate to come and talk with us. We want to be a resource for you. Remember that you can also submit anonymous feedback (which will lead to us making a general announcement to the class, if necessary, to address your concerns). If you prefer to speak with someone outside of the course, you may find helpful resources at the Harvard Office of Diversity and Inclusion. We (like many people) are still learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please talk to us about it. As a participant in course discussions, you are expected to respect your classmates' diverse backgrounds and perspectives. Our course will discuss diversity, inclusion, and ethics in data science. Please contact us (in person or electronically) or submit anonymous feedback if you have any suggestions for how we can improve. Extension School Policies Accommodation Requests. Harvard Extension School is committed to providing an inclusive, accessible academic community for students with disabilities and chronic health conditions. The Accessibility Services Office (ASO) (https://extension.harvard.edu/for-students/support-and-services/accessibility-services/) offers accommodations and support to students with documented disabilities. If you have a need for accommodations or adjustments, contact Accessibility Services directly via email at accessibility@extension.harvard.edu or by phone at 617-998-9640. Academic Integrity. You are responsible for understanding Harvard Extension School policies on academic integrity (https://extension.harvard.edu/for-students/student-policies-conduct/academic-integrity/) and how to use sources responsibly. Stated most broadly, academic integrity means that all course work submitted, whether a draft or a final version of a paper, project, take-home exam, online exam, computer program, oral presentation, or lab report, must be your own words and ideas, or the sources must be clearly acknowledged. The potential outcomes for violations of academic integrity are serious and ordinarily include all of the following: required withdrawal (RQ), which means a failing grade in the course (with no refund), the suspension of registration privileges, and a notation on your transcript. Using sources responsibly (https://extension.harvard.edu/for-students/support-and-services/using-sources-effectively-and-responsibly/) is an essential part of your Harvard education. We provide additional information about our expectations regarding academic integrity on our website. We invite you to review that information and to check your understanding of academic citation rules by completing two free online 15-minute tutorials that are also available on our site. (The tutorials are anonymous open-learning tools.)","tags":"pages","url":"pages/syllabus.html"},{"title":"Lab 06: Principal Components Analysis (PCA)","text":"Jupyter Notebooks Lab 6: Principal Components Analysis (PCA)","tags":"labs","url":"labs/lab06/"},{"title":"Lecture 8: Inference in Regression and Hypothesis Testing","text":"Slides Lecture 8 : Inference in Linear Regression (PDF) Lecture 8 : Bootstrapping and Confidence Intervals (PDF) Lecture 8 : Prediction Intervals (PDF) Lecture 8 : Evaluating Significance of Predictors Hypothesis Testing (PDF) Exercises Lecture 8: Exercise: Beta Values for Data from Random Universe [Notebook] Lecture 8: Exercise: Beta Values for Data using Bootstrapping [Notebook] Lecture 8: Exercise: Confidence Intervals for Beta value [Notebook] Lecture 8: Exercise: Computing the CI [Notebook] Lecture 8: Exercise: Hypothesis Testing [Notebook]","tags":"lectures","url":"lectures/lecture08/"},{"title":"Lecture 8: Inference in Regression and Hypothesis Testing","text":"s2-ex1 Title : Exercise: Beta Values for Data from Random Universe Description : Given a RandomUniverse(dataframe)->dataframe function that gives a new dataset from a \"parallel\" universe, calculate the $\\beta_0$'s and $\\beta_1$'s and plot a histogram like the one below. Data Description: Instructions: Get a new dataframe using the RandomUniverse function already provided in the exercise Calculate $\\beta_0$, $\\beta_1$ for that particular dataframe Add the calculated $\\beta_0$ and $\\beta_1$ values to a python list Plot a histogram using the lists calculated above Hints: $${\\widehat {\\beta_1 }}={\\frac {\\sum _{i=1}&#94;{n}(x_{i}-{\\bar {x}})(y_{i}-{\\bar {y}})}{\\sum _{i=1}&#94;{n}(x_{i}-{\\bar {x}})&#94;{2}}}$$$${\\widehat {\\beta_0 }}={\\bar {y}}-{\\widehat {\\beta_1 }}\\,{\\bar {x}}$$ plt.subplots() Create a figure and a set of subplots ax.hist() Plot a histogram from a list or series. In [1]: # Import necessary libraries import numpy as np import pandas as pd import matplotlib.pyplot as plt from randomuniverse import RandomUniverse % matplotlib inline In [2]: # Read the advertising dataset as a pandas dataframe df = pd . read_csv ( 'Advertising_adj.csv' ) # Take a quick look at the dataframe df . head () Out[2]: tv sales 0 230.1 465.26 1 44.5 218.95 2 17.2 195.79 3 151.5 389.47 4 180.8 271.58 In [10]: # Create two empty lists that will store the beta values beta0_list , beta1_list = [],[] # Choose the number of \"parallel\" Universes to generate # that many new versions of the dataset parallelUniverses = 10000 # Loop over the maximum number of parallel Universes for i in range ( parallelUniverses ): # Call the RandomUniverse helper function with the dataframe # read from the data file df_new = RandomUniverse ( df ) # Find the mean of the predictor values i.e. tv xmean = df_new . tv . mean () # Find the mean of the response values i.e. sales ymean = df_new . sales . mean () # Compute the analytical values of beta0 and beta1 using the # equation given in the hints beta1 = np . sum (( df_new . tv - xmean ) * ( df_new . sales - ymean )) / np . sum (( df_new . tv - xmean ) ** 2 ) beta0 = ymean - beta1 * xmean # Append the calculated values of beta1 and beta0 to the appropriate lists beta0_list . append ( beta0 ) beta1_list . append ( beta1 ) In [11]: ### edTest(test_beta) ### # Compute the mean of the beta values beta0_mean = np . mean ( beta0_list ) beta1_mean = np . mean ( beta1_list ) In [12]: # Plot histograms of beta_0 and beta_1 using lists created above fig , ax = plt . subplots ( 1 , 2 , figsize = ( 18 , 8 )) ax [ 0 ] . hist ( beta0_list ) ax [ 1 ] . hist ( beta1_list ) ax [ 0 ] . set_xlabel ( 'Beta 0' ) ax [ 1 ] . set_xlabel ( 'Beta 1' ) ax [ 0 ] . set_ylabel ( 'Frequency' ); Out[12]: Text(0, 0.5, 'Frequency') ⏸ Increase the number of parallelUniverses . Which of the following do you observe? A. The spread increases B. The frequency of points decreases C. The spread decreases D. There is no change In [13]: ### edTest(test_chow1) ### # Submit an answer choice as a string below # (Eg. if you choose option C, put 'C') answer1 = 'D' In [0]:","tags":"lectures","url":"lectures/lecture08/notebook-1/"},{"title":"Lecture 8: Inference in Regression and Hypothesis Testing","text":"s2-ex2 Title : Exercise: Beta Values for Data using Bootstrapping Description : Solve the previous exercise by building your own bootstrap function. Data Description: Instructions: Define a function bootstrap that takes a dataframe as the input. Use NumPy's random.randint() function to generate random integers in the range of the length of the dataset. These integers will be used as the indices to access the rows of the dataset. Similar to the previous exercise, compute the $\\beta_0$ and $\\beta_1$ values for each instance of the dataframe. Plot the $\\beta_0$, $\\beta_1$ histograms. Hints: To compute the beta values use the following equations: $\\beta_{0}=\\bar{y}-\\left(b_{1} * \\bar{x}\\right)$ $\\beta_{1}=\\frac{\\sum(x-\\bar{x}) *(y-\\bar{y})}{\\sum(x-\\bar{x})&#94;{2}}$ where $\\bar{x}$ is the mean of $x$ and $\\bar{y}$ is the mean of $y$ np.random.randint() Returns list of integers as per mentioned size np.dot() Computes the dot product of two arrays df.iloc[] Purely integer-location based indexing for selection by position ax.hist() Plots a histogram ax.set_xlabel() Sets label for x-axthe is ax.set_ylabel() Sets label for the y-axis Note: This exercise is auto-graded and you can try multiple attempts. In [1]: # Import necessary libraries import pandas as pd import numpy as np import matplotlib.pyplot as plt from randomuniverse import RandomUniverse % matplotlib inline In [2]: # Read the file \"Advertising_csv\" df = pd . read_csv ( 'Advertising_adj.csv' ) # Take a quick look at the data df . head () Out[2]: tv sales 0 230.1 465.26 1 44.5 218.95 2 17.2 195.79 3 151.5 389.47 4 180.8 271.58 In [4]: # Define a bootstrap function, which takes as input a dataframe # It must output a bootstrapped version of the input dataframe def bootstrap ( df ): selectionIndex = np . random . randint ( 0 , df . shape [ 0 ], size = df . __len__ ()) new_df = df . iloc [ selectionIndex ] return new_df Alternate approach to $\\beta$ computation In [18]: # Initialize two empty lists to store the beta values beta0_list , beta1_list = [],[] # Choose the number of \"parallel\" Universes to generate the new dataset number_of_bootstraps = 1000 # Loop through the number of bootstraps for i in range ( number_of_bootstraps ): # Call the bootstrap function to get a bootstrapped version of the data df_new = bootstrap ( df ) # Find the mean of the predictor values i.e. tv xmean = df_new . tv . mean () # Find the mean of the response values i.e. sales ymean = df_new . sales . mean () #'X' is the predictor variable given by df_new.tv values X = df_new . tv #'y' is the reponse variable given by df_new.sales values y = df_new . sales # Compute the analytical values of beta0 and beta1 using the # equation given in the hints beta1 = ((( df_new . tv - xmean ) * ( df_new . sales - ymean )) . sum ()) / ((( df_new . tv - xmean ) ** 2 ) . sum ()) beta0 = ymean - beta1 * xmean # Append the calculated values of beta1 and beta0 to the appropriate lists beta0_list . append ( beta1 ) beta1_list . append ( beta0 ) In [19]: ### edTest(test_beta) ### # Compute the mean of the beta values beta0_mean = np . mean ( beta0_list ) beta1_mean = np . mean ( beta1_list ) In [20]: # Plot histograms of beta_0 and beta_1 using lists created above fig , ax = plt . subplots ( 1 , 2 , figsize = ( 18 , 8 )) ax [ 0 ] . hist ( beta0_list ) ax [ 1 ] . hist ( beta1_list ) ax [ 0 ] . set_xlabel ( 'beta 0' ) ax [ 1 ] . set_xlabel ( 'beta 1' ) ax [ 0 ] . set_ylabel ( 'Frequency' ) plt . show (); Compare the plots with the results from the RandomUniverse() function In [21]: # Helper code to visualise the similarity between the bootstrap # function here & the RandomUniverse() function from last exercise beta0_randUni , beta1_randUni = [],[] parallelUniverses = 1000 for i in range ( parallelUniverses ): df_new = RandomUniverse ( df ) xmean = df_new . tv . mean () ymean = df_new . sales . mean () # Using linear algebra result as discussed in lecture beta1 = ((( df_new . tv - xmean ) * ( df_new . sales - ymean )) . sum ()) / ((( df_new . tv - xmean ) ** 2 ) . sum ()) beta0 = ymean - beta1 * xmean beta0_randUni . append ( beta0 ) beta1_randUni . append ( beta1 ) In [22]: # Helper code to plot the bootstrapped beta values & the ones from random universe def plotmulti ( list1 , list2 ): fig , axes = plt . subplots ( 1 , 2 , figsize = ( 10 , 4 ), sharey = 'row' ) axes [ 0 ] . hist ( list1 ); axes [ 0 ] . set_xlabel ( 'Beta Distribution' ) axes [ 0 ] . set_ylabel ( 'Frequency' ) axes [ 0 ] . set_title ( 'Bootstrap' ) axes [ 1 ] . hist ( list2 ); axes [ 1 ] . set_xlabel ( 'Beta Distribution' ) axes [ 1 ] . set_title ( 'Random Universe' ) plt . show (); In [23]: # Call the 'plotmulti' function above to compare the two histograms for beta0 plotmulti ( beta0_list , beta0_randUni ) In [24]: # Call the 'plotmulti' function above to compare the two histograms for beta1 plotmulti ( beta1_list , beta1_randUni ) In [0]:","tags":"lectures","url":"lectures/lecture08/notebook-2/"},{"title":"Lecture 8: Inference in Regression and Hypothesis Testing","text":"s2-ex3 Title : Exercise: Confidence Intervals for Beta value Description : The goal of this exercise is to create a plot like the one given below for $\\beta_0$ and $\\beta_1$. Data Description: Instructions: Follow the steps from the previous exercise to get the lists of beta values. Sort the list of beta values in ascending order (from low to high). To compute the 95% confidence interval, find the 2.5 percentile and the 97.5 percentile using np.percentile() . Use the helper code plot_simulation() to visualise the $\\beta$ values along with its confidence interval Hints: $${\\widehat {\\beta_1 }}={\\frac {\\sum _{i=1}&#94;{n}(x_{i}-{\\bar {x}})(y_{i}-{\\bar {y}})}{\\sum _{i=1}&#94;{n}(x_{i}-{\\bar {x}})&#94;{2}}}$$$${\\widehat {\\beta_0 }}={\\bar {y}}-{\\widehat {\\beta_1 }}\\,{\\bar {x}}$$ np.random.randint() Returns list of integers as per mentioned size df.iloc[] Purely integer-location based indexing for selection by position plt.hist() Plots a histogram plt.axvline() Adds a vertical line across the axes plt.axhline() Add a horizontal line across the axes plt.xlabel() Sets the label for the x-axis plt.ylabel() Sets the label for the y-axis plt.legend() Place a legend on the axes ndarray.sort() Returns the sorted ndarray. np.percentile(list, q) Returns the q-th percentile value based on the provided ascending list of values. Note: This exercise is auto-graded and you can try multiple attempts . In [1]: # Import necessary libraries import numpy as np import pandas as pd import matplotlib.pyplot as plt % matplotlib inline Reading the standard Advertising dataset In [2]: # Read the 'Advertising_adj.csv' file df = pd . read_csv ( 'Advertising_adj.csv' ) # Take a quick look at the data df . head ( 3 ) In [7]: # Use the bootstrap function defined in the previous exercise def bootstrap ( df ): selectionIndex = np . random . randint ( len ( df ), size = len ( df )) new_df = df . iloc [ selectionIndex ] return new_df In [8]: # Initialize empty lists to store beta values from 100 bootstraps # of the original data beta0_list , beta1_list = [],[] # Set the number of bootstraps numberOfBootstraps = 100 # Loop over the number of bootstraps for i in range ( numberOfBootstraps ): # Call the function bootstrap with the original dataframe df_new = bootstrap ( df ) # Compute the mean of the predictor i.e. the TV column xmean = df_new . tv . mean () # Compute the mean of the response i.e. the Sales column ymean = df_new . sales . mean () # Compute beta1 analytical using the equation in the hints beta1 = ((( df_new . tv - xmean ) * ( df_new . sales - ymean )) . sum ()) / ((( df_new . tv - xmean ) ** 2 ) . sum ()) # Compute beta1 analytical using the equation in the hints beta0 = ymean - beta1 * xmean # Append the beta values to their appropriate lists beta0_list . append ( beta0 ) beta1_list . append ( beta1 ) In [9]: ### edTest(test_sort) ### # Sort the two lists of beta values from the lowest value to highest beta0_list . ___ ; beta1_list . ___ ; In [10]: ### edTest(test_beta) ### # Find the 95% percent confidence for beta0 interval using the # percentile function beta0_CI = ( np . ___ , np . ___ ) # Find the 95% percent confidence for beta1 interval using the # percentile function beta1_CI = ( np . ___ , np . ___ ) In [0]: # Print the confidence interval of beta0 upto 3 decimal points print ( f 'The beta0 confidence interval is { ___ } ' ) In [0]: # Print the confidence interval of beta1 upto 3 decimal points print ( f 'The beta1 confidence interval is { ___ } ' ) In [15]: # Helper function to plot the histogram of beta values along with # the 95% confidence interval def plot_simulation ( simulation , confidence ): plt . hist ( simulation , bins = 30 , label = 'beta distribution' , align = 'left' , density = True ) plt . axvline ( confidence [ 1 ], 0 , 1 , color = 'r' , label = 'Right Interval' ) plt . axvline ( confidence [ 0 ], 0 , 1 , color = 'red' , label = 'Left Interval' ) plt . xlabel ( 'Beta value' ) plt . ylabel ( 'Frequency' ) plt . title ( 'Confidence Interval' ) plt . legend ( frameon = False , loc = 'upper right' ) In [0]: # Call the function plot_simulation to get the histogram for beta 0 # with the confidence interval plot_simulation ( ___ , ___ ) In [0]: # Call the function plot_simulation to get the histogram for beta 1 # with the confidence interval plot_simulation ( ___ , ___ )","tags":"lectures","url":"lectures/lecture08/notebook-3/"},{"title":"Lecture 8: Inference in Regression and Hypothesis Testing","text":"s3-exd1-challenge Title : Exercise: Hypothesis Testing Description : The goal of this exercise is to identify the relevant features of the dataset using Hypothesis testing and to plot a bar plot like the one given below: Data Description: Instructions: Read the file Advertising.csv as a dataframe. Fit a simple multi-linear regression with \"medv\" as the response variable and the remaining columns as the predictor variables. Compute the coefficients of the model and plot a bar chart to depict these values. To find the distributions of the coefficients perform bootstrap. For each bootstrap: Fit a simple multi-linear regression with the same conditions as before. Compute the coefficient values and store as a list. Compute the |t|∣t∣ values for each of the coefficient value in the list. Plot a bar chart of the varying |t|∣t∣ values. Compute the p-value from the |t|∣t∣ values. Plot a bar chart of 1-p1−p values of the coefficients. Also mark the 0.95 line on the chart as shown above. Hints: pd.read_csv(filename) Returns a pandas dataframe containing the data and labels from the file data sklearn.preprocessing.normalize() Scales input vectors individually to unit norm (vector length). np.interp() Returns one-dimensional linear interpolation sklearn.train_test_split() Splits the data into random train and test subsets sklearn.LinearRegression() LinearRegression fits a linear model sklearn.fit() Fits the linear model to the training data sklearn.predict() Predict using the linear model. Note: This exercise is auto-graded and you can try multiple attempts . In [2]: # Import necessary libraries % matplotlib inline import numpy as np import pandas as pd from scipy import stats import matplotlib.pyplot as plt from sklearn import preprocessing from sklearn.linear_model import Lasso from sklearn.linear_model import Ridge from sklearn.metrics import mean_squared_error from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.preprocessing import PolynomialFeatures In [3]: # Read the file \"Advertising.csv\" as a dataframe df = pd . read_csv ( \"Advertising.csv\" , index_col = 0 ) # Take a quick look at the dataframe df . head () In [333]: # Get all the columns except 'sales' as the predictors X = df . drop ([ 'sales' ], axis = 1 ) # Select 'sales' as the response variable y = df [ 'sales' ] In [0]: # Initialize a linear regression model with normalize=True lreg = LinearRegression ( normalize = True ) # Fit the model on the entire data lreg . fit ( X , y ) In [335]: # Get the coefficient of each predictor as a dictionary coef_dict = dict ( zip ( df . columns [: - 1 ], np . transpose ( lreg . coef_ ))) predictors , coefficients = list ( zip ( * sorted ( coef_dict . items (), key = lambda x : x [ 1 ]))) In [0]: # Helper code to visualize the coefficients of all predictors fig , ax = plt . subplots () ax . barh ( predictors , coefficients , align = 'center' , color = \"#336600\" , alpha = 0.7 ) ax . grid ( linewidth = 0.2 ) ax . set_xlabel ( \"Coefficient\" ) ax . set_ylabel ( \"Predictors\" ) plt . show () In [337]: # Helper function to compute the t-statistic def get_t ( arr ): means = np . abs ( arr . mean ( axis = 0 )) stds = arr . std ( axis = 0 ) return np . divide ( means , stds ) In [338]: # Initialize an empty list to store the coefficient values coef_dist = [] # Set the number of bootstraps numboot = 1000 # Loop over the all the bootstraps for i in range ( ___ ): # Get a bootstrapped version of the dataframe df_new = df . sample ( frac = 1 , replace = True ) # Get all the columns except 'sales' as the predictors X = df_new . drop ( ___ , axis = 1 ) # Select 'sales' as the response variable y = df_new [ ___ ] # Initialize a linear regression model with normalize=True lreg = LinearRegression ( normalize = ___ ) # Fit the model on the entire data lreg . fit ( ___ , ___ ) # Append the coefficients of all predictors to the list coef_dist . append ( lreg . coef_ ) # Convert the list to a numpy array coef_dist = np . array ( coef_dist ) In [339]: # Use the helper function get_t to find the T-test values tt = get_t ( ___ ) n = df . shape [ 0 ] In [340]: # Get the t-value associated with each predictor tt_dict = dict ( zip ( df . columns [: - 1 ], tt )) predictors , tvalues = list ( zip ( * sorted ( tt_dict . items (), key = lambda x : x [ 1 ]))) In [0]: # Helper code below to visualise the t-values fig , ax = plt . subplots () ax . barh ( predictors , tvalues , align = 'center' , color = \"#336600\" , alpha = 0.7 ) ax . grid ( linewidth = 0.2 ) ax . set_xlabel ( \"T-test values\" ) ax . set_ylabel ( \"Predictors\" ) plt . show (); In [342]: ### edTest(test_pval) ### # From t-test values compute the p values using scipy.stats # T-distribution function pval = stats . t . sf ( tt , n - 1 ) * 2 # Here we use sf i.e 'Survival function' which is 1 - CDF of the t distribution. # We also multiply by two because its a two tailed test. # Please refer to lecture notes for more information # Since p values are in reversed order, we find the 'confidence' # which is 1-p conf = ___ In [343]: # Get the 'confidence' values associated with each predictor conf_dict = dict ( zip ( df . columns [: - 1 ], conf )) predictors , confs = list ( zip ( * sorted ( conf_dict . items (), key = lambda x : x [ 1 ]))) In [0]: # Helper code below to visualise the confidence values fig , ax = plt . subplots () ax . barh ( predictors , confs , align = 'center' , color = \"#336600\" , alpha = 0.7 ) ax . grid ( linewidth = 0.2 ) ax . axvline ( x = 0.95 , linewidth = 3 , linestyle = '--' , color = 'black' , alpha = 0.8 , label = '0.95' ) ax . set_xlabel ( \"$1-p$ value\" ) ax . set_ylabel ( \"Predictors\" ) ax . legend () plt . show ();","tags":"lectures","url":"lectures/lecture08/notebook-4/"},{"title":"Lecture 8: Inference in Regression and Hypothesis Testing","text":"s3-exd2-challenge Title : Exercise: Computing the CI Description : You are the manager of the Advertising division of your company, and your boss asks you the question, \"How much more sales will we have if we invest $1000 dollars in TV advertising?\" The goal of this exercise is to estimate the Sales with a 95% confidence interval using the Advertising.csv dataset. Data Description: Instructions: Read the file Advertising.csv as a dataframe. Fix a budget amount of 1000 dollars for TV advertising as variable called Budget. Select the number of bootstraps. For each bootstrap: Select a new dataframe with the predictor as TV and the response as Sales. Fit a simple linear regression on the data. Predict on the budget and compute the error estimate using the helper function error_func() . Store the sales as a sum of the prediction and the error estimate and append to sales_list . Sort the sales_list which is a distribution of predicted sales over numboot bootstraps. Compute the 95% confidence interval of sales_list . Use the helper function plot_simulation to visualize the distribution and print the estimated sales. Hints: np.random.randint() Returns list of integers as per mentioned size df.sample() Get a new sample from a dataframe plt.hist() Plots a histogram plt.axvline() Adds a vertical line across the axes plt.axhline() Add a horizontal line across the axes plt.legend() Place a legend on the axes ndarray.sort() Returns the sorted ndarray. np.percentile(list, q) Returns the q-th percentile value based on the provided ascending list of values. Note: This exercise is auto-graded and you can try multiple attempts . In [2]: # Import necessary libraries % matplotlib inline import numpy as np import pandas as pd from scipy import stats import matplotlib.pyplot as plt from sklearn import preprocessing from sklearn.metrics import mean_squared_error from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.preprocessing import PolynomialFeatures In [95]: # Read the `Advertising.csv` dataframe df = pd . read_csv ( 'Advertising.csv' ) # Take a quick look at the data df . head () In [134]: # Helper function to compute the variance of the error term def error_func ( y , y_p ): n = len ( y ) return np . sqrt ( np . sum (( y - y_p ) ** 2 / ( n - 2 ))) In [147]: # Set the number of bootstraps numboot = 1000 # Set the budget as per the instructions given # Use 2D list to facilitate model prediction (sklearn.LinearRegression requires input as a 2d array) budget = [[ ___ ]] # Initialize an empty list to store sales predictions for each bootstrap sales_list = [] In [148]: # Loop through each bootstrap for i in range ( ___ ): # Create bootstrapped version of the data using the sample function # Set frac=1 and replace=True to get a bootstrap df_new = df . sample ( ___ , replace = ___ ) # Get the predictor data ('TV') from the new bootstrapped data x = df_new [[ ___ ]] # Get the response data ('Sales') from the new bootstrapped data y = df_new . ___ # Initialize a Linear Regression model linreg = LinearRegression () # Fit the model on the new data linreg . fit ( ___ , ___ ) # Predict on the budget from the original data prediction = linreg . predict ( budget ) # Predict on the bootstrapped data y_pred = linreg . predict ( x ) # Compute the error using the helper function error_func error = np . random . normal ( 0 , error_func ( y , y_pred )) # The final sales prediction is the sum of the model prediction # and the error term sales = ___ # Convert the sales to float type and append to the list sales_list . append ( np . float64 ( ___ )) In [137]: ### edTest(test_sales) ### # Sort the list containing sales predictions in ascending order sales_list . sort () # Find the 95% confidence interval using np.percentile function # at 2.5% and 97.5% sales_CI = ( np . percentile ( ___ , ___ ), np . percentile ( ___ , ___ )) In [138]: # Helper function to plot the histogram of beta values along # with the 95% confidence interval def plot_simulation ( simulation , confidence ): plt . hist ( simulation , bins = 30 , label = 'beta distribution' , align = 'left' , density = True , edgecolor = 'k' ) plt . axvline ( confidence [ 1 ], 0 , 1 , color = 'r' , label = 'Right Interval' ) plt . axvline ( confidence [ 0 ], 0 , 1 , color = 'red' , label = 'Left Interval' ) plt . xlabel ( 'Beta value' ) plt . ylabel ( 'Frequency' ) plt . legend ( frameon = False , loc = 'upper right' ) plt . show (); In [0]: # Call the plot_simulation function above with the computed sales # distribution and the confidence intervals computed earlier plot_simulation ( sales_list , sales_CI ) In [0]: # Print the computed values print ( f \"With a TV advertising budget of $ { budget [ 0 ][ 0 ] } ,\" ) print ( f \"we can expect an increase of sales anywhere between { sales_CI [ 0 ] : 0.2f } to { sales_CI [ 1 ] : .2f } \\ with a 95% confidence interval\" ) ⏸ The sales predictions here is based on the Simple-Linear regression model between TV and Sales . Re-run the above exercise by fitting the model considering all variables in Advertising.csv . Keep the budget the same, i.e $1000 for 'TV' advertising. You may have to change the budget variable to something like [[1000,0,0]] for proper computation. Does your predicted sales interval change? Why, or why not? In [149]: ### edTest(test_chow1) ### # Type your answer within in the quotes given answer1 = '___'","tags":"lectures","url":"lectures/lecture08/notebook-5/"},{"title":"Lecture 7: Probability","text":"Slides Lecture 7 : From Probability to Maximum Likelihood Estimation (MLE) (PDF) Lecture 7 : Debug_Except (PDF) Exercises Lecture 7: Exercise: CS109A Olympics [Notebook] Lecture 7: Exercise: CS109A Olympics - Solution [Notebook]","tags":"lectures","url":"lectures/lecture07/"},{"title":"Lecture 7: Probability","text":"100mDash Title : Exercise: CS109A Olympics Description : Data Description: Instructions: In this exercise, you will simulate the 100m sprint race discussed during the lecture. We have already defined for you a Sprinter() class which has two characteristics for each sprinter: Base time Performance variance Run the code cell that makes four instances of the Sprinter() class. You will work with those for the entire exercise. Call the time attribute of the helper class to get the time taken by a competitor in the actual race. First run the race simulation five times; you will do this by creating a dictionary with participant name as keys, and time taken in a simulated race as the values. You will sort this dictionary by values and determine the winner of the simulated race. Repeat the simulation of the race for 10,000 times and count who won the race for how many times. Based on this observation, you will then investigate why a particular participant won as many times? Repeat the simulation for 10,000 times, but this time get the distribution of times for each participant over these runs. Calculate the mean race time, standard deviation of the race time and the confidence interval for each participant. Use the helper code to observe a plot similar to the one given below: Hints: Counter() Helps accumulating counts of objects in a certain data structure. np.mean() Used to calculate the mean of an array. sorted() Used to sort data. np.std() Used to calculate the std deviation of an array. np.percentile Used to calculate percentile of data inbetween a given range. Frequently used for calculating confidence intervals. CS109A Olympics : 100m dash We are going to have 4 of our team members compete against each other in the 100m dash. In [1]: # Importing libraries import numpy as np from time import sleep import os from IPython.display import clear_output from collections import Counter from helper import Sprinter from helper import run_sim import matplotlib.pyplot as plt from prettytable import PrettyTable plt . xkcd ( scale = 0 , randomness = 4 ) Out[1]: <matplotlib.pyplot._xkcd at 0x7f8dbe7bdfd0> Taking a look at the competitors Each participant has a characteristic assigned to them. The characteristic has 2 parts : Base speed : This is the time they gave in a non-competitive environment. Performance variance : Based on the mood, weather and other conditions this measure determines how much a participant's time will vary. In [2]: # Name of sprinters sprinters = [ 'Pavlos' , 'Tale' , 'Varshini' , 'Hayden' ] # Defining charactersistics, ('Base pace','performance variance') characteristics = [( 13 , 0.25 ),( 12.5 , 0.5 ),( 12.25 , 1 ),( 14.5 , 1 )] sprinters_dict = {} for idx , sprinter in enumerate ( sprinters ): # Take note of the * before characteristics sprinters_dict [ sprinter ] = Sprinter ( * characteristics [ idx ]) Running a race sprinters_dict has keys as the name of each participant, and the value as a class. The time attribute of the class is the time taken by that person to run a race. Call sprinters_dict['Pavlos'].time for 10 different times. In [3]: # Call time attribute ___ ⏸ Pause & Think Run the cell above, once again. What do you observe? A. Output is different because the python compile memory location has changed B. Output is the same C. Output changes because it is a new sample from random process In [0]: ### edTest(test_chow0) ### # Submit an answer choice as a string below (eg. if you choose option A put 'A') answer = '___' Get the times for each participant by calling the time attribute and create a dictionary called race , which has the key as the name of the participant and value as the time taken by participant to run the race. Sort race.items() according to time and get the item in dictionary with the least time taken to finish and assign it to winner . In [4]: ### edTest(test_race) ### # Get the times for each participant and make a dictionary race = ___ # Sort the items of the dictionary to get the winner # Hint: Remember to sort by the values and not the keys winner = ___ Race simulation As you would have noticed, every time you make a new dictionary race , the results would differ. Redefine the race dictionary, and run the cell below for a simulation of the race! In [5]: # Get the times for each participant and make a dictionary race = { sprinter : dash . time for sprinter , dash in sprinters_dict . items ()} # Sort the items of the dictionary to get the winner winner = sorted ( race . items (), key = lambda x : x [ 1 ])[ 0 ] # Uncomment and execute the following code # run_sim(race,winner) Multiple simulations Earlier was just one race, we want to find out who performs better over multiple races. So let's run the race 5 times Run a loop for 5 times In each loop generate the race dictionary as done earlier, and get the winner after sorting race.items() Append name of the winners to the winner_list Keep track of everyone's timings In [6]: # Run the simulation and append winners to the winner_list # Create an empty list winner_list = [] # Run a simulation for 5 loops for simulation in range ( 5 ): # Create a race dictionary race = { k : v . time for k , v in sprinters_dict . items ()} # Sort the items winner = sorted ( race . items (), key = lambda x : x [ 1 ])[ 0 ] # Append the name of the winner to winners_list winner_list . append ( winner ) # Take a look at the winners list winner_list Even more simulations We will run 10,000 simulations and use the Counter to see who wins how many times. Check the hints for how to use Counter() . In [7]: # Run the simulation and append winners to the winner_list # Create an empty list winner_list = [] # Run a simulation for 10000 loops for simulation in range ( 10000 ): # Create race dictionary race = { k : v . time for k , v in sprinters_dict . items ()} # Sort the items winner = sorted ( race . items (), key = lambda x : x [ 1 ])[ 0 ] # Append the name of the winner to winners_list winner_list . append ( winner [ 0 ]) # Display first 5 entries from winner_list winner_list___ In [8]: ### edTest(test_wins) ### # Get the counts for each person winning the race # Hint: Use counter, look at the hints wins = Counter ( winner_list ) # Print wins to see the output of the simulation print ( ___ ) In [9]: # Helper code to plot the wins of each sprinter plt . bar ( list ( wins . keys ()), list ( wins . values ()), alpha = 0.5 ) plt . xlabel ( 'Sprinters' ) plt . ylabel ( 'Race wins' , rotation = 0 , labelpad = 30 ) plt . show (); Why is Varshini winning so much ? Let us analyze why exactly is Varshini winning so frequently in our simulations. But first, we will need to record the sprint timings for each sprinter in every simulation. We will again run 10,000 simulations but this time record the individual sprint timings for each simulation instead. Make a new dictionary race_results with keys as the name of sprinters and the value as an empty list. We will append race results to this list after each simulation. Run a simulation loop for 10000 times In each simulation loop over sprinters_dict.items() and for each participant: Calculate time by calling .time append time to the list for particular key of race_results In [10]: # Run the earlier simulation loop for 10000 times # Loop over the sprinters_dict items and for each participant # Call time and append to the corresponding list in race_results race_results = { k :[] for k in sprinters_dict . keys ()} for simulation in range ( 10000 ): for sprinter , dash in sprinters_dict . items (): # For a given participant call the .time attribute sprint_timing = dash . time race_results [ sprinter ] . append ( sprint_timing ) Sample mean $\\bar{x}$ sample standard deviation $s$ Now we have a list of times given by each participant. We have the complete distribution, so let's calculate the mean, standard deviation and confidence interval. As discussed in the lecture, if we have a given sample, we can quickly compute the mean and standard deviation using np.mean() and np.std() . Let's begin with the race results for Pavlos . In [11]: # Using the race_results dictionary, find the mean # and std for 'Pavlos' pavlos_mean = ___ pavlos_std = ___ print ( f 'The average pace of Pavlos is { pavlos_mean : .2f } and the sample std is { pavlos_std : 2f } ' ) Sample mean $\\bar{x}$ sample standard deviation $s$ for all sprinters For each sprinter in the race_results dicitionary, find the mean and standard deviation of the 10,000 simulations using the np.mean() and np.std() functions. Store your findings in a new dictionary called race_stats . In [12]: # Calculate mean and std of each participant # Initialize an empty dictionary race_stats = {} # Loop over race_results.keys() for sprinter in race_results . keys (): sprinter_mean = np . mean ( race_results [ sprinter ]) sprinter_std = np . std ( race_results [ sprinter ]) # Store it as a list [mean,std] corresponding to each # participant key in race_stats race_stats [ sprinter ] = [ sprinter_mean , sprinter_std ] In [13]: # Use the helper code below to print your findings pt = PrettyTable () pt . field_names = [ \"Sprinter\" , \"Sample mean\" , \"Sample std\" ] for sprinter , stats in race_stats . items (): pt . add_row ([ sprinter , round ( stats [ 0 ], 3 ), round ( stats [ 1 ], 3 )]) print ( pt ) Confidence Interval Confidence interval is the range of values for which we can claim a certain confidence level(95% mostly). The confidence interval represents values for the population parameter for which the difference between the parameter and the observed estimate is not significant at the 5% level. Calculate the 95% CI by getting np.percentile at 2.5 and 97.5. Calculate and append these to the list of stats in the race_stats dictionary, for each participant In [14]: #By using the race_results dictionary defined above, # Find the 2.5 and 97.5 percentile of Tale's race runs. # Hint : Use race_results['Tale's'] CI = np . percentile ( ___ ,[ ___ , ___ ]) print ( f 'The 95% confidence interval for Tale is { round ( CI [ 0 ], 2 ), round ( CI [ 1 ], 2 ) } ' ) Confidence intervals for all sprinters. Let's repeat the above for each sprinter. You will add this information to your race_stats dictionary. We expect you to extend stats list with the $2.5$ and the $97.5$ percentile values for each sprinter. For e.g., if for Pavlos , we have mean=13.00 , std=0.1 , and CI as (12.8,13.2) , your race_stats['Pavlos'] must look like: [13.00,0.1,12.8,13.2] . In [15]: # Repeat the same as above, but for every sprinter # run through the race_results dictionary for each sprinter # find the confidence interval, and add it to the race_stats dictionary # defined above for sprinter , runs in race_results . items (): ci = np . percentile ( runs ,[ 2.5 , 97.5 ]) # Hint: You can use the .extend() method to add it to the # existing list of stats race_stats [ sprinter ] . extend ( ci ) In [16]: # Use the helper code below to print your findings pt = PrettyTable () pt . field_names = [ \"Sprinter\" , \"Sample mean\" , \"Sample std\" , \"95% CI\" ] for sprinter , stats in race_stats . items (): mean = round ( stats [ 0 ], 3 ) std = round ( stats [ 1 ], 3 ) confidence_interval = ( round ( stats [ 2 ], 3 ), round ( stats [ 3 ], 3 )) pt . add_row ([ sprinter , mean , std , confidence_interval ]) print ( pt ) Histogram plot for each sprinter Run the following cell to get a cool plot for distribution of times. In [17]: # Helper code to plot the distribution of times fig = plt . gcf () fig . set_size_inches ( 10 , 6 ) bins = np . linspace ( 10 , 17 , 50 ) for sprinter , runs in race_results . items (): height , bins , patches = plt . hist ( runs , bins , alpha = 0.5 , \\ label = sprinter , density = True , edgecolor = 'k' ) plt . fill_betweenx ([ 0 , height . max ()], race_stats [ sprinter ][ 2 ], race_stats [ sprinter ][ 3 ], alpha = 0.2 ) plt . legend ( loc = 'upper left' , fontsize = 16 ) plt . xlabel ( 'Seconds' ) plt . ylabel ( 'Frequency' , rotation = 0 , labelpad = 25 ) ax = plt . gca () ax . spines [ 'right' ] . set_visible ( False ) ax . spines [ 'top' ] . set_visible ( False ) ax . set_title ( 'Time distribution for sprinters' ) plt . show () ⏸ Pause & Think Take a look at the histograms for each participant and comment on why do you think is Varshini winning more races? In [0]: ### edTest(test_chow1) ### # Write your answer as a string below answer = '___' ⏸ What one parameter should Tale change in order to win more races? Note : Pick one that is most influential A. Improve consistency B. Reduce base time C. Increase base time D. Relax and hydrate before the race In [0]: ### edTest(test_chow2) ### # Submit an answer choice as a string below (eg. if you choose option A put 'A') answer = '___' In [0]: # Before you click mark, please comment out the run_sim function above 👩🏻‍🎓 Bonus (Not graded) Find out who among has would have the most podium finishes (top 3). In [18]: # Your code here","tags":"lectures","url":"lectures/lecture07/notebook-1/"},{"title":"Lecture 7: Probability","text":"simulation_solution Title : Exercise: CS109A Olympics Description : Data Description: Instructions: In this exercise, you will simulate the 100m sprint race discussed during the lecture. We have already defined for you a Sprinter() class which has two characteristics for each sprinter: Base time Performance variance Run the code cell that makes four instances of the Sprinter() class. You will work with those for the entire exercise. Call the time attribute of the helper class to get the time taken by a competitor in the actual race. First run the race simulation five times; you will do this by creating a dictionary with participant name as keys, and time taken in a simulated race as the values. You will sort this dictionary by values and determine the winner of the simulated race. Repeat the simulation of the race for 10,000 times and count who won the race for how many times. Based on this observation, you will then investigate why a particular participant won as many times? Repeat the simulation for 10,000 times, but this time get the distribution of times for each participant over these runs. Calculate the mean race time, standard deviation of the race time and the confidence interval for each participant. Use the helper code to observe a plot similar to the one given below: Hints: Counter() Helps accumulating counts of objects in a certain data structure. np.mean() Used to calculate the mean of an array. sorted() Used to sort data. np.std() Used to calculate the std deviation of an array. np.percentile Used to calculate percentile of data inbetween a given range. Frequently used for calculating confidence intervals. PyDS Olymipics : 100m dash We are going to have 4 of our team members compete against each other in the 100m dash. In [1]: # Importing libraries import numpy as np from time import sleep import os from IPython.display import clear_output from collections import Counter from helper import Sprinter import matplotlib.pyplot as plt from prettytable import PrettyTable plt . xkcd ( scale = 0 , randomness = 4 ) Taking a look at the competitors Each participant has a characteristic assigned to him. The characteristic has 2 parts : Base speed : This is the time they gave in a non-competitive environment. Performance variance : Based on the mood, weather and other conditions this measure determines how much a participant's time will vary. In [2]: # Name of sprinters sprinters = [ 'Pavlos' , 'Hargun' , 'Joy' , 'Hayden' ] # Defining charactersistics, ('Base pace','performance variance') characteristics = [( 13 , 0.25 ),( 12.5 , 0.5 ),( 12.25 , 1 ),( 14.5 , 1 )] sprinters_dict = {} for idx , sprinter in enumerate ( sprinters ): sprinters_dict [ sprinter ] = Sprinter ( * characteristics [ idx ]) Running a race sprinters_dict has keys as the name of each participant, and the value as a class. The time attribute of the class is the time taken by that person to run a race. Call sprinters_dict['Pavlos'].time for 10 different times. In [3]: # Call time attribute ___ Get the times for each participant by calling the time attribute. Create a dictionary called race , which has the key as the name of the participant and value as the time taken by participant to run the race. Sort race.items() according to time and get the item in dictionary with the least time taken to finish and assign it to winner . Note: The time taken by a participant to finish the race is the value of the dictionary so remember to sort by values In [4]: # Get the times for each participant and make a dictionary race = ___ # Then sort the items of the dictionary to get the winner # Hint: Remember to sort by the values and not the keys winner = ___ Race simulation As you would have noticed, every time you make a new dictionary race , the results would differ. Redefine the race dictionary, and run the cell below for a simulation of the race! In [5]: # Again get the times for each participant and make a dictionary race = ___ # Then sort the items of the dictionary to get the winner winner = ___ # Execute the following code for i in range ( 1 , 11 ): clear_output ( wait = True ) print ( \"|START|\" + \" \\n |START|\" . join ([ '----' * min ( 10 , int (( 15 * i ) / race [ runner ])) + ' ' * ( 10 - min ( 10 , int (( 15 * i ) / race [ runner ]))) + '|' + runner for runner in race . keys ()])) sleep ( 0.5 ) print ( f ' \\n The winner is { winner [ 0 ] } with a time of { winner [ 1 ] : .2f } s!' ) Multiple simulations Earlier was just one race, we want to find out who performs better over multiple races. So let's run the race 5 times Run a loop for 5 times In each loop generate the race dictionary as done earlier, and get the winner after sorting race.items() Append winners to the winner_list Keep track of everyone's timings In [6]: # Run the simulation and append winners to the winner_list winner_list = [] for simulation in range ( 5 ): race = ___ winner = ___ ___ winner_list Even more simulations We will run 10,000 simulations and use the Counter to see who wins how many times. Check the hints for how to use Counter() . In [7]: # Run the simulation and append winners to the winner_list ___ In [8]: # Get the counts for each person winning the race wins = Counter ( ___ ) print ( wins ) In [9]: # Execute the code plt . bar ( list ( wins . keys ()), list ( wins . values ()), alpha = 0.5 ) plt . xlabel ( 'Sprinters' ) plt . ylabel ( 'Race wins' , rotation = 0 , labelpad = 30 ) Why is Joy winning so much ? Let us analyze why exactly is Joy winning so frequently in our simulations. But first, we will need to record the sprint timings for each sprinter in every simulation. We will again run 10,000 simulations but this time record the individual sprint timings for each simulation instead. Make a new dictionary race_results with keys as the name of sprinters and the value as an empty list. We will append race results to this list after each simulation. Inside the simulation loop, loop through the items of the race_results dictionary, and for each participant : Calculate time by calling .time append time to the list for participant in race_results In [10]: # Run the earlier simulation and store all 10000 times given by a participant # race_results has a list of times as values for a given key( i.e participant) # So for a key it has a corresponding list of times for that participant. race_results = { ___ : ___ for ___ in sprinters_dict . ___ } for simulation in range ( 10000 ): for sprinter , dash in sprinters_dict . items (): sprint_timing = ___ race_results [ ___ ] . append ( ___ ) Sample mean $\\bar{x}$ sample standard deviation $s$ Now we have a list of times given by each participant. We have the complete distribution, so let's calculate the mean, std and confidence interval. As discussed in the lecture, if we have a given sample, we can quickly compute the mean and standard deviation using np.mean() and np.std() . Let's begin with the race results for Pavlos . In [11]: # Using the race_results dictionary, find the mean # and std for 'Pavlos' pavlos_mean = np . mean ( ___ ) pavlos_std = np . std ( ___ ) print ( f 'The average pace of Pavlos is { pavlos_mean : .2f } and the sample std is { pavlos_std : 2f } ' ) Sample mean $\\bar{x}$ sample standard deviation $s$ for all sprinters For each sprinter in the race_results dicitionary, find the mean and standard deviation of the 10,000 simulations using the np.mean() and np.std() functions. Store your findings in a new dictionary called race_stats as a list. So the race_stats dictionary has a list of corresponding stats for each participant(key) In [12]: # loop through the keys of race_results # calculate mean and std of each participant using np.mean() and np.std() # Assign these stats to the key, as a list race_stats = {} for sprinter in race_results . keys (): sprinter_mean = ___ sprinter_std = ___ race_stats [ sprinter ] = [ ___ , ___ ] In [13]: # Use the helper code below to print your findings pt = PrettyTable () pt . field_names = [ \"Sprinter\" , \"Sample mean\" , \"Sample std\" ] for sprinter , stats in race_stats . items (): pt . add_row ([ sprinter , round ( stats [ 0 ], 3 ), round ( stats [ 1 ], 3 )]) print ( pt ) Confidence Interval Confidence interval is the range of values for which we can claim a certain confidence level(95% mostly). The confidence interval represents values for the population parameter for which the difference between the parameter and the observed estimate is not significant at the 5% level. Use np.percentile() to calculate the 95% CI. Calculate np.percentile at 2.5 and 97.5 to get the interval. Calculate and append these to the list of stats in the race_stats dictionary, for each participant In [14]: #By using the race_results dictionary defined above, # Find the 2.5 and 97.5 percentile of Hargun's race runs. CI = np . percentile ( ___ ,[ ___ , ___ ]) print ( f 'The 95% confidence interval for Hargun is { round ( CI [ 0 ], 2 ), round ( CI [ 1 ], 2 ) } ' ) Confidence intervals for all sprinters. Let's repeat the above for each sprinter. You will add this information to your race_stats dictionary. We expect you to append the $2.5$ and the $97.5$ percentile values to the existing stats list for each sprinter. For e.g., if for Pavlos , we have mean=13.00 , std=0.1 , and CI as (12.8,13.2) , your race_stats['Pavlos'] must look like: [13.00,0.1,12.8,13.2] . In [15]: # Now lets repeat the same, but for every sprinter # run through the race_results dictionary for each sprinter # find the confidence interval, and add it to the race_stats dictionary # defined above # Hint: You can use the .extend() method to add it to the existing list of stats for sprinter , runs in race_results . items (): ci = np . percentile ( ___ ) race_stats [ ___ ] . ___ In [16]: # Use the helper code below to print your findings pt = PrettyTable () pt . field_names = [ \"Sprinter\" , \"Sample mean\" , \"Sample std\" , \"95% CI\" ] for sprinter , stats in race_stats . items (): mean = round ( stats [ 0 ], 3 ) std = round ( stats [ 1 ], 3 ) confidence_interval = ( round ( stats [ 2 ], 3 ), round ( stats [ 3 ], 3 )) pt . add_row ([ sprinter , mean , std , confidence_interval ]) print ( pt ) Histogram plot for each sprinter Run the following cell to get a cool plot for distribution of times. In [17]: fig = plt . gcf () fig . set_size_inches ( 10 , 6 ) bins = np . linspace ( 10 , 17 , 50 ) for sprinter , runs in race_results . items (): height , bins , patches = plt . hist ( runs , bins , alpha = 0.5 , \\ label = sprinter , density = True , edgecolor = 'k' ) plt . fill_betweenx ([ 0 , height . max ()], race_stats [ sprinter ][ 2 ], race_stats [ sprinter ][ 3 ], alpha = 0.2 ) plt . legend ( loc = 'upper left' , fontsize = 16 ) plt . xlabel ( 'Seconds' ) plt . ylabel ( 'Frequency' , rotation = 0 , labelpad = 25 ) ax = plt . gca () ax . spines [ 'right' ] . set_visible ( False ) ax . spines [ 'top' ] . set_visible ( False ) plt . show () ⏸ Take a look at the histograms for each participant and comment on why do you think Joy is winning the most races? A. Very consistent distribution B. Low base time and not a very high spread C. High base time but variation causes lower times to show more frequently D. Joy is not winning the most races In [0]: ### edTest(test_chow1) ### # Submit an answer choice as a string below (eg. if you choose option A put 'A') answer = '___' ⏸ What one parameter should Hargun change in order to win more races? A. Reduce base time B. Reduce consistency C. Relax before the race D. Increase consistency In [0]: ### edTest(test_chow2) ### # Submit an answer choice as a string below (eg. if you choose option A put 'A') answer = '___' 👩🏻‍🎓 Bonus (Not graded) Find out who among has would have the most podium finishes (top 3). In [18]: # Your code here","tags":"lectures","url":"lectures/lecture07/notebook-2/"},{"title":"Lab 4","text":"cs109a_lab_04_solutions_part2 CS109A Introduction to Data Science Lab 4: Bonus material: Polynomial Regression Harvard University Fall 2021 Instructors : Pavlos Protopapas and Natesh Pillai Lab Team : Marios Mattheakis, Hayden Joy, Chris Gumb, and Eleni Kaxiras Authors : Eleni Kaxiras, Rahul Dave, David Sondak, Will Claybaugh, and Pavlos Protopapas In [1]: ## RUN THIS CELL TO GET THE RIGHT FORMATTING import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\" ) . text HTML ( styles ) Out[1]: Learning Objectives Practice Polynomial Regression. In [2]: import numpy as np import pandas as pd import matplotlib.pyplot as plt # from sklearn import preprocessing from sklearn.preprocessing import PolynomialFeatures , MinMaxScaler from sklearn.preprocessing import OneHotEncoder , OrdinalEncoder , StandardScaler from sklearn.metrics import r2_score , mean_squared_error from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from pandas.api.types import CategoricalDtype from sklearn.compose import make_column_transformer , TransformedTargetRegressor from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline , make_pipeline from sklearn.linear_model import Ridge from sklearn.impute import SimpleImputer from pandas.plotting import scatter_matrix import seaborn as sns % matplotlib inline Polynomial Regression, and Revisiting the Cab Data In [3]: # read in the data, break into train and test cab_df = pd . read_csv ( \"../data/dataset_1.txt\" ) train_data , test_data = train_test_split ( cab_df , test_size = .2 , random_state = 42 ) cab_df . head () Out[3]: TimeMin PickupCount 0 860.0 33.0 1 17.0 75.0 2 486.0 13.0 3 300.0 5.0 4 385.0 10.0 In [4]: cab_df . shape Out[4]: (1250, 2) In [5]: # do some data cleaning X_train = train_data [ 'TimeMin' ] . values . reshape ( - 1 , 1 ) / 60 y_train = train_data [ 'PickupCount' ] . values X_test = test_data [ 'TimeMin' ] . values . reshape ( - 1 , 1 ) / 60 y_test = test_data [ 'PickupCount' ] . values def plot_cabs ( cur_model , poly_transformer = None ): # build the x values for the prediction line x_vals = np . arange ( 0 , 24 , .1 ) . reshape ( - 1 , 1 ) # if needed, build the design matrix if poly_transformer : design_mat = poly_transformer . fit_transform ( x_vals ) else : design_mat = x_vals # make the prediction at each x value prediction = cur_model . predict ( design_mat ) # plot the prediction line, and the test data plt . plot ( x_vals , prediction , color = 'k' , label = \"Prediction\" ) plt . scatter ( X_test , y_test , label = \"Test Data\" ) # label your plots plt . ylabel ( \"Number of Taxi Pickups\" ) plt . xlabel ( \"Time of Day (Hours Past Midnight)\" ) plt . legend () plt . show () In [6]: from sklearn.linear_model import LinearRegression fitted_cab_model0 = LinearRegression () . fit ( X_train , y_train ) plot_cabs ( fitted_cab_model0 ) In [7]: fitted_cab_model0 . score ( X_test , y_test ) Out[7]: 0.240661535615741 We can see that there's still a lot of variation in cab pickups that's not being caught by a linear fit. And the linear fit is predicting massively more pickups at 11:59pm than at 12:00am. However, we can add columns to our design matrix for $TimeMin&#94;2$ and $TimeMin&#94;3$ and so on, allowing a wigglier polynomial that will better fit the data. We'll be using sklearn's PolynomialFeatures to take some of the tedium out of building the new design matrix. In fact, if all we want is a formula like $y \\approx \\beta_0 + \\beta_1 x + \\beta_2 x&#94;2 + ...$ it will directly return the new design matrix. In [8]: degree = 3 transformer_3 = PolynomialFeatures ( degree , include_bias = False ) new_features = transformer_3 . fit_transform ( X_train ) new_features Out[8]: array([[6.73333333e+00, 4.53377778e+01, 3.05274370e+02], [2.18333333e+00, 4.76694444e+00, 1.04078287e+01], [1.41666667e+00, 2.00694444e+00, 2.84317130e+00], ..., [1.96666667e+01, 3.86777778e+02, 7.60662963e+03], [1.17333333e+01, 1.37671111e+02, 1.61534104e+03], [1.42000000e+01, 2.01640000e+02, 2.86328800e+03]]) A few notes on PolynomialFeatures : The interface is a bit strange. PolynomialFeatures is a 'transformer' in sklearn. We'll be using several transformers that learn a transformation on the training data and then apply that transformation on future data. On these (more typical) transformers it makes sense to have a .fit() and a separate .transform() . With PolynomialFeatures, the .fit() is pretty trivial, and we often fit and transform in one command, as seen above. You rarely want to include_bias (a column of all 1s), since sklearn will add it automatically. If you want polynomial features for a several different variables, you should call .fit_transform() separately on each column and append all the results to the design matrix (unless you also want interaction terms between the newly-created features). See np.concatenate for joining arrays. In [9]: fitted_cab_model3 = LinearRegression () . fit ( new_features , y_train ) plot_cabs ( fitted_cab_model3 , transformer_3 ) Questions: Calculate the polynomial model's $R&#94;2$ performance on the test set. Does the polynomial model improve on the purely linear model? Make a residual plot for the polynomial model. What does this plot tell us about the model? your answer here See code below Yes, the test set $R&#94;2$ is higher, and the visual fit to both data sets is much better. It even looks like the predicted number of pickups at 11:59 pm and 12:00 am are nearly equal. See the code below. The residuals are much more evenly spread than with the linear model [not shown], but they still don't look like an even spread of gaussian noise. This makes it unlikely that the statsmodel assumptions are valid, and we might want to be careful about trusting confidence intervals, etc, and we may want to search for other models entirely. In [10]: # your code here # test r-squared print ( \"Test R-squared:\" , fitted_cab_model3 . score ( transformer_3 . fit_transform ( X_test ), y_test )) Test R-squared: 0.33412512570778774 In [11]: # your code here design_mat = transformer_3 . fit_transform ( X_train ) prediction = fitted_cab_model3 . predict ( design_mat ) residual = y_train - prediction plt . scatter ( X_train , residual , label = \"Residual\" ) plt . axhline ( 0 , color = 'k' ) plt . title ( \"Residuals for the Cubic Model\" ) plt . ylabel ( \"Residual Number of Taxi Pickups\" ) plt . xlabel ( \"Time of Day (Hours Past Midnight)\" ) plt . legend () Out[11]: <matplotlib.legend.Legend at 0x160eed760> In [12]: # your code here design_mat = X_train prediction = fitted_cab_model0 . predict ( design_mat ) residual = y_train - prediction plt . scatter ( X_train , residual , label = \"Residual\" ) plt . axhline ( 0 , color = 'k' ) plt . title ( \"Residuals for the Linear Model\" ) plt . ylabel ( \"Residual Number of Taxi Pickups\" ) plt . xlabel ( \"Time of Day (Hours Past Midnight)\" ) plt . legend () Out[12]: <matplotlib.legend.Legend at 0x160fc6d60>","tags":"labs","url":"labs/lab4/notebook-1/"},{"title":"Lab 4","text":"cs109a_lab_04_students CS109A Introduction to Data Science Lab 4: Multiple Regression and Feature engineering Harvard University Fall 2021 Instructors : Pavlos Protopapas and Natesh Pillai Lab Team : Marios Mattheakis, Hayden Joy, Chris Gumb, and Eleni Kaxiras Authors : Eleni Kaxiras, Rahul Dave, David Sondak, Will Claybaugh, and Pavlos Protopapas In [0]: ## RUN THIS CELL TO GET THE RIGHT FORMATTING import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\" ) . text HTML ( styles ) Learning Objectives After this lab, you should be able to Implement multiple regression models with sklearn . Work with categorical variables including transforming them. Incorporate pipelines into your workflow In [0]: import numpy as np import pandas as pd import matplotlib.pyplot as plt # from sklearn import preprocessing from sklearn.preprocessing import PolynomialFeatures , MinMaxScaler from sklearn.preprocessing import OneHotEncoder , OrdinalEncoder , StandardScaler from sklearn.metrics import r2_score , mean_squared_error from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from pandas.api.types import CategoricalDtype from sklearn.compose import make_column_transformer , TransformedTargetRegressor from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline , make_pipeline from sklearn.linear_model import Ridge from sklearn.impute import SimpleImputer from pandas.plotting import scatter_matrix import seaborn as sns % matplotlib inline 1 - Exploring the Football data Introduction The data imported below were scraped by Shubham Maurya and record various facts about players in the English Premier League. Our goal is to fit models that predict the players' market value (what the player could earn when hired by a new team). There are all sorts of questions we could answer, for example is there a relationship between a player's popularity and his market value? Or we could make interesting observations about players in the top 6 teams. The data were scraped by Shubham Maurya from a variety of sources, including transfermrkt.com and Fantasy Premier League (FPL) . They record various facts about players in the English Premier League. Data description name : Name of the player club : Club of the player age : Age of the player position : The usual position on the pitch position_cat : 1 for attackers, 2 for midfielders, 3 for defenders, 4 for goalkeepers page_views : Average daily Wikipedia page views from September 1, 2016 to May 1, 2017 fpl_points : FPL points accumulated over the previous season region : 1 for England, 2 for EU, 3 for Americas, 4 for Rest of World nationality : Player's nationality new_signing : Whether a new signing for 2017/18 (till 20th July) new_foreign : Whether a new signing from a different league, for 2017/18 (till 20th July) club_id : a numerical version of the Club feature Our return variable market_value : As on transfermrkt.com on July 20th, 2017 Import the data In [0]: league_df = pd . read_csv ( \"league_data.csv\" ) league_df . head () In [0]: league_df . shape In [0]: league_df . isnull () . sum () We have not talked about handling missing values so we will just drop this here. In [0]: league_df = league_df . dropna () In [0]: league_df . isnull () . sum () In [0]: response = 'market_value' y = league_df [ response ] In [0]: league_df . describe ( include = \"all\" ) 🏋🏻‍♂️ TEAM ACTIVITY 1: Let's start with some feature engineering. The people that hired us to predict on this data want to know if being in a big club affects the market value of a player. So we need to create a new binary categorical variable named big_clubs with values $0$ or $1$ designating if a club belongs to the Top 6 clubs: big_clubs = ['Arsenal', 'Chelsea', 'Liverpool', 'Manchester+City', 'Manchester+United', 'Tottenham'] They also want to look at players in age groups and not just by age. Put the age feature in bins according to the values below, and name the variable age_cat : pandas has the .cut() method that breaks a variable into bins with labels age_bins = [___] age_labels = [___] league_df['age_cat'] = pd.cut(x=league_df['age'],\\ bins=age_bins, labels=age_labels) In [0]: # 1. your code here In [0]: # check list ( league_df [[ 'club' , 'big_club' ]] . groupby ([ 'big_club' ]) . apply ( np . unique )) Applying functions to pandas DataFrames and Series A simpler but less generic way to do the previous exercise would be league_df['big_club2'] = league_df.apply(lambda row: 1 if row['club'] in big_clubs else 0, axis=1) If the function to create the new column is simple, there is a more direct way to create the new column (feature), e.g.: df['new_column'] = df['column']**2 In [0]: # 2. your code here In [0]: # check list ( league_df [[ 'age_cat' , 'age' , ]] . sort_values ( by = 'age_cat' ) . groupby ([ 'age_cat' ]) . apply ( np . unique )) Looking at data types more closely In [0]: league_df . dtypes In [0]: # let's see what features we want to use in the model categorical_cols = [ 'position_cat' , 'new_signing' , 'big_club' , 'age_cat' , 'region' ] # non-ordinal numerical_cols = [ 'age' , 'page_views' , 'fpl_points' ] ordinal_cols = [] # we do not have any In [0]: league_df . head () In [0]: # cast categorical variables as pandas type `category` cat_type = CategoricalDtype ( ordered = False ) for var in categorical_cols : league_df [ var ] = league_df [ var ] . astype ( cat_type ) In [0]: league_df [ categorical_cols + numerical_cols ] . dtypes In [0]: # Shape of things league_df . age . values . reshape ( - 1 , 1 ) . shape Stratified train/test split We want to split before we do any EDA since, ideally, we do not want our test set to influence our design decisions. Also, to make sure that the training and test data have appropriate representation of each region; it would be bad for the training data to entirely miss a region. This is especially important because some regions are rather rare. train and test subsets = sklearn.model_selection.train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)[source] https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html 🏋🏻‍♂️ TEAM ACTIVITY 2: Practice stratified train-test split. Stratify by `region`. Use the train_test_split function and its stratify argument to split the data, keeping equal representation of each region. Note: This will not work if the dataset contains missing data. In [0]: # your code here In [0]: # check train_data . shape , test_data . shape , y_train . shape , y_test . shape Now that we won't be peeking at the test set, let's explore and look for patterns! We'll practice a number of useful pandas and numpy functions along the way. We notice that our dataset contains columns with different data types. We need to apply a specific preprocessing for each one of them. Categorical variables that are ordinal need to be coded as integers, while the rest of them need to be one-hot-encoded. We can do this sequentially or better use sklearn's pipeline structure. Our pipeline could conveniently include any standardization/normalisation of numerical values. For now we will let them as they are. In [0]: train_data . head () In [0]: sns . pairplot ( train_data [[ 'age' , 'page_views' , 'market_value' ]], \\ kind = 'reg' , diag_kind = 'hist' ); In [0]: train_data . columns In [0]: train_data [[ 'club' , 'club_id' ]] . \\ groupby ([ 'club_id' ]) . agg ({ 'club' : np . unique , }) In [0]: train_data . groupby ( 'position' ) . agg ({ 'market_value' : np . mean , 'page_views' : np . median , 'fpl_points' : np . max }) 2 - Transform categorical variables In [0]: categorical_cols , numerical_cols In [0]: X_train = train_data [ categorical_cols + numerical_cols ] . copy () X_test = test_data [ categorical_cols + numerical_cols ] . copy () X_train . shape , X_test . shape , y_train . shape , y_test . shape Using sklearn OneHotEncoder() By default, keeps all one-hot created columns. Fine-grained drop mechanism, can drop only binary variables, or the first in the list of categories, or even a specific one ($cats[i]$). drop{‘first', ‘if_binary'} or a array-like of shape (n_features,), default=None It also has a mechanism for handling the presence of unknown categories in the test set. handle_unknown{‘error', ‘ignore'}, default='error' In [0]: oh = OneHotEncoder ( drop = 'if_binary' , sparse = False , handle_unknown = 'error' ) oh_train = oh . fit_transform ( train_data [ categorical_cols ]) oh_train [: 10 ] In [0]: list ( zip ( categorical_cols , oh . categories_ )) In [0]: oh_train . shape , train_data [ categorical_cols ] . shape In [0]: oh_test = oh . transform ( test_data [ categorical_cols ]) oh_test . shape , test_data [ categorical_cols ] . shape In [0]: # remember these are \"views\" of the dataframe # the dataframe remains unchanged train_data [ categorical_cols ] . head ( 5 ) In [0]: train_data [ numerical_cols ] . values . shape , oh_train . shape Using pandas get_dummies() By default keeps all $k$ dummies out of $k$ categorical levels. Can be made to remove the first level, so that we have $k-1 dummies$. In [0]: dummies_train = pd . get_dummies ( train_data [ categorical_cols ]) #drop_first=True dummies_train . head () In [0]: # transform the test set dummies_test = pd . get_dummies ( test_data [ categorical_cols ]) Note : if the test dataset has a category that does not exist in the training set, this will throw an error. In [0]: pd . set_option ( 'display.max_columns' , None ) # create the design matrix for the train set design_train_df = pd . concat ([ train_data [ numerical_cols ], dummies_train ], axis = 1 ) design_train_df . head () In [0]: # for the test set design_test_df = pd . concat ([ test_data [ numerical_cols ], dummies_test ], axis = 1 ) In [0]: design_train_df . dtypes In [0]: # the dataframe remains unchanged train_data [ categorical_cols ] . head ( 5 ) In [0]: list ( zip ( categorical_cols , oh . categories_ )) Now, let's run the model using our design matrices In [0]: #create linear model regression = LinearRegression () #fit linear model regression . fit ( design_train_df , y_train ) y_pred = regression . predict ( design_test_df ) r2_train = regression . score ( design_train_df , y_train ) r2_test = regression . score ( design_test_df , y_test ) print ( f 'R&#94;2 train = { r2_train : .5 } ' ) print ( f 'R&#94;2 test = { r2_test : .5 } ' ) 3 - Using Transformation Pipelines There could be many transformations that need to be executed sequentialy in order to construct the design matrix. As we saw, it is possible to handcraft the design matrix ourselves by transforming individual columns, it is more efficient and error-free to create an sklearn pipeline to do this for you. Sklearn can work directly with $numpy$ arrays or $DataFrames$. When using the latter, sklearn.compose.ColumnTransformer is useful, as it applies transformers to columns of an array or pandas DataFrame. This estimator allows different columns or column subsets of the input to be transformed separately and the features generated by each transformer will be concatenated to form the design matrix. Making a pipeline from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler my_pipeline = Pipeline([ ('imputer', Imputer(strategy='median')), # we will be using later ('std_scaler', StandardScaler()), # optional ('selector', ColumnTransformer()) # for one-hot encoding ('regressor', lr) # actual regressor model ]) In [0]: # transform categoricals categorical_encoder = OneHotEncoder ( drop = 'if_binary' , handle_unknown = 'error' ) #handle_unknown='ignore' # transform numericals numerical_pipe = Pipeline ([ ( 'imputer' , SimpleImputer ( strategy = 'mean' )), # for later #('stdscaler', StandardScaler()) # for later ]) # bring all transformations together preprocessor = ColumnTransformer ([ ( 'cat' , categorical_encoder , categorical_cols ), ( 'num' , numerical_pipe , numerical_cols ) ]) # add a regressor lr = LinearRegression () model = Pipeline ([ ( 'preprocessor' , preprocessor ), ( 'regressor' , lr ) ]) model . fit ( X_train , y_train ) In [0]: ohe = ( model . named_steps [ 'preprocessor' ] . named_transformers_ [ 'cat' ]) feature_names = ohe . get_feature_names ( input_features = categorical_cols ) feature_names = np . r_ [ feature_names , numerical_cols ] feature_names = list ( feature_names ) feature_names In [0]: print ( f 'LR train R&#94;2: { model . score ( X_train , y_train ) : .3f } ' ) print ( f 'LR test R&#94;2: { model . score ( X_test , y_test ) : .3f } ' ) In [0]: # grab the linear regressor linear_regressor = model . named_steps [ 'regressor' ] linear_regressor . coef_ . shape In [0]: pd . DataFrame ( zip ( feature_names + numerical_cols , linear_regressor . coef_ ), columns = [ 'feature' , 'coeff' ]) A different way to construct the pipeline In [0]: preprocessor = make_column_transformer ( ( OneHotEncoder ( drop = 'if_binary' , handle_unknown = 'error' ), categorical_cols ), #(StandardScaler(), numerical_columns), ( SimpleImputer ( strategy = 'mean' ), numerical_cols ), remainder = 'passthrough' ) In [0]: model = make_pipeline ( preprocessor , LinearRegression () ) model . fit ( X_train , y_train ) In [0]: feature_names = ( model . named_steps [ 'columntransformer' ] . named_transformers_ [ 'onehotencoder' ] . get_feature_names ( input_features = categorical_cols )) feature_names = np . concatenate ( [ feature_names , numerical_cols ]) coefs = pd . DataFrame ( model . named_steps [ 'linearregression' ] . coef_ , columns = [ 'Coefficients' ], index = feature_names ) coefs In [0]: print ( f 'LR train R&#94;2: { model . score ( X_train , y_train ) : .3f } ' ) print ( f 'LR test R&#94;2: { model . score ( X_test , y_test ) : .3f } ' ) 4 - Feature Engineering 🏋🏻‍♂️ TEAM ACTIVITY 4: Let's focus on introducing new features to see if our model performs better. After talking to our client for four hours and doing some some thought, we concluded that the mean predicted market value should be: $$\\hat{y} = \\beta_0 + \\beta_1\\cdot \\text{fpl_points} + \\beta_2\\cdot\\text{age} + \\beta_3\\cdot\\text{age}&#94;2 + \\beta_4\\cdot \\text{new_signing} +\\beta_5\\cdot \\text{big_club} + \\beta_6\\cdot \\text{position_cat} \\\\ + \\beta_7\\cdot \\text{age_cat} + \\beta_8\\cdot \\text{page_views}\\times \\text{fpl_points}$$ We're including a 2nd degree polynomial in age because we expect pay to increase as a player gains experience, but then decrease as they continue aging. We also include an interaction term between page_views and fpl_points . Build a design matrix function and fit this model to the training data. How good is the overall model? Interpret the regression model. What is the meaning of the coefficient for: age and age$&#94;2$ big_club What should a player do in order to improve their market value? How many page views should a player go get to increase their market value by 10? In [0]: # load a fresh train and test set. train_data = pd . read_csv ( \"train_data.csv\" ) test_data = pd . read_csv ( \"test_data.csv\" ) train_data . head ( 2 ) In [0]: # your code here In [0]: # check print ( f 'LR train R&#94;2: { model . score ( X_train , y_train ) : .3f } ' ) print ( f 'LR test R&#94;2: { model . score ( X_test , y_test ) : .3f } ' ) Conceptual questions The model is reasonably good. We're capturing about 76% of the variation in market values, and the test set confirms that we're not overfitting too badly. Look at the coefficients, depends upon your split.. Linear regression on non-experimental data can't determine causation, so we can't prove that a given relationship runs in the direction we might think. For instance, doing whatever it takes to get more page views probably doesn't meaningfully increase market value; it's likely the causation runs in the other direction and great players get more views. Even so, we can use page views to help us tell who is a great player and thus likely to be paid well. In [0]: agecoef = float ( coefs . loc [ 'age' ] . values ) age2coef = float ( coefs . loc [ 'age_sq' ] . values ) agecoef , age2coef In [0]: x_vals = np . linspace ( - 100 , 100 , 1000 ) y_vals = agecoef * x_vals + age2coef * x_vals ** 2 plt . plot ( x_vals , y_vals ) plt . title ( \"Effect of Age on Player Market value\" ) plt . xlabel ( \"Age\" ) plt . ylabel ( \"Contribution to Predicted Market Value\" ) plt . show () Conceptual questions If our model does not have a constant, we must include all four dummy variable columns. If we drop one, we're not modeling any effect of being in that category, and effectively assuming the dropped category's effect is 0. Being in position 2 (instead of position 1) has an impact between -1.54 and +2.38 on a player's market value. Since we're using an intercept, the dropped category becomes the baseline and the effect of any dummy variable is the effect of being in that category instead of the baseline category. END OF LAB 04","tags":"labs","url":"labs/lab4/notebook-2/"},{"title":"Lecture 6: Regularization Ridge and Lasso Regression","text":"Slides Lecture 6 : Multi-Linear Regression (PDF) Lecture 6 : Ridge and Lasso - Hyperparameters (PDF) Lecture 6 : Comparison of Ridge and Lasso (PDF) Exercises Lecture 6: Exercise: Bias Variance Tradeoff [Notebook] Lecture 6: Exercise: Simple Lasso and Ridge Regularization [Notebook] Lecture 6: Exercise: Variation of Coefficients for Lasso and Ridge Regression [Notebook] Lecture 6: Exercise: Hyper-parameter Tuning for Ridge Regression [Notebook] Lecture 6: Exercise: Regularization with Cross-validation [Notebook]","tags":"lectures","url":"lectures/lecture06/"},{"title":"Lecture 6: Regularization Ridge and Lasso Regression","text":"s4_ex1_challenge Title : Exercise: Simple Lasso and Ridge Regularization Description : The aim of this exercise is to understand Lasso and Ridge regularization. For this we will plot the predictor vs coefficient as a horizontal bar chart. The graph will look similar to the one given below. Data Description: Instructions: Read the dataset and assign the predictor and response variables. Split the dataset into train and validation sets. Fit a multi-linear regression model. Compute the validation MSE of the model. Compute the coefficient of the predictors and store to the plot later. Implement Lasso regularization by specifying an alpha value. Repeat steps 4 and 5. Implement Ridge regularization by specifying the same alpha value. Repeat steps 4 and 5. Plot the coefficient of all the 3 models in one graph as shown above. Hints: sklearn.normalize() Scales input vectors individually to the unit norm (vector length) sklearn.train_test_split() Splits the data into random train and test subsets sklearn.LinearRegression() LinearRegression fits a linear model sklearn.fit() Fits the linear model to the training data sklearn.predict() Predict using the linear modReturns the coefficient of the predictors in the model. mean_squared_error() Mean squared error regression loss sklearn.Lasso() Linear Model trained with L1 prior as a regularizer sklearn.Ridge() Linear least squares with L2 regularization Note: This exercise is auto-graded and you can try multiple attempts. In [99]: # Import necessary libraries % matplotlib inline import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn import preprocessing from sklearn.linear_model import Lasso from sklearn.linear_model import Ridge from sklearn.metrics import mean_squared_error from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.preprocessing import PolynomialFeatures Reading the dataset In [100]: # Read the file \"Boston_housing.csv\" as a Pandas dataframe df = pd . read_csv ( \"Boston_housing.csv\" ) Predictors & Response variables Select the following columns as predictors crim indus nox rm age dis rad tax ptratio black lstat Select the 'medv' column as response variable In [101]: # Select a subdataframe of predictors mentioned above X = df [ ___ ] # Normalize the values of the dataframe X_norm = preprocessing . normalize ( ___ ) # Select medv as the response variable y = df [ ___ ] Split the dataset into train and validation sets In [102]: ### edTest(test_random) ### # Split the data into train and validation sets with 70% train data and # random_state as 31 X_train , X_val , y_train , y_val = train_test_split ( ___ ) Multi-linear Regression Analysis In [103]: # Initialize a Linear Regression model lreg = LinearRegression () # Fit the linear model on the train data lreg . fit ( ___ ) # Predict on the validation data y_val_pred = lreg . predict ( ___ ) In [0]: # Use the mean_squared_error function to compute the validation mse mse = mean_squared_error ( ___ , ___ ) # Print the MSE value print ( \"Multi-linear regression validation MSE is\" , mse ) Obtaining the coefficients of the predictors In [105]: # Helper code to create a dictionary of the coefficients # along with the predictors as keys lreg_coef = dict ( zip ( X . columns , np . transpose ( lreg . coef_ ))) # Linear regression coefficients for plotting lreg_x = list ( lreg_coef . keys ()) lreg_y = list ( lreg_coef . values ()) Implementing Lasso regularization In [106]: # Create a Lasso Regression model with alpha as 0.008 lasso_reg = Lasso ( ___ ) # Fit the model on the train data lasso_reg . fit ( ___ ) # Predict on the validation data using the trained model y_val_pred = lasso_reg . predict ( ___ ) Computing the MSE with Lasso regularization In [0]: # Calculate the validation MSE mse_lasso = mean_squared_error ( ___ , ___ ) # Print the validation MSE print ( \"Lasso validation MSE is\" , mse_lasso ) In [108]: # Hhelper code to make a dictionary of the predictors # along with the coefficients associated with them lasso_coef = dict ( zip ( X . columns , np . transpose ( lasso_reg . coef_ ))) # Get the Lasso regularisation coefficients for plotting lasso_x = list ( lasso_coef . keys ()) lasso_y = list ( lasso_coef . values ()) Implementing Ridge regularization In [109]: # Create a Ridge Regression model with alpha as 0.008 ridgeReg = Ridge ( ___ ) # Fit the model on the train data ridgeReg . fit ( ___ ) # Predict the trained model on the validation data y_val_pred = ridgeReg . predict ( ___ ) Computing the MSE with Ridge regularization In [0]: ### edTest(test_mse) ### # Calculate the validation MSE mse_ridge = mean_squared_error ( ___ ) # Print he valdiation MSE print ( \"Ridge validation MSE is\" , mse_ridge ) Obtaining the coefficients of the predictors In [111]: # Helper code to make a dictionary of the predictors # along with the coefficients associated with them ridge_coef = dict ( zip ( X . columns , np . transpose ( ridgeReg . coef_ ))) # Ridge regularisation coefficients for plotting ridge_x = list ( ridge_coef . keys ()) ridge_y = list ( ridge_coef . values ()) Plotting the graph In [0]: # Helper code below to visualise your results plt . rcdefaults () plt . barh ( lreg_x , lreg_y , 1.0 , align = 'edge' , color = \"#D3B4B4\" , label = \"Linear Regression\" ) plt . barh ( lasso_x , lasso_y , 0.75 , align = 'edge' , color = \"#81BDB2\" , label = \"Lasso regularisation\" ) plt . barh ( ridge_x , ridge_y , 0.25 , align = 'edge' , color = \"#7E7EC0\" , label = \"Ridge regularisation\" ) plt . grid ( linewidth = 0.2 ) plt . xlabel ( \"Coefficient\" ) plt . ylabel ( \"Predictors\" ) plt . legend ( loc = 'best' ) plt . xlim ( - 6500 , 3500 ) plt . show () ⏸ How does the performance of Lasso and Ridge regression compare with that of Linear regression? In [0]: ### edTest(test_chow1) ### # Submit an answer choice as a string below answer1 = '___' ⏸ Change the alpha values for both, Lasso and Ridge, to 1000. What happens to the coefficients? In [0]: ### edTest(test_chow2) ### # Submit an answer choice as a string below answer2 = '___'","tags":"lectures","url":"lectures/lecture06/notebook-1/"},{"title":"Lecture 6: Regularization Ridge and Lasso Regression","text":"s4-exa2-challenge Title : Exercise: Bias Variance Tradeoff Description : The aim of this exercise is to understand bias variance tradeoff . For this, you will fit a polynomial regression model with different degrees on the same data and plot them as given below. Data Description: Instructions: Read the file noisypopulation.csv as a Pandas dataframe. Assign the response and predictor variables appropriately as mentioned in the scaffold. Perform sampling on the dataset to get a subset. For each sampled version fo the dataset: For degree of the chosen degree value: Compute the polynomial features for the training Fit the model on the given data Select a set of random points in the data to predict the model Store the predicted values as a list Plot the predicted values along with the random data points and true function as given above. Hints: FUNCTION SIGNATURE: gen(degree, number of samples, number of points, x, y) sklearn.PolynomialFeatures() Generates polynomial and interaction features sklearn.LinearRegression() LinearRegression fits a linear model sklearn.fit() Fits the linear model to the training data sklearn.predict() Predict using the linear model. Note: This exercise is auto-graded and you can try multiple attempts. In [31]: #Import necessary libraries % matplotlib inline import scipy as sp import numpy as np import pandas as pd import matplotlib as mpl import matplotlib.cm as cm import matplotlib.pyplot as plt from sklearn.linear_model import LinearRegression from sklearn.preprocessing import PolynomialFeatures In [32]: # Helper function to define plot characteristics def make_plot (): fig , axes = plt . subplots ( figsize = ( 20 , 8 ), nrows = 1 , ncols = 2 ); axes [ 0 ] . set_ylabel ( \"$p_R$\" , fontsize = 18 ) axes [ 0 ] . set_xlabel ( \"$x$\" , fontsize = 18 ) axes [ 1 ] . set_xlabel ( \"$x$\" , fontsize = 18 ) axes [ 1 ] . set_yticklabels ([]) axes [ 0 ] . set_ylim ([ 0 , 1 ]) axes [ 1 ] . set_ylim ([ 0 , 1 ]) axes [ 0 ] . set_xlim ([ 0 , 1 ]) axes [ 1 ] . set_xlim ([ 0 , 1 ]) plt . tight_layout (); return axes In [33]: # Reading the file into a dataframe df = pd . read_csv ( \"noisypopulation.csv\" ) In [34]: ###edTest(get_data)### # Set column x is the predictor and column y is the response variable. # Column f is the true function of the given data # Select the values of the columns x = df . ___ f = df . ___ y = df . ___ In [36]: # Function to compute the Polynomial Features for the data x # for the given degree d def polyshape ( d , x ): return PolynomialFeatures ( ___ ) . fit_transform ( ___ . reshape ( - 1 , 1 )) In [37]: # Function to fit a Linear Regression model def make_predict_with_model ( x , y , x_pred ): # Create a Linear Regression model with fit_intercept as False lreg = ___ # Fit the model to the data x and y got parameters to the function lreg . fit ( ___ , ___ ) # Predict on the x_pred data got as a parameter to this function y_pred = lreg . predict ( ___ ) # Return the linear model and the prediction on the test data return lreg , y_pred In [38]: # Function to perform sampling and fit the data, with the following parameters # degree is the maximum degree of the model # num_sample is the number of samples # size is the number of random points selected from the data for each sample # x is the predictor variable # y is the response variable def gen ( degree , num_sample , size , x , y ): # Create 2 lists to store the prediction and model predicted_values , linear_models = [], [] # Loop over the number of samples for i in range ( num_sample ): # Helper code to call the make_predict_with_model function to fit on the data indexes = np . sort ( np . random . choice ( x . shape [ 0 ], size = size , replace = False )) # lreg and y_pred hold the model and predicted values for the current sample lreg , y_pred = make_predict_with_model ( polyshape ( degree , x [ indexes ]), y [ indexes ], polyshape ( degree , x )) # Append the model and predicted values to the appropriate lists predicted_values . append ( ___ ) linear_models . append ( ___ ) # Return the 2 lists, one for predicted values and one for the model return predicted_values , linear_models In [39]: # Call the function gen() twice with x and y as the # predictor and response variable respectively # Set the number of samples to 200 and the number of points as 30 # Store the return values in appropriate variables # Get results for degree 1 predicted_1 , model_1 = gen ( ___ ); # Get results for degree 100 predicted_100 , model_100 = gen ( ___ ); In [0]: # Helper code to plot the data indexes = np . sort ( np . random . choice ( x . shape [ 0 ], size = 30 , replace = False )) plt . figure ( figsize = ( 12 , 8 )) axes = make_plot () # Plot for Degree 1 axes [ 0 ] . plot ( x , f , label = \"f\" , color = 'darkblue' , linewidth = 4 ) axes [ 0 ] . plot ( x , y , '.' , label = \"Population y\" , color = '#009193' , markersize = 8 ) axes [ 0 ] . plot ( x [ indexes ], y [ indexes ], 's' , color = 'black' , label = \"Data y\" ) for i , p in enumerate ( predicted_1 [: - 1 ]): axes [ 0 ] . plot ( x , p , alpha = 0.03 , color = '#FF9300' ) axes [ 0 ] . plot ( x , predicted_1 [ - 1 ], alpha = 0.3 , color = '#FF9300' , label = \"Degree 1 from different samples\" ) # Plot for Degree 100 axes [ 1 ] . plot ( x , f , label = \"f\" , color = 'darkblue' , linewidth = 4 ) axes [ 1 ] . plot ( x , y , '.' , label = \"Population y\" , color = '#009193' , markersize = 8 ) axes [ 1 ] . plot ( x [ indexes ], y [ indexes ], 's' , color = 'black' , label = \"Data y\" ) for i , p in enumerate ( predicted_100 [: - 1 ]): axes [ 1 ] . plot ( x , p , alpha = 0.03 , color = '#FF9300' ) axes [ 1 ] . plot ( x , predicted_100 [ - 1 ], alpha = 0.2 , color = '#FF9300' , label = \"Degree 100 from different samples\" ) axes [ 0 ] . legend ( loc = 'best' ) axes [ 1 ] . legend ( loc = 'best' ) plt . show (); ⏸ Does changing the degree from 100 to 10 reduce variance? Why or why not? In [0]: ### edTest(test_chow1) ### # Submit an answer choice as a string below answer1 = '___'","tags":"lectures","url":"lectures/lecture06/notebook-2/"},{"title":"Lecture 6: Regularization Ridge and Lasso Regression","text":"s4-ex3-challenge Title : Exercise: Hyper-parameter Tuning for Ridge Regression Description : The goal of this exercise is to perform hyper-parameter tuning and produce a plot similar to the one below: Data Description: The dataset has a total of 3 columns with names - x,y and f \"$x$\" represents the predictor variable \"$y$\" is the response variable \"$f$\" denotes the true values of the underlying function Instructions: Read the dataset polynomial50.csv as a dataframe. Assign the predictor and response variables. Visualize the dataset by making plots using the predictor and response variables along with the true function. Split the data into train and validation sets using random_state=42 . For each value of alpha from a given list: Estimate a Ridge regression on the training data with the alpha value. Calculate the MSE of training and validation data. Append to separate lists appropriately. Use the given plot_functions function to plot the value of parameters. Compute the best hyperparameter for this data based on the lowest MSE Make a plot of the MSE values for each value of hyper-parameter alpha from the list above. It should look similar to the one given above. Hints: sklearn.Ridge() Linear least squares with L2 regularization. sklearn.train_test_split() Splits the data into random train and test subsets. ax.plot() Plot y versus x as lines and/or markers. sklearn.PolynomialFeatures() Generates a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. sklearn.fit_transform() Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. sklearn.Ridge() Linear least squares with L2 regularization. sklearn.predict() Predict using the linear model. mean_squared_error() Mean squared error regression loss. Note: This exercise is auto-graded and you can try multiple attempts. In [9]: # Import necessary libraries % matplotlib inline import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from sklearn.linear_model import Ridge , Lasso from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split from sklearn.preprocessing import PolynomialFeatures plt . style . use ( 'seaborn-white' ) # These are custom functions made to help you visualise your results from helper import plot_functions from helper import plot_coefficients In [52]: # Open the file 'polynomial50.csv' as a Pandas dataframe df = pd . read_csv ( 'polynomial50.csv' ) In [0]: # Take a quick look at the data df . head () In [54]: # Assign the values of the 'x' column as the predictor x = df [[ 'x' ]] . values # Assign the values of the 'y' column as the response y = df [ 'y' ] . values # Also assign the true value of the function (column 'f') to the variable f f = df [ 'f' ] . values In [0]: # Visualise the distribution of the x, y values & also the value of the true function f fig , ax = plt . subplots () # Plot x vs y values ax . plot ( ___ , ___ , '.' , label = 'Observed values' , markersize = 10 ) # Plot x vs true function value ax . plot ( ___ , ___ , 'k-' , label = 'Function description' ) # Helper code to annotate the plot ax . legend ( loc = 'best' ) ax . set_xlabel ( 'Predictor - $X$' , fontsize = 16 ) ax . set_ylabel ( 'Response - $Y$' , fontsize = 16 ) ax . set_title ( 'Predictor vs Response plot' , fontsize = 16 ) plt . show (); In [132]: # Split the data into train and validation sets with # training size 80% and random_state = 42 x_train , x_val , y_train , y_val = train_test_split ( x , y , train_size = 0.8 , random_state = 42 ) In [0]: ### edTest(test_mse) ### fig , rows = plt . subplots ( 6 , 2 , figsize = ( 16 , 24 )) # Select the degree for polynomial features degree = ___ # List of hyper-parameter values alphas = [ 0.0 , 1e-7 , 1e-5 , 1e-3 , 0.1 , 1 ] # Create two lists for training and validation error training_error , validation_error = [],[] # Compute the polynomial features train and validation sets x_poly_train = PolynomialFeatures ( ___ ) . fit_transform ( ___ ) x_poly_val = PolynomialFeatures ( ___ ) . fit_transform ( ___ ) # Loop over all the alpha values for i , alpha in enumerate ( alphas ): # Code to get the plot grid l , r = rows [ i ] # Initialize a Ridge regression with the current alpha ridge = Ridge ( fit_intercept = False , alpha = ___ ) # Fit the model on the transformed training data ridge . fit ( ___ , ___ ) # Predict on the transformed training set y_train_pred = ridge . predict ( ___ ) # Predict on the transformed validation set y_val_pred = ridge . predict ( ___ ) # Compute the training and validation errors mse_train = mean_squared_error ( ___ , ___ ) mse_val = mean_squared_error ( ___ , ___ ) # Add the error values to the appropriate list training_error . append ( ___ ) validation_error . append ( ___ ) # Calling the helper functions plot_functions & # plot_coefficients to visualise the plots plot_functions ( degree , ridge , l , df , alpha , x_val , y_val , x_train , y_train ) plot_coefficients ( ridge , r , alpha ) sns . despine (); In [136]: ### edTest(test_hyper) ### # Find the best value of hyper parameter, which # gives the least error on the validdata best_parameter = ___ # Print the best hyper parameter print ( f 'The best hyper parameter value, alpha = { best_parameter } ' ) In [0]: # Plot the errors as a function of increasing d value # to visualise the training and validation errors fig , ax = plt . subplots ( figsize = ( 12 , 8 )) # Plot the training errors for each alpha value ax . plot ( ___ , ___ , 's--' , label = 'Training error' , color = 'Darkblue' , linewidth = 2 ) # Plot the validation errors for each alpha value ax . plot ( ___ , ___ , 's-' , label = 'validation error' , color = '#9FC131FF' , linewidth = 2 ) # Draw a vertical line at the best parameter ax . axvline ( ___ , 0 , 0.5 , color = 'r' , label = f 'Min validation error at alpha = { best_parameter } ' ) ax . set_xlabel ( 'Value of Alpha' , fontsize = 15 ) ax . set_ylabel ( 'Mean Squared Error' , fontsize = 15 ) ax . set_ylim ([ 0 , 0.010 ]) ax . legend ( loc = 'upper left' , fontsize = 16 ) ax . set_title ( 'Mean Squared Error' , fontsize = 20 ) ax . set_xscale ( 'log' ) plt . tight_layout () plt . show ();","tags":"lectures","url":"lectures/lecture06/notebook-3/"},{"title":"Lecture 6: Regularization Ridge and Lasso Regression","text":"s5_ex1_challenge Title : Exercise: Variation of Coefficients for Lasso and Ridge Regression Description : The goal of this exercise is to understand the variation of the coefficients of predictors with varying values of regularization parameter in Lasso and Ridge regularization. Below is a sample plot for Ridge ($L_2$ regularization) Data Description: Instructions: Read the dataset bateria_train.csv and assign the predictor and response variables. The predictor is the 'Spreading factor' and the response variable is the 'Perc_population' Use a maximum degree of 7 to make polynomial features and make a new predictor x_poly Make a list of alpha values. For each value of $\\alpha$ : Fit a multi-linear regression using $L_2$ regularization Compute the coefficient of the predictors and store to the plot later Make a plot of the coefficients along with the alpha values Make a new alpha list as per the code in the exercise Implement Lasso regularization by repeating the above steps for each value of alpha Make another plot of the coefficients along with the new alpha values Hints: np.linspace() Return evenly spaced numbers over a specified interval. np.transpose() Reverse or permute the axes of an array; returns the modified array. sklearn.PolynomialFeatures() Generates a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. sklearn.fit_transform() Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. sklearn.LinearRegression() LinearRegression fits a linear model. sklearn.fit() Fits the linear model to the training data. sklearn.predict() Predict using the linear modReturns the coefficient of the predictors in the model. mean_squared_error() Mean squared error regression loss. sklearn.coef_ Returns the coefficients of the predictors. sklearn.Lasso() Linear Model trained with L1 prior as a regularizer. sklearn.Ridge() Linear least squares with L2 regularization. Note: This exercise is auto-graded and you can try multiple attempts. In [0]: # Import necessary libraries % matplotlib inline import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.linear_model import Lasso from sklearn.linear_model import Ridge from sklearn.preprocessing import PolynomialFeatures In [0]: # Helper code to alter plot properties large = 22 ; med = 16 ; small = 10 params = { 'axes.titlesize' : large , 'legend.fontsize' : med , 'figure.figsize' : ( 16 , 10 ), 'axes.labelsize' : med , 'axes.titlesize' : med , 'axes.linewidth' : 2 , 'xtick.labelsize' : med , 'ytick.labelsize' : med , 'figure.titlesize' : large } plt . style . use ( 'seaborn-white' ) plt . rcParams . update ( params ) % matplotlib inline In [0]: # Read the file \"bacteria_train.csv\" as a dataframe df = pd . read_csv ( \"bacteria_train.csv\" ) In [0]: # Take a quick look of your dataset df . head () In [0]: # Set the values of 'Spreading_factor' as the predictor x = df [[ ___ ]] # Set the values of 'Perc_population' as the response y = df [ ___ ] In [0]: # Select the degree of the polynomial maxdeg = 4 # Compute the polynomial features on the data x_poly = PolynomialFeatures ( ___ ) . fit_transform ( ___ ) In [0]: # Get a list of 1000 alpha values ranging from 10 to 120 # np.linspace is inclusive by default unlike arange alpha_list = np . linspace ( ___ , ___ , ___ ) In [0]: ### edTest(test_ridge_fit) ### # Make an empty list called coeff_list to store the coefficients of each model coeff_list = [] # Loop over all alpha values for i in alpha_list : # Initialize a Ridge regression object with the current alpha value # and set normalize as True ridge_reg = Ridge ( alpha = ___ , normalize = ___ ) # Fit on the transformed data ridge_reg . fit ( ___ , ___ ) # Append the coeff_list with the coefficients of the trained model coeff_list . append ( ___ ) In [0]: # Take the transpose of the list to get the variation in the # coefficient values per degree trend = np . array ( coeff_list ) . T In [0]: # Helper code to plot the variation of the coefficients as per the alpha value # Just adding some nice colors. make sure to comment this cell out if you plan to use degree more than 7 colors = [ '#5059E8' , '#9FC131FF' , '#D91C1C' , '#9400D3' , '#FF2F92' , '#336600' , 'black' ] fig , ax = plt . subplots ( figsize = ( 10 , 6 )) for i in range ( maxdeg ): ax . plot ( alpha_list , np . abs ( trend [ i + 1 ]), color = colors [ i ], alpha = 0.9 , label = f 'Degree { i + 1 } ' , lw = 2.2 ) ax . legend ( loc = 'best' , fontsize = 10 ) ax . set_xlabel ( r '$\\alpha$ values' , fontsize = 20 ) ax . set_ylabel ( r '$\\beta$ values' , fontsize = 20 ) fig . suptitle ( r 'Ridge ($L_2$) Regression' ) plt . show (); Compare the results of Ridge regression with the Lasso variant In [0]: # Select a list of 1000 alpha values ranging from 1e-4 to 1e-1 alpha_list = np . linspace ( ___ , ___ , ___ ) In [0]: ### edTest(test_lasso_fit) ### # Initialize a list called to store the alpha value of each model coeff_list = [] # Loop over all the alpha values for i in alpha_list : # Initialize a Lasso regression model with the current alpha # Set normalize as True lasso_reg = Lasso ( alpha = ___ , max_iter = 250000 , normalize = ___ ) # Fit on the transformed data lasso_reg . fit ( ___ , ___ ) # Append the coeff_list with the coefficients of the model coeff_list . append ( ___ ) In [0]: # Get the transpose of the list to get the variation in the # coefficient values per degree trend = np . array ( coeff_list ) . T In [0]: # Helper code below to plot the variation of the coefficients as per the alpha value colors = [ '#5059E8' , '#9FC131FF' , '#D91C1C' , '#9400D3' , '#FF2F92' , '#336600' , 'black' ] fig , ax = plt . subplots ( figsize = ( 10 , 6 )) for i in range ( maxdeg ): ax . plot ( alpha_list , np . abs ( trend [ i + 1 ]), color = colors [ i ], alpha = 0.9 , label = f 'Degree { i + 1 } ' , lw = 2 ) ax . legend ( loc = 'best' , fontsize = 10 ) ax . set_xlabel ( r '$\\alpha$ values' , fontsize = 20 ) ax . set_ylabel ( r '$\\beta$ values' , fontsize = 20 ) fig . suptitle ( r 'Lasso ($L_1$) Regression' ) plt . show ();","tags":"lectures","url":"lectures/lecture06/notebook-4/"},{"title":"Lecture 6: Regularization Ridge and Lasso Regression","text":"reg_with_cv Title : Exercise: Regularization with Cross-validation Description : The aim of this exercise is to understand regularization with cross-validation. Data Description: Instructions: Initialising the required parameters for this exercise. This can be viewed in the scaffold. Read the data file polynomial50.csv and assign the predictor and response variables. Use the helper code to visualise the data. Define a function reg_with_validation that performs Ridge regularization by taking a random_state parameter. Split the data into train and validation sets by specifying the random_state. Compute the polynomial features for the train and validation sets. Run a loop for the alpha values. Within the loop: Initialise the Ridge regression model with the specified alpha. Fit the model on the training data and predict and on the train and validation set. Compute the MSE of the train and validation prediction. Store these values in lists. Run reg_with_validation for varying random states and plot a graph that depicts the best alpha value and the best MSE. The graph will be similar to the one given above. Define a function reg_with_cross_validation that performs Ridge regularization with cross-validation by taking a random_state parameter. Sample the data using the specified random state. Assign the predictor and response variables using the sampled data. Run a loop for the alpha values. Within the loop: Initialise the Ridge regression model with the specified alpha. Fit the model on the entire data and using cross-validation with 5 folds. Get the train and validation MSEs by taking their mean. Store these values in lists. Run reg_with_cross_validation for varying random states and plot a graph that depicts the best alpha value and the best MSE. Use the helper code given to print your best MSEs in the case of simple validation and cross-validation for different random states. Hints: df.sample() Returns a random sample of items from an axis of the object. sklearn.cross_validate() Evaluate metric(by cross-validation and also record fit/score times. np.mean() Compute the arithmetic mean along the specified axis. sklearn.RidgeRegression() Linear least squares with l2 regularization. sklearn.fit() Fit Ridge egression model. sklearn.predict() Predict using the linear model. sklearn.mean_squared_error() Mean squared error regression loss. sklearn.PolynomialFeatures() Generate polynomial and interaction features. sklearn.fit_transform() Fit to data, then transform it. In [0]: # Import necessary libraries import numpy as np import pandas as pd import matplotlib.pyplot as plt from prettytable import PrettyTable from sklearn.linear_model import Ridge from sklearn.metrics import mean_squared_error from sklearn.model_selection import cross_validate from sklearn.model_selection import train_test_split from sklearn.preprocessing import PolynomialFeatures % matplotlib inline In [0]: # Initialising required parameters # The list of random states ran_state = [ 0 , 10 , 21 , 42 , 66 , 109 , 310 , 1969 ] # The list of alpha for regularization alphas = [ 1e-7 , 1e-5 , 1e-3 , 0.01 , 0.1 , 1 ] # The degree of the polynomial degree = 30 In [0]: # Read the file 'polynomial50.csv' as a dataframe df = pd . read_csv ( 'polynomial50.csv' ) # Assign the values of the 'x' column as the predictor x = df [[ 'x' ]] . values # Assign the values of the 'y' column as the response y = df [ 'y' ] . values # Also assign the true value of the function (column 'f') to the variable f f = df [ 'f' ] . values In [0]: # Helper code below to visualise the distribution of the x, y values & also the value of the true function f fig , ax = plt . subplots () # Plot x vs y values ax . plot ( x , y , 'o' , label = 'Observed values' , markersize = 10 , color = 'Darkblue' ) # Plot x vs true function value ax . plot ( x , f , 'k-' , label = 'True function' , linewidth = 4 , color = '#9FC131FF' ) ax . legend ( loc = 'best' ); ax . set_xlabel ( 'Predictor - $X$' , fontsize = 16 ) ax . set_ylabel ( 'Response - $Y$' , fontsize = 16 ) ax . set_title ( 'Predictor vs Response plot' , fontsize = 16 ) plt . show (); In [0]: # Function to perform regularization with simple validation def reg_with_validation ( rs ): # Split the data into train and validation sets with train size # as 80% and random_state as the value given as the function parameter x_train , x_val , y_train , y_val = train_test_split ( x , y , train_size = 0.8 , random_state = rs ) # Create two lists for training and validation error training_error , validation_error = [],[] # Compute the polynomial features for the train and validation sets x_poly_train = ___ x_poly_val = ___ # Run a loop for all alpha values for alpha in alphas : # Initialise a Ridge regression model by specifying the current # alpha and with fit_intercept=False ridge_reg = ___ # Fit on the modified training data ___ # Predict on the training set y_train_pred = ___ # Predict on the validation set y_val_pred = ___ # Compute the training and validation mean squared errors mse_train = ___ mse_val = ___ # Append the MSEs to their respective lists training_error . append ( mse_train ) validation_error . append ( mse_val ) # Return the train and validation MSE return training_error , validation_error In [0]: ### edTest(test_validation) ### # Initialise a list to store the best alpha using simple validation for varying random states best_alpha = [] # Run a loop for different random_states for i in range ( len ( ran_state )): # Get the train and validation error by calling the # function reg_with_validation training_error , validation_error = ___ # Get the best mse from the validation_error list best_mse = ___ # Get the best alpha value based on the best mse best_parameter = ___ # Append the best alpha to the list best_alpha . append ( best_parameter ) # Use the helper code given below to plot the graphs fig , ax = plt . subplots ( figsize = ( 6 , 4 )) # Plot the training errors for each alpha value ax . plot ( alphas , training_error , 's--' , label = 'Training error' , color = 'Darkblue' , linewidth = 2 ) # Plot the validation errors for each alpha value ax . plot ( alphas , validation_error , 's-' , label = 'Validation error' , color = '#9FC131FF' , linewidth = 2 ) # Draw a vertical line at the best parameter ax . axvline ( best_parameter , 0 , 0.5 , color = 'r' , label = f 'Min validation error at alpha = { best_parameter } ' ) ax . set_xlabel ( 'Value of Alpha' , fontsize = 15 ) ax . set_ylabel ( 'Mean Squared Error' , fontsize = 15 ) ax . set_ylim ([ 0 , 0.010 ]) ax . legend ( loc = 'upper left' , fontsize = 16 ) bm = round ( best_mse , 5 ) ax . set_title ( f 'Best alpha is { best_parameter } with mse { bm } ' , fontsize = 16 ) ax . set_xscale ( 'log' ) plt . tight_layout () plt . show () In [0]: # Function to perform regularization with cross validation def reg_with_cross_validation ( rs ): # Sample the data to get different splits using the random state df_new = ___ # Assign the values of the 'x' column as the predictor from your sampled dataframe x = df_new [[ 'x' ]] . values # Assign the values of the 'y' column as the response from your sampled dataframe y = df_new [ 'y' ] . values # Create two lists for training and validation error training_error , validation_error = [],[] # Compute the polynomial features on the entire data x_poly = ___ # Run a loop for all alpha values for alpha in alphas : # Initialise a Ridge regression model by specifying the alpha value and with fit_intercept=False ridge_reg = ___ # Perform cross validation on the modified data with neg_mean_squared_error as the scoring parameter and cv=5 # Set return_train_score to True ridge_cv = ___ # Compute the training and validation errors got after cross validation mse_train = ___ mse_val = ___ # Append the MSEs to their respective lists training_error . append ( mse_train ) validation_error . append ( mse_val ) # Return the train and validation MSE return training_error , validation_error In [0]: ### edTest(test_cross_validation) ### # Initialise a list to store the best alpha using cross validation for varying random states best_cv_alpha = [] # Run a loop for different random_states for i in range ( len ( ran_state )): # Get the train and validation error by calling the function reg_with_cross_validation training_error , validation_error = ___ # Get the best mse from the validation_error list best_mse = ___ # Get the best alpha value based on the best mse best_parameter = ___ # Append the best alpha to the list best_cv_alpha . append ( ___ ) # Use the helper code given below to plot the graphs fig , ax = plt . subplots ( figsize = ( 6 , 4 )) # Plot the training errors for each alpha value ax . plot ( alphas , training_error , 's--' , label = 'Training error' , color = 'Darkblue' , linewidth = 2 ) # Plot the validation errors for each alpha value ax . plot ( alphas , validation_error , 's-' , label = 'Validation error' , color = '#9FC131FF' , linewidth = 2 ) # Draw a vertical line at the best parameter ax . axvline ( best_parameter , 0 , 0.5 , color = 'r' , label = f 'Min validation error at alpha = { best_parameter } ' ) ax . set_xlabel ( 'Value of Alpha' , fontsize = 15 ) ax . set_ylabel ( 'Mean Squared Error' , fontsize = 15 ) ax . legend ( loc = 'upper left' , fontsize = 16 ) bm = round ( best_mse , 5 ) ax . set_title ( f 'Best alpha is { best_parameter } with mse { bm } ' , fontsize = 16 ) ax . set_xscale ( 'log' ) plt . tight_layout () In [0]: # Helper code to print your findings pt = PrettyTable () pt . field_names = [ \"Random State\" , \"Best Alpha with Validation\" , \"Best Alpha with Cross-Validation\" ] for i in range ( 6 ): pt . add_row ([ ran_state [ i ], best_alpha [ i ], best_cv_alpha [ i ]]) print ( pt ) ⏸ Comment on the results of regularization with simple validation and cross-validation after changing the random state and alpha values. In [0]: ### edTest(test_chow1) ### # Submit an answer choice as a string below answer1 = '___'","tags":"lectures","url":"lectures/lecture06/notebook-5/"},{"title":"Lecture 5: Model Selection and Cross Validation","text":"Slides Lecture 5 : Model Selection (PDF) Lecture 5 : Model Selection with Cross Validation (PDF) Exercises Lecture 5: Exercise: Exercise: Best Degree of Polynomial with Train and Validation sets [Notebook] Lecture 5: Exercise: Best Degree of Polynomial using Cross-validation [Notebook]","tags":"lectures","url":"lectures/lecture05/"},{"title":"Lecture 5: Model Selection and Cross Validation","text":"s2_exc1_challenge Title : Exercise: Best Degree of Polynomial with Train and Validation sets Description : The aim of this exercise is to find the best degree of polynomial based on the MSE values. Further, plot the train and validation error graphs as a function of degree of the polynomial as shown below. Data Description: Instructions: Read the dataset and split into train and validation sets. Select a max degree value for the polynomial model. Fit a polynomial regression model on the training data for each degree and predict on the validation data. Compute the train and validation error as MSE values and store in separate lists. Find out the best degree of the model. Plot the train and validation errors for each degree. Hints: pd.read_csv(filename) Returns a pandas dataframe containing the data and labels from the file data sklearn.train_test_split() Splits the data into random train and test subsets sklearn.PolynomialFeatures() Generates a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree sklearn.fit_transform() Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X sklearn.LinearRegression(fit_intercept=False) LinearRegression fits a linear model with no intercept calculation sklearn.fit() Fits the linear model to the training data sklearn.predict() Predict using the linear model plt.subplots() Create a figure and a set of subplots Note: This exercise is auto-graded and you can try multiple attempts. In [1]: # Import necessary libraries import operator import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.metrics import mean_squared_error from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.preprocessing import PolynomialFeatures % matplotlib inline Reading the dataset In [2]: # Read the file \"dataset.csv\" as a Pandas dataframe df = pd . read_csv ( \"dataset.csv\" ) # Take a quick look at the dataset df . head () Out[2]: x y 0 4.98 24.0 1 9.14 21.6 2 4.03 34.7 3 2.94 33.4 4 5.33 36.2 In [5]: # Assign the values of the x and y column values to the # predictor and response variables x = df [[ 'x' ]] . values y = df . y . values Train-validation split In [25]: ### edTest(test_random) ### # Split the dataset into train and validation sets with 75% training set # Set random_state=1 x_train , x_val , y_train , y_val = train_test_split ( x , y , train_size = 0.75 , random_state = 1 ) Computing the train and validation error in terms of MSE In [26]: ### edTest(test_regression) ### # To iterate over the range, select the maximum degree of the polynomial maxdeg = 10 # Create two empty lists to store training and validation MSEs training_error , validation_error = [],[] # Loop through the degrees of the polynomial to create different models for d in range ( maxdeg ): # Compute the polynomial features for the current degree # for the train set x_poly_train = PolynomialFeatures ( degree = d ) . fit_transform ( x_train ) # Compute the polynomial features for the validation set x_poly_val = PolynomialFeatures ( degree = d ) . fit_transform ( x_val ) # Initialize a linear regression model lreg = LinearRegression ( fit_intercept = False ) # Fit the model on the train data lreg . fit ( x_poly_train , y_train ) # Use the trained model to predict on the transformed train data y_train_pred = lreg . predict ( x_poly_train ) # Use the trained model to predict on the transformed validation data y_val_pred = lreg . predict ( x_poly_val ) # Compute the MSE on the train predictions training_error . append ( mean_squared_error ( y_train , y_train_pred )) # Compute the MSE on the validation predictions validation_error . append ( mean_squared_error ( y_val , y_val_pred )) Finding the best degree In [27]: ### edTest(test_best_degree) ### # Helper code to compute the best degree, which is the model # with the lowest validation error min_mse = min ( validation_error ) best_degree = validation_error . index ( min_mse ) # Print the degree of the best model computed above print ( \"The best degree of the model is\" , best_degree ) The best degree of the model is 2 Plotting the error graph In [28]: # Plot the errors as a function of increasing d value to visualise the training # and testing errors fig , ax = plt . subplots () # Plot the training error with labels ax . plot ( np . arange ( 0 , maxdeg ), training_error , label = \"Training Error\" ) # Plot the validation error with labels ax . plot ( np . arange ( 0 , maxdeg ), validation_error , label = \"Validation Error\" ) # Set the plot labels and legends ax . set_xlabel ( 'Degree of Polynomial' ) ax . set_ylabel ( 'Mean Squared Error' ) ax . legend ( loc = 'best' ) ax . set_yscale ( 'log' ) plt . show (); ⏸ If you run the exercise with a random state of 0, do you notice any change? What would you attribute this change to? In [29]: ### edTest(test_chow1) ### # Submit an answer choice as a string below answer1 = 'No good, it change a lot' In [0]:","tags":"lectures","url":"lectures/lecture05/notebook-1/"},{"title":"Lecture 5: Model Selection and Cross Validation","text":"s2_exc2_challenge Title : Exercise: Best Degree of Polynomial using Cross-validation Description : The aim of this exercise is to find the best degree of polynomial based on the MSE values. Further, plot the train and cross-validation error graphs as shown below. Data Description: Instructions: Read the dataset and split into train and validation sets. Select a max degree value for the polynomial model. For each degree: Perform k-fold cross validation Fit a polynomial regression model for each degree on the training data and predict on the validation data Compute the train, validation and cross-validation error as MSE values and store them in separate lists. Print the best degree of the model for both validation and cross-validation approaches. Plot the train and cross-validation errors for each degree. Hints: pd.read_csv(filename)</a> Returns a pandas dataframe containing the data and labels from the file data. sklearn.train_test_split() Splits the data into random train and test subsets. sklearn.PolynomialFeatures() Generates a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. sklearn.cross_validate() Evaluate metric(s) by cross-validation and also record fit/score times. sklearn.fit_transform() Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. sklearn.LinearRegression(fit_intercept=False) LinearRegression fits a linear model. sklearn.fit() Fits the linear model to the training data. sklearn.predict() Predict using the linear model. plt.subplots() Create a figure and a set of subplots. operator.itemgetter() Return a callable object that fetches item from its operand. zip() Makes an iterator that aggregates elements from each of the iterables. Note: This exercise is auto-graded and you can try multiple attempts. In [39]: # Import necessary libraries % matplotlib inline import operator import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.metrics import mean_squared_error from sklearn.linear_model import LinearRegression from sklearn.model_selection import cross_validate from sklearn.model_selection import train_test_split from sklearn.preprocessing import PolynomialFeatures Reading the dataset In [40]: # Read the file \"dataset.csv\" as a Pandas dataframe df = pd . read_csv ( \"dataset.csv\" ) In [41]: # Assign the values of column x as the predictor x = df [[ 'x' ]] . values # Assign the values of column y as the response variable y = df . y . values Train-validation split In [42]: ### edTest(test_random) ### # Split the data into train and validation sets with 75% for training # and with a random_state=1 x_train , x_val , y_train , y_val = train_test_split ( ___ ) Computing the MSE In [43]: ### edTest(test_regression) ### # To iterate over the range, select the maximum degree of the polynomial maxdeg = 10 # Create three empty lists to store training, validation and cross-validation MSEs training_error , validation_error , cross_validation_error = [],[],[] # Loop through the degrees of the polynomial for d in range ( ___ ): # Compute the polynomial features for the entire data x_poly = PolynomialFeatures ( ___ ) . fit_transform ( ___ ) # Compute the polynomial features for the train data x_poly_train = PolynomialFeatures ( ___ ) . fit_transform ( ___ ) # Compute the polynomial features for the validation data x_poly_val = PolynomialFeatures ( ___ ) . fit_transform ( ___ ) # Initialize a Linear Regression object lreg = LinearRegression () # Fit model on the training set lreg . fit ( ___ ) # Predict on the training data y_train_pred = lreg . predict ( ___ ) # Predict on the validation set y_val_pred = lreg . predict ( ___ ) # Compute the mse on the train data training_error . append ( mean_squared_error ( ___ )) # Compute the mse on the validation data validation_error . append ( mean_squared_error ( ___ )) # Perform cross-validation on the entire data with 10 folds and # get the mse_scores mse_score = cross_validate ( ___ ) # Compute the mean of the cross validation error and store in list # Remember to take into account the sign of the MSE metric returned by the cross_validate function cross_validation_error . append ( ___ ) Finding the best degree In [0]: ### edTest(test_best_degree) ### # Get the best degree associated with the lowest validation error min_mse = min ( ___ ) best_degree = validation_error . index ( ___ ) # Get the best degree associated with the lowest cross-validation error min_cross_val_mse = min ( ___ ) best_cross_val_degree = cross_validation_error . index ( ___ ) # Print the values print ( \"The best degree of the model using validation is\" , best_degree ) print ( \"The best degree of the model using cross-validation is\" , best_cross_val_degree ) Plotting the error graph In [0]: # Plot the errors as a function of increasing d value to visualise the training and validation errors fig , ax = plt . subplots ( 1 , 2 , figsize = ( 16 , 8 )) # Plot the training error with labels ax [ 0 ] . plot ( range ( maxdeg ), np . log ( training_error ), label = 'Training error' , linewidth = 3 , color = '#FF7E79' , alpha = 0.4 ) # Plot the validation error with labels ax [ 0 ] . plot ( range ( maxdeg ), np . log ( validation_error ), label = 'Validation error' , linewidth = 3 , color = \"#007D66\" , alpha = 0.4 ) # Plot the training error with labels ax [ 1 ] . plot ( range ( maxdeg ), np . log ( training_error ), label = 'Training error' , linewidth = 3 , color = '#FF7E79' , alpha = 0.4 ) # Plot the cross-validation error with labels ax [ 1 ] . plot ( range ( maxdeg ), np . log ( cross_validation_error ), label = 'Cross-Validation error' , linewidth = 3 , color = \"#007D66\" , alpha = 0.4 ) # Set the plot labels and legends ax [ 0 ] . set_xlabel ( 'Degree of Polynomial' , fontsize = 12 ) ax [ 0 ] . set_ylabel ( 'Log Mean Squared Error' , fontsize = 12 ) ax [ 0 ] . set_title ( \"Log of validation error as a function of degree\" ) ax [ 1 ] . set_xlabel ( 'Degree of Polynomial' , fontsize = 12 ) ax [ 1 ] . set_ylabel ( 'Log Mean Squared Error' , fontsize = 12 ) ax [ 1 ] . set_title ( \"Log of CV error as a function of degree\" ) ax [ 0 ] . legend () ax [ 1 ] . legend () plt . show (); ⏸ If you run the exercise with a random state of 0, do you notice any change? What conclusion can you draw from this experiment? In [0]: ### edTest(test_chow1) ### # Submit an answer choice as a string below answer1 = '___'","tags":"lectures","url":"lectures/lecture05/notebook-2/"},{"title":"Lab 3","text":"cs109a_lab_03_students CS109A Introduction to Data Science Lab 3: kNN and Linear Regression Student's version Harvard University Fall 2021 Instructors : Pavlos Protopapas and Natesh Pillai Lab Team : Marios Mattheakis, Hayden Joy, Chris Gumb, and Eleni Kaxiras Authors : Marios Mattheakis and Eleni Kaxiras In [1]: #RUN THIS CELL import requests from IPython.core.display import HTML styles = requests . get ( \"http://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\" ) . text HTML ( styles ) Out[1]: Learning Objectives For this lab, our goal is for you to try out your first models, and specifically k-Nearest Neighbors (kNN) and Linear Regression. In the course thus far, we have discussed some aspects of dealing with data, including scraping data from the web, organizing it using dictionaries and Pandas dataframes, and visualizing it using Matplotlib. Now we're moving on to data modeling! We make models to fit the training data and predict on the test data . We will see what those two types of data are. Specifically, our learning objectives are: 1 - Performing exploratory data analysis (EDA) on our dataset . 2 - Splitting this dataset into a training and test set . 3 - Training you first models (kNN and Linear Regression) on your data using the sklearn library . Using these models to understand relationships between the response variable and the predictors (also called features). Evaluating model performance on the test set using metrics such as $R&#94;2$ and MSE. In [2]: import numpy as np import pandas as pd import matplotlib import matplotlib.pyplot as plt import seaborn as sns % matplotlib inline from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsRegressor from sklearn.linear_model import LinearRegression from sklearn import metrics from pandas.api.types import CategoricalDtype from sklearn.metrics import mean_squared_error from sklearn.preprocessing import MinMaxScaler from statsmodels.api import OLS import statsmodels.api as sm # uncomment to display full dataframe # pd.set_option('display.max_rows', None) 1 - The Bikeshare dataset and preliminary EDA Our dataset was collected by the Capital Bikeshare program in Washington D.C ( source: UCI repository ). It contains over two years of data on the total number of bike rentals per day, as well as 10 attributes describing the day and its weather (see below for a description). The dataset is provided in the file bikeshare.csv . The task is to build a regression model to predict the total number of bike rentals in a given day (known as the response variable) based on attributes about the day (known as the features). Such a forecasting model would be useful in planning the number of bikes that need to be available in the system on any given day, and also in monitoring traffic in the city. Description of variables season (1:winter, 2:spring, 3:summer, 4:fall) month (1 through 12, with 1 denoting Jan) holiday (1 = the day is a holiday, 0 = otherwise, extracted from https://dchr.dc.gov/page/holiday-schedules ) day_of_week (0 through 6, with 0 denoting Sunday) workingday (1 = the day is neither a holiday or weekend, 0 = otherwise) weather 1: Clear, Few clouds, Partly cloudy, Partly cloudy 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog temp (temperature in Celsius) atemp (apparent, or relative outdoor, or real feel temperature, in Celsius) humidity (relative humidity) windspeed (wind speed) count (response variable i.e. total number of bike rentals on the day) Load and inspect the BikeShare dataset In [3]: bikeshare = pd . read_csv ( 'data/bikeshare.csv' ) print ( f 'Length of Dataset: { bikeshare . shape [ 0 ] } ' ) display ( bikeshare . head ()) Length of Dataset: 731 season month holiday day_of_week workingday weather temp atemp humidity windspeed count 0 2 5 0 2 1 2 24.0 26.0 76.58 0.12 6073 1 4 12 0 2 1 1 15.0 19.0 73.38 0.17 6606 2 2 6 0 4 1 1 26.0 28.0 56.96 0.25 7363 3 4 12 0 0 0 1 0.0 4.0 58.63 0.17 2431 4 3 9 0 3 1 3 23.0 23.0 91.71 0.10 1996 In [4]: bikeshare . columns Out[4]: Index(['season', 'month', 'holiday', 'day_of_week', 'workingday', 'weather', 'temp', 'atemp', 'humidity', 'windspeed', 'count'], dtype='object') Let's check for missing values It's always a good practice to check the types of your features and make sure they are what they are supposed to be. If not, we can cast them to the correct type using .astype() In [5]: bikeshare . isnull () . sum () Out[5]: season 0 month 0 holiday 0 day_of_week 0 workingday 0 weather 0 temp 0 atemp 0 humidity 0 windspeed 0 count 0 dtype: int64 Let's check the types of the features It's always a good practice to check the types of your features and make sure they are what they are supposed to be. If not, we can cast them to the correct type using .astype() In [6]: bikeshare . dtypes Out[6]: season int64 month int64 holiday int64 day_of_week int64 workingday int64 weather int64 temp float64 atemp float64 humidity float64 windspeed float64 count int64 dtype: object In [7]: ## here we designate the types that we want # categorical variables cat_type = CategoricalDtype ( ordered = False ) cat_vars = [ 'weather' , 'day_of_week' , 'holiday' , 'season' , 'month' , 'workingday' , 'weather' ] for var in cat_vars : bikeshare [ var ] = bikeshare [ var ] . astype ( cat_type ) # integer variables int_vars = [ 'count' ] for var in int_vars : bikeshare [ var ] = bikeshare [ var ] . astype ( 'int' ) In [8]: bikeshare . dtypes Out[8]: season category month category holiday category day_of_week category workingday category weather category temp float64 atemp float64 humidity float64 windspeed float64 count int64 dtype: object In [9]: ## OPTIONAL for good housekeeping - make a dict with the feature descriptions names_dict = { 'season' : 'Season (1 for spring, 2 for summer, 3 for fall, 4 for winter)' , 'month' : 'Month of the year (1 through 12, with 1 denoting Jan)' , 'holiday' : '1 = the day is a holiday, 0 = otherwise' , 'count' : 'Total number of bike rentals on the day' , 'temp' : 'Temperature (degrees C)' } ##... fill out the rest of the features ## useful in explaining the features in plots, tables, etc. print ( names_dict [ 'month' ]) Month of the year (1 through 12, with 1 denoting Jan) How many categories does day_of_the_week have? In [10]: # see how many categories we have bikeshare [ 'day_of_week' ] . unique () Out[10]: [2, 4, 0, 3, 6, 5, 1] Categories (7, int64): [0, 1, 2, 3, 4, 5, 6] In [11]: bikeshare [ bikeshare . season == 4 ] . shape Out[11]: (178, 11) Example of why .groupby() is powerfull You can think of .groupby() and agg (aggregate) as a way to flatten a part of the DataFrame. Is we group by month and then aggreegate the values for each month we effectively flatten all the rows that are the same month together. When you flatten the DataFrame you need to specify how to aggregate the values, e.g. sum them up, count them, etc? DataFrame.groupby(by=None, axis=0, level=None, as_index=True, sort=True, group_keys=True, squeeze=NoDefault.no_default, observed=False, dropna=True) For more see: https://pandas.pydata.org/pandas-docs/stable/reference/groupby.html In [12]: bikeshare . sort_values ([ 'season' ], ascending = False ) . groupby ( 'season' ) . head ( 2 ) Out[12]: season month holiday day_of_week workingday weather temp atemp humidity windspeed count 595 4 10 0 1 1 2 18.0 21.0 64.92 0.09 6778 211 4 12 0 1 1 2 9.0 14.0 82.71 0.06 3811 280 3 8 0 6 0 2 29.0 29.0 73.29 0.21 6299 520 3 8 0 5 1 2 29.0 30.0 61.50 0.07 7582 552 2 5 0 1 1 1 21.0 24.0 78.79 0.13 3958 652 2 4 0 4 1 2 16.0 19.0 75.67 0.18 5026 478 1 3 0 5 1 1 19.0 22.0 52.52 0.23 3239 87 1 2 1 1 0 2 4.0 7.0 60.50 0.31 1107 In [13]: ### very useful import calendar calendar . month_name [ 1 ] Out[13]: 'January' In [14]: bikeshare [[ 'season' , 'month' , 'day_of_week' , 'temp' ]] . \\ groupby ([ 'month' ]) . agg ({ 'day_of_week' : np . size , 'season' : np . unique , 'temp' : np . mean }) Out[14]: day_of_week season temp month 1 62 1 -0.080645 2 57 1 3.912281 3 62 [1, 2] 9.580645 4 60 2 14.583333 5 62 2 22.532258 6 60 [2, 3] 28.150000 7 62 3 32.580645 8 62 3 29.629032 9 60 [3, 4] 23.850000 10 62 4 15.580645 11 60 4 8.316667 12 62 [1, 4] 5.451613 Plot the count of bike rentals by month In [15]: ave_rentals_month = bikeshare . groupby ( 'month' ) . mean ()[ 'count' ] ave_rentals_month Out[15]: month 1 2498.306452 2 2824.315789 3 3822.241935 4 4348.600000 5 5268.790323 6 5547.033333 7 5563.677419 8 5584.887097 9 5653.316667 10 5199.225806 11 4454.633333 12 3781.741935 Name: count, dtype: float64 In [16]: import matplotlib.ticker as ticker fontsize = 15 fig , ax = plt . subplots ( figsize = [ 8 , 5 ]) ax . plot ( ave_rentals_month , 'bo-' , linewidth = 1 , markersize = 7 ) # labels # ax.set_ylabel('Count (x 1000)', fontsize=fontsize) # ax.set_xlabel('Month', fontsize=fontsize) ## for more meaningful names ax . set_xlabel ( names_dict [ 'month' ], fontsize = fontsize ) ax . set_ylabel ( names_dict [ 'count' ], fontsize = fontsize ) ax . set_title ( 'Bikeshare Rentals per Month' , fontsize = fontsize + 3 ) ax . yaxis . set_major_formatter ( lambda y , pos : y / 1000 ) ax . xaxis . set_major_locator ( ticker . MaxNLocator ( 12 )) ax . xaxis . set_major_formatter ( lambda x , pos : calendar . month_name [ int ( x )]) ax . tick_params ( axis = 'x' , rotation = 45 , length = 6 , width = 1 ) plt . xlim ([ 0 , 12 ]) plt . show (); /usr/lib/python3.9/site-packages/matplotlib/axes/_base.py:507: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version. Convert to a numpy array before indexing instead. x = x[:, np.newaxis] In [17]: bikeshare . temp . unique () . shape [ 0 ] Out[17]: 50 TOP 2 - Split the dataset into a training set and a test set Now that we have an idea of what the data looks like, we want to predict the number of rentals on a given day, i.e. the value of count . We will split the data randomly into a training and a testing set. We will use the train_test_split function from sklearn ( Scikit learn (sklearn) ). What is the need for training and testing data sets? The training set will be used to train the model, while the testing set will be used to quantify how well that model does on data it has never seen before. By setting random_state = 42 we ensure reproducibility of your results. Let us first call a function that will randomly split the data up into a 70-30 split, with 70% of the data going into the training set: from sklearn.model_selection import train_test_split In [18]: # set the response variable response = 'count' In [19]: # split whole data set train_data , test_data = train_test_split ( bikeshare , test_size = 0.30 , random_state = 42 ) # then create the training feature set (X_train) and the response vector (y_train) X_train , y_train = train_data . drop ( response , axis = 1 ), train_data [ response ] X_test , y_test = test_data . drop ( response , axis = 1 ), test_data [ response ] print ( X_train . shape , y_train . shape , X_test . shape , y_test . shape ) print ( f 'Training set = { train_data . shape [ 0 ] } \\ ( { ( 100 * train_data . shape [ 0 ] / bikeshare . shape [ 0 ]) : .2f } % of the total dataset)' ) print ( f 'Testing set = { test_data . shape [ 0 ] } \\ ( { ( 100 * test_data . shape [ 0 ] / bikeshare . shape [ 0 ]) : .2f } % of the total dataset)' ) (511, 10) (511,) (220, 10) (220,) Training set = 511 (69.90% of the total dataset) Testing set = 220 (30.10% of the total dataset) In [20]: # plot split results plt . figure ( figsize = [ 8 , 5 ]) sns . scatterplot ( data = train_data , x = 'temp' , y = 'count' , label = 'train' ) sns . scatterplot ( data = test_data , x = 'temp' , y = 'count' , label = 'test' ) plt . xlabel ( names_dict [ 'temp' ]) plt . ylabel ( names_dict [ 'count' ]) plt . title ( \"Data distribution for a 70/30 dataset split\" ) plt . legend () plt . show () TEAM ACTIVITY 1: Split the bikeshare data into X_train, y_train, X_test, and y_test but include only one feature (\"windspeed\") In [37]: # your code here temp_df = bikeshare [[ \"temp\" , response ]] train_data , test_data = train_test_split ( temp_df , test_size = 0.30 , random_state = 42 ) Out[37]: temp count 0 24.0 6073 1 15.0 6606 2 26.0 7363 3 0.0 2431 4 23.0 1996 In [45]: X_train , X_test , y_train , y_test = train_test_split ( bikeshare [[ \"temp\" ]], bikeshare . temp , test_size = 0.33 ) print ( X_train . shape , X_test . shape , y_train . shape , y_test . shape ) (489, 1) (242, 1) (489,) (242,) TOP 3 - Building a model with sklearn We will work with scikit-learn (sklearn) . import sklearn According to its website, Scikit-learn is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection and evaluation, and many other utilities. Why do we need to add a constant in our simple linear regression model? Let's say we a data set of two obsevations with one predictor and one response variable each. We would then have the following two equations if we run a simple linear regression model. $$y_1=\\beta_0 + \\beta_1*x_1$$ $$y_2=\\beta_0 + \\beta_1*x_2$$ For simplicity and calculation efficiency we want to \"absorb\" the constant $b_0$ into an array with $b_1$ so we have only multiplication. To do this we introduce the constant ${x}&#94;0=1$ $$y_1=\\beta_0*{x_1}&#94;0 + \\beta_1*x_1$$ $$y_2=\\beta_0 * {x_2}&#94;0 + \\beta_1*x_2$$ That becomes: $$y_1=\\beta_0*1 + \\beta_1*x_1$$ $$y_2=\\beta_0 * 1 + \\beta_1*x_2$$ In matrix notation: $$ \\left [ \\begin{array}{c} y_1 \\\\ y_2 \\\\ \\end{array} \\right] = \\left [ \\begin{array}{cc} 1& x_1 \\\\ 1 & x_2 \\\\ \\end{array} \\right] \\cdot \\left [ \\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\\\ \\end{array} \\right] $$ sklearn adds the constant for us. Steps to training a model 1 - Break the sets into one containing the predictor(s) and one with the response variable. Do separately for train and test. 2 - Define the Model model = sklearn_model_name(hyper_parameter1 = value1, hyper_parameter2 = value2) 3 - Scale the features (if needed) from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler(feature_range=(0, 1)) X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) X_train = pd.DataFrame(X_train_scaled) X_test = pd.DataFrame(X_test_scaled) 4 - Fit Model model.fit(x_train, y_train) 5 - Get the Predictions y_pred_train = model.predict(X_train) y_pred_test = model.predict(X_test) 6 - Evaluate the Model The metrics that you will use to evaluate the model depend on the task at hand, i.e. Regression or Classification. For Regression, $R&#94;2$ Score and $MSE$ are commonly used, while for Classification, Accuracy (%) is one popular metric. # Return the coefficient of determination R&#94;2 of the prediction r2_train = model.score(y_train, y_pred_train) r2_test = model.score(y_test, y_pred_test) # Calculating MSE on the test set y_pred = model.predict(X_test) MSE = mean_squared_error(y_test, y_pred) 7 - Print Results print(\"Score for Model (Training):\", r2_train) print(\"Score for Model (Testing) :\", r2_test) The scikit-learn library and the shape of things Before diving in, let's discuss more of the details of sklearn . Scikit-learn consists of estimators , Python objects, that implements the methods fit(X, y) and predict() Let's see the structure of scikit-learn needed to make these fits. fit() always takes two arguments: estimator . fit ( Xtrain , ytrain ) We will consider two estimators in this lab: LinearRegression and KNeighborsRegressor . It is very important to understand that Xtrain must be in the form of a 2x2 array with each row corresponding to one sample, and each column corresponding to the feature values for that sample. ytrain on the other hand is a simple array of responses. These are continuous for regression problems. This is the reason we write for e.g., X_train=train_data[['temp']] # two brackets and not train_data['temp'] # one bracket $k$-nearest neighbors regression Using sklearn We will use temp from our available features to predict total bike rental count . In [46]: # set the response variable response = 'count' In [47]: # from sklearn.neighbors import KNeighborsRegressor # split again train_data , test_data = train_test_split ( bikeshare , test_size = 0.30 , random_state = 42 ) # then create the training feature set (X_train) and the response vector (y_train) X_train = train_data [[ 'temp' ]] # note the two brackets y_train = train_data [ response ] # one bracket X_test = test_data [[ 'temp' ]] y_test = test_data [ response ] X_train . shape , y_train . shape , X_test . shape , y_test . shape Out[47]: ((511, 1), (511,), (220, 1), (220,)) In [48]: #from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler ( feature_range = ( 0 , 1 )) X_train_scaled = scaler . fit_transform ( X_train ) X_test_scaled = scaler . transform ( X_test ) In [49]: # from sklearn.neighbors import KNeighborsRegressor # Set kNN hyperparameter: k = 20 # First, we create the classifier object: neighbors = KNeighborsRegressor ( n_neighbors = k ) # Then, we fit the model using x_train as training data and y_train as target values: neighbors . fit ( X_train , y_train ) # Retrieve our predictions: prediction_knn = neighbors . predict ( X_test ) prediction_train_knn = neighbors . predict ( X_train ) # This returns the mean accuracy on the given test data and labels, or in other words, # the R squared value -- A constant model that always predicts the expected value of y, # disregarding the input features, would get a R&#94;2 score of 1. r2_train = neighbors . score ( X_train , y_train ) r2_test = neighbors . score ( X_test , y_test ) mse_test = mean_squared_error ( y_test , prediction_knn ) mse_train = mean_squared_error ( y_train , prediction_train_knn ) print ( f '------ kNN model score for k = { k } ' ) print ( f 'R2 training set: { r2_train : .3f } ' ) print ( f 'R2 testing set: { r2_test : .3f } ' ) print ( f 'MSE training set: { mse_train : .2f } ' ) print ( f 'MSE testing set: { mse_test : .2f } ' ) ------ kNN model score for k = 20 R2 training set: 0.348 R2 testing set: 0.248 MSE training set: 2754139.71 MSE testing set: 2825000.18 In [50]: # SubPlots fig , axes = plt . subplots ( nrows = 1 , ncols = 2 , figsize = ( 20 , 6 )) # train data axes [ 0 ] . set_ylim ([ 0 , 10000 ]) axes [ 0 ] . plot ( train_data [ 'temp' ], train_data [ 'count' ], 'bo' , alpha = 0.5 , label = 'Data' ) sorted_temp = train_data . sort_values ([ 'temp' ]) prediction_knn = neighbors . predict ( sorted_temp [[ 'temp' ]]) axes [ 0 ] . plot ( sorted_temp [ 'temp' ], prediction_knn , 'k-' , linewidth = 5 , markersize = 10 , label = 'Prediction' ) axes [ 0 ] . set_xlabel ( 'Temperature' , fontsize = 15 ) axes [ 0 ] . set_ylabel ( '# of Rentals' , fontsize = 15 ) axes [ 0 ] . set_title ( \"Train Set\" , fontsize = 18 ) axes [ 0 ] . legend ( loc = 'upper right' , fontsize = 12 ) # test data axes [ 1 ] . set_ylim ([ 0 , 10000 ]) axes [ 1 ] . plot ( test_data [ 'temp' ], test_data [ 'count' ], 'r*' , alpha = 0.5 , label = 'Data' ) sorted_temp = test_data . sort_values ([ 'temp' ]) prediction_knn = neighbors . predict ( sorted_temp [[ 'temp' ]]) axes [ 1 ] . plot ( sorted_temp [ 'temp' ], prediction_knn , 'g-' , linewidth = 5 , markersize = 10 , label = 'Prediction' ) axes [ 1 ] . set_xlabel ( 'Temperature' , fontsize = 15 ) axes [ 1 ] . set_ylabel ( '# of Rentals' , fontsize = 15 ) axes [ 1 ] . set_title ( \"Test Set\" , fontsize = 18 ) axes [ 1 ] . legend ( loc = 'upper right' , fontsize = 12 ) fig . suptitle ( \"kNN Regression (k= {} ): Temp vs Rental Count\" . format ( k ), fontsize = 20 ) plt . show () TEAM ACTIVITY 2: Practice using sklearn's kNN regression Let's now try different $k$s for our model. Load a simple dataset sim_data.csv Perform 80-20 train-test split using a random state of 42 Create a function that implements kNN regression with your choice of k (explore a few different k's) Predict on both training and test data For all kNN models generated, plot plot for the test and train set next to one other: Calculate $R&#94;2$ score Hints: dont forget to sort! you can make plot colors more transparent using \"alpha\" and lines thicker using \"linewidth\" In [53]: # read in data data = pd . read_csv ( \"data/sim_data.csv\" ) # drop Unnamed column data . drop ( columns = [ \"Unnamed: 0\" ], inplace = True ) # split into 70/30, random_state=42 sim_train_data , sim_test_data = train_test_split ( data , test_size = 0.3 , random_state = 42 ) def knn_model ( k , train_data , test_data ): # create the classifier object neighbors = KNeighborsRegressor ( n_neighbors = k ) # fit the model using x_train as training data and y_train as target values neighbors . fit ( train_data [[ 'x' ]], train_data [ 'y' ]) sorted_train = train_data . sort_values ([ 'x' ]) sorted_test = test_data . sort_values ([ 'x' ]) # Retreieve our predictions: train_preds = neighbors . predict ( sorted_train [[ 'x' ]]) test_preds = neighbors . predict ( sorted_test [[ 'x' ]]) # find r&#94;2 r2_train = neighbors . score ( train_data [[ 'x' ]], train_data [ 'y' ]) r2_test = neighbors . score ( test_data [[ 'x' ]], test_data [ 'y' ]) print ( f 'R2 training set: { r2_train : .3f } ' ) print ( f 'R2 testing set: { r2_test : .3f } ' ) return sorted_train , sorted_test , train_preds , test_preds , r2_train , r2_test def plot_predictions_same_plot ( k , train_data , test_data , train_preds , test_preds ): fig , axes = plt . subplots ( 1 , 2 , figsize = ( 20 , 6 )) # train data axes [ 0 ] . plot ( train_data [ 'x' ], train_data [ 'y' ], 'bo' , alpha = 0.5 , label = 'Train Set' ) axes [ 0 ] . plot ( train_data [ 'x' ], train_preds , 'k-' , linewidth = 2 , markersize = 10 , label = 'Train Preds' ) axes [ 1 ] . plot ( test_data [ 'x' ], test_data [ 'y' ], 'r*' , alpha = 0.5 , label = 'Test Set' ) axes [ 1 ] . plot ( test_data [ 'x' ], test_preds , 'g-' , linewidth = 2 , markersize = 10 , label = 'Test Preds' ) for i in [ 0 , 1 ]: axes [ i ] . set_xlabel ( 'x' ) axes [ i ] . set_ylabel ( 'y' ) axes [ i ] . set_title ( f 'x vs y for kNN Regression (k= { k } )' ) axes [ i ] . legend () plt . show () fig . suptitle ( 'THIS' , fontsize = 20 ) #plt.show() knn_train_preds = [] knn_test_preds = [] knn_r2_train_scores = [] knn_r2_test_scores = [] list_of_ks = [ 1 , 2 , 4 , 6 , 8 , 10 , 15 , 20 , 30 ] for k in list_of_ks : sim_sorted_train , sim_sorted_test , sim_train_preds , sim_test_preds , knn_r2_train , knn_r2_test = knn_model ( k , sim_train_data , sim_test_data ) plot_predictions_same_plot ( k , sim_sorted_train , sim_sorted_test , sim_train_preds , sim_test_preds ) knn_train_preds . append ( sim_train_preds ) knn_test_preds . append ( sim_test_preds ) knn_r2_train_scores . append ( knn_r2_train ) knn_r2_test_scores . append ( knn_r2_test ) R2 training set: 1.000 R2 testing set: -0.244 R2 training set: 0.601 R2 testing set: 0.212 R2 training set: 0.493 R2 testing set: 0.268 R2 training set: 0.463 R2 testing set: 0.415 R2 training set: 0.437 R2 testing set: 0.360 R2 training set: 0.416 R2 testing set: 0.396 R2 training set: 0.440 R2 testing set: 0.400 R2 training set: 0.425 R2 testing set: 0.394 R2 training set: 0.375 R2 testing set: 0.359 Linear Regression We just went over the kNN prediction method. Now, we will fit the same data using a linear regression model. What is the main difference between a kNN model and linear regression model? Linear regression specifies the model (whatever the data is, the model will fit a linear line) whereas kNN does not specify a model, it just assigns a value based on how close this value is to k points in the training set (based on some closeness metric). Advantages of linear regression models are that they are very fast and yield an exact optimal solution. For a more in-depth discussion on generalized linear models, please see the Advanced Section on this. We will use the same training/testing dataset as before and create linear regression objects. We can do this using sklearn (as we did for kNN) as well as with another package called statsmodels . In [54]: response = 'count' # Label data as X,Y for ease x_train , y_train = train_data [[ 'temp' ]], train_data [ response ] x_test , y_test = test_data [[ 'temp' ]], test_data [ response ] You can also split into train test by x and y using train test split X_train, X_test, y_train, y_test = train_test_split( bikeshare['temp']), bikeshare['count'] ) Linear Regression using sklearn In [55]: from sklearn.linear_model import LinearRegression lr_sklearn = LinearRegression () . fit ( x_train , y_train ) # x data must be 2D array print ( 'Coefficients:' , lr_sklearn . coef_ ) print ( 'Intercept:' , lr_sklearn . intercept_ ) Coefficients: [94.82327417] Intercept: 2996.286973239966 Note: only one coefficient here since only using one descriptor variable ( temp ) Use model to predict on training and testing data and plot prediction In [56]: # predict y_preds_train = lr_sklearn . predict ( x_train ) y_preds_test = lr_sklearn . predict ( x_test ) # plot predictions fig , axes = plt . subplots ( 1 , 2 , figsize = ( 20 , 6 )) axes = axes . ravel () axes [ 0 ] . scatter ( x_train , y_train , color = 'b' , alpha = 0.5 , label = 'Data' ) axes [ 0 ] . plot ( x_train , y_preds_train , 'k' , linewidth = 5 , label = 'Fit' ) axes [ 0 ] . set_title ( 'Train Set' , fontsize = 18 ) axes [ 1 ] . scatter ( x_test , y_test , color = 'r' , marker = '*' , alpha = 0.5 , label = 'Data' ) axes [ 1 ] . plot ( x_test , y_preds_test , 'g' , linewidth = 5 , label = 'Prediction' ) axes [ 1 ] . set_title ( 'Test Set' , fontsize = 18 ) for i , ax in enumerate ( axes ): ax . set_ylim ( 0 , 10000 ) ax . set_xlabel ( 'Temperature' , fontsize = 15 ) ax . set_ylabel ( names_dict [ response ], fontsize = 15 ) ax . legend ( loc = 'upper right' , fontsize = 12 ) fig . suptitle ( f 'Linear Regression: Temp vs { names_dict [ response ] } ' , fontsize = 20 ) plt . show () Compute performance metrics for both training and testing In [57]: # from sklearn import metrics # Mean Squared Error (MSE) print ( \"MSE Train: {:.3f} \" . format ( metrics . mean_squared_error ( y_train , y_preds_train ))) print ( \"MSE Test: {:.3f} \" . format ( metrics . mean_squared_error ( y_test , y_preds_test ))) # R&#94;2 score print ( \"R&#94;2 Train: {:.3f} \" . format ( metrics . r2_score ( y_train , y_preds_train ))) print ( \"R&#94;2 Test: {:.3f} \" . format ( metrics . r2_score ( y_test , y_preds_test ))) MSE Train: 3031302.440 MSE Test: 2915373.463 R&#94;2 Train: 0.282 R&#94;2 Test: 0.224 Residual plots Residual plots are useful for identifying non-linearity between the predictors and the response variable. After we fit our model, we calculate the predicted (of fitted ) values $\\hat{y}_{i}$. Then we calculate the residuals $e_i = y_i - \\hat{y}_{i}$ We plot the residuals $e_1$ versus: the predictor $x_i$, if we have one predictor in our model, OR the predicted values $\\hat{y}_{i}$, if we have multiple predictors. TEAM ACTIVITY 3: Plot the residuals In [59]: # plot the residuals y_pred = lr_sklearn . predict ( test_data [[ 'temp' ]]) # predicted (fitted) values y = test_data [ response ] # test values residuals = y - y_pred fig , axes = plt . subplots ( 1 , 1 , figsize = ( 8 , 5 )) plt . scatter ( y_pred , residuals , color = 'blue' ) plt . axhline () plt . title ( \"Residual Plot for Linear Regression Model\" ) plt . annotate ( 'Outlier' , xy = ( 6400 , - 5500 ), xytext = ( 6000 , - 4000 ), fontsize = 14 , arrowprops = dict ( arrowstyle = '->' , ec = 'orange' , lw = 1 )) #, bbox = dict(boxstyle=\"round\", fc=\"0.8\")) plt . ylabel ( 'Residuals' ); Out[59]: Text(0, 0.5, 'Residuals') In [60]: # find the outliers test_data [( residuals <- 5000 ) | ( residuals > 5500 )] Out[60]: season month holiday day_of_week workingday weather temp atemp humidity windspeed count 333 2 6 0 4 1 2 36.0 37.0 56.83 0.15 915 533 1 1 0 5 1 1 2.0 5.0 50.75 0.38 8961 In [61]: plt . hist ( residuals ); Out[61]: (array([ 2., 2., 11., 50., 62., 40., 35., 12., 1., 5.]), array([-5494.92484335, -4367.92571118, -3240.926579 , -2113.92744682, -986.92831464, 140.07081753, 1267.06994971, 2394.06908189, 3521.06821407, 4648.06734624, 5775.06647842]), <BarContainer object of 10 artists>) Recall that more accurate models will have higher $R&#94;2$ scores (value of 1 is perfect fitted line) and lower MSEs (meaning lower error). For more info on these, check out sklearn metrics documentation. Take a look at the end of the notebook for calculations of MSE and $R&#94;2$ metrics by hand. TEAM ACTIVITY 4: Compare kNN and linear regression for the same dataset For the sim_data dataset, split 70-30 train-test split and use random state of 42 Create a function that implements linear regression with sklearn Predict on both training and test data Create 2 subplots with the following plotted: Subplot 1: Train set Plot training data in blue Plot linear regression prediction in black Plot kNN prediction (using k = 10) in magenta ('m') Subplot 2: Test set Plot testing data in red Plot linear regression prediction in green Plot kNN prediction (using k = 10) in yellow ('y') Calculate $MSE$ scores for both train and test sets for both kNN and linear regression Hints: don't forget sort! plt.subplots(...) creates subplots In [62]: def linreg_model ( train_data , test_data ): # sort sorted_train = train_data . sort_values ([ 'x' ]) sorted_test = test_data . sort_values ([ 'x' ]) x_train , x_test , y_train , y_test = sorted_train [ 'x' ], sorted_test [ 'x' ], sorted_train [ 'y' ], sorted_test [ 'y' ] x_train_ca = x_train . values . reshape ( - 1 , 1 ) x_test_ca = x_test . values . reshape ( - 1 , 1 ) # Create Linear Regression object results = LinearRegression () . fit ( x_train_ca , y_train ) # predict train_preds = results . predict ( x_train_ca ) test_preds = results . predict ( x_test_ca ) # find r&#94;2 r2_train = metrics . r2_score ( y_train , results . predict ( x_train_ca )) r2_test = metrics . r2_score ( y_test , results . predict ( x_test_ca )) return train_preds , test_preds , r2_train , r2_test def plot_predictions2 ( k , train_data , test_data , knn_train_preds , knn_test_preds , linreg_train_preds , linreg_test_preds ): # SubPlots fig , axes = plt . subplots ( nrows = 1 , ncols = 2 , figsize = ( 20 , 6 )) axes [ 0 ] . plot ( train_data [ 'x' ], train_data [ 'y' ], 'bo' , alpha = 0.5 , label = 'Data' ) axes [ 0 ] . plot ( train_data [ 'x' ], knn_train_preds , 'm-' , linewidth = 2 , markersize = 10 , label = 'KNN Preds' ) axes [ 0 ] . plot ( train_data [ 'x' ], linreg_train_preds , 'k-' , linewidth = 2 , markersize = 10 , label = 'Linreg Preds' ) axes [ 0 ] . set_xlabel ( 'x' ) axes [ 0 ] . set_ylabel ( 'y' ) axes [ 0 ] . set_title ( \"Train Data\" ) axes [ 0 ] . legend () axes [ 1 ] . plot ( test_data [ 'x' ], test_data [ 'y' ], 'r*' , alpha = 0.5 , label = 'Data' ) axes [ 1 ] . plot ( test_data [ 'x' ], knn_test_preds , 'y-' , linewidth = 2 , markersize = 10 , label = 'KNN Preds' ) axes [ 1 ] . plot ( test_data [ 'x' ], linreg_test_preds , 'g-' , linewidth = 2 , markersize = 10 , label = 'Test Preds' ) axes [ 1 ] . set_xlabel ( 'x' ) axes [ 1 ] . set_ylabel ( 'y' ) axes [ 1 ] . set_title ( \"Test Data\" ) axes [ 1 ] . legend () fig . suptitle ( \"KNN vs Linear Regression\" ) plt . show () # get predictions linreg_train_preds , linreg_test_preds , linreg_r2_train , linreg_r2_test = linreg_model ( sim_train_data , sim_test_data ) # plot linreg predictions side by side with knn predictions k = 10 plot_predictions2 ( k , sim_sorted_train , sim_sorted_test , knn_train_preds [ 1 ], knn_test_preds [ 1 ], linreg_train_preds , linreg_test_preds ) # print r2 scores for knn with k=10 and linreg print ( \"R&#94;2 Score of kNN on training set with k= {} :\" . format ( k ), knn_r2_train_scores [ 1 ]) print ( \"R&#94;2 Score of kNN on testing set: with k= {} \" . format ( k ), knn_r2_test_scores [ 1 ]) print ( \"R&#94;2 Score of linear regression on training set\" , linreg_r2_train ) print ( \"R&#94;2 Score of linear regression on testing set\" , linreg_r2_test ) R&#94;2 Score of kNN on training set with k=10: 0.6006116858176433 R&#94;2 Score of kNN on testing set: with k=10 0.21197194463555347 R&#94;2 Score of linear regression on training set 0.3816018646727135 R&#94;2 Score of linear regression on testing set 0.46326892325303626 END of lab Bonus Material: Train-Test Split using a mask In [63]: #Function to Split data into Train and Test Set def split_data ( data ): #Calculate Length of Dataset length = len ( data ) #Define Split split = 0.7 #Set a random Seed For Shuffling np . random . seed ( 9001 ) #Generate a Mask with a X:Y Split mask = np . random . rand ( length ) < split #Separate train and test data data_train = data [ mask ] data_test = data [ ~ mask ] #Return Separately return data_train , data_test In [64]: #Split data using defined function train_data_manual , test_data_manual = split_data ( bikeshare ) print ( \"Length of Training set:\" , len ( train_data_manual )) print ( \"Length of Testing set:\" , len ( test_data_manual )) Length of Training set: 507 Length of Testing set: 224 In [65]: ## Check that the ratio between test and train sets is right test_data_manual . shape [ 0 ] / ( test_data_manual . shape [ 0 ] + train_data_manual . shape [ 0 ]) Out[65]: 0.3064295485636115 Extra: Implementing the kNN Algorithm by hand 🏋🏻‍♂️ To really understand how the kNN algorithm works, it helps to go through the algorithm line by line in code. In [66]: #kNN Algorithm def knn_algorithm ( train , test , k ): #Create any empty list to store our predictions in predictions = [] #Separate the response and predictor variables from training and test set: train_x = train [ 'temp' ] train_y = train [ 'count' ] test_x = test [ 'temp' ] test_y = test [ 'count' ] for i , ele in enumerate ( test_x ): #For each test point, store the distance between all training points and test point distances = pd . DataFrame (( train_x . values - ele ) ** 2 , index = train . index ) distances . columns = [ 'dist' ] #display(distances) #Then, we sum across the columns per row to obtain the Euclidean distance squared ##distances = vec_distances.sum(axis = 1) #Sort the distances to training points (in ascending order) and take first k points nearest_k = distances . sort_values ( by = 'dist' ) . iloc [: k ] #For simplicity, we omitted the square rooting of the Euclidean distance because the #square root function preserves order. #Take the mean of the y-values of training set corresponding to the nearest k points k_mean = train_y [ nearest_k . index ] . mean () #Add on the mean to our predicted y-value list predictions . append ( k_mean ) #Create a dataframe with the x-values from test and predicted y-values predict = test . copy () predict [ 'predicted_count' ] = pd . Series ( predictions , index = test . index ) return predict Now to run the algorithm on our dataset with $k = 5$: In [67]: #Run the kNN function k = 5 predicted_knn = knn_algorithm ( train_data , test_data , k ) predicted_knn . head () Out[67]: season month holiday day_of_week workingday weather temp atemp humidity windspeed count predicted_count 703 3 7 0 2 1 1 34.0 33.0 49.21 0.13 6660 5269.2 33 4 11 0 6 0 1 6.0 9.0 51.92 0.19 3926 2958.6 300 2 6 0 1 1 2 21.0 23.0 77.79 0.17 5099 5577.4 456 4 12 0 4 1 1 2.0 5.0 58.00 0.24 7938 2377.2 633 2 5 0 0 0 2 20.0 23.0 74.00 0.15 6359 4693.4 We want to have a way to evaluate our predictions from the kNN algorithm with $k=5$. One way is to compute the $R&#94;2$ coefficient. Let's create a function for that: In [68]: #Test predictions in comparison to true value of test set def evaluate ( predicted , true ): #Find the squared error: squared_error = ( predicted [ 'predicted_count' ] - true [ 'count' ]) ** 2 #Finding the mean squared error: error_var = squared_error . sum () sample_var = (( true [ 'count' ] - true [ 'count' ] . mean ()) ** 2 ) . sum () r = ( 1 - ( error_var / sample_var )) return r Then let's apply this function to our predictions: In [69]: print ( \"Length of Test Data:\" , len ( test_data )) print ( \"R&#94;2 Score of kNN test:\" , evaluate ( predicted_knn , test_data )) Length of Test Data: 220 R&#94;2 Score of kNN test: 0.15616203314647292 In [70]: predicted_knn_train = knn_algorithm ( test_data , train_data , k ) print ( \"R&#94;2 Score of kNN train:\" , evaluate ( predicted_knn_train , train_data )) R&#94;2 Score of kNN train: 0.14054289261925446 Extra: pandas tricks In [71]: # seasons and months overlap pd . set_option ( 'display.max_rows' , None ) bikeshare . groupby ([ 'season' , 'month' ]) . size () Out[71]: season month 1 1 62 2 57 3 40 4 0 5 0 6 0 7 0 8 0 9 0 10 0 11 0 12 22 2 1 0 2 0 3 22 4 60 5 62 6 40 7 0 8 0 9 0 10 0 11 0 12 0 3 1 0 2 0 3 0 4 0 5 0 6 20 7 62 8 62 9 44 10 0 11 0 12 0 4 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 16 10 62 11 60 12 40 dtype: int64 Exercise: Look at the table above. Do you notice anything strange? answer What is the connection between seasons and months? Depends on which definition you use and if you are north or south of the equator. For more details see Seasons: Meteorological and Astronomical How to change values in specific places in the dataframe create a mask to change values to individual rows change the value by applying a function to all the rows Apply a function: .apply(lambda row: funct(row, *args)) In [72]: show_names_dict = { 1 : 'work' , 0 : 'fun' } In [73]: ## OPTIONAL EXAMPLE on how to create a new column by applying a function to the rows # def my_function(df, column1, column2, .... more columns): # s = df[column1] # b = df[column2] # ## OR some complicated function of s and b # return np.round(s*b, PRECISION) # df[new_column_name] = df.apply(lambda row: my_function(row, column1, column2), axis=1) In [74]: # we are making a copy so not to change our initial dataframe df = bikeshare . copy () mask = ( df . workingday == 1 ) # change only the rows with workingday==1 # df.loc[mask, 'workingday'] = \\ # df.loc[mask, 'workingday'].apply(lambda row: show_names_dict[row]) # change all the rows df [ 'workingday' ] = \\ df [ 'workingday' ] . apply ( lambda row : show_names_dict [ row ]) df . head () Out[74]: season month holiday day_of_week workingday weather temp atemp humidity windspeed count 0 2 5 0 2 work 2 24.0 26.0 76.58 0.12 6073 1 4 12 0 2 work 1 15.0 19.0 73.38 0.17 6606 2 2 6 0 4 work 1 26.0 28.0 56.96 0.25 7363 3 4 12 0 0 fun 1 0.0 4.0 58.63 0.17 2431 4 3 9 0 3 work 3 23.0 23.0 91.71 0.10 1996 In [0]:","tags":"labs","url":"labs/lab3/notebook-1/"},{"title":"Lecture 4: Multi-linear and Polynomial Regression","text":"Slides Lecture 4 : Multi-Linear Regression (PDF) Lecture 4 : Polynomial Regression (PDF) Exercises Lecture 4: Exercise: Simple Multi-linear Regression [Notebook] Lecture 4: Exercise: Linear and Polynomial Regression with Residual Analysis [Notebook] Lecture 4: Exercise: Multi-collinearity vs Model Predictions [Notebook]","tags":"lectures","url":"lectures/lecture04/"},{"title":"Lecture 4: Multi-linear and Polynomial Regression","text":"s2_exa1_challenge Title : Exercise: Simple Multi-linear Regression Description : The aim of this exercise is to understand how to use multi regression. Here we will observe the difference in MSE for each model as the predictors change. Data Description: Instructions: Read the file Advertisement.csv as a dataframe. For each instance of the predictor combination, form a model. For example, if you have 2 predictors, A and B, you will end up getting 3 models - one with only A, one with only B, and one with both A and B. Split the data into train and test sets. Compute the MSE of each model. Print the Predictor - MSE value pair Hints: pd.read_csv(filename) Returns a pandas dataframe containing the data and labels from the file data. sklearn.preprocessing.normalize() Scales input vectors individually to unit norm (vector length). sklearn.model_selection.train_test_split() Splits the data into random train and test subsets. sklearn.linear_model.LinearRegression LinearRegression fits a linear model. sklearn.linear_model.LinearRegression.fit() Fits the linear model to the training data. sklearn.linear_model.LinearRegression.predict() Predict using the linear model. sklearn.metrics.mean_squared_error() Computes the mean squared error regression loss Note: This exercise is auto-graded and you can try multiple attempts. In [1]: # Import necessary libraries import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn import preprocessing from prettytable import PrettyTable from sklearn.metrics import mean_squared_error from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split % matplotlib inline Reading the dataset In [2]: # Read the file \"Advertising.csv\" df = pd . read_csv ( \"Advertising.csv\" ) In [3]: # Take a quick look at the data to list all the predictors df . head () Out[3]: TV Radio Newspaper Sales 0 230.1 37.8 69.2 22.1 1 44.5 39.3 45.1 10.4 2 17.2 45.9 69.3 9.3 3 151.5 41.3 58.5 18.5 4 180.8 10.8 58.4 12.9 Create different multi predictor models In [11]: ### edTest(test_mse) ### # Initialize a list to store the MSE values mse_list = [] # List of all predictor combinations to fit the curve cols = [[ 'TV' ],[ 'Radio' ],[ 'Newspaper' ],[ 'TV' , 'Radio' ],[ 'TV' , 'Newspaper' ],[ 'Radio' , 'Newspaper' ],[ 'TV' , 'Radio' , 'Newspaper' ]] # Loop over all the predictor combinations for i in cols : # Set each of the predictors from the previous list as x x = df [ i ] # Set the \"Sales\" column as the reponse variable y = df [ \"Sales\" ] # Split the data into train-test sets with 80% training data and 20% testing data. # Set random_state as 0 x_train , x_test , y_train , y_test = train_test_split ( x , y , test_size = 0.2 , random_state = 0 ) # Initialize a Linear Regression model lreg = LinearRegression () # Fit the linear model on the train data lreg . fit ( x_train , y_train ) # Predict the response variable for the test set using the trained model y_pred = lreg . predict ( x_test ) # Compute the MSE for the test data MSE = mean_squared_error ( y_test , y_pred ) # Append the computed MSE to the list mse_list . append ( MSE ) Display the MSE with predictor combinations In [12]: # Helper code to display the MSE for each predictor combination t = PrettyTable ([ 'Predictors' , 'MSE' ]) for i in range ( len ( mse_list )): t . add_row ([ cols [ i ], mse_list [ i ]]) print ( t ) +------------------------------+-------------------+ | Predictors | MSE | +------------------------------+-------------------+ | ['TV'] | 10.18618193453022 | | ['Radio'] | 24.23723303713214 | | ['Newspaper'] | 32.13714634300907 | | ['TV', 'Radio'] | 4.391429763581883 | | ['TV', 'Newspaper'] | 8.687682675690592 | | ['Radio', 'Newspaper'] | 24.78339548293816 | | ['TV', 'Radio', 'Newspaper'] | 4.402118291449686 | +------------------------------+-------------------+ In [13]: In [0]:","tags":"lectures","url":"lectures/lecture04/notebook-1/"},{"title":"Lecture 4: Multi-linear and Polynomial Regression","text":"s2_exb1_challenge Title : Exercise: Linear and Polynomial Regression with Residual Analysis Description : The goal of this exercise is to fit linear regression and polynomial regression to the given data. Plot the fit curves of both the models along with the data and observe what the residuals tell us about the two fits. Data Description: Instructions: Read the poly.csv file into a dataframe. Split the data into train and test subsets. Fit a linear regression model on the entire data, using LinearRegression() object from Sklearn library. Guesstimate the degree of the polynomial which would best fit the data. Fit a polynomial regression model on the computed Polynomial Features using LinearRegression() object from sklearn library. Plot the linear and polynomial model predictions along with the test data. Compute the polynomial and linear model residuals using the formula below $\\epsilon = y_i - \\hat{y}$ Plot the histogram of the residuals and comment on your choice of the polynomial degree. Hints: pd.DataFrame.head() Returns a pandas dataframe containing the data and labels from the file data. sklearn.model_selection.train_test_split() Splits the data into random train and test subsets. plt.subplots() Create a figure and a set of subplots. sklearn.preprocessing.PolynomialFeatures() Generates a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. sklearn.preprocessing.StandardScaler.fit_transform() Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. sklearn.linear_model.LinearRegression LinearRegression fits a linear model. sklearn.linear_model.LinearRegression.fit() Fits the linear model to the training data. sklearn.linear_model.LinearRegression.predict() Predict using the linear model. plt.plot() Plots x versus y as lines and/or markers. plt.axvline() Add a vertical line across the axes. ax.hist() Plots a histogram. Note: This exercise is auto-graded and you can try multiple attempts. In [7]: # Import necessary libraries import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.preprocessing import PolynomialFeatures % matplotlib inline In [8]: # Read the data from 'poly.csv' into a Pandas dataframe df = pd . read_csv ( 'poly.csv' ) # Take a quick look at the dataframe df . head () Out[8]: x y 0 -3.292157 -46.916988 1 0.799528 -3.941553 2 -0.936214 -2.800522 3 -4.722680 -103.030914 4 -3.602674 -54.020819 In [9]: # Get the column values for x & y as numpy arrays x = df [[ 'x' ]] . values y = df [ 'y' ] . values In [10]: # Helper code to plot x & y to visually inspect the data fig , ax = plt . subplots () ax . plot ( x , y , 'x' ) ax . set_xlabel ( '$x$ values' ) ax . set_ylabel ( '$y$ values' ) ax . set_title ( '$y$ vs $x$' ) plt . show (); In [11]: # Split the data into train and test sets # Set the train size to 0.8 and random state to 22 x_train , x_test , y_train , y_test = train_test_split ( x , y , train_size = 0.8 , random_state = 22 ) In [22]: # Initialize a linear model model = LinearRegression () # Fit the model on the train data model . fit ( x_train , y_train ) # Get the predictions on the test data using the trained model y_lin_pred = model . predict ( x_test ) In [42]: ### edTest(test_deg) ### # Guess the correct polynomial degree based on the above graph guess_degree = 4 # Generate polynomial features on the train data x_poly_train = PolynomialFeatures ( degree = guess_degree ) . fit_transform ( x_train ) # Generate polynomial features on the test data x_poly_test = PolynomialFeatures ( degree = guess_degree ) . fit_transform ( x_test ) In [43]: # Initialize a model to perform polynomial regression polymodel = LinearRegression ( fit_intercept = False ) # Fit the model on the polynomial transformed train data polymodel . fit ( x_poly_train , y_train ) # Predict on the entire polynomial transformed test data y_poly_pred = polymodel . predict ( x_poly_test ) In [44]: # Helper code to visualise the results idx = np . argsort ( x_test [:, 0 ]) x_test = x_test [ idx ] # Use the above index to get the appropriate predicted values for y_test # y_test values corresponding to sorted test data y_test = y_test [ idx ] # Linear predicted values y_lin_pred = y_lin_pred [ idx ] # Non-linear predicted values y_poly_pred = y_poly_pred [ idx ] In [45]: # First plot x & y values using plt.scatter plt . scatter ( x , y , s = 10 , label = \"Test Data\" ) # Plot the linear regression fit curve plt . plot ( x_test , y_lin_pred , label = \"Linear fit\" , color = 'k' ) # Plot the polynomial regression fit curve plt . plot ( x_test , y_poly_pred , label = \"Polynomial fit\" , color = 'red' , alpha = 0.6 ) # Assigning labels to the axes plt . xlabel ( \"x values\" ) plt . ylabel ( \"y values\" ) plt . legend () plt . show (); In [46]: ### edTest(test_poly_predictions) ### # Calculate the residual values for the polynomial model poly_residuals = y_test - y_poly_pred In [47]: ### edTest(test_linear_predictions) ### # Calculate the residual values for the linear model lin_residuals = y_test - y_lin_pred In [48]: # Helper code to plot the residual values # Plot the histograms of the residuals for the two cases # Distribution of residuals fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 4 )) bins = np . linspace ( - 20 , 20 , 20 ) ax [ 0 ] . set_xlabel ( 'Residuals' ) ax [ 0 ] . set_ylabel ( 'Frequency' ) # Plot the histograms for the polynomial regression ax [ 0 ] . hist ( poly_residuals , bins , label = \"poly_residuals\" , color = '#B2D7D0' , alpha = 0.6 ) # Plot the histograms for the linear regression ax [ 0 ] . hist ( lin_residuals , bins , label = \"lin_residuals\" , color = '#EFAEA4' , alpha = 0.6 ) ax [ 0 ] . legend ( loc = 'upper left' ) # Distribution of predicted values with the residuals ax [ 1 ] . scatter ( y_poly_pred , poly_residuals , s = 10 , color = '#B2D7D0' , label = 'Polynomial predictions' ) ax [ 1 ] . scatter ( y_lin_pred , lin_residuals , s = 10 , color = '#EFAEA4' , label = 'Linear predictions' ) ax [ 1 ] . set_xlim ( - 75 , 75 ) ax [ 1 ] . set_xlabel ( 'Predicted values' ) ax [ 1 ] . set_ylabel ( 'Residuals' ) ax [ 1 ] . legend ( loc = 'upper left' ) fig . suptitle ( 'Residual Analysis (Linear vs Polynomial)' ) plt . show (); ⏸ Do you think that polynomial degree is appropriate. Experiment with a degree of polynomial of 2 and comment on what you observe for the residuals? In [49]: ### edTest(test_chow1) ### # Type your answer within in the quotes given answer1 = '2 is better' In [0]:","tags":"lectures","url":"lectures/lecture04/notebook-2/"},{"title":"Lecture 4: Multi-linear and Polynomial Regression","text":"s2_exb2_challenge Title : Exercise: Multi-collinearity vs Model Predictions Description : The goal of this exercise is to see how multi-collinearity can affect the predictions of a model. For this, perform a multi-linear regression on the given dataset and compare the coefficients with those from simple linear regression of the individual predictors. Data Description: Instructions: Read the dataset colinearity.csv as a dataframe. For each of the predictor variable, create a linear regression model with the same response variable. Compute the coefficients for each model and store in a list. Fit all predictors using a separate multi-linear regression object. Calculate the coefficients of each model. Compare the coefficients of the multi-linear regression model with those of the simple linear regression model. DISCUSSION: Why do you think the coefficients change and what does it mean? Hints: pd.read_csv(filename) Returns a pandas dataframe containing the data and labels from the file data. pd.DataFrame.drop() Drop specified labels from rows or columns. sklearn.linear_model.LinearRegression Returns a linear regression object from the sklearn library. sklearn.linear_model.LinearRegression.coef_ This attribute returns the coefficient(s) of the linear regression object. sklearn.linear_model.LinearRegression.fit() Fit linear model to the data. pd.Series.reshape() Return a np.ndndarray with the values in the specified shape. Note: This exercise is auto-graded and you can try multiple attempts. In [0]: # Import necessary libraries import numpy as np import pandas as pd import seaborn as sns from pprint import pprint import matplotlib.pyplot as plt from sklearn.linear_model import LinearRegression % matplotlib inline In [0]: # Read the file named \"colinearity.csv\" into a Pandas dataframe df = pd . read_csv ( ___ ) In [0]: # Take a quick look at the dataset df . head () Creation of Linear Regression Objects In [0]: # Choose all the predictors as the variable 'X' (note capitalization of X for multiple features) X = df . drop ([ ___ ], axis = 1 ) # Choose the response variable 'y' y = df . ___ In [0]: ### edTest(test_coeff) ### # Initialize a list to store the beta values for each linear regression model linear_coef = [] # Loop over all the predictors # In each loop \"i\" holds the name of the predictor for i in X : # Set the current predictor as the variable x x = df [[ ___ ]] # Create a linear regression object linreg = ____ # Fit the model with training data # Remember to choose only one column at a time i.e. given by x linreg . fit ( ___ , ___ ) # Add the coefficient value of the model to the list linear_coef . append ( linreg . coef_ ) Multi-Linear Regression using all variables In [0]: # Perform multi-linear regression with all predictors multi_linear = LinearRegression () # Fit the multi-linear regression on all features of the entire data multi_linear . fit ( ___ , ___ ) # Get the coefficients (plural) of the model multi_coef = multi_linear . coef_ Printing the individual $\\beta$ values In [0]: # Helper code to see the beta values of the linear regression models print ( 'By simple(one variable) linear regression for each variable:' , sep = ' \\n ' ) for i in range ( 4 ): pprint ( f 'Value of beta { i + 1 } = { linear_coef [ i ][ 0 ] : .2f } ' ) In [0]: ### edTest(test_multi_coeff) ### # Helper code to compare with the values from the multi-linear regression print ( 'By multi-Linear regression on all variables' ) for i in range ( 4 ): pprint ( f 'Value of beta { i + 1 } = { round ( multi_coef [ i ], 2 ) } ' ) ⏸ Why do you think the $\\beta$ values are different in the two cases? A. Because the random seed selected is not as random as we would imagine. B. Because of collinearity between $\\beta_1$ and $\\beta_4$ C. Because multi-linear regression is not a stable model D. Because of the measurement error in the data In [0]: ### edTest(test_chow1) ### # Submit an answer choice as a string below # (Eg. if you choose option C, put 'C') answer1 = '___' In [0]: # Helper code to visualize the heatmap of the covariance matrix corrMatrix = df [[ 'x1' , 'x2' , 'x3' , 'x4' ]] . corr () sns . heatmap ( corrMatrix , annot = True ) plt . show ()","tags":"lectures","url":"lectures/lecture04/notebook-3/"},{"title":"Lecture 3: Introduction to Regression kNN and Linear Regression","text":"Slides Lecture 3 : Introduction to Regression Part A - kNN (PDF) Lecture 3 : Introduction to Regression Part B - Error Evaluation and Model Comparison (PDF) Lecture 3 : Introduction to Regression Part C – Linear Models (PDF) Exercises Lecture 3: Exercise: Simple Data Plotting [Notebook] Lecture 3: Exercise: Simple kNN Regression [Notebook] Lecture 3: Exercise: Finding the Best k in kNN Regression [Notebook] Lecture 3: Exercise: MSE for Varying Beta Values [Notebook] Lecture 3: Exercise: Linear Regression using Sklearn [Notebook] References Lecture 3 : List (PDF) Lecture 3 : Dictionaries (PDF) Lecture 3 : Numpy (PDF) Lecture 3 : Zip & Enumerate (PDF)","tags":"lectures","url":"lectures/lecture03/"},{"title":"Lecture 3: Introduction to Regression kNN and Linear Regression","text":"s1_exc3_challenge Title : Exercise: Linear Regression using Sklearn Description : The goal of this exercise is to use the sklearn package to fit a Linear Regression on the previously used Advertising.csv datafile and produce a plot like the one given below. Data Description: Instructions: Use train_test_split() function to split the dataset into training and testing sets Use the LinearRegression function to make a model Fit the model on the training set Predict on the testing set using the fit model Estimate the fit of the model using mean_squared_error function Plot the dataset along with the predictions to visualize the fit Hints: pd.read_csv(filename) Returns a pandas dataframe containing the data and labels from the file data sklearn.train_test_split() Splits the data into random train and test subsets sklearn.LinearRegression() LinearRegression fits a linear model sklearn.fit() Fits the linear model to the training data sklearn.predict() Predict using the linear model mean_squared_error() Computes the mean squared error regression loss plt.plot() Plot y versus x as lines and/or markers Note: This exercise is auto-graded, hence please remember to set all the parameters to the values mentioned in the scaffold before marking. In [1]: # Import necessary libraries import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.metrics import r2_score from sklearn.metrics import mean_squared_error from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split % matplotlib inline In [2]: # Read the data from the file \"Advertising.csv\" df = pd . read_csv ( 'Advertising.csv' ) In [0]: # Take a quick look at the data df . head () In [7]: # Assign TV advertising as predictor variable 'x' x = df [[ 'TV' ]] # Set the Sales column as the response variable 'y' y = df [ 'Sales' ] In [13]: # Split the dataset in train and test data with 80% training set x_train , x_test , y_train , y_test = train_test_split ( ___ , ___ , train_size = 0.8 ) In [14]: # Initialize a Linear Regression model using Sklearn model = LinearRegression () # Fit the linear model on the train data model . fit ( ___ , ___ ) # Peedict on the test data using the trained model y_pred_test = model . predict ( ___ ) In [0]: ### edTest(test_mse) ### # Compute the MSE of the predicted test values mse = mean_squared_error ( ___ , ___ ) # Print the computed MSE print ( f 'The test MSE is { ___ } ' ) In [0]: # Make a plot of the data along with the predicted linear regression fig , ax = plt . subplots () ax . scatter ( x , y , label = 'data points' ) # Plot the test data and the predicted output of test data ax . plot ( ___ , ___ , color = 'red' , linewidth = 2 , label = 'model predictions' ) ax . set_xlabel ( 'Advertising' ) ax . set_ylabel ( 'Sales' ) ax . legend () ⏸ How does your $MSE$ change when the size of the training set is change to 60% instead of 80%? In [17]: ### edTest(test_chow1) ### # Type your answer within in the quotes given answer1 = '___'","tags":"Lectures","url":"lectures/lecture03/notebook/"},{"title":"Lecture 3: Introduction to Regression kNN and Linear Regression","text":"s1_ex1a_challenge Title : Description : The aim of this exercise is to plot TV Ads vs Sales based on the Advertisement dataset which should look similar to the graph given below. Data Description: Instructions: Read the Advertisement data and view the top rows of the dataframe to get an understanding of the data and the columns. Select the first 7 observations and the columns TV and Sales to make a new data frame. Create a scatter plot of the new data frame TV budget vs Sales . Hints: pd.read_csv(filename) Returns a pandas dataframe containing the data and labels from the file data df.iloc[] Returns a subset of the dataframe that is contained in the row range passed as the argument np.linspace() Returns evenly spaced numbers over a specified interval df.head() Returns the first 5 rows of the dataframe with the column names plt.scatter() A scatter plot of y vs. x with varying marker size and/or color plt.xlabel() This is used to specify the text to be displayed as the label for the x-axis plt.ylabel() This is used to specify the text to be displayed as the label for the y-axis Note: This exercise is auto-graded and you can try multiple attempts. In [1]: # Import necessary libraries import pandas as pd import matplotlib.pyplot as plt % matplotlib inline Reading the Advertisement dataset In [2]: # \"Advertising.csv\" containts the data set used in this exercise data_filename = 'Advertising.csv' # Read the file \"Advertising.csv\" file using the pandas library df = pd . read_csv ( \"Advertising.csv\" ) In [3]: # Get a quick look of the data df . describe () Out[3]: TV Radio Newspaper Sales count 200.000000 200.000000 200.000000 200.000000 mean 147.042500 23.264000 30.554000 14.022500 std 85.854236 14.846809 21.778621 5.217457 min 0.700000 0.000000 0.300000 1.600000 25% 74.375000 9.975000 12.750000 10.375000 50% 149.750000 22.900000 25.750000 12.900000 75% 218.825000 36.525000 45.100000 17.400000 max 296.400000 49.600000 114.000000 27.000000 In [4]: ### edTest(test_pandas) ### # Create a new dataframe by selecting the first 7 rows of # the current dataframe df_new = df . head ( 7 ) In [5]: # Print your new dataframe to see if you have selected 7 rows correctly print ( df_new ) TV Radio Newspaper Sales 0 230.1 37.8 69.2 22.1 1 44.5 39.3 45.1 10.4 2 17.2 45.9 69.3 9.3 3 151.5 41.3 58.5 18.5 4 180.8 10.8 58.4 12.9 5 8.7 48.9 75.0 7.2 6 57.5 32.8 23.5 11.8 Plotting the graph In [7]: # Use a scatter plot for plotting a graph of TV vs Sales plt . scatter ( df_new . TV , df_new . Sales ) # Add axis labels for clarity (x : TV budget, y : Sales) plt . xlabel ( \"TV budget\" ) plt . ylabel ( \"Sales\" ) Out[7]: Text(0, 0.5, 'Sales') Post-Exercise Question Instead of just plotting seven points, experiment to plot all points. In [8]: # Your code here plt . scatter ( df . TV , df . Sales ) # Add axis labels for clarity (x : TV budget, y : Sales) plt . xlabel ( \"TV budget\" ) plt . ylabel ( \"Sales\" ) Out[8]: Text(0, 0.5, 'Sales') In [0]:","tags":"Lectures","url":"lectures/lecture03/notebook-1/"},{"title":"Lecture 3: Introduction to Regression kNN and Linear Regression","text":"s1_exa2_challenge Title : Exercise: Simple kNN Regression Description : The goal of this exercise is to re-create the plots given below. You would have come across these graphs in the lecture as well. Data Description: Instructions: Part 1: KNN by hand for k=1 Read the Advertisement data. Get a subset of the data from row 5 to row 13. Apply the kNN algorithm by hand and plot the first graph as given above. Part 2: Using sklearn package Read the Advertisement dataset. Split the data into train and test sets using the train_test_split() function. Set k_list as the possible k values ranging from 1 to 70. For each value of k in k_list : Use sklearn KNearestNeighbors() to fit train data. Predict on the test data. Use the helper code to get the second plot above for k=1,10,70. Hints: np.argsort() Returns the indices that would sort an array. df.iloc[] Returns a subset of the dataframe that is contained in the column range passed as the argument. plt.plot() Plot y versus x as lines and/or markers. df.values Returns a Numpy representation of the DataFrame. pd.idxmin() Returns index of the first occurrence of minimum over requested axis. np.min() Returns the minimum along a given axis. np.max() Returns the maximum along a given axis. model.fit() Fit the k-nearest neighbors regressor from the training dataset. model.predict() Predict the target for the provided data. np.zeros() Returns a new array of given shape and type, filled with zeros. train_test_split(X,y) Split arrays or matrices into random train and test subsets. np.linspace() Returns evenly spaced numbers over a specified interval. KNeighborsRegressor(n_neighbors=k_value) Regression-based on k-nearest neighbors. Note: This exercise is auto-graded, hence please remember to set all the parameters to the values mentioned in the scaffold before marking. In [2]: # Import required libraries import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.neighbors import KNeighborsRegressor from sklearn.model_selection import train_test_split % matplotlib inline In [3]: # Read the data from the file \"Advertising.csv\" filename = 'Advertising.csv' df_adv = pd . read_csv ( filename ) In [4]: # Take a quick look of the dataset df_adv . head () Out[4]: TV Radio Newspaper Sales 0 230.1 37.8 69.2 22.1 1 44.5 39.3 45.1 10.4 2 17.2 45.9 69.3 9.3 3 151.5 41.3 58.5 18.5 4 180.8 10.8 58.4 12.9 Part 1: KNN by hand for $k=1$ In [5]: # Get a subset of the data i.e. rows 5 to 13 # Use the TV column as the predictor x_true = df_adv . TV . iloc [ 5 : 13 ] # Use the Sales column as the response y_true = df_adv . Sales . iloc [ 5 : 13 ] # Sort the data to get indices ordered from lowest to highest TV values idx = np . argsort ( x_true ) . values # Get the predictor data in the order given by idx above x_true = x_true . iloc [ idx ] . values # Get the response data in the order given by idx above y_true = y_true . iloc [ idx ] . values In [6]: ### edTest(test_findnearest) ### # Define a function that finds the index of the nearest neighbor # and returns the value of the nearest neighbor. # Note that this is just for k = 1 where the distance function is # simply the absolute value. def find_nearest ( array , value ): # Hint: To find idx, use .idxmin() function on the series idx = pd . Series ( np . abs ( array - value )) . idxmin () # Return the nearest neighbor index and value return idx , array [ idx ] In [7]: # Create some synthetic x-values (might not be in the actual dataset) x = np . linspace ( np . min ( x_true ), np . max ( x_true )) # Initialize the y-values for the length of the synthetic x-values to zero y = np . zeros (( len ( x ))) In [8]: # Apply the KNN algorithm to predict the y-value for the given x value for i , xi in enumerate ( x ): # Get the Sales values closest to the given x value y [ i ] = y [ find_nearest ( x , xi )[ 0 ]] Plotting the data In [12]: # Plot the synthetic data along with the predictions plt . plot ( x , y , '-.' ) # Plot the original data using black x's. plt . plot ( x_true , Y_true , 'kx' ) # Set the title and axis labels plt . title ( 'TV vs Sales' ) plt . xlabel ( 'TV budget in $1000' ) plt . ylabel ( 'Sales in $1000' ) --------------------------------------------------------------------------- NameError Traceback (most recent call last) /tmp/ipykernel_30/3566749308.py in <module> 3 4 # Plot the original data using black x's. ----> 5 plt . plot ( x_true , Y_true , 'kx' ) 6 7 # Set the title and axis labels NameError : name 'Y_true' is not defined Part 2: KNN for $k\\ge1$ using sklearn In [0]: # Read the data from the file \"Advertising.csv\" data_filename = 'Advertising.csv' df = pd . read_csv ( data_filename ) # Set 'TV' as the 'predictor variable' x = df [[ ___ ]] # Set 'Sales' as the response variable 'y' y = df [ ___ ] In [0]: ### edTest(test_shape) ### # Split the dataset in training and testing with 60% training set # and 40% testing set with random state = 42 x_train , x_test , y_train , y_test = train_test_split ( ___ , ___ , train_size = ___ , random_state = ___ ) In [0]: ### edTest(test_nums) ### # Choose the minimum k value based on the instructions given on the left k_value_min = ___ # Choose the maximum k value based on the instructions given on the left k_value_max = ___ # Create a list of integer k values betwwen k_value_min and k_value_max using linspace k_list = np . linspace ( k_value_min , k_value_max , 70 ) In [0]: # Set the grid to plot the values fig , ax = plt . subplots ( figsize = ( 10 , 6 )) # Variable used to alter the linewidth of each plot j = 0 # Loop over all the k values for k_value in k_list : # Creating a kNN Regression model model = KNeighborsRegressor ( n_neighbors = int ( ___ )) # Fitting the regression model on the training data model . fit ( ___ , ___ ) # Use the trained model to predict on the test data y_pred = model . predict ( ___ ) # Helper code to plot the data along with the model predictions colors = [ 'grey' , 'r' , 'b' ] if k_value in [ 1 , 10 , 70 ]: xvals = np . linspace ( x . min (), x . max (), 100 ) ypreds = model . predict ( xvals ) ax . plot ( xvals , ypreds , '-' , label = f 'k = { int ( k_value ) } ' , linewidth = j + 2 , color = colors [ j ]) j += 1 ax . legend ( loc = 'lower right' , fontsize = 20 ) ax . plot ( x_train , y_train , 'x' , label = 'train' , color = 'k' ) ax . set_xlabel ( 'TV budget in $1000' , fontsize = 20 ) ax . set_ylabel ( 'Sales in $1000' , fontsize = 20 ) plt . tight_layout () ⏸ In the plotting code above, re-run ax.plot(x_train, y_train,'x',label='train',color='k') with x_test and y_test instead. According to you, which k value is the best and why? In [0]: ### edTest(test_chow1) ### # Type your answer within in the quotes given answer1 = '___'","tags":"Lectures","url":"lectures/lecture03/notebook-2/"},{"title":"Lecture 3: Introduction to Regression kNN and Linear Regression","text":"s1_exb1_challenge Title : Exercise: Finding the Best k in kNN Regression Description : The goal here is to find the value of k of the best performing model based on the test MSE. Data Description: Instructions: Read the data into a Pandas dataframe object. Select the sales column as the response variable and TV budget column as the predictor variable. Make a train-test split using sklearn.model_selection.train_test_split . Create a list of integer k values using numpy.linspace . For each value of k Fit a kNN regression on train set. Calculate MSE on test set and store it. Plot the test MSE values for each k. Find the k value associated with the lowest test MSE. Hints: train_test_split(X,y) Split arrays or matrices into random train and test subsets. np.linspace() Returns evenly spaced numbers over a specified interval. KNeighborsRegressor(n_neighbors=k_value) Regression-based on k-nearest neighbors. model.predict() Predict the target for the provided data. mean_squared_error() Computes the mean squared error regression loss. dict.keys() Returns a view object that displays a list of all the keys in the dictionary. dict.values() Returns a list of all the values available in a given dictionary. plt.plot() Plot y versus x as lines and/or markers. dict.items() Returns a list of dict's (key, value) tuple pairs. Note: This exercise is auto-graded and you can try multiple attempts. In [1]: # Import necessary libraries import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.utils import shuffle from sklearn.metrics import r2_score from sklearn.metrics import mean_squared_error from sklearn.neighbors import KNeighborsRegressor from sklearn.model_selection import train_test_split % matplotlib inline Reading the standard Advertising dataset In [0]: # Read the file 'Advertising.csv' into a Pandas dataset df = pd . read_csv ( 'Advertising.csv' ) In [0]: # Take a quick look at the data df . head () In [3]: # Set the 'TV' column as predictor variable x = df [[ ___ ]] # Set the 'Sales' column as response variable y = df [ ___ ] Train-Test split In [21]: ### edTest(test_shape) ### # Split the dataset in training and testing with 60% training set and # 40% testing set x_train , x_test , y_train , y_test = train_test_split ( ___ , ___ , train_size = ___ , random_state = 66 ) In [28]: ### edTest(test_nums) ### # Choosing k range from 1 to 70 k_value_min = 1 k_value_max = 70 # Create a list of integer k values between k_value_min and # k_value_max using linspace k_list = np . linspace ( k_value_min , k_value_max , num = 70 , dtype = int ) Model fit In [0]: # Setup a grid for plotting the data and predictions fig , ax = plt . subplots ( figsize = ( 10 , 6 )) # Create a dictionary to store the k value against MSE fit {k: MSE@k} knn_dict = {} # Variable used for altering the linewidth of values kNN models j = 0 # Loop over all k values for k_value in k_list : # Create a KNN Regression model for the current k model = KNeighborsRegressor ( n_neighbors = int ( ___ )) # Fit the model on the train data model . fit ( x_train , y_train ) # Use the trained model to predict on the test data y_pred = model . predict ( ___ ) # Calculate the MSE of the test data predictions MSE = ____ # Store the MSE values of each k value in the dictionary knn_dict [ k_value ] = ___ # Helper code to plot the data and various kNN model predictions colors = [ 'grey' , 'r' , 'b' ] if k_value in [ 1 , 10 , 70 ]: xvals = np . linspace ( x . min (), x . max (), 100 ) ypreds = model . predict ( xvals ) ax . plot ( xvals , ypreds , '-' , label = f 'k = { int ( k_value ) } ' , linewidth = j + 2 , color = colors [ j ]) j += 1 ax . legend ( loc = 'lower right' , fontsize = 20 ) ax . plot ( x_train , y_train , 'x' , label = 'test' , color = 'k' ) ax . set_xlabel ( 'TV budget in $1000' , fontsize = 20 ) ax . set_ylabel ( 'Sales in $1000' , fontsize = 20 ) plt . tight_layout () Graph plot In [0]: # Plot a graph which depicts the relation between the k values and MSE plt . figure ( figsize = ( 8 , 6 )) plt . plot ( ___ , ___ , 'k.-' , alpha = 0.5 , linewidth = 2 ) # Set the title and axis labels plt . xlabel ( 'k' , fontsize = 20 ) plt . ylabel ( 'MSE' , fontsize = 20 ) plt . title ( 'Test $MSE$ values for different k values - KNN regression' , fontsize = 20 ) plt . tight_layout () Find the best knn model In [0]: ### edTest(test_mse) ### # Find the lowest MSE among all the kNN models min_mse = min ( ___ ) # Use list comprehensions to find the k value associated with the lowest MSE best_model = [ key for ( key , value ) in knn_dict . items () if value == min_mse ] # Print the best k-value print ( \"The best k value is \" , best_model , \"with a MSE of \" , min_mse ) ⏸ From the options below, how would you classify the \"goodness\" of your model? A. Good B. Satisfactory C. Bad In [0]: ### edTest(test_chow1) ### # Submit an answer choice as a string below (eg. if you choose option A, put 'A') answer1 = '___' In [0]: # Helper code to compute the R2_score of your best model model = KNeighborsRegressor ( n_neighbors = best_model [ 0 ]) model . fit ( x_train , y_train ) y_pred_test = model . predict ( x_test ) # Print the R2 score of the model print ( f \"The R2 score for your model is { r2_score ( y_test , y_pred_test ) } \" ) ⏸ After observing the $R&#94;2$ value, how would you now classify your model? A. Good B. Satisfactory C. Bad In [1]: ### edTest(test_chow2) ### # Submit an answer choice as a string below (eg. if you choose option A, put 'A') answer2 = '___'","tags":"Lectures","url":"lectures/lecture03/notebook-3/"},{"title":"Lecture 3: Introduction to Regression kNN and Linear Regression","text":"s1_exc2_challenge Title : Exercise: MSE for Varying Beta Values Description : The goal of this exercise is to produce a plot like the one given below. Data Description: Instructions: Fix $\\beta_0 = 2.2$. Change $\\beta_1$ in a range $[-2, 3]$. Estimate the fit of the model by following the steps below: Create empty lists to store the MSE and $\\beta_1$. Set a range of values for $\\beta_1$ and compute MSE for each one. Hints: np.linspace(start, stop, num) Return evenly spaced numbers over a specified interval. np.arange(start, stop, increment) Return evenly spaced values within a given interval. list_name.append(item) Add an item to the end of the list. plt.xlabel() This is used to specify the text to be displayed as the label for the x-axis. plt.ylabel() This is used to specify the text to be displayed as the label for the y-axis. Note: This exercise is auto-graded, hence please remember to set all the parameters to the values mentioned in the scaffold before marking. In [13]: # Import necessary libraries import numpy as np import pandas as pd import matplotlib.pyplot as plt % matplotlib inline Reading the dataset In [14]: # Read data file 'Advertising.csv' into a Pandas Dataframe df = pd . read_csv ( 'Advertising.csv' ) In [0]: # Take a quick look at the data df . head () In [17]: # Create a new dataframe called `df_new` with the columns 'TV' and 'sales' df_new = df [[ 'TV' , 'sales' ]] Beta and MSE Computation In [6]: # Set beta0 to 2.2 beta0 = 2.2 In [12]: # Create an empty list to store the MSE mse_list = ___ # Create an empty list to store the beta1 values beta1_list = ___ In [0]: ### edTest(test_beta) ### # This loops runs from -2 to 3.0 with an increment of 0.1 # i.e a total of 51 steps for beta1 in ___ : # Calculate prediction of x using beta0 and beta1 # Recall the formula y = beta0 + beta1*x y_predict = ___ # Calculate the Mean Squared Error mean_squared_error = ___ # Append the new MSE to the list initialized above mse_list . ___ # Append the beta1 value to the appropriate list initialized above beta1_list . ___ Plotting the graph In [0]: ### edTest(test_mse) ### # Helper code to plot the MSE as a function of beta1 plt . plot ( beta1_list , mse_list ) plt . xlabel ( 'Beta1' ) plt . ylabel ( 'MSE' ) ⏸ Go back and change your $\\beta_0$ value of your choice and report your new optimal $\\beta_1$ value and new lowest $MSE$. Is the MSE lower than before, or more? In [0]: ### edTest(test_chow1) ### # Submit an answer choice as a string below answer1 = '___'","tags":"Lectures","url":"lectures/lecture03/notebook-4/"},{"title":"Lab 2","text":"cs109a_lab02_seaborn CS109A Introduction to Data Science Lab 2: EDA with Pandas (+seaborn) Harvard University Fall 2021 Instructors : Pavlos Protopapas and Natesh Pillai Authors : Natesh Pillai In [2]: ## RUN THIS CELL TO GET THE RIGHT FORMATTING import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\" ) . text HTML ( styles ) Out[2]: In [3]: import pandas as pd import seaborn as sns import matplotlib.pyplot as plt In this lecture we will look at tools for plotting using both matplotlib and seaborn. Load data The file quartets.csv contains 4 different tiny datasets that we will use to quickly understand the value of ploting. In [4]: quartets = pd . read_csv ( 'quartets.csv' , index_col = 0 ) Exploration In [5]: quartets . info () <class 'pandas.core.frame.DataFrame'> Int64Index: 44 entries, 1 to 11 Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 x 44 non-null int64 1 y 44 non-null float64 2 quartet 44 non-null object dtypes: float64(1), int64(1), object(1) memory usage: 1.4+ KB We see there are 44 entries, two numerical columns x and y and one column to potentially identify every quartet dataset. How does this dataframe look like? In [106]: quartets . head () Out[106]: x y quartet 1 10 8.04 I 2 8 6.95 I 3 13 7.58 I 4 9 8.81 I 5 11 8.33 I How do random samples look like? In [7]: quartets . sample ( 5 ) Out[7]: x y quartet 8 19 12.50 IV 3 13 7.58 I 3 13 12.74 III 5 11 8.33 I 2 8 5.76 IV Quartet's names In [8]: quartets [ 'quartet' ] . unique () . tolist () Out[8]: ['I', 'II', 'III', 'IV'] Display the first 3 samples from every dataset In [9]: quartets . groupby ( 'quartet' ) . head ( 3 ) Out[9]: x y quartet 1 10 8.04 I 2 8 6.95 I 3 13 7.58 I 1 10 9.14 II 2 8 8.14 II 3 13 8.74 II 1 10 7.46 III 2 8 6.77 III 3 13 12.74 III 1 8 6.58 IV 2 8 5.76 IV 3 8 7.71 IV Display 2 random samples from every dataset In [10]: quartets . groupby ( 'quartet' ) . sample ( 2 ) Out[10]: x y quartet 5 11 8.33 I 10 7 4.82 I 3 13 8.74 II 7 6 6.13 II 10 7 6.42 III 2 8 6.77 III 10 8 7.91 IV 4 8 8.84 IV Display every quartet's dataset size In [11]: quartets . groupby ( 'quartet' ) . size () Out[11]: quartet I 11 II 11 III 11 IV 11 dtype: int64 Descriptive Statistics In [12]: quartets . groupby ( 'quartet' ) . agg ([ 'mean' , 'std' ]) . round ( 3 ) Out[12]: x y mean std mean std quartet I 9 3.317 7.501 2.032 II 9 3.317 7.501 2.032 III 9 3.317 7.500 2.030 IV 9 3.317 7.501 2.031 Almost same mean and standard deviation for every quartet. This looks like all quartets samples could be sampled from the same distribution. These are tiny datasets so we could read them all In [13]: quartets [ quartets [ 'quartet' ] == 'I' ] Out[13]: x y quartet 1 10 8.04 I 2 8 6.95 I 3 13 7.58 I 4 9 8.81 I 5 11 8.33 I 6 14 9.96 I 7 6 7.24 I 8 4 4.26 I 9 12 10.84 I 10 7 4.82 I 11 5 5.68 I In [14]: quartets [ quartets [ 'quartet' ] == 'II' ] Out[14]: x y quartet 1 10 9.14 II 2 8 8.14 II 3 13 8.74 II 4 9 8.77 II 5 11 9.26 II 6 14 8.10 II 7 6 6.13 II 8 4 3.10 II 9 12 9.13 II 10 7 7.26 II 11 5 4.74 II In [15]: quartets [ quartets [ 'quartet' ] == 'III' ] Out[15]: x y quartet 1 10 7.46 III 2 8 6.77 III 3 13 12.74 III 4 9 7.11 III 5 11 7.81 III 6 14 8.84 III 7 6 6.08 III 8 4 5.39 III 9 12 8.15 III 10 7 6.42 III 11 5 5.73 III In [16]: quartets [ quartets [ 'quartet' ] == 'IV' ] Out[16]: x y quartet 1 8 6.58 IV 2 8 5.76 IV 3 8 7.71 IV 4 8 8.84 IV 5 8 8.47 IV 6 8 7.04 IV 7 8 5.25 IV 8 19 12.50 IV 9 8 5.56 IV 10 8 7.91 IV 11 8 6.89 IV Plot or not to plot? Pandas by default comes with matplotlib incorporated. Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics. BoxPlot In [17]: quartets . groupby ( 'quartet' ) . boxplot ( grid = False ); Seaborn's palettes In [18]: sns . color_palette () Out[18]: In [19]: sns . color_palette ( 'pastel' ) Out[19]: In [20]: palette = 'pastel' Seaborn's boxplots Similar boxplots with Matplotlib and Seaborn In [21]: fig , axes = plt . subplots ( 2 , 2 , figsize = ( 8 , 7 )) axes = axes . flatten () . tolist () for quartet , g in quartets . groupby ( 'quartet' ): ax = axes . pop ( 0 ) sns . boxplot ( data = g , ax = ax , palette = palette ); ax . set_title ( f 'quartet { quartet } ' ) plt . suptitle ( \"Quartets' boxplots\" ); Using seaborn boxplots to compare quartes's shared features seaborn.boxplot() pandas.melt() In [22]: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 16 , 4 )) sns . boxplot ( x = 'x' , y = 'value' , hue = 'quartet' , data = pd . melt ( quartets , id_vars = 'quartet' , var_name = 'x' , value_name = 'value' ), ax = ax , palette = palette ) ax . set_title ( \"quartets' features\" ); The problem with the plot above is that we are forcing different features (like x and y ) to share the same y-axis. So, another way to acomplish the goal could be this one In [23]: fig , axes = plt . subplots ( 1 , 2 , figsize = ( 16 , 4 )) for i , col in enumerate ([ 'x' , 'y' ]): sns . boxplot ( x = 'quartet' , y = col , data = quartets , ax = axes [ i ], palette = palette ); axes [ i ] . set_title ( f 'variable { col } ' ) Histograms Pandas let us easily plot the individual quartet's feature histogram in one line of code. In [24]: quartets . groupby ( 'quartet' ) . hist (); The histograms allows us to start to see some differences Seaborn's histograms seaborn.histplot() We could do the same with seaborn with this code In [25]: for quartet , g in quartets . groupby ( 'quartet' ): fig , axes = plt . subplots ( 1 , 2 , figsize = ( 8 , 2.5 )) sns . histplot ( data = g , x = \"x\" , hue = 'quartet' , ax = axes [ 0 ], palette = palette , bins = 10 , kde = True ); sns . histplot ( data = g , x = \"y\" , hue = 'quartet' , ax = axes [ 1 ], palette = palette , bins = 10 , kde = True ); plt . suptitle ( f 'Quartet { quartet } ' ) We can plot all quartets's two features x and y in two different plots moving out the subplots creation In [26]: # some elements are 'bars' (default but too noisy when plotting so many features), 'step', 'poly' element = 'step' fig , axes = plt . subplots ( 1 , 2 , figsize = ( 12 , 5 )) legends = [] for quartet , g in quartets . groupby ( 'quartet' ): legends . append ( f 'quartet { quartet } ' ) sns . histplot ( data = g , x = \"x\" , hue = 'quartet' , ax = axes [ 0 ], palette = palette , bins = 10 , kde = False , alpha = .2 , element = element ); sns . histplot ( data = g , x = \"y\" , hue = 'quartet' , ax = axes [ 1 ], palette = palette , bins = 10 , kde = False , alpha = .2 , element = element ); axes [ 0 ] . legend ( legends ) axes [ 1 ] . legend ( legends ); FacetGrid This is a powerful tool that can be used in combination with ploting method from seaborn or even matplotlib to plot multiple subplots based on some conditional relationship. seaborn.FacetGrid() : Multi-plot grid for plotting conditional relationships. Grid of histograms In [27]: for feature in [ 'x' , 'y' ]: # create the grid with condition quartet g = sns . FacetGrid ( quartets , col = \"quartet\" , palette = palette , col_wrap = 4 ) # for every condition we are going to create a subplot for the grid for column \"feature\" g . map ( sns . histplot , feature , bins = 10 ); # col_wrap define the number of columns. Change the value to 3 and 2 to understand visually its behaviour We can create one FacetGrid for all. For that we need to convert the dataframe to access values based on conditions. In [28]: melted = pd . melt ( quartets , id_vars = 'quartet' , var_name = 'x' , value_name = 'value' ) . rename ( columns = { 'x' : 'variable' }) melted Out[28]: quartet variable value 0 I x 10.00 1 I x 8.00 2 I x 13.00 3 I x 9.00 4 I x 11.00 ... ... ... ... 83 IV y 5.25 84 IV y 12.50 85 IV y 5.56 86 IV y 7.91 87 IV y 6.89 88 rows × 3 columns In [29]: # create the grid with quartets as columns and variable as rows g = sns . FacetGrid ( melted , row = \"variable\" , col = 'quartet' , palette = palette , sharex = False ) g . map ( sns . histplot , 'value' , bins = 10 ); # we need set sharex to False to avoid distorting shapes between rows (you can try changing it to True) Scatter plots Knowing that we have x and y features, we can think about using other kind of helpful plots. Why not a scatter plot? In [30]: quartets . groupby ( 'quartet' ) . plot . scatter ( x = 'x' , y = 'y' , s = 50 ); Scatter plots with seaborn We can combine matplotlib with seaborn to improve the aesthetic. In [31]: fig , axes = plt . subplots ( 2 , 2 , figsize = ( 7 , 7 )) axes = axes . flatten () . tolist () for quartet , g in quartets . groupby ( 'quartet' ): ax = axes . pop ( 0 ) sns . scatterplot ( data = g , x = 'x' , y = 'y' , ax = ax ) ax . set_title ( f 'quartet { quartet } ' ) plt . subplots_adjust ( hspace = 0.3 ); Scatter plots with FacetGrid FacetGrid is great to avoid writting too many lines of matplotlib code. In this case we can force the grid to share x and y domain to simplify features domains comparison. In [32]: g = sns . FacetGrid ( quartets , col = 'quartet' , palette = palette , col_wrap = 2 , sharex = True , sharey = True ) g . map ( sns . scatterplot , 'x' , 'y' ); Line plots We could also use a lineplot but to do that we need to know that dots should be ordered in the x axis. In [33]: quartets . sort_values ( by = 'x' ) . groupby ( 'quartet' ) . plot ( x = 'x' , y = 'y' , marker = 'o' , lw = .7 ); All in one We also can use matplotlib to plot all groups in the same plot In [34]: # create one figure of 1 x 1 size. fig , ax = plt . subplots ( 1 , 1 , figsize = ( 16 , 6 )) # plot all 4 quartets in the same ax quartets . sort_values ( by = 'x' ) . groupby ( 'quartet' ) . plot ( x = 'x' , y = 'y' , marker = 'o' , ms = 10 , lw = .7 , alpha = .7 , ax = ax ) plt . ylabel ( 'y' ) plt . title ( 'All in one quartets' ); Lineplots with seaborn Seaborn.lineplot() simplifies the creation of the same plot. In [35]: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 16 , 6 )) sns . lineplot ( data = quartets , x = 'x' , y = 'y' , hue = 'quartet' , marker = 'o' , ms = 10 , lw = .7 , alpha = .7 , ax = ax ) plt . title ( 'All in one quartets' ); And we can plot all quartets together (removing the conditional hue for seaborn) In [36]: fig , axes = plt . subplots ( 1 , 2 , figsize = ( 16 , 4 )) sns . lineplot ( data = quartets , x = 'x' , y = 'y' , lw = .7 , ax = axes [ 0 ]) axes [ 0 ] . set_title ( 'one line of seaborn' ) quartets . plot ( x = 'x' , y = 'y' , lw = .7 , ax = axes [ 1 ]) axes [ 1 ] . set_title ( 'one line of matplotlib' ); Seaborn is built on matplotlib, so using more lines of matplotlib should let you arrive to the same seaborn plot. Let´s use a different dataset We will load an already known dataset. Source: https://www.kaggle.com/spscientist/students-performance-in-exams Original source generator: http://roycekimmons.com/tools/generated_data/exams If you go to the original source you will find this is a fictitious dataset created specifically for data science training purposes. In [37]: df = pd . read_csv ( 'StudentsPerformance.csv' ) . rename ( columns = { 'race/ethnicity' : 'group' , 'parental level of education' : 'parental' , 'test preparation course' : 'course' , 'math score' : 'math' , 'reading score' : 'reading' , 'writing score' : 'writing' } ) In [38]: df . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 1000 entries, 0 to 999 Data columns (total 8 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 gender 1000 non-null object 1 group 1000 non-null object 2 parental 1000 non-null object 3 lunch 1000 non-null object 4 course 1000 non-null object 5 math 1000 non-null int64 6 reading 1000 non-null int64 7 writing 1000 non-null int64 dtypes: int64(3), object(5) memory usage: 62.6+ KB In [39]: df . head () Out[39]: gender group parental lunch course math reading writing 0 female group B bachelor's degree standard none 72 72 74 1 female group C some college standard completed 69 90 88 2 female group B master's degree standard none 90 95 93 3 male group A associate's degree free/reduced none 47 57 44 4 male group C some college standard none 76 78 75 In [40]: df [ 'group' ] . unique () . tolist () Out[40]: ['group B', 'group C', 'group A', 'group D', 'group E'] Let's simplify the dataframe We can simplify the group values to the group letter Series.str Series.str : Vectorized string functions for Series and Index. In [41]: df [ 'group' ] = df [ 'group' ] . str [ - 1 ] df [ 'group' ] . unique () . tolist () Out[41]: ['B', 'C', 'A', 'D', 'E'] In [42]: df . head () Out[42]: gender group parental lunch course math reading writing 0 female B bachelor's degree standard none 72 72 74 1 female C some college standard completed 69 90 88 2 female B master's degree standard none 90 95 93 3 male A associate's degree free/reduced none 47 57 44 4 male C some college standard none 76 78 75 In [43]: df [ 'course' ] . unique () Out[43]: array(['none', 'completed'], dtype=object) Series.apply Series.apply :Invoke function on values of Series. Series . apply ( func , convert_dtype = True , args = (), ** kwargs ) In [44]: # we verify that we have never change this column values yet if 'completed' in df [ 'course' ] . unique () . tolist (): df [ 'course' ] = df [ 'course' ] . apply ( lambda x : 1 if x == 'completed' else 0 ) # we can change the column values type to boolean df [ 'course' ] = df [ 'course' ] . astype ( bool ) df [ 'course' ] . unique () Out[44]: array([False, True]) In [45]: df . head () Out[45]: gender group parental lunch course math reading writing 0 female B bachelor's degree standard False 72 72 74 1 female C some college standard True 69 90 88 2 female B master's degree standard False 90 95 93 3 male A associate's degree free/reduced False 47 57 44 4 male C some college standard False 76 78 75 Missing values In [46]: df . isna () . sum () Out[46]: gender 0 group 0 parental 0 lunch 0 course 0 math 0 reading 0 writing 0 dtype: int64 None of the column series present missing values Some questions: Does gender affect math scores? Does math scores affect gender? Does reading and writing scores affect math scores? Do math scores affect reading and writing scores? Does a group perform better at math than the rest? Does parental level education affect math scores? In [47]: df [[ 'reading' , 'math' ]] . sample ( 5 ) Out[47]: reading math 340 61 58 370 77 84 186 76 80 687 78 77 499 71 76 In [48]: df [[ 'reading' , 'math' ]] . describe () Out[48]: reading math count 1000.000000 1000.00000 mean 69.169000 66.08900 std 14.600192 15.16308 min 17.000000 0.00000 25% 59.000000 57.00000 50% 70.000000 66.00000 75% 79.000000 77.00000 max 100.000000 100.00000 It's not common at all to see a zero on scores. Here we see a 0 found at math In [49]: df [ df [ 'math' ] == 0 ] Out[49]: gender group parental lunch course math reading writing 59 female C some high school free/reduced False 0 17 10 Does this sample look possible? Why? Histograms for our selected variables In [50]: df [[ 'reading' , 'math' ]] . hist ( bins = 50 , grid = False ); Histograms for our selected variables (seaborn) seaborn.histplot() We can plot histogram in different plots using matplotlib subplots In [51]: plt . figure ( figsize = ( 12 , 4 )) sns . histplot ( df [[ 'reading' ]], bins = 50 , ax = plt . subplot ( 121 ), palette = palette ) sns . histplot ( df [[ 'math' ]], bins = 50 , ax = plt . subplot ( 122 ), palette = palette ); But knowing that by default sns.histplot merges all features into the same plot, it could be simpler In [52]: sns . histplot ( df [[ 'reading' , 'math' ]], bins = 50 , palette = palette ); Kernel Density Estimate In [53]: df [[ 'reading' , 'math' ]] . plot . kde () plt . title ( 'KDEs' ); Seaborn comes with the method seaborn.kdeplot() to create Kernel Density Plots but we can just set the histplot params kde to True to combine them. In [54]: sns . histplot ( df [[ 'reading' , 'math' ]], bins = 50 , kde = True , palette = palette ); BoxPlot In [55]: df [[ 'reading' , 'math' ]] . boxplot (); At first glance distributions looks similar as one could expect. Math scores distribution looks a bit shifted down. Boxplots with seaborn seanborn.boxplot() In [56]: sns . boxplot ( data = df [[ 'reading' , 'math' ]], palette = palette ); Boxenplots or Letter values seaborn.boxenplot() In [57]: sns . boxenplot ( data = df [[ 'reading' , 'math' ]], palette = palette ); Violinplots seaborn.violinplot() In [58]: sns . violinplot ( data = df [[ 'reading' , 'math' ]], palette = palette ); What about the relation between the scores? Do they interact? Scatter to the rescue In [59]: df . plot . scatter ( x = 'reading' , y = 'math' , s = 10 , alpha = .5 , figsize = ( 6 , 5 )) plt . title ( 'reading vs math' ); There is visual correlation between these variables. Correlation Pandas has implemented a method named corr() . DataFrame.corr() : Compute pairwise correlation of columns, excluding NA/null values. DataFrame . corr ( method = 'pearson' , min_periods = 1 ) In [60]: df [[ 'reading' , 'math' ]] . corr () Out[60]: reading math reading 1.00000 0.81758 math 0.81758 1.00000 Pandas corr() offers different correlation methods. In most cases pearson or/and spearman are the methods to go. In [61]: for method in [ 'pearson' , 'kendall' , 'spearman' ]: # iloc is used to access value at first row second column. corr = df [[ 'reading' , 'math' ]] . corr ( method = method ) . iloc [ 0 , 1 ] print ( f ' { method } correlation: { corr : .3f } ' ) pearson correlation: 0.818 kendall correlation: 0.617 spearman correlation: 0.804 We've confirmed there is a strong (linear) correlation between reading and math scores. Each variable could work as a proxy of the other variable. Boxplot on the whole dataframe In [62]: df . boxplot (); Correlation between all variables In [63]: df . corr () Out[63]: course math reading writing course 1.000000 0.177702 0.241780 0.312946 math 0.177702 1.000000 0.817580 0.802642 reading 0.241780 0.817580 1.000000 0.954598 writing 0.312946 0.802642 0.954598 1.000000 Reading and writing have a really strong correlation. Of course one could use plots In [64]: cols = [ 'math' , 'reading' , 'writing' ] for i , c1 in enumerate ( cols ): c2 = cols [ i + 1 ] if i < len ( cols ) - 1 else cols [ 0 ] df . plot . scatter ( x = c1 , y = c2 , s = 10 , alpha = .5 ) plt . title ( f ' { c1 } vs { c2 } ' ) Scatter plots with seaborn Seaborn comes with seaborn.scatterplot() . In [65]: sns . pairplot ( df . select_dtypes ( 'number' ), palette = palette ); In [66]: df [[ 'gender' , 'math' , 'reading' , 'writing' ]] . sample ( 5 ) Out[66]: gender math reading writing 663 female 65 69 67 903 female 93 100 100 443 female 73 83 76 362 female 52 58 58 137 male 70 55 56 In [67]: df [[ 'gender' , 'math' , 'reading' , 'writing' ]] . describe () Out[67]: math reading writing count 1000.00000 1000.000000 1000.000000 mean 66.08900 69.169000 68.054000 std 15.16308 14.600192 15.195657 min 0.00000 17.000000 10.000000 25% 57.00000 59.000000 57.750000 50% 66.00000 70.000000 69.000000 75% 77.00000 79.000000 79.000000 max 100.00000 100.000000 100.000000 Pie plot In [68]: df [ 'gender' ] . value_counts ( normalize = True ) . plot . pie ( figsize = ( 6 , 6 )); Seaborn doesn't come with a method to plot pie plots In [69]: df . groupby ( 'gender' ) . mean () Out[69]: course math reading writing gender female 0.355212 63.633205 72.608108 72.467181 male 0.360996 68.728216 65.473029 63.311203 In [70]: df . groupby ( 'gender' ) . boxplot (); pandas.melt() is a powerful method to unpivot a dataframe. We are going to use it to simplify use of some seaborn plots. In [71]: score_cols = df . select_dtypes ( 'number' ) . columns . tolist () id_vars = [ c for c in df . columns if c not in score_cols ] score_cols , id_vars melted = pd . melt ( df , id_vars = id_vars , var_name = 'skill' , value_name = 'score' ) melted . head () Out[71]: gender group parental lunch course skill score 0 female B bachelor's degree standard False math 72 1 female C some college standard True math 69 2 female B master's degree standard False math 90 3 male A associate's degree free/reduced False math 47 4 male C some college standard False math 76 When you make things easier to read for seaborn, seaborn will make the plots easier to read for you. In [72]: for func in [ sns . boxplot , sns . boxenplot , sns . violinplot ]: g = sns . FacetGrid ( melted , col = \"skill\" ) g . map ( func , 'score' , 'gender' , order = None , palette = palette ); In [73]: sns . pairplot ( df , palette = palette , hue = 'gender' ); In [74]: df . groupby ( 'gender' ) . plot . kde (); In [75]: df [ 'is_female' ] = df [ 'gender' ] . apply ( lambda x : 1 if x == 'female' else 0 ) df [ 'is_female' ] = df [ 'is_female' ] . astype ( float ) df [ 'is_male' ] = df [ 'gender' ] . apply ( lambda x : 1 if x == 'male' else 0 ) df [ 'is_male' ] = df [ 'is_male' ] . astype ( float ) df . head () Out[75]: gender group parental lunch course math reading writing is_female is_male 0 female B bachelor's degree standard False 72 72 74 1.0 0.0 1 female C some college standard True 69 90 88 1.0 0.0 2 female B master's degree standard False 90 95 93 1.0 0.0 3 male A associate's degree free/reduced False 47 57 44 0.0 1.0 4 male C some college standard False 76 78 75 0.0 1.0 Instead of looking at correlation between all variables we want to see how this new variables is_female correlates with the scores. Pandas gives us the method DataFrame.corrwith() for this kind of cases. In [76]: df [[ 'math' , 'reading' , 'writing' ]] . corrwith ( df [ 'is_female' ]) Out[76]: math -0.167982 reading 0.244313 writing 0.301225 dtype: float64 In [77]: df [[ 'math' , 'reading' , 'writing' ]] . corrwith ( df [ 'is_male' ]) Out[77]: math 0.167982 reading -0.244313 writing -0.301225 dtype: float64 One Hot Encoding What we have just done is known as One Hot Encoding of the gender variable. Pandas has a method to simplify this kind of conversion under the name: get_dummies() pd.get_dummies() : Convert categorical variable into dummy/indicator variables. pandas . get_dummies ( data , prefix = None , prefix_sep = '_' , dummy_na = False , columns = None , sparse = False , drop_first = False , dtype = None ) In [78]: pd . get_dummies ( df [ 'gender' ]) . head () Out[78]: female male 0 1 0 1 1 0 2 1 0 3 0 1 4 0 1 In example, we could make use of this method to create a new DataFrame with the scores and the one hot encoded version of gender variable. We use get_dummies() to encode the categorical gender variable and then we use the pd.concat() method to concatenate two DataFrames on the horizonal axis. In [79]: df_encoded = pd . concat ([ df [[ 'math' , 'reading' , 'writing' ]], pd . get_dummies ( df [ 'gender' ])], axis = 1 ) df_encoded . head () Out[79]: math reading writing female male 0 72 72 74 1 0 1 69 90 88 1 0 2 90 95 93 1 0 3 47 57 44 0 1 4 76 78 75 0 1 And then using one line of code more we could arrive to the same conclusions In [80]: df_encoded . corr () Out[80]: math reading writing female male math 1.000000 0.817580 0.802642 -0.167982 0.167982 reading 0.817580 1.000000 0.954598 0.244313 -0.244313 writing 0.802642 0.954598 1.000000 0.301225 -0.301225 female -0.167982 0.244313 0.301225 1.000000 -1.000000 male 0.167982 -0.244313 -0.301225 -1.000000 1.000000 Extra Most of ML models don't work with categorial variables. You will become familiar with method like get_dummies() from pandas or similar ones from other libraries to prepare the data that will feed your models. Sometimes, it is convenient to normalize or standardize the data. We already known that the new dataset ranges, so we could normalize it using one line of code. In [81]: df_normalized = df_encoded . div ( df_encoded . max () - df_encoded . min ()) df_normalized . head () Out[81]: math reading writing female male 0 0.72 0.867470 0.822222 1.0 0.0 1 0.69 1.084337 0.977778 1.0 0.0 2 0.90 1.144578 1.033333 1.0 0.0 3 0.47 0.686747 0.488889 0.0 1.0 4 0.76 0.939759 0.833333 0.0 1.0 The new dataset df_normalized looks like a generic dataset for any kind of ML algorithm And we can check that we don't change correlations after normalizing them. In [82]: df_normalized . corr () . round ( 14 ) == df_encoded . corr () . round ( 14 ) Out[82]: math reading writing female male math True True True True True reading True True True True True writing True True True True True female True True True True True male True True True True True Heatmap seaborn.heatmap() : Plot rectangular data as a color-encoded matrix. Heatmap is a great tool for plotting features' correlations In [83]: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 6 )) sns . heatmap ( df_normalized . corr (), annot = True , fmt = '.2f' , cmap = 'Blues' , ax = ax ); Who will approve? In [84]: approval_threshold = 40 In [85]: df [ 'approved' ] = df [ 'math' ] >= approval_threshold df [ 'approved' ] = df [ 'approved' ] . astype ( int ) df . head () Out[85]: gender group parental lunch course math reading writing is_female is_male approved 0 female B bachelor's degree standard False 72 72 74 1.0 0.0 1 1 female C some college standard True 69 90 88 1.0 0.0 1 2 female B master's degree standard False 90 95 93 1.0 0.0 1 3 male A associate's degree free/reduced False 47 57 44 0.0 1.0 1 4 male C some college standard False 76 78 75 0.0 1.0 1 In [86]: df [ 'approved' ] . value_counts ( normalize = True ) . plot . bar (); Seaborn has a method for plotting counts of feature's values. In [87]: sns . countplot ( data = df , x = 'group' , palette = palette ); The problem is that the countplot method doesn't count with a normalize parameter. So trying to plot a normalized version is to so simple as when using pandas ( Series.value_counts(normalize=True).plot.bar() ) In [88]: df [ 'approved' ] . value_counts ( normalize = True ) . to_frame () Out[88]: approved 1 0.96 0 0.04 In [89]: sns . barplot ( data = (( df [ 'approved' ] . value_counts ( normalize = True ) * 100 ) . to_frame () . reset_index () . rename ( columns = { 'approved' : '%' , 'index' : 'approved' })), x = 'approved' , y = '%' , palette = palette ); In [90]: df [[ 'gender' , 'course' , 'reading' , 'writing' , 'math' ]] . groupby ( 'gender' ) . corrwith ( df [ 'approved' ]) Out[90]: course reading writing math gender female 0.102233 0.513812 0.549594 0.548292 male 0.071767 0.317447 0.331337 0.339371 In [91]: df . groupby ( 'approved' )[ 'gender' ] . value_counts ( normalize = True ) . plot . bar (); We will try to do the same plot with seaborn In [92]: tmp = ( df . groupby ( 'approved' )[ 'gender' ] . value_counts ( normalize = True ) . to_frame () . rename ( columns = { 'gender' : '%' }) * 100 ) . reset_index () sns . barplot ( data = tmp , x = 'approved' , y = '%' , hue = 'gender' , palette = palette ); Years in a cell When plotting you can think of using one of these four approaches: Pandas Pandas + Matplotlib Pandas + Seaborn Pandas + Seaborn + Matplotlib Pandas Learning: easy Default Visual: bad Custom Visual: regular TIP: just knowing what are the plotting methods implemented in pandas is enough to start plotting many things to extract information for you (but maybe not for a presentation). Pandas + Matplotlib Learning: difficult Default Visual: regular Custom Visual: excellent but tricky (it's all about learning matplotlib, not easy to start from scratch) TIP: Think the plot you want and then using DataFrame.groupby or some condition applied to the dataframe will be enough to feed your plots. Pandas + Seaborn Learning: good Default Visual: good Custom Visual: very good TIP: Seaborn is almost about preparing a DataFrame to feed the seaborn plot you are looking for. So you need to learn about Seaborn's available plots and probably expend some time learning pandas methods like melt and pivot to transform the dataframe in an input kind of the ones seaborn likes. Pandas + Matplotlib + Seaborn Learning: difficult Default Visual: good Custom Visual: excellent TIP: Sky is the limit. Remember that seaborn was built on matplotlib. In [93]: sns . countplot ( data = df , x = 'approved' , hue = 'gender' , palette = palette ); In [94]: ax = plt . subplot () for group , g in df . groupby ([ 'approved' , 'gender' ]): g [[ 'math' ]] . hist ( bins = 50 , ax = ax , alpha = .3 , label = f ' { group [ 0 ] } { group [ 1 ] } ' ); plt . legend (); To do the same plot with seaborn we will need to convert the dataframe like the melted one and add some new column that represents a combination of gender and approved. Sometimes it's better to look for alternatives that let us do the same analysis without too much coding. In [95]: g = sns . FacetGrid ( df , col = 'approved' , row = 'gender' ) g . map ( sns . histplot , 'math' , palette = palette ); In [96]: g = sns . FacetGrid ( df , col = 'approved' , row = 'gender' ) g . map ( sns . histplot , 'reading' , palette = palette ); In [97]: g = sns . FacetGrid ( df , col = 'approved' , row = 'gender' ) g . map ( sns . histplot , 'writing' , palette = palette ); In [98]: # let's repeat the three features with violinplots for feature in [ 'reading' , 'writing' , 'math' ]: g = sns . FacetGrid ( df , col = 'approved' , row = 'gender' , sharex = True , sharey = True ) g . map ( sns . violinplot , feature , order = None , palette = palette ); If we prepare data for seaborn, seaborn will give what we want. For instance, seaborn.violinplot() permits to split the violin distribution using a secondary binary hue feature. But this just can be done when using parameters x and y . In this case we can use a dummy feature to plot what we want. Knowing this will help us to improve our previous plot. In [99]: df [ 'dummy' ] = '' # let's repeat the three features with violinplots for feature in [ 'reading' , 'writing' , 'math' ]: g = sns . FacetGrid ( df , col = 'approved' , sharey = True ) g . map ( sns . violinplot , data = df , x = 'dummy' , y = feature , hue = 'gender' , split = True , order = None , palette = palette ); g . add_legend () # we want to display the gender legend g . set_ylabels ( 'score' ) g . fig . subplots_adjust ( top = 0.8 ) g . fig . suptitle ( f 'feature: { feature } ' , fontsize = 12 , font = 'verdana' ) del df [ 'dummy' ] PairGrid seaborn.PairGrid() is a great tool that let us extend seaborn plots easily. In [100]: # this should do something similar to pairplot() but without setting the histogram in the diagonal g = sns . PairGrid ( df ) g . map ( sns . scatterplot ); In [101]: del df [ 'is_female' ] del df [ 'is_male' ] Maybe you didn't see the power of PairGrids. Let's try again with a new custom PairGrid plot with multivariate KDE subplots In [102]: # Create a cubehelix colormap to use with kdeplot cmap = sns . cubehelix_palette ( start = 0 , light = .95 , as_cmap = True ) g = sns . PairGrid ( df , diag_sharey = False ) g . map_upper ( sns . kdeplot , cmap = cmap , fill = True ) g . map_lower ( sns . kdeplot , cmap = cmap , fill = True ) g . map_diag ( sns . kdeplot , color = '#aa0000' , fill = True ); Summary In this lecture you've learnt: Some important things about using SEABORN with PANDAS! Importance of plots Using pandas for EDA Notion of One Hot Encoding Pandas pandas.read_csv() pandas.concat() pandas.get_dummies() DataFrame.info() DataFrame.head() DataFrame.sample() DataFrame.describe() DataFrame.unique() DataFrame.str DataFrame.grouby() DataFrame.sourt_values() DataFrame.corr() DataFrame.corrwith() DataFrameGroupBy.size() Pandas (plotting) DataFrame.boxplot() DataFrame.hist() DataFrame.plot() DataFrame.plot.kde() DataFrame.plot.pie() DataFrame.plot.scatter() matplotlib matplotlib.pyplot.subplots() matplotlib.pyplot.title() matplotlib.pyplot.plot() matplotlib.pyplot.suptitle() matplotlib.pyplot.subplot() matplotlib.pyplot.subplots_adjust() matplotlib.pyplot.ylabel() matplotlib.pyplot.legend() matplotlib.pyplot.figure() seaborn seaborn.boxplot() seaborn.boxenplot() seaborn.histplot() seaborn.barplot() seaborn.countplot() seaborn.scatterplot() seaborn.violinplot() seaborn.lineplot() seaborn.pairplot() seaborn.heatmap() seaborn.kdeplot() seaborn.FacetGrid() seaborn.PairGrid() In [ ]:","tags":"labs","url":"labs/lab2/notebook-1/"},{"title":"Lab 2","text":"Ex1_lab2 Tracking Covid-19 at U.S. Colleges and Universities The New York Times is releasing counts of Covid-19 cases reported on college and university campuses in the United States. Since late July, they have been conducting a rolling survey of American colleges and universities — including every four-year public institution and every private college that competes in N.C.A.A. sports — to track the number of coronavirus cases reported among students and employees. The survey now includes more than 1,900 colleges. Starting in 2021 the number of cases in 2021 is also included. Colleges and universities that have reported zero cases will be listed with a zero for cases. We have excluded missing values. This is an exercise meant to make you think of ways of grouping, aggregating, plotting, and presenting this data in a way that an audience would appreciate. Data Data can be found in the colleges.csv file. date,state,county,city,ipeds_id,college,cases,cases_2021,notes 2021-02-26,Alabama,Madison,Huntsville,100654,Alabama A&M University,41,, … 2021-02-26,Alabama,Jefferson,Birmingham,100663,University of Alabama at Birmingham,2856,570,\"Total is known to include one or more cases from a medical school, medical center, teaching hospital, clinical setting or other academic program in health sciences.\" The fields have the following definitions: date : The date of the last update. state : The state where the college is located. county : The county where the college is located. city : The city where the college is located. ipeds_id : The Integrated Postsecondary Education Data System (IPEDS) ID number for the college. college : The name of the college or university. cases : The total number of reported Covid-19 cases among university students and employees in all fields, including those whose roles as doctors, nurses, pharmacists or medical students put them at higher risk of contracting the virus, since the beginning of the pandemic. In [2]: % matplotlib inline import pandas as pd ; pd . set_option ( 'max_columns' , 6 ) import seaborn as sns import matplotlib.pyplot as plt In [3]: colleges = pd . read_csv ( 'colleges.csv' ) In [4]: colleges . shape Out[4]: (1946, 7) In [5]: from IPython.core.display import display , HTML display ( HTML ( colleges . head ( 10 ) . to_html ())) date state county city ipeds_id college cases 0 2021-05-26 Alabama Madison Huntsville 100654 Alabama A&M University 41 1 2021-05-26 Alabama Montgomery Montgomery 100724 Alabama State University 2 2 2021-05-26 Alabama Limestone Athens 100812 Athens State University 45 3 2021-05-26 Alabama Lee Auburn 100858 Auburn University 2742 4 2021-05-26 Alabama Montgomery Montgomery 100830 Auburn University at Montgomery 220 5 2021-05-26 Alabama Walker Jasper 102429 Bevill State Community College 4 6 2021-05-26 Alabama Jefferson Birmingham 100937 Birmingham-Southern College 263 7 2021-05-26 Alabama Limestone Tanner 101514 Calhoun Community College 137 8 2021-05-26 Alabama Tallapoosa Alexander City 100760 Central Alabama Community College 49 9 2021-05-26 Alabama Coffee Enterprise 101143 Enterprise State Community College 76 Some ways of grouping the data would be by state or by county . When presenting, a plot is usually better that just arrays of numbers. In [6]: colleges . groupby ( '_____' ) . agg ([ 'mean' , 'std' ]) . shape Out[6]: (55, 2) In [53]: colleges . groupby ( '_____' ) . agg ({ 'cases' : sum }) Out[53]: cases county Abbeville 0 Acadia 149 Ada 1642 Adair 749 Adams 948 ... ... Yellow Medicine 93 Yellowstone 246 Yolo 678 York 804 Yuma 41 736 rows × 1 columns You are encouraged to make your own story with plots. Optional: add geospatial data and maps to your plots People have written libraries to display geospatial data and nice visualizations of maps. To run the following cells in your environment you will need to install the libraries geoplot and geopandas using conda or pip . Feel free to try this example, although, for this class it's entirely optional . The example below is from the geoplot website. In [2]: #!conda install geoplot -c conda-forge #!conda install geopandas In [1]: import geopandas as gpd import geoplot as gplt In [2]: usa_cities = gpd . read_file ( gplt . datasets . get_path ( 'usa_cities' )) usa_cities . head () Out[2]: id POP_2010 ELEV_IN_FT STATE geometry 0 53 40888.0 1611.0 ND POINT (-101.29627 48.23251) 1 101 52838.0 830.0 ND POINT (-97.03285 47.92526) 2 153 15427.0 1407.0 ND POINT (-98.70844 46.91054) 3 177 105549.0 902.0 ND POINT (-96.78980 46.87719) 4 192 17787.0 2411.0 ND POINT (-102.78962 46.87918) In [3]: continental_usa_cities = usa_cities . query ( 'STATE not in [\"HI\", \"AK\", \"PR\"]' ) gplt . pointplot ( continental_usa_cities ); Out[3]: <AxesSubplot:> In [4]: contiguous_usa = gpd . read_file ( gplt . datasets . get_path ( 'contiguous_usa' )) ax = gplt . polyplot ( contiguous_usa ) gplt . pointplot ( continental_usa_cities , ax = ax ); Out[4]: <AxesSubplot:> In [5]: import geoplot.crs as gcrs ax = gplt . webmap ( contiguous_usa , projection = gcrs . WebMercator ()) gplt . pointplot ( continental_usa_cities , ax = ax ) Out[5]: <GeoAxesSubplot:> In [0]:","tags":"labs","url":"labs/lab2/notebook-2/"},{"title":"Lab 2","text":"Lec2_Ex2 CS109A Introduction to PANDAS Lecture 2, Exercise 2: PANDAS Intro 2 Harvard University Fall 2021 Instructors : Pavlos Protopapas and Natesh Pillai Exercise 2: PANDAS Intro 2 Let's get some more practice with a few of the core PANDAS functions. In [2]: import pandas as pd We'll continue working with StudentsPerformance dataset from Exercise 1. In [3]: # import the CSV file df = pd . read_csv ( \"StudentsPerformance.csv\" ) df . head () Indexing - iloc and loc It's very important to understand the differences between loc and iloc. Looking at the next cell code one could think that they do the same thing. (When you query just for one row you obtain an object whose name is the index of the selected row.) In [133]: df . iloc [ 10 ] == df . loc [ 10 ] In [134]: all ( df . iloc [ 10 ] == df . loc [ 10 ]) The first time we loaded the CSV into a DataFrame we didn't tell pandas to interpret a specific column as an index, so pandas created an index for us. Whether the ordering imposed on our data by this index should be respected is a matter on which iloc and loc disagree. To really learn the difference of iloc and loc we need to shuffle the rows To do that we can use the sample method to take a new sample of the dataframe original size (frac=1 means that) _Bonus: Stop and consider: what is the purpose of setting a random_state when we call them sample method?_ In [135]: df = df . sample ( frac = 1 , random_state = 109 ) df Now let's repeat our code from ealier. In [136]: df . iloc [ 10 ] == df . loc [ 10 ] In [137]: all ( df . iloc [ 10 ] == df . loc [ 10 ]) It turns out that loc filters by index value (something like where df.index == value ). That is, loc 's results are depend only on the indices (which are now scrambled after sampling). The actual positions of elements in the DataFrame are ignored. In [147]: df . loc [ 10 ] By contrast, iloc filters by row position (something like where df.index == df.index[value] ) So iloc 's results depend on the actual positions of elements in a pandas data structure. The indices of these elements are ignored. It's this difference that explains counterintuitive results like this: In [150]: df . index [ 10 ] Consider a single row index with iloc In [146]: df . iloc [ 10 ] And take note of where you can find the index in output formatted like this. Enough chat. Time for... Exercise In the cell below, fill in the blank so that the row5 variable stores the 5th row of df. To be clear, imagine our DataFrame looked as follows: Index Words 0 this 1 is 2 not 3 easy We'd say the 1st row is the one with this word, the 2nd row is the one with is word, the 3rd row is the one with not word, etc. In [0]: ### edTest(test_a) ### row5 = ________ row5 You can display the first rows to have a better understanding of what you did. Can you find the row you've just selected? In [0]: df . iloc [: 5 ] Notice how we can use familar Python slice notation with iloc and loc ! Sorting We scrambled out df earlier with sample . We should also know how to tidy things up. Exercise In the cell below, fill in the blank so that sorted_df now stores df after sorting it by the math score column in decreasing order ( HINT ) In [11]: ### edTest(test_b) ### sorted_df = ________ sorted_df Exercise In the cell below, fill in the blank so that sorted_row5 stores the 5th row of sorted_df . To be clear, imagine our sorted DataFrame looked as follows: Index Words 3 easy 1 is 2 not 0 this We'd say the 1st row is the one with easy , the 2nd row is the one with is , the 3rd row is the one with not , etc. In [12]: ### edTest(test_c) ### sorted_row5 = ________ sorted_row5 Can you find the row you've just selected? In [149]: # len('head()') < len('iloc[:5]') :) sorted_df . head () Column Naming Conventions How you've named your df columns can affect the amount of typing required to manipulate your data, the readability of your code, and even the syntax optons you have. Let's take a look at some best practices for naming columns. In [10]: df We can see that there are columns names whose lengths are not confortable for coding. What we can do to make our life easier: Try to work with short columns names Try to avoid characters like spaces that will allow us to use column access without brackets Try working in only lower or upper case (prefably lower case; there's no need to shout) Our df already conforms to this last suggestion. We'll find that, after some renaming, things should become easier for us, making: expressions like these ones: condition = ( df [ 'test preparation course' ] != 'completed' ) & ( df [ 'writing score' ] > df [ 'writing score' ] . median ()) to become: condition = ( df . course != 'completed' ) & ( df . writing > df . writing . median ()) Exercise In the cell below fill in the blank with these goals in mind: rename column race/ethnicity to race rename column parental level of education to peduc raname column test preparation course to course remove score (included the left space char) from the rest of the columns HINT1 : Don't by shy, check the documentation if you need some help HINT2 : in many cases, it's faster to access docstrings using help help ( df . rename ) HINT3 : TAB autocomplete can be ued to you explore an object's available methods and attributes.\\ HINT4 : Still more exciting, place your cursor after the opening paranthesis of a function call and press SHIFT+TAB once or twice. Instantly, you're presented with a docstring. It's a whole new world! 🌈 In [0]: ### edTest(test_d) ### df = df . rename ( columns = { ____ }) df Look for missing values Missing values in our DataFrames can cause many issues. They can cause certain operations and function calls to fail and throw an error. Perhaps worse, this problems can happen 'silently,' affecting our results without us realizing there is a problem. Unless we take precautions of course! The first step is locating missingness. This dataset doesn't have any missing values. So we'll make some ourselves and 'poke a holes' in the Dataframe. In [19]: # 'poking holes' in the data df . iloc [ 0 , 5 ] = None df . iloc [ 2 , 2 ] = None Exercise Fill in the blank to display whether or not entries in the first 5 rows are missing ( HINT ) Solution In [0]: df . ___ . ___ Exercise Fill in the blanks to sum the total number of missing values in each of the dataset's columns. In [0]: ### edTest(test_e) ### resultE = df . ___ . ___ display ( resultE ) Now let's deal with these 'holes' we've just made Exercise Fill the missing math entry with that column's mean . Hint: Select subsets of the data frame using a column name or names Note: The blanks here represent just one way of doing this. Don't feel constrain by the blanks here. Solution In [0]: df___ = df___ . ___ ( df___ . ___ ) df . head () Exercise Drop the row with the missing peduc from the DataFrame. Hint: to make it easier, consider that this is now the only remaining missing value Solution In [0]: df = df___ df . head () Categorical Columns - nunique() and unique() If some of your data is categorical (e.g., taking on a descredte set of values with an inherent ordering) you'll often what to know exactly how many unique values you're dealing with. nunique and unique are here to help and can be used on a single column or across multiple columns. Please consider, unique will return all unique values. What if you ask for the uniques of a column with 1 million different values? 🤔 This particular method should be used wisely. In [46]: df . nunique () In [63]: df [ 'gender' ] . unique () . tolist () In [66]: # the line below represents the usage that should be avoided # df['math'].unique() Exercise Fill in the blanks using unique() and nunique() to complete the method print_uniques to help us to learn more about the categorical variables in our data. In [0]: ### edTest(test_f) ### def print_uniques ( df , col , limit = 10 ): \"\"\"Print column's uniques values when the number of column's uniques is lower or equal than limit \"\"\" n = df [ col ] . ______ if n <= limit : print ( f ' { col } :' , df [ col ] . ______ . tolist ()) else : print ( f ' { col } :' , f 'more than { limit } uniques' ) for col in df . columns : print_uniques ( df , col ) Descriptive statistics In Pandas, DataFrames have a simple method for displaying summary statistics. In [0]: df . describe () Exercise Sometimes we don't want to access all these statistics. In the cell below fill in the blanks to get the mean and the standard deviation of the writing and reading columns ( HINT ). In [0]: ### edTest(test_g) ### resultG = df [[ ____ ]] . aggregate ([ ____ ]) display ( resultG ) Now we can group our dataframe by a specific column(s) value's and aggregate the other columns. Hint: Try using agg() as an alternative to spare some typing Exercise Group the dataframe by peduc and gender while aggregating math , reading , and writing with the mean and course with the mode. Tip: Again, don't feel constained by the blanks. This command may span multiple lines. In [33]: df . ___ ( _____ ) . ___ ( _________ ) Out[33]: math reading writing peduc gender associate's degree female 65.250000 74.120690 74.000000 male 70.764151 67.433962 65.405660 bachelor's degree female 68.349206 77.285714 78.380952 male 70.581818 68.090909 67.654545 high school female 59.322581 68.268817 66.731183 male 64.705882 61.480392 58.539216 master's degree female 66.500000 76.805556 77.638889 male 74.826087 73.130435 72.608696 some college female 65.406780 73.550847 74.050847 male 69.009259 64.990741 63.148148 some high school female 59.296703 69.109890 68.285714 male 67.955672 64.693182 61.375000 We would likely want to treat peduc as an 'ordinal' rather than a categorical variable as it does have an inherent ordering. Feel free to try using indexing to sort the rows. But after the grouping we now have multiple indices for each row! There's still more Pandas to discover 🐼","tags":"labs","url":"labs/lab2/notebook-3/"},{"title":"Lecture 2: Introduction to PANDAS and EDA","text":"Slides Lecture 2: Introduction to PANDAS and EDA (Notebook) Exercises Lecture 2: Introduction to Data Science (Notebook) Lecture 2: Introduction to PANDAS 2 (Notebook)","tags":"lectures","url":"lectures/lecture02/"},{"title":"Lecture 2: Introduction to Data Science","text":"Lec2_Ex1 CS109a Introduction to Data Science Lecture 2, Exercise 1: PANDAS Intro Harvard University Fall 2021 Instructors : Pavlos Protopapas and Natesh Pillai Exercise 1: PANDAS Intro As discussed in class, PANDAS is Python library that contains highly useful data structures, including DataFrames, which makes Exploratory Data Analysis (EDA) easy. Here, we get practice with some of the elementary functions. In [1]: import pandas as pd For this exercise, we will be working with StudentsPerformance dataset made available through Kaggle . It contains information about the exame score of ( fictional ) high school students. In [2]: # import the CSV file df = pd . read_csv ( \"StudentsPerformance.csv\" ) PANDAS Basics 🐼 Let's get started with basic functionality of PANDAS! Exercise In the cell below fill in the blank to display general dataframe info rmation. _Tip: The Pandas documention will be your best friend. But in many cases, a simple tab autocomplete can find what your looking for._ In [5]: ### edTest(test_a) ### df . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 1000 entries, 0 to 999 Data columns (total 8 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 gender 1000 non-null object 1 race/ethnicity 1000 non-null object 2 parental level of education 1000 non-null object 3 lunch 1000 non-null object 4 test preparation course 1000 non-null object 5 math score 1000 non-null int64 6 reading score 1000 non-null int64 7 writing score 1000 non-null int64 dtypes: int64(3), object(5) memory usage: 62.6+ KB Examine the output carefully. There's a lot in there. Can you interpret each column? What about the details in header footer? Exercise In the cell below, fill in the blank so that the variable cols stores the df 's column names. NOTE: Please keep the type of the data structure as a <class 'pandas.core.indexes.base.Index'> . Do not have to convert this to a list.\\ Tip: Reviewing the DataFrame object itself might help In [6]: ### edTest(test_b) ### cols = df . columns # Check at least the type is the right one assert type ( cols ) == pd . core . indexes . base . Index Exercise In the cell below, fill in the blank so that: num_cols stores the number of columns in df \\ ( HINT ) In [7]: ### edTest(test_c) ### num_rows = df . shape [ 0 ] num_cols = df . shape [ 1 ] Exercise In the cell below, fill in the blank so that first_seven is equal to the first 7 rows. ( HINT ) In [9]: ### edTest(test_d) ### first_seven = df . head ( 7 ) Exercise In the cell below, fill in the blank so that last_four is equal to the last 4 rows. ( HINT ) In [8]: ### edTest(test_e) ### last_four = df . tail ( 4 ) Exercise In the cell below, fill in the blank so that the unique_parental_education_levels variable stores a list of the 6 distinct values found within the parental level of education column of df .\\ Tip: Again, try searching the documentation In [26]: ### edTest(test_f) ### unique_parental_education_levels = df [ 'parental level of education' ] . unique () unique_parental_education_levels Out[26]: array([\"bachelor's degree\", 'some college', \"master's degree\", \"associate's degree\", 'high school', 'some high school'], dtype=object) In [27]: # we can check if they are really 6 print ( 'Are there 6 unique values?:' , len ( unique_parental_education_levels ) == 6 ) # we can display them here print () print ( unique_parental_education_levels ) Are there 6 unique values?: True [\"bachelor's degree\" 'some college' \"master's degree\" \"associate's degree\" 'high school' 'some high school'] Exercise In the cell below, fill in the blank so that the scored_100_at_math variable stores the DataFrame row(s) that correspond to everyone who scored 100 at math.\\ Hint: Think 'indexing.' Specifically, boolean indexing In [13]: ### edTest(test_g) ### scored_100_at_math = df [ df [ \"math score\" ] == 100 ] scored_100_at_math Out[13]: gender race/ethnicity parental level of education lunch test preparation course math score reading score writing score 149 male group E associate's degree free/reduced completed 100 100 93 451 female group E some college standard none 100 92 97 458 female group E bachelor's degree standard none 100 100 100 623 male group A some college standard completed 100 96 86 625 male group D some college standard completed 100 97 99 916 male group E bachelor's degree standard completed 100 100 100 962 female group E associate's degree standard none 100 100 100 Some observations about conditions In [14]: # this shows that using the condition with 'loc' is the same as not using 'loc' condition = df [ 'math score' ] == 100 all ( df [ condition ] == df . loc [ condition ]) Out[14]: True In [15]: # the condition is a boolean series to be used with the whole dataframe # the condition will be used to filter the dataframe to those rows where condition value is True # we can see that condition has as many rows as the original dataframe condition Out[15]: 0 False 1 False 2 False 3 False 4 False ... 995 False 996 False 997 False 998 False 999 False Name: math score, Length: 1000, dtype: bool Exercise In the cell below, fill in the blank to display scores' descriptive statistics ( HINT ). In [16]: ### edTest(test_h) ### df . describe () Out[16]: math score reading score writing score count 1000.00000 1000.000000 1000.000000 mean 66.08900 69.169000 68.054000 std 15.16308 14.600192 15.195657 min 0.00000 17.000000 10.000000 25% 57.00000 59.000000 57.750000 50% 66.00000 70.000000 69.000000 75% 77.00000 79.000000 79.000000 max 100.00000 100.000000 100.000000 Exercise In the cell below, fill in the blanks so that the uncompleted_with_good_writing_score variable stores the DataFrame rows that correspond to everyone who hasn't completed the preparation course and there writing score is above the median. In [25]: ### edTest(test_i) ### # not completed preparation course condition cond1 = df [ 'test preparation course' ] != 'completed' # writing score above the median cond2 = df [ 'writing score' ] > df [ 'writing score' ] . median () uncompleted_with_good_writing_score = df . loc [ cond1 & cond2 ] uncompleted_with_good_writing_score Out[25]: gender race/ethnicity parental level of education lunch test preparation course math score reading score writing score 0 female group B bachelor's degree standard none 72 72 74 2 female group B master's degree standard none 90 95 93 4 male group C some college standard none 76 78 75 5 female group B associate's degree standard none 71 83 78 12 female group B high school standard none 65 81 73 ... ... ... ... ... ... ... ... ... 981 male group D some high school standard none 81 78 78 984 female group C some high school standard none 74 75 82 992 female group D associate's degree free/reduced none 55 76 76 993 female group D bachelor's degree free/reduced none 62 72 74 999 female group D some college free/reduced none 77 86 86 251 rows × 8 columns Obvervation: the '&' operator differs from the 'and' operator In [0]:","tags":"Lectures","url":"lectures/lecture02/notebook-1/"},{"title":"Lecture 2: Introduction to PANDAS 2","text":"Lec2_Ex2 CS109A Introduction to PANDAS Lecture 2, Exercise 2: PANDAS Intro 2 Harvard University Fall 2021 Instructors : Pavlos Protopapas and Natesh Pillai Exercise 2: PANDAS Intro 2 Let's get some more practice with a few of the core PANDAS functions. In [1]: import pandas as pd We'll continue working with StudentsPerformance dataset from Exercise 1. In [2]: # import the CSV file df = pd . read_csv ( \"StudentsPerformance.csv\" ) df . head () Out[2]: gender race/ethnicity parental level of education lunch test preparation course math score reading score writing score 0 female group B bachelor's degree standard none 72 72 74 1 female group C some college standard completed 69 90 88 2 female group B master's degree standard none 90 95 93 3 male group A associate's degree free/reduced none 47 57 44 4 male group C some college standard none 76 78 75 Indexing - iloc and loc It's very important to understand the differences between loc and iloc. Looking at the next cell code one could think that they do the same thing. (When you query just for one row you obtain an object whose name is the index of the selected row.) In [3]: df . iloc [ 10 ] == df . loc [ 10 ] Out[3]: gender True race/ethnicity True parental level of education True lunch True test preparation course True math score True reading score True writing score True Name: 10, dtype: bool In [4]: all ( df . iloc [ 10 ] == df . loc [ 10 ]) Out[4]: True The first time we loaded the CSV into a DataFrame we didn't tell pandas to interpret a specific column as an index, so pandas created an index for us. Whether the ordering imposed on our data by this index should be respected is a matter on which iloc and loc disagree. To really learn the difference of iloc and loc we need to shuffle the rows To do that we can use the sample method to take a new sample of the dataframe original size (frac=1 means that) _Bonus: Stop and consider: what is the purpose of setting a random_state when we call them sample method?_ In [5]: df = df . sample ( frac = 1 , random_state = 109 ) df Out[5]: gender race/ethnicity parental level of education lunch test preparation course math score reading score writing score 301 male group D some high school free/reduced none 56 54 52 895 female group E some high school free/reduced none 32 34 38 763 female group B high school standard none 62 62 63 854 male group C some high school standard none 62 64 55 49 male group C high school standard completed 82 84 82 ... ... ... ... ... ... ... ... ... 399 male group D some high school standard none 60 59 54 141 female group C some college free/reduced none 59 62 64 757 male group E bachelor's degree free/reduced completed 70 68 72 245 male group C associate's degree standard none 85 76 71 262 female group C some high school free/reduced none 44 50 51 1000 rows × 8 columns Now let's repeat our code from ealier. In [6]: df . iloc [ 10 ] == df . loc [ 10 ] Out[6]: gender False race/ethnicity False parental level of education False lunch True test preparation course False math score False reading score False writing score False dtype: bool In [7]: all ( df . iloc [ 10 ] == df . loc [ 10 ]) Out[7]: False It turns out that loc filters by index value (something like where df.index == value ). That is, loc 's results are depend only on the indices (which are now scrambled after sampling). The actual positions of elements in the DataFrame are ignored. In [8]: df . loc [ 10 ] Out[8]: gender male race/ethnicity group C parental level of education associate's degree lunch standard test preparation course none math score 58 reading score 54 writing score 52 Name: 10, dtype: object By contrast, iloc filters by row position (something like where df.index == df.index[value] ) So iloc 's results depend on the actual positions of elements in a pandas data structure. The indices of these elements are ignored. It's this difference that explains counterintuitive results like this: In [9]: df . index [ 10 ] Out[9]: 342 Consider a single row index with iloc In [10]: df . iloc [ 10 ] Out[10]: gender female race/ethnicity group B parental level of education high school lunch standard test preparation course completed math score 69 reading score 76 writing score 74 Name: 342, dtype: object And take note of where you can find the index in output formatted like this. Enough chat. Time for... Exercise In the cell below, fill in the blank so that the row5 variable stores the 5th row of df. To be clear, imagine our DataFrame looked as follows: Index Words 0 this 1 is 2 not 3 easy We'd say the 1st row is the one with this word, the 2nd row is the one with is word, the 3rd row is the one with not word, etc. In [16]: ### edTest(test_a) ### row5 = df . iloc [ 4 ] row5 Out[16]: gender male race/ethnicity group C parental level of education high school lunch standard test preparation course completed math score 82 reading score 84 writing score 82 Name: 49, dtype: object You can display the first rows to have a better understanding of what you did. Can you find the row you've just selected? In [17]: df . iloc [: 5 ] Out[17]: gender race/ethnicity parental level of education lunch test preparation course math score reading score writing score 301 male group D some high school free/reduced none 56 54 52 895 female group E some high school free/reduced none 32 34 38 763 female group B high school standard none 62 62 63 854 male group C some high school standard none 62 64 55 49 male group C high school standard completed 82 84 82 Notice how we can use familar Python slice notation with iloc and loc ! Sorting We scrambled out df earlier with sample . We should also know how to tidy things up. Exercise In the cell below, fill in the blank so that sorted_df now stores df after sorting it by the math score column in decreasing order ( HINT ) In [18]: ### edTest(test_b) ### sorted_df = df . sort_values ( by = 'math score' , ascending = False ) sorted_df Out[18]: gender race/ethnicity parental level of education lunch test preparation course math score reading score writing score 623 male group A some college standard completed 100 96 86 625 male group D some college standard completed 100 97 99 149 male group E associate's degree free/reduced completed 100 100 93 458 female group E bachelor's degree standard none 100 100 100 962 female group E associate's degree standard none 100 100 100 ... ... ... ... ... ... ... ... ... 145 female group C some college free/reduced none 22 39 33 787 female group B some college standard none 19 38 32 17 female group B some high school free/reduced none 18 32 28 980 female group B high school free/reduced none 8 24 23 59 female group C some high school free/reduced none 0 17 10 1000 rows × 8 columns Exercise In the cell below, fill in the blank so that sorted_row5 stores the 5th row of sorted_df . To be clear, imagine our sorted DataFrame looked as follows: Index Words 3 easy 1 is 2 not 0 this We'd say the 1st row is the one with easy , the 2nd row is the one with is , the 3rd row is the one with not , etc. In [21]: ### edTest(test_c) ### sorted_row5 = sorted_df . iloc [ 4 ] sorted_row5 Out[21]: gender female race/ethnicity group E parental level of education associate's degree lunch standard test preparation course none math score 100 reading score 100 writing score 100 Name: 962, dtype: object Can you find the row you've just selected? In [22]: # len('head()') < len('iloc[:5]') :) sorted_df . head () Out[22]: gender race/ethnicity parental level of education lunch test preparation course math score reading score writing score 623 male group A some college standard completed 100 96 86 625 male group D some college standard completed 100 97 99 149 male group E associate's degree free/reduced completed 100 100 93 458 female group E bachelor's degree standard none 100 100 100 962 female group E associate's degree standard none 100 100 100 Column Naming Conventions How you've named your df columns can affect the amount of typing required to manipulate your data, the readability of your code, and even the syntax optons you have. Let's take a look at some best practices for naming columns. In [23]: df Out[23]: gender race/ethnicity parental level of education lunch test preparation course math score reading score writing score 301 male group D some high school free/reduced none 56 54 52 895 female group E some high school free/reduced none 32 34 38 763 female group B high school standard none 62 62 63 854 male group C some high school standard none 62 64 55 49 male group C high school standard completed 82 84 82 ... ... ... ... ... ... ... ... ... 399 male group D some high school standard none 60 59 54 141 female group C some college free/reduced none 59 62 64 757 male group E bachelor's degree free/reduced completed 70 68 72 245 male group C associate's degree standard none 85 76 71 262 female group C some high school free/reduced none 44 50 51 1000 rows × 8 columns We can see that there are columns names whose lengths are not confortable for coding. What we can do to make our life easier: Try to work with short columns names Try to avoid characters like spaces that will allow us to use column access without brackets Try working in only lower or upper case (prefably lower case; there's no need to shout) Our df already conforms to this last suggestion. We'll find that, after some renaming, things should become easier for us, making: expressions like these ones: condition = ( df [ 'test preparation course' ] != 'completed' ) & ( df [ 'writing score' ] > df [ 'writing score' ] . median ()) to become: condition = ( df . course != 'completed' ) & ( df . writing > df . writing . median ()) Exercise In the cell below fill in the blank with these goals in mind: rename column race/ethnicity to race rename column parental level of education to peduc raname column test preparation course to course remove score (included the left space char) from the rest of the columns HINT1 : Don't by shy, check the documentation if you need some help HINT2 : in many cases, it's faster to access docstrings using help help ( df . rename ) HINT3 : TAB autocomplete can be ued to you explore an object's available methods and attributes.\\ HINT4 : Still more exciting, place your cursor after the opening paranthesis of a function call and press SHIFT+TAB once or twice. Instantly, you're presented with a docstring. It's a whole new world! 🌈 In [24]: df . columns Out[24]: Index(['gender', 'race/ethnicity', 'parental level of education', 'lunch', 'test preparation course', 'math score', 'reading score', 'writing score'], dtype='object') In [27]: ### edTest(test_d) ### df = df . rename ( columns = { 'race/ethnicity' : 'race' , 'parental level of education' : 'peduc' , 'test preparation course' : 'course' , 'math score' : 'math' , 'reading score' : 'reading' , 'writing score' : 'writing' , }) df Out[27]: gender race peduc lunch course math reading writing 301 male group D some high school free/reduced none 56 54 52 895 female group E some high school free/reduced none 32 34 38 763 female group B high school standard none 62 62 63 854 male group C some high school standard none 62 64 55 49 male group C high school standard completed 82 84 82 ... ... ... ... ... ... ... ... ... 399 male group D some high school standard none 60 59 54 141 female group C some college free/reduced none 59 62 64 757 male group E bachelor's degree free/reduced completed 70 68 72 245 male group C associate's degree standard none 85 76 71 262 female group C some high school free/reduced none 44 50 51 1000 rows × 8 columns Look for missing values Missing values in our DataFrames can cause many issues. They can cause certain operations and function calls to fail and throw an error. Perhaps worse, this problems can happen 'silently,' affecting our results without us realizing there is a problem. Unless we take precautions of course! The first step is locating missingness. This dataset doesn't have any missing values. So we'll make some ourselves and 'poke a holes' in the Dataframe. In [28]: # 'poking holes' in the data df . iloc [ 0 , 5 ] = None df . iloc [ 2 , 2 ] = None Exercise Fill in the blank to display whether or not entries in the first 5 rows are missing ( HINT ) Solution In [30]: df . isna () . head () Out[30]: gender race peduc lunch course math reading writing 301 False False False False False True False False 895 False False False False False False False False 763 False False True False False False False False 854 False False False False False False False False 49 False False False False False False False False Exercise Fill in the blanks to sum the total number of missing values in each of the dataset's columns. In [31]: ### edTest(test_e) ### resultE = df . isna () . sum () display ( resultE ) gender 0 race 0 peduc 1 lunch 0 course 0 math 1 reading 0 writing 0 dtype: int64 Now let's deal with these 'holes' we've just made Exercise Fill the missing math entry with that column's mean . Hint: Select subsets of the data frame using a column name or names Note: The blanks here represent just one way of doing this. Don't feel constrain by the blanks here. Solution In [32]: df [ 'math' ] = df [ 'math' ] . fillna ( df [ 'math' ] . mean ()) df . head () Out[32]: gender race peduc lunch course math reading writing 301 male group D some high school free/reduced none 66.099099 54 52 895 female group E some high school free/reduced none 32.000000 34 38 763 female group B None standard none 62.000000 62 63 854 male group C some high school standard none 62.000000 64 55 49 male group C high school standard completed 82.000000 84 82 Exercise Drop the row with the missing peduc from the DataFrame. Hint: to make it easier, consider that this is now the only remaining missing value Solution In [33]: df = df . dropna () df . head () Out[33]: gender race peduc lunch course math reading writing 301 male group D some high school free/reduced none 66.099099 54 52 895 female group E some high school free/reduced none 32.000000 34 38 854 male group C some high school standard none 62.000000 64 55 49 male group C high school standard completed 82.000000 84 82 790 female group B high school standard none 48.000000 62 60 Categorical Columns - nunique() and unique() If some of your data is categorical (e.g., taking on a descredte set of values with an inherent ordering) you'll often what to know exactly how many unique values you're dealing with. nunique and unique are here to help and can be used on a single column or across multiple columns. Please consider, unique will return all unique values. What if you ask for the uniques of a column with 1 million different values? 🤔 This particular method should be used wisely. In [34]: df . nunique () Out[34]: gender 2 race 5 peduc 6 lunch 2 course 2 math 82 reading 72 writing 77 dtype: int64 In [35]: df [ 'gender' ] . unique () . tolist () Out[35]: ['male', 'female'] In [36]: # the line below represents the usage that should be avoided # df['math'].unique() Exercise Fill in the blanks using unique() and nunique() to complete the method print_uniques to help us to learn more about the categorical variables in our data. In [37]: ### edTest(test_f) ### def print_uniques ( df , col , limit = 10 ): \"\"\"Print column's uniques values when the number of column's uniques is lower or equal than limit \"\"\" n = df [ col ] . nunique () if n <= limit : print ( f ' { col } :' , df [ col ] . unique () . tolist ()) else : print ( f ' { col } :' , f 'more than { limit } uniques' ) for col in df . columns : print_uniques ( df , col ) gender: ['male', 'female'] race: ['group D', 'group E', 'group C', 'group B', 'group A'] peduc: ['some high school', 'high school', \"bachelor's degree\", 'some college', \"associate's degree\", \"master's degree\"] lunch: ['free/reduced', 'standard'] course: ['none', 'completed'] math: more than 10 uniques reading: more than 10 uniques writing: more than 10 uniques Descriptive statistics In Pandas, DataFrames have a simple method for displaying summary statistics. In [38]: df . describe () Out[38]: math reading writing count 999.000000 999.000000 999.000000 mean 66.103202 69.176176 68.059059 std 15.166754 14.605740 15.202426 min 0.000000 17.000000 10.000000 25% 57.000000 59.000000 57.500000 50% 66.000000 70.000000 69.000000 75% 77.000000 79.000000 79.000000 max 100.000000 100.000000 100.000000 Exercise Sometimes we don't want to access all these statistics. In the cell below fill in the blanks to get the mean and the standard deviation of the writing and reading columns ( HINT ). In [39]: ### edTest(test_g) ### resultG = df [[ 'writing' , 'reading' ]] . aggregate ([ 'mean' , 'std' ]) display ( resultG ) writing reading mean 68.059059 69.176176 std 15.202426 14.605740 Now we can group our dataframe by a specific column(s) value's and aggregate the other columns. Hint: Try using agg() as an alternative to spare some typing Exercise Group the dataframe by peduc and gender while aggregating math , reading , and writing with the mean and course with the mode. Tip: Again, don't feel constained by the blanks. This command may span multiple lines. In [42]: df . groupby ([ 'peduc' , 'gender' ]) . agg ({ 'math' : 'mean' , 'reading' : 'mean' , 'writing' : 'mean' }) Out[42]: math reading writing peduc gender associate's degree female 65.250000 74.120690 74.000000 male 70.764151 67.433962 65.405660 bachelor's degree female 68.349206 77.285714 78.380952 male 70.581818 68.090909 67.654545 high school female 59.322581 68.268817 66.731183 male 64.705882 61.480392 58.539216 master's degree female 66.500000 76.805556 77.638889 male 74.826087 73.130435 72.608696 some college female 65.406780 73.550847 74.050847 male 69.009259 64.990741 63.148148 some high school female 59.296703 69.109890 68.285714 male 67.955672 64.693182 61.375000 We would likely want to treat peduc as an 'ordinal' rather than a categorical variable as it does have an inherent ordering. Feel free to try using indexing to sort the rows. But after the grouping we now have multiple indices for each row! There's still more Pandas to discover 🐼","tags":"Lectures","url":"lectures/lecture02/notebook-2/"},{"title":"Lecture 2: Introduction to PANDAS","text":"Lec2_notebook CS109a Introduction to PANDAS Lecture 1, Pandas Intro Harvard University Fall 2021 Instructors : Pavlos Protopapas and Natesh Pillai Pandas PANDAS is Python library that contains highly useful data structures, including DataFrames, which makes Exploratory Data Analysis (EDA) easy. Here we will see some of the elementary functions in practice. Installing Using conda conda install pandas Using pip pip install pandas TIP: You can try installing a library from a jupyter notebook cell adding \"!\" # using conda !conda install pandas # or using pip !pip install pandas PANDAS Basics Let's get started with basic functionality of PANDAS! Importing pandas importing pandas is as simple as next line import pandas But because of lazyness for convenience we usually import it as pd In [4]: import pandas as pd You can always check for the version of almost any library using __version__ In [5]: pd . __version__ Out[5]: '1.3.2' Pandas data structures The main data structures in pandas are the Series (useful for time series) and the DataFrame . Series Formal: One-dimensional ndarray with axis labels (including time series). Roughly: You can think of it as kind of spreadsheet column or a relational database table of one column DataFrame Formal: Two-dimensional, size-mutable, potentially heterogeneous tabular data. Roughly: to a relational database table. Where every DataFrame's column is a Series . Both DataFrames and Series always have an index . pd.Series pd.Series(data=None, index=None, dtype=None, name=None, copy=False) When not using an index pandas will add an index for us: >>> s1 = pd . Series ( range ( 0 , 50 , 10 )) 0 0 1 10 2 20 3 30 4 40 dtype : int64 The data can be strings not just numbers The index can be anything, but the data and index should have the same length. In [6]: s = pd . Series ( data = [ 'A' , 'B' , 'C' , 'D' , 'E' ], index = range ( 10 , 5 , - 1 )) s Out[6]: 10 A 9 B 8 C 7 D 6 E dtype: object We can independently access the series' values or its index In [7]: s . values Out[7]: array(['A', 'B', 'C', 'D', 'E'], dtype=object) In [8]: s . index Out[8]: RangeIndex(start=10, stop=5, step=-1) pd.DataFrame pd.DataFrame(data=None, index=None, columns=None, dtype=None, copy=None) This data structure also contains labeled axes (rows and columns). index First Name Last Name 0 Ann Gatton 1 John Fosa 2 Zack Kaufman DataFrame class offers powerful ways to create them. For instance the two code lines belows generate the same DataFrame object. # using rows pd . DataFrame ( data = [[ 1 , 2 ], [ 3 , 4 ], [ 5 , 6 ]], columns = [ 'A' , 'B' ]) # using columns pd . DataFrame ( data = { 'A' :[ 1 , 3 , 5 ], 'B' : [ 2 , 4 , 6 ]}) A B 0 1 2 1 3 4 2 5 6 Loading data It's common to create DataFrames, but usually we read data from external sources. This is easy to do in Pandas. In [9]: tpl = 'https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas. {} .html' for m in [ 'clipboard' , 'csv' , 'excel' , 'feather' , 'fwf' , 'gbq' , 'hdf' , 'html' , 'json' , 'parquet' , 'pickle' , 'spss' , 'sql' , 'sql_query' , 'sql_table' , 'stata' , 'table' , 'xml' ]: method = f 'read_ { m } ' url = tpl . format ( method ) print ( f ' { method } \\t { url } ' ) read_clipboard https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_clipboard.html read_csv https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html read_excel https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html read_feather https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_feather.html read_fwf https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_fwf.html read_gbq https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_gbq.html read_hdf https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_hdf.html read_html https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_html.html read_json https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_json.html read_parquet https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_parquet.html read_pickle https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_pickle.html read_spss https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_spss.html read_sql https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql.html read_sql_query https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql_query.html read_sql_table https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql_table.html read_stata https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_stata.html read_table https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_table.html read_xml https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_xml.html Example The method read_html is powerful and requires a bit of expirience. The first line processes the url and extracts html that match the criteria into a DataFrames The header will come as first row of the DataFrame, so in line 2 we use the first row values as columns names for the dataframe and finally we remove the first row. In [10]: df = pd . read_html ( 'https://en.wikipedia.org/wiki/Harvard_University' , match = 'School' )[ 0 ] In [11]: df Out[11]: 0 1 0 School Founded 1 Harvard College 1636 2 Medicine 1782 3 Divinity 1816 4 Law 1817 5 Dental Medicine 1867 6 Arts and Sciences 1872 7 Business 1908 8 Extension 1910 9 Design 1914 10 Education 1920 11 Public Health 1922 12 Government 1936 13 Engineering and Applied Sciences 2007 In [15]: df = pd . read_html ( 'https://en.wikipedia.org/wiki/Harvard_University' , match = 'School' )[ 0 ] df = df . rename ( columns = df . iloc [ 0 ])[ 1 :] df Out[15]: School Founded 1 Harvard College 1636 2 Medicine 1782 3 Divinity 1816 4 Law 1817 5 Dental Medicine 1867 6 Arts and Sciences 1872 7 Business 1908 8 Extension 1910 9 Design 1914 10 Education 1920 11 Public Health 1922 12 Government 1936 13 Engineering and Applied Sciences 2007 pd.read_csv read_csv is the recommended starting point for anyone learning pandas. You can read its docs here . Let's use it to load Avocado prices It is a well known fact that Millenials LOVE Avocado Toast. It's also a well known fact that all Millenials live in their parents basements. Clearly, they aren't buying home because they are buying too much Avocado Toast! But maybe there's hope… if a Millenial could find a city with cheap avocados, they could live out the Millenial American Dream. The table below represents weekly 2018 retail scan data for National retail volume (units) and price. Retail scan data comes directly from retailers' cash registers based on actual retail sales of Hass avocados. Starting in 2013, the table below reflects an expanded, multi-outlet retail data set. Multi-outlet reporting includes an aggregation of the following channels: grocery, mass, club, drug, dollar and military. The Average Price (of avocados) in the table reflects a per unit (per avocado) cost, even when multiple units (avocados) are sold in bags. The Product Lookup codes (PLU's) in the table are only for Hass avocados. Other varieties of avocados (e.g. greenskins) are not included in this table. Some relevant columns in the dataset: Date : The date of the observation AveragePrice : the average price of a single avocado type : conventional or organic year : the year Region : the city or region of the observation Total Volume : Total number of avocados sold 4046 : Total number of avocados with PLU 4046 sold 4225 : Total number of avocados with PLU 4225 sold 4770 : Total number of avocados with PLU 4770 sold Load dataset Read a compressed csv file. We ask pandas to use first csv column as index to avoid creating a new one by default. TIP : when you are blind about what you are loading or you already know it is a big dataset you can fix the number of rows to be loaded using the parameter nrows ( nrows=None to load all and it's the default value) In [12]: df = pd . read_csv ( 'avocado.csv.zip' , index_col = 0 , compression = 'zip' , nrows = None ) Roughly exploring the data We can quickly see the dataframe's dimension In [13]: df . shape Out[13]: (18249, 13) The shape is a tuple with the number of rows and the number of columns In [14]: len ( df . index ), len ( df . columns ) Out[14]: (18249, 13) Show only the columns' names In [15]: df . columns Out[15]: Index(['Date', 'AveragePrice', 'Total Volume', '4046', '4225', '4770', 'Total Bags', 'Small Bags', 'Large Bags', 'XLarge Bags', 'type', 'year', 'region'], dtype='object') The columns attribute is not a python list. In [16]: type ( df . columns ) == pd . Index Out[16]: True Show only the index In [17]: df . index Out[17]: Int64Index([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, ... 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], dtype='int64', length=18249) Sometimes some column type is incorrect and possible ways to detect it is using df.info() or df.dtypes . In [20]: df . dtypes Out[20]: Date object AveragePrice float64 Total Volume float64 4046 float64 4225 float64 4770 float64 Total Bags float64 Small Bags float64 Large Bags float64 XLarge Bags float64 type object year int64 region object dtype: object In example, here Date is an object (the way pandas save strings). We can use a better column type for that. In [21]: df [ 'Date' ] = pd . to_datetime ( df [ 'Date' ]) df . dtypes Out[21]: Date datetime64[ns] AveragePrice float64 Total Volume float64 4046 float64 4225 float64 4770 float64 Total Bags float64 Small Bags float64 Large Bags float64 XLarge Bags float64 type object year int64 region object dtype: object show first (by default: 5) rows In [22]: df . head () Out[22]: Date AveragePrice Total Volume 4046 4225 4770 Total Bags Small Bags Large Bags XLarge Bags type year region 0 2015-12-27 1.33 64236.62 1036.74 54454.85 48.16 8696.87 8603.62 93.25 0.0 conventional 2015 Albany 1 2015-12-20 1.35 54876.98 674.28 44638.81 58.33 9505.56 9408.07 97.49 0.0 conventional 2015 Albany 2 2015-12-13 0.93 118220.22 794.70 109149.67 130.50 8145.35 8042.21 103.14 0.0 conventional 2015 Albany 3 2015-12-06 1.08 78992.15 1132.00 71976.41 72.58 5811.16 5677.40 133.76 0.0 conventional 2015 Albany 4 2015-11-29 1.28 51039.60 941.48 43838.39 75.78 6183.95 5986.26 197.69 0.0 conventional 2015 Albany show last 2 rows In [24]: df . tail ( 2 ) Out[24]: Date AveragePrice Total Volume 4046 4225 4770 Total Bags Small Bags Large Bags XLarge Bags type year region 10 2018-01-14 1.93 16205.22 1527.63 2981.04 727.01 10969.54 10919.54 50.00 0.0 organic 2018 WestTexNewMexico 11 2018-01-07 1.62 17489.58 2894.77 2356.13 224.53 12014.15 11988.14 26.01 0.0 organic 2018 WestTexNewMexico display some data info Sometimes the Dataframe method info() is a great way to take a first data snapshot with few columns datasets. It displays: columns names number of rows (as entries) number of non null values data type per column (per Series) memory usage TIP : if you know that the number of columns is high (maybe when printing df.shape[1] ), then you can pass a False flag to the method info() to reduce the information just to global information. In [25]: few_columns = True df . info ( verbose = few_columns ) <class 'pandas.core.frame.DataFrame'> Int64Index: 18249 entries, 0 to 11 Data columns (total 13 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Date 18249 non-null datetime64[ns] 1 AveragePrice 18249 non-null float64 2 Total Volume 18249 non-null float64 3 4046 18249 non-null float64 4 4225 18249 non-null float64 5 4770 18249 non-null float64 6 Total Bags 18249 non-null float64 7 Small Bags 18249 non-null float64 8 Large Bags 18249 non-null float64 9 XLarge Bags 18249 non-null float64 10 type 18249 non-null object 11 year 18249 non-null int64 12 region 18249 non-null object dtypes: datetime64[ns](1), float64(9), int64(1), object(2) memory usage: 1.9+ MB Descriptive statistics We can take a fast look at some data statistics with one line of code In [26]: df . describe () Out[26]: AveragePrice Total Volume 4046 4225 4770 Total Bags Small Bags Large Bags XLarge Bags year count 18249.000000 1.824900e+04 1.824900e+04 1.824900e+04 1.824900e+04 1.824900e+04 1.824900e+04 1.824900e+04 18249.000000 18249.000000 mean 1.405978 8.506440e+05 2.930084e+05 2.951546e+05 2.283974e+04 2.396392e+05 1.821947e+05 5.433809e+04 3106.426507 2016.147899 std 0.402677 3.453545e+06 1.264989e+06 1.204120e+06 1.074641e+05 9.862424e+05 7.461785e+05 2.439660e+05 17692.894652 0.939938 min 0.440000 8.456000e+01 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000 2015.000000 25% 1.100000 1.083858e+04 8.540700e+02 3.008780e+03 0.000000e+00 5.088640e+03 2.849420e+03 1.274700e+02 0.000000 2015.000000 50% 1.370000 1.073768e+05 8.645300e+03 2.906102e+04 1.849900e+02 3.974383e+04 2.636282e+04 2.647710e+03 0.000000 2016.000000 75% 1.660000 4.329623e+05 1.110202e+05 1.502069e+05 6.243420e+03 1.107834e+05 8.333767e+04 2.202925e+04 132.500000 2017.000000 max 3.250000 6.250565e+07 2.274362e+07 2.047057e+07 2.546439e+06 1.937313e+07 1.338459e+07 5.719097e+06 551693.650000 2018.000000 Data Selection Column names They represent a powerful tool to access subgroups or individual values (in combination with other methods) in a DataFrame. $[]$ vs $[[]]$ Using column name as key will return the column values as type Series # returns a Series with dataframe values for column 'my_col' df [ 'my_col' ] # this gives the same access but is not recommended. Can't work when there is a space or a not allowed char in the name. df . my_col Using a python list of column names as key will return a sub dataframe with that columns # returns a DataFrame with the two columns df [[ 'my_col_A' , 'my_col_B' ]] # returns a Series with my_col_A values df [[ 'my_col_A' ]] In [27]: # this should be False because we just say that column name inside brackets returns a Series type ( df [ 'AveragePrice' ]) == pd . DataFrame Out[27]: False In [28]: type ( df [ 'AveragePrice' ]) == pd . Series Out[28]: True In [29]: # this should be True because we say that a list of column names inside brackets returns a sub dataframe type ( df [[ 'AveragePrice' ]]) == pd . DataFrame Out[29]: True Accessing column Series In [30]: df [ 'AveragePrice' ] . head () Out[30]: 0 1.33 1 1.35 2 0.93 3 1.08 4 1.28 Name: AveragePrice, dtype: float64 Accessing subdataframe of one column In [31]: df [[ 'AveragePrice' ]] . head () Out[31]: AveragePrice 0 1.33 1 1.35 2 0.93 3 1.08 4 1.28 Let's try to visualize the difference once more using the method values that return the data as numpy array. In [32]: df [ 'AveragePrice' ] . values Out[32]: array([1.33, 1.35, 0.93, ..., 1.87, 1.93, 1.62]) In [33]: df [[ 'AveragePrice' ]] . values Out[33]: array([[1.33], [1.35], [0.93], ..., [1.87], [1.93], [1.62]]) This is because Series.values returns a one dimensional array with the column values and DataFrame.values returns a two dimensional array that could be thought as an array of rows. In [34]: df [ 'AveragePrice' ] . values . shape , df [[ 'AveragePrice' ]] . values . shape Out[34]: ((18249,), (18249, 1)) Exercise In the cell below fill in the blanks to display the first 10 rows of a sub-dataframe with columns Date and AveragePrice . Remember that DataFrame is a class that allows chaining composition. In [35]: df [[ 'Date' , 'AveragePrice' ]] . head ( 10 ) Out[35]: Date AveragePrice 0 2015-12-27 1.33 1 2015-12-20 1.35 2 2015-12-13 0.93 3 2015-12-06 1.08 4 2015-11-29 1.28 5 2015-11-22 1.26 6 2015-11-15 0.99 7 2015-11-08 0.98 8 2015-11-01 1.02 9 2015-10-25 1.07 Filtering An expresion like the one show below represents a condition that will return a boolean list with many boolean values as values in the Series df['Date'] . And this number, its length is the same size of the number of rows in the DataFrame df. df [ 'Date' ] == '2015-10-25' The boolean list will be True for rows where the condition is True and False otherwise. A list of boolean values let as filter a DataFrame based on the condition. In [36]: condition = df [ 'Date' ] == '2015-10-25' condition Out[36]: 0 False 1 False 2 False 3 False 4 False ... 7 False 8 False 9 False 10 False 11 False Name: Date, Length: 18249, dtype: bool In [38]: df [ condition ] . head () Out[38]: Date AveragePrice Total Volume 4046 4225 4770 Total Bags Small Bags Large Bags XLarge Bags type year region 9 2015-10-25 1.07 74338.76 842.40 64757.44 113.00 8625.92 8061.47 564.45 0.0 conventional 2015 Albany 9 2015-10-25 1.09 358478.08 236814.29 64607.97 304.36 56751.46 31826.88 24924.58 0.0 conventional 2015 Atlanta 9 2015-10-25 1.19 656892.03 53766.25 397911.35 49085.74 156128.69 149987.55 6141.14 0.0 conventional 2015 BaltimoreWashington 9 2015-10-25 1.11 59874.45 29521.58 10089.82 6551.57 13711.48 13660.98 0.00 50.5 conventional 2015 Boise 9 2015-10-25 1.02 534249.47 4005.39 430725.78 191.31 99326.99 94581.94 4745.05 0.0 conventional 2015 Boston It's common to find this kind of expressions directly df [ df [ 'Date' ] == '2015-10-25' ] Logical expressions Example of conditions condition = df [ col ] > value condition = df [ col ] <= value condition = df [ col ] == value condition = df [ col ] != value # in list condition = df [ col ] . isin ([ value1 , value2 ]) # not in list condition = ~ df [ col ] . isin ([ value1 , value2 ]) # between (inclusive) condition = df [ col ] . between ( value1 , value2 ) Then we can combine different conditions with logical operators like \"&\" or \"|\". df . loc [ cond1 & cond2 ] df . loc [ cond1 | cond2 ] These above expressions can be executed without the loc operator df [ cond1 & cond2 ] df [ cond1 | cond2 ] TIP: many problems can be avoided using parenthesis for each simple condition in situations where we need to combine two or more conditions. In [39]: df [( df [ 'Date' ] == '2015-10-25' ) & ( df [ 'AveragePrice' ] < .90 )] . head () Out[39]: Date AveragePrice Total Volume 4046 4225 4770 Total Bags Small Bags Large Bags XLarge Bags type year region 9 2015-10-25 0.86 1010394.81 557469.46 301143.50 49959.10 101822.75 96417.63 5279.41 125.71 conventional 2015 DallasFtWorth 9 2015-10-25 0.88 933623.58 437329.85 313129.29 81274.85 101889.59 57577.21 44260.60 51.78 conventional 2015 Houston 9 2015-10-25 0.83 761261.71 435986.90 240689.98 19968.66 64616.17 64585.35 30.82 0.00 conventional 2015 PhoenixTucson 9 2015-10-25 0.86 4912068.04 2542914.87 1537781.45 247539.31 583832.41 475267.20 108231.39 333.82 conventional 2015 SouthCentral 9 2015-10-25 0.82 635873.60 363487.08 166607.85 31960.04 73818.63 72717.86 1100.77 0.00 conventional 2015 WestTexNewMexico # be careful with expressions like this that will fail when doing the bit operation df [ df [ 'Date' ] == '2015-10-25' & df [ 'AveragePrice' ] < .90 ] .loc[] vs .iloc[] Accessing rows .loc[] This operator allows us to access information by index label, but by definition it could be used with a boolean array as we saw with conditions: Access a group of rows and columns by label(s) or a boolean array. .loc[] is primarily label based, but may also be used with a boolean array. When using df.info() we discover that the number of unique values for index (index domain values) is between 0 and 11 included. So, we can use .loc to filter the rows where the index value is 9. In [43]: df . loc [ 9 ] Out[43]: Date AveragePrice Total Volume 4046 4225 4770 Total Bags Small Bags Large Bags XLarge Bags type year region 9 2015-10-25 1.07 74338.76 842.40 64757.44 113.00 8625.92 8061.47 564.45 0.00 conventional 2015 Albany 9 2015-10-25 1.09 358478.08 236814.29 64607.97 304.36 56751.46 31826.88 24924.58 0.00 conventional 2015 Atlanta 9 2015-10-25 1.19 656892.03 53766.25 397911.35 49085.74 156128.69 149987.55 6141.14 0.00 conventional 2015 BaltimoreWashington 9 2015-10-25 1.11 59874.45 29521.58 10089.82 6551.57 13711.48 13660.98 0.00 50.50 conventional 2015 Boise 9 2015-10-25 1.02 534249.47 4005.39 430725.78 191.31 99326.99 94581.94 4745.05 0.00 conventional 2015 Boston ... ... ... ... ... ... ... ... ... ... ... ... ... ... 9 2018-01-21 1.27 3159.80 92.12 73.17 0.00 2994.51 2117.69 876.82 0.00 organic 2018 Syracuse 9 2018-01-21 1.52 6871.05 76.66 407.09 0.00 6387.30 6375.55 11.75 0.00 organic 2018 Tampa 9 2018-01-21 1.63 1283987.65 108705.28 259172.13 1490.02 914409.26 710654.40 203526.59 228.27 organic 2018 TotalUS 9 2018-01-21 1.83 189317.99 27049.44 33561.32 439.47 128267.76 76091.99 51947.50 228.27 organic 2018 West 9 2018-01-21 1.87 13766.76 1191.92 2452.79 727.94 9394.11 9351.80 42.31 0.00 organic 2018 WestTexNewMexico 432 rows × 13 columns .iloc[] This operator allows us to access information by index position in the way we usually do with other programing languages like C. Purely integer-location based indexing for selection by position. .iloc[] is primarily integer position based (from 0 to length-1 of the axis), but may also be used with a boolean array. When using df.iloc[9] we are going to access to the 10th row in the DataFrame df. The returned value should be of type Series with the row values ( df.iloc[9].values ) as values and the columns names as the Series index. In [47]: df . iloc [ 9 ] Out[47]: Date 2015-10-25 00:00:00 AveragePrice 1.07 Total Volume 74338.76 4046 842.4 4225 64757.44 4770 113.0 Total Bags 8625.92 Small Bags 8061.47 Large Bags 564.45 XLarge Bags 0.0 type conventional year 2015 region Albany Name: 9, dtype: object In [36]: type ( df . iloc [ 9 ]) Out[36]: pandas.core.series.Series The name of the series is the index label value of the original dataframe. TIP: practice to really learn how and when to use .loc vs i.loc Mathematical and other methods on a DataFrame Pandas Series and DataFrame offers access to hundred of methods to operate on them like: sum(), mul(), mean(), std(), max(), min(), etc . All of these methods usually operate by default over columns but they can operate over rows. Look at the next cell results and try to think about what happened (take a look at fields like type or region). In [48]: df . sum () /tmp/ipykernel_32/1703867807.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. df.sum() Out[48]: AveragePrice 25657.7 Total Volume 15523402593.400002 4046 5347110739.26 4225 5386275717.93 4770 416802342.13 Total Bags 4373175798.389999 Small Bags 3324870837.51 Large Bags 991615770.55 XLarge Bags 56689177.33 type conventionalconventionalconventionalconvention... year 36792683 region AlbanyAlbanyAlbanyAlbanyAlbanyAlbanyAlbanyAlba... dtype: object In [49]: df [ 'AveragePrice' ] . mean () Out[49]: 1.4059784097758825 Missing Data This is a critical problem for any Data Scientist and deserves its own Lecture. What to do when some of the data are missing? Pandas offers some options to explore a dataframe looking for missing data. # returns a boolean dataframe of the same size with True values for cells where values are NaN df . isna () # returns a boolean dataframe of the same size with True values for cells where values aren't NaN df . notna () # alias of the above methods df . isnull () df . notnull () Count the number of NaN values for every column In [50]: df . isna () . sum () Out[50]: Date 0 AveragePrice 0 Total Volume 0 4046 0 4225 0 4770 0 Total Bags 0 Small Bags 0 Large Bags 0 XLarge Bags 0 type 0 year 0 region 0 dtype: int64 Count the number of NaN values per row In [51]: df . isna () . sum ( axis = 1 ) Out[51]: 0 0 1 0 2 0 3 0 4 0 .. 7 0 8 0 9 0 10 0 11 0 Length: 18249, dtype: int64 Count the total number of NaN values in the dataframe In [52]: df . isna () . sum () . sum () Out[52]: 0 Select the rows with at least one NaN value In [53]: df [ df . isna () . any ( axis = 1 )] Out[53]: Date AveragePrice Total Volume 4046 4225 4770 Total Bags Small Bags Large Bags XLarge Bags type year region There are specific methods related to face this problem like: fillna() bfill() ffill() dropna() It's important to learn to handle missing data Dropping Sometimes you'll want to discard information. Here is an example of how to use the drop method to do that. df . drop ( labels = None , axis = 0 , index = None , columns = None , level = None , inplace = False , errors = 'raise' ) In [54]: drop_columns = [ '4046' , '4225' , '4770' , 'Total Bags' , 'Small Bags' , 'Large Bags' , 'XLarge Bags' ] df = df . drop ( columns = drop_columns ) df Out[54]: Date AveragePrice Total Volume type year region 0 2015-12-27 1.33 64236.62 conventional 2015 Albany 1 2015-12-20 1.35 54876.98 conventional 2015 Albany 2 2015-12-13 0.93 118220.22 conventional 2015 Albany 3 2015-12-06 1.08 78992.15 conventional 2015 Albany 4 2015-11-29 1.28 51039.60 conventional 2015 Albany ... ... ... ... ... ... ... 7 2018-02-04 1.63 17074.83 organic 2018 WestTexNewMexico 8 2018-01-28 1.71 13888.04 organic 2018 WestTexNewMexico 9 2018-01-21 1.87 13766.76 organic 2018 WestTexNewMexico 10 2018-01-14 1.93 16205.22 organic 2018 WestTexNewMexico 11 2018-01-07 1.62 17489.58 organic 2018 WestTexNewMexico 18249 rows × 6 columns Sorting sort_values and sort_index are common methods when using pandas. sort_values : Sort by the values along either axis. df . sort_values ( by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' , ignore_index = False , key = None ) Next cell filter data for a particular Date and type. Then sort values by region in ascending order and finally display only the first 10 rows. In [55]: condition = ( df [ 'Date' ] == '2018-01-07' ) & ( df [ 'type' ] == 'organic' ) df [ condition ] . sort_values ( by = 'region' ) . head ( 10 ) Out[55]: Date AveragePrice Total Volume type year region 11 2018-01-07 1.54 4816.90 organic 2018 Albany 11 2018-01-07 1.53 15714.11 organic 2018 Atlanta 11 2018-01-07 1.15 82282.71 organic 2018 BaltimoreWashington 11 2018-01-07 1.77 2553.90 organic 2018 Boise 11 2018-01-07 1.91 30096.00 organic 2018 Boston 11 2018-01-07 1.17 9115.92 organic 2018 BuffaloRochester 11 2018-01-07 1.95 156341.57 organic 2018 California 11 2018-01-07 1.08 28741.11 organic 2018 Charlotte 11 2018-01-07 1.83 41573.25 organic 2018 Chicago 11 2018-01-07 1.71 13141.82 organic 2018 CincinnatiDayton Sorting can use more columns using a python list with some parameters. In [56]: df [ condition ] . sort_values ( by = [ 'region' , 'AveragePrice' ], ascending = [ True , False ]) . head () Out[56]: Date AveragePrice Total Volume type year region 11 2018-01-07 1.54 4816.90 organic 2018 Albany 11 2018-01-07 1.53 15714.11 organic 2018 Atlanta 11 2018-01-07 1.15 82282.71 organic 2018 BaltimoreWashington 11 2018-01-07 1.77 2553.90 organic 2018 Boise 11 2018-01-07 1.91 30096.00 organic 2018 Boston sort_index : Sort object by labels (along an axis) df . sort_index ( axis = 0 , level = None , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' , sort_remaining = True , ignore_index = False , key = None ) Next cell sort rows based on the index values (in ascending order) In [57]: df . sort_index () Out[57]: Date AveragePrice Total Volume type year region 0 2015-12-27 1.33 64236.62 conventional 2015 Albany 0 2016-12-25 1.85 8657.87 organic 2016 PhoenixTucson 0 2015-12-27 1.25 73109.90 conventional 2015 Pittsburgh 0 2016-12-25 1.90 11376.97 organic 2016 Philadelphia 0 2016-12-25 1.27 5601.65 organic 2016 Orlando ... ... ... ... ... ... ... 52 2017-01-01 2.06 39260.55 organic 2017 NewYork 52 2017-01-01 1.11 476239.03 conventional 2017 NorthernNewEngland 52 2017-01-01 2.00 115256.09 organic 2017 Northeast 52 2017-01-01 0.93 547565.88 conventional 2017 Atlanta 52 2017-01-01 0.97 142347.90 conventional 2017 Roanoke 18249 rows × 6 columns Renaming It's very common to rename things for convenience. In example, when column names are too long or use special characters it could come in handy to short them. The method rename() is a great tool to help us in many situations. Let's see a simple example In [58]: df . rename ( columns = { 'AveragePrice' : 'price' , 'Total Volume' : 'volume' }, inplace = True ) df Out[58]: Date price volume type year region 0 2015-12-27 1.33 64236.62 conventional 2015 Albany 1 2015-12-20 1.35 54876.98 conventional 2015 Albany 2 2015-12-13 0.93 118220.22 conventional 2015 Albany 3 2015-12-06 1.08 78992.15 conventional 2015 Albany 4 2015-11-29 1.28 51039.60 conventional 2015 Albany ... ... ... ... ... ... ... 7 2018-02-04 1.63 17074.83 organic 2018 WestTexNewMexico 8 2018-01-28 1.71 13888.04 organic 2018 WestTexNewMexico 9 2018-01-21 1.87 13766.76 organic 2018 WestTexNewMexico 10 2018-01-14 1.93 16205.22 organic 2018 WestTexNewMexico 11 2018-01-07 1.62 17489.58 organic 2018 WestTexNewMexico 18249 rows × 6 columns Counting Counting number of values without NaNs We already saw different ways to access to the number of rows. But what if you want to count the number of rows with not NaN values? The count() method counts non-NA cells for each column or row In [59]: df . count () Out[59]: Date 18249 price 18249 volume 18249 type 18249 year 18249 region 18249 dtype: int64 Counting number of unique values per column (Series) in the DataFrame Number of unique for one Series In [60]: df . region . nunique () Out[60]: 54 Number of unique values for every Series in the DataFrame In [61]: df . nunique () Out[61]: Date 169 price 259 volume 18237 type 2 year 4 region 54 dtype: int64 Unique values In [62]: df [ 'type' ] . unique () Out[62]: array(['conventional', 'organic'], dtype=object) Remember that we can easily change a DataFrame or Series output into a python list In [63]: df [ 'type' ] . unique () . tolist () Out[63]: ['conventional', 'organic'] Counting rows based on unique values value_counts() return a Series containing counts of unique rows in the DataFrame DataFrame . value_counts ( subset = None , normalize = False , sort = True , ascending = False , dropna = True ) This method is simple but powerful for simple exploration. Let's look some examples In [64]: df . value_counts ( subset = 'type' ) Out[64]: type conventional 9126 organic 9123 dtype: int64 In [65]: df . value_counts ( subset = 'year' , sort = False ) Out[65]: year 2015 5615 2016 5616 2017 5722 2018 1296 dtype: int64 We can use a bigger subset for a more detailed view In [66]: df . value_counts ( subset = [ 'year' , 'type' ], sort = False ) Out[66]: year type 2015 conventional 2808 organic 2807 2016 conventional 2808 organic 2808 2017 conventional 2862 organic 2860 2018 conventional 648 organic 648 dtype: int64 And we just need add one more flag to access to the same but normalized values In [67]: df . value_counts ( subset = [ 'year' , 'type' ], sort = False , normalize = True ) * 100 Out[67]: year type 2015 conventional 15.387145 organic 15.381665 2016 conventional 15.387145 organic 15.387145 2017 conventional 15.683051 organic 15.672092 2018 conventional 3.550880 organic 3.550880 dtype: float64 Grouping Have you been looking for power: meet groupby() A groupby operation involves some combination of splitting the object, applying a function, and combining the results. This can be used to group large amounts of data and compute operations on these groups. DataFrame . groupby ( by = None , axis = 0 , level = None , as_index = True , sort = True , group_keys = True , squeeze =< no_default > , observed = False , dropna = True ) Next cell mimics in some way the value_counts() behaviour. Split dataset into subdaframes where each subdataframe.year is unique Count the number of rows without NaNs for every column In [68]: df . groupby ( 'year' ) . count () Out[68]: Date price volume type region year 2015 5615 5615 5615 5615 5615 2016 5616 5616 5616 5616 5616 2017 5722 5722 5722 5722 5722 2018 1296 1296 1296 1296 1296 The above rows present the same values because the original dataset is free of NaN values. It's a great dataset: No NaNs and Avocados everywhere. Functions What about max()? In [69]: df . groupby ( 'year' ) . max () Out[69]: Date price volume type region year 2015 2015-12-27 2.79 44655461.51 organic WestTexNewMexico 2016 2016-12-25 3.25 52288697.89 organic WestTexNewMexico 2017 2017-12-31 3.17 61034457.10 organic WestTexNewMexico 2018 2018-03-25 2.30 62505646.52 organic WestTexNewMexico And mean()? In [70]: df . groupby ( 'year' ) . mean () Out[70]: price volume year 2015 1.375590 7.810274e+05 2016 1.338640 8.584206e+05 2017 1.515128 8.623393e+05 2018 1.347531 1.066928e+06 Do you find something different between using max and mean? What are your thoughts? In [74]: df . groupby ( 'year' ) . describe () Out[74]: price volume count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max year 2015 5615.0 1.375590 0.375595 0.49 1.07 1.300 1.67 2.79 5615.0 7.810274e+05 3.171256e+06 84.56 6931.6300 76146.82 400176.6800 44655461.51 2016 5616.0 1.338640 0.393708 0.51 1.04 1.300 1.56 3.25 5616.0 8.584206e+05 3.478732e+06 385.55 10643.6850 109597.29 451107.2925 52288697.89 2017 5722.0 1.515128 0.432906 0.44 1.22 1.490 1.77 3.17 5722.0 8.623393e+05 3.481957e+06 515.01 13790.6975 122915.75 426454.5125 61034457.10 2018 1296.0 1.347531 0.305858 0.56 1.13 1.345 1.56 2.30 1296.0 1.066928e+06 4.285501e+06 2064.90 17690.9825 157175.09 529462.2450 62505646.52 There exist other methods that can be chain to gropuby(). In example first and last will return the first and the last row of each group respectivily. In [78]: df . groupby ( 'year' ) . first () Out[78]: Date price volume type region year 2015 2015-12-27 1.33 64236.62 conventional Albany 2016 2016-12-25 1.52 73341.73 conventional Albany 2017 2017-12-31 1.47 113514.42 conventional Albany 2018 2018-03-25 1.57 149396.50 conventional Albany Previous call is similar to using head, but head() keeps the group index where first set a new index: the year In [85]: df . groupby ( 'year' ) . head ( 1 ) Out[85]: Date price volume type year region 0 2015-12-27 1.33 64236.62 conventional 2015 Albany 0 2016-12-25 1.52 73341.73 conventional 2016 Albany 0 2017-12-31 1.47 113514.42 conventional 2017 Albany 0 2018-03-25 1.57 149396.50 conventional 2018 Albany Aggregate aggregate : Aggregate using one or more operations over the specified axis ( agg is an alias). Next cell shows the aggregated avocado price and volume values from year 2018 In [81]: condition = ( df [ 'year' ] == 2018 ) df [ condition ][[ 'price' , 'volume' ]] . agg ([ 'min' , 'mean' , 'std' , 'max' ]) Out[81]: price volume min 0.560000 2.064900e+03 mean 1.347531 1.066928e+06 std 0.305858 4.285501e+06 max 2.300000 6.250565e+07 aggregate can be applied to dataframes though it can be chained with groupby() In [86]: df . groupby ( 'year' )[[ 'price' , 'volume' ]] . agg ([ 'min' , 'mean' , 'std' , 'max' ]) Out[86]: price volume min mean std max min mean std max year 2015 0.49 1.375590 0.375595 2.79 84.56 7.810274e+05 3.171256e+06 44655461.51 2016 0.51 1.338640 0.393708 3.25 385.55 8.584206e+05 3.478732e+06 52288697.89 2017 0.44 1.515128 0.432906 3.17 515.01 8.623393e+05 3.481957e+06 61034457.10 2018 0.56 1.347531 0.305858 2.30 2064.90 1.066928e+06 4.285501e+06 62505646.52 Suppose you need a way to extract percentiles. The quantile() method can be applied directly on a dataframe to extract the thing you want. In [87]: df [ condition ][[ 'price' , 'volume' ]] . quantile ( .10 ) Out[87]: price 0.970 volume 8174.655 Name: 0.1, dtype: float64 But there are cases where you need to extract more than that. For those cases it can be convenient to create custom methods to be used with aggregation. In [88]: def percentil_10 ( x ): return x . quantile ( .10 ) def percentil_90 ( x ): return x . quantile ( .90 ) df [ condition ][[ 'price' , 'volume' ]] . agg ([ percentil_10 , 'median' , percentil_90 ]) Out[88]: price volume percentil_10 0.970 8174.655 median 1.345 157175.090 percentil_90 1.750 1810981.615 In [89]: df . groupby ( 'year' )[[ 'price' , 'volume' ]] . agg ([ percentil_10 , 'median' , percentil_90 ]) Out[89]: price volume percentil_10 median percentil_90 percentil_10 median percentil_90 year 2015 0.96 1.300 1.90 2431.434 76146.82 1285267.958 2016 0.88 1.300 1.86 4146.935 109597.29 1351865.735 2017 0.98 1.490 2.07 5889.687 122915.75 1398304.817 2018 0.97 1.345 1.75 8174.655 157175.09 1810981.615 Summary In this lecture you've learnt: How to install pandas About pandas What are Series and DataFrame data structures How to create a simple DataFrame How to load a DataFrame with an external data source How to access column Series How to access row Series (index) The differences between loc[] and iloc[] Different ways to start exploring the data general structure Different ways to access to description statistics How to look for missing data How to do data filtering using conditions How to do sorting How to do counting How to group information How to do aggregation Facts: We've only imported pandas! Almost everything was about accessing and processing the data, and not creating it. Topics left out maybe for other lectures: DataFrame operations: How to add a column to a DataFrame DataFrame inter columns operations apply() applymap() pipe() Merging DataFrames (merge) Concatenating DataFrames (concat) Appending DataFrames, Series or a simple row (append) Using loops with: iterrows() itertuples() groupby() Next lecture Plotting with PANDAS (showing the importance of plots) EDA with PANDAS (using Seaborn if possible) In [0]:","tags":"Lectures","url":"lectures/lecture02/notebook-3/"},{"title":"Lab 1","text":"cs109a_section_scaffold_1 CS109A Introduction to Data Science Lab 01: Introduction to Web Scraping Harvard University Fall 2021 Instructors : Pavlos Protopapas and Natesh Pillai Lab Team : Marios Mattheakis, Hayden Joy, Chris Gumb, and Eleni Kaxiras Authors : Varshini Reddy, Marios Mattheakis and Pavlos Protopapas In [21]: ## RUN THIS CELL TO GET THE RIGHT FORMATTING import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\" ) . text HTML ( styles ) Out[21]: Lab Learning Objectives When we're done today, you will approach messy real-world data with confidence that you can get it into a format that you can manipulate. Specifically, our learning objectives are: Understand the tree-like structure of an HTML document and use that structure to extract desired information. Use Python data structures such as lists, dictionaries to store and manipulate information. Practice using Python packages such as BeautifulSoup , including how to navigate their documentation to find functionality. Identify other (semi-)structured formats commonly used for storing and transferring data, such as CSV. Pre-Requisites Before you start working on the lab, we expect you to be familiar with Python programming. Following is the list of topics you need to brush up on before attending the lab session. We have provided some quick start references as well. Python Data Structures Lists Dictionaries Functions in python Python classes Files and strings In [16]: # Importing necessary libraries import numpy as np import matplotlib.pyplot as plt import pandas as pd from bs4 import BeautifulSoup import requests import json from IPython.display import HTML % matplotlib inline In [2]: # Setting up 'requests' to make HTTPS requests properly takes some # extra steps. requests . packages . urllib3 . disable_warnings () import warnings warnings . filterwarnings ( \"ignore\" ) Lab Data Analysis Questions Is science becoming more collaborative over time? How about literature? Are there a few \"geniuses\" or lots of hard workers? One way we might answer those questions is by looking at Nobel Prizes winners. We could ask questions like: 1) Has anyone won a prize more than once? 2) How has the total number of recipients changed over time? 3) How has the number of recipients per award changed over time? To answer these questions, we will need data: who received what award and when . When possible: find a structured dataset (.csv, .json, .xls) After a google search we stumble upon this dataset on github . It is also in the lab folder named github-nobel-prize-winners.csv . We use Pandas to read it. Pandas will be covered next week in more details. In [5]: df = pd . read_csv ( \"data/github-nobel-prize-winners.csv\" ) df . head () Out[5]: year discipline winner desc 0 1901 chemistry Jacobus H. van 't Hoff in recognition of the extraordinary services h... 1 1901 literature Sully Prudhomme in special recognition of his poetic compositi... 2 1901 medicine Emil von Behring for his work on serum therapy, especially its ... 3 1901 peace Henry Dunant NaN 4 1901 peace Fr&eacute;d&eacute;ric Passy NaN Research Question 1: Did anyone recieve the Nobel Prize more than once? How would you check if anyone recieved more than one nobel prize? We will be using Python lists for this, which is a pre-requisite for this lab as mentioned earlier. If you have any questions with regards to lists or list comprehensions, refer to the slides from us here . In [6]: # Initialize the list storing all the names name_winners = [] for name in df . winner : # Check if we already encountered this name: if name in name_winners : # (TODO) If so, print the name print ( ___ ) else : # (TODO) Otherwise append the name to the list name_winners . append ( ___ ) We don't want to print \"No Prize was Awarded\" all the time. In [7]: # List storing all the names name_winners = [] for name in df . winner : # (TODO) Check if we already encountered this name and the name is not \"No Prize was Awarded\": if name in name_winners and name != ___ : # (TODO) If so, print the name print ( ___ ) else : # (TODO) Otherwise append the name to the list name_winners . append ( ___ ) we can use .split() on a string to separate the words into individual strings and store them in a list. Experiment with the .split() below before using it. In [9]: UN_string = \"Office of the United Nations\" print ( UN_string . split ()) n_words = len ( UN_string . split ()) print ( \"Number of words: \" + str ( n_words )); ['Office', 'of', 'the', 'United', 'Nations'] Number of words: 5 Let us only print winners with only two words in their name: In [8]: name_winners = [] for name in df . winner : # (TODO) Check if we already encountered this name and the name consists of no more than 2 words: if name in name_winners and len ( ___ ) <= 2 : # (TODO) If so, print the name print ( ___ ) else : # (TODO) Otherwise append the name to the list name_winners . append ( ___ ) Marie Curie recieved the nobel prize in physics in 1903 and chemistry in 1911. She is one of only four people to recieve two Nobel Prizes. All questions, such as \"did anyone receive the Noble Price more than once?\", are easy to answer when the data is present in such a clean tabular form. However, many times (if not most) we do not find the data we need in such a format. In such cases, we need to perform web scraping and cleaning to get the data we desire. The end result of this lab is to create a pandas dataframe after web scraping and cleaning. WEB SCRAPING The first step in web scraping is to understand the HTML structure of the webpage. But, what is HTML? HTML stands for Hyper Text Markup Language. It is the standard markup language for documents designed to be displayed in a web browser. It can be assisted by technologies such as Cascading Style Sheets and scripting languages such as JavaScript. Standard HTML documents HTML documents generally have the following structure: **\\ ** **\\ ** **\\ ** **\\ Page Title\\ ** **\\ ** **\\ ** **\\ Page Heading\\ ** **\\ The first paragraph of page\\ ** **.** **.** **.** **.** **\\ ** **\\ ** What does each of these tags indicate? The \\<!DOCTYPE html> declaration defines that this document is an HTML5 document The \\ element is the root element of an HTML page The \\ element contains meta information about the HTML page The \\ element specifies a title for the HTML page (which is shown in the browser's title bar or in the page's tab) The \\ element defines the document's body, and is a container for all the visible contents, such as headings, paragraphs, images, hyperlinks, tables, lists, etc. The \\ element defines a large heading. There are other heading tags in html, \\ , \\ , \\ , \\ , \\ The \\ element defines a paragraph What is an HTML Element? An HTML element is defined by a start tag, some content, and an end tag: \\ Tag content \\ An example of an HTML element is as follows: \\ The Page Heading \\ WEB SCRAPING The official Nobel website has the data we want, but in 2018 and 2019 the physics prize was awarded to multiple groups so we will use an archived version of the web-page for an easier introduction to web scraping. The Internet Archive periodically crawls most of the Internet and saves what it finds. (That's a lot of data!) So let's grab the data from the Archive's \"Wayback Machine\" (great name!). We've just given you the direct URL, but at the very end you'll see how we can get it out of a JSON response from the Wayback Machine API. Let's take a look at the 2018 version of the Nobel website and to look at the underhood HTML: right-click and click on inspect .You should see something like this. Mapping the HTML tags to the webpage When you inspect, try to map each element on the webpage to its HTML. In [12]: # here is what we will get after selecting using the class by year tag. # we use the HTML parser module to render the html einstein = HTML ( ' \\ <div class =\"Class: by year\"> \\ <h3> \\ <a href=\"http://web.archive.org/web/20180820111639/https://www.nobelprize.org/nobel_prizes/physics/laureates/1921/\"> \\ The Nobel Prize in Physics 1921 \\ </a> \\ </h3> \\ <h6> \\ <a href=\"http://web.archive.org/web/20180820111639/https://www.nobelprize.org/nobel_prizes/physics/laureates/1921/einstein-facts.html\"> \\ Albert Einstein</a> \\ </h6> \\ <p> \\ \"for his services to Theoretical Physics, and especially for his discovery of the law of the photoelectric effect\" \\ </p> \\ ' ) display ( einstein ) The Nobel Prize in Physics 1921 Albert Einstein \"for his services to Theoretical Physics, and especially for his discovery of the law of the photoelectric effect\" In [23]: snapshot_url = 'http://web.archive.org/web/20180820111639/https://www.nobelprize.org/prizes/lists/all-nobel-prizes/' In [0]: # (TODO) make a GET request to snapshot_url snapshot = requests . get ( ___ ) snapshot Response [200] is a success status code. Let's google: response 200 meaning . All possible codes here . In [14]: type ( snapshot ) Try to request \"www.xoogle.be\". What happens? In [15]: snapshot_url2 = 'http://web.archive.org/web/20180820111639/https://www.xoogle.be' # (TODO) make a GET request to snapshot_url2 snapshot = requests . get ( ___ ) snapshot Always remember to \"not to be evil\" when scraping with requests! If downloading multiple pages (like you will be doing on HW1), always put a delay between requests (e.g., time.sleep(1) , with the time library), so you do not unwittingly hammer someone's webserver and/or get blocked. Let's look at the content we just scraped! In [0]: snapshot = requests . get ( snapshot_url ) raw_html = snapshot . text print ( raw_html [: 5000 ]) What makes Python special ? In [15]: import this The Zen of Python, by Tim Peters Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Readability counts. Special cases aren't special enough to break the rules. Although practicality beats purity. Errors should never pass silently. Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess. There should be one-- and preferably only one --obvious way to do it. Although that way may not be obvious at first unless you're Dutch. Now is better than never. Although never is often better than *right* now. If the implementation is hard to explain, it's a bad idea. If the implementation is easy to explain, it may be a good idea. Namespaces are one honking great idea -- let's do more of those! Regular Expressions You can find specific patterns or strings in text by using Regular Expressions (or re, regex, regexp): This is a pattern matching mechanism used throughout Computer Science and programming (it's not just specific to Python). A short summary of regular expressions from us can be found here . Some great resources that we recommend, if you are interested in them (could be very useful for a homework problem): https://docs.python.org/3.3/library/re.html https://regexone.com https://docs.python.org/3/howto/regex.html . Specify a specific sequence with the help of regex special characters. Some examples: \\S : Matches any character which is not a Unicode whitespace character: spaces, tabs, newlines \\d : Matches any Unicode decimal digit, 0 , 1 , ..., 9 * : Causes the resulting RE to match 0 or more repetitions of the preceding RE, as many repetitions as are possible. Let's find all the occurances of 'Marie' in our raw_html: In [17]: import re In [18]: re . findall ( r 'Marie' , raw_html ) Note we use an r before the string to get the raw text. Using \\S to match 'Marie' + ' ' + 'any character which is not a Unicode whitespace character': In [19]: re . findall ( r 'Marie \\S' , raw_html ) How would we find the lastnames that come after Marie? In [20]: # Your code here Hint: The \\w character represents any alpha-numeric character. \\w* is greedy and gets a repeat of the characters until the next bit of whitespace. Now, we have all our data in the notebook. Unfortunately, it is the form of one really long string, which is hard to work with directly. This is where BeautifulSoup comes in. This is an example of code that grabs the first title. Regex can quickly become complex, which motivates beautiful soup. In [21]: first_title = re . findall ( r '<h3><a.*>.*<\\/a><\\/h3>' , raw_html )[ 0 ] print ( first_title ) #you can do this via regex, but it gets complicated fast! This motivates Beautiful Soup. Parse the HTML with BeautifulSoup BeautifulSoup works by parsing the raw html text into a tree. Every tag in the raw html becomes a node in the tree. We can then navigate the tree by selecting a node and querying its parent, children, siblings, etc. In [0]: soup = BeautifulSoup ( raw_html , 'html.parser' ) Key BeautifulSoup functions we'll be using in this lab: tag.prettify() : Returns cleaned-up version of raw HTML, useful for printing tag.select(selector) : Return a list of nodes matching a CSS selector tag.select_one(selector) : Return the first node matching a CSS selector tag.text/soup.get_text() : Returns visible text of a node (e.g.,\" <p>Some text</p> \" -> \"Some text\") tag.contents : A list of the immediate children of this node You can also use these functions to find nodes. tag.find_all(tag_name, attrs=attributes_dict) : Returns a list of matching nodes tag.find(tag_name, attrs=attributes_dict) : Returns first matching node BeautifulSoup is a very powerful library -- much more info here: https://www.crummy.com/software/BeautifulSoup/bs4/doc/ Let's practice some BeautifulSoup commands, Print a cleaned-up version of the raw HTML Which function should we use from above? In [0]: pretty_soup = soup . prettify () print ( pretty_soup [: 500 ]) #what about negative indices? Find the first \"title\" object In [0]: soup . select ( \"title\" )[: 50 ] Extract the text of first \"heading\" object given by $<h3>$ In [0]: soup . select_one ( 'a h3' ) Extracting award data Let's use the structure of the HTML document to extract the data we want. From inspecting the page in DevTools, we found that each award is in a div with a by_year class. Let's get all of them. In [26]: award_nodes = soup . select ( '.by_year' ) #<div class =\"by year\" len ( award_nodes ) Let's pull out an example. In [27]: award_node = award_nodes [ 200 ] In [0]: award_node . prettify () We use the HTML library to render the HTML below In [0]: HTML ( award_node . prettify ()) Let's practice getting data out of a BS node (award_node) The prize title Check the html from above and note that the prize title is in the h3 tag. In [29]: award_node . select_one ( 'h3' ) . text How do we separate the year from the selected prize title? In [30]: award_node . select_one ( 'h3' ) . text [ - 4 :] How do we drop the year from the title? In [31]: award_node . select_one ( 'h3' ) . text [: - 4 ] . strip () Let's put them into functions: In [32]: # wrap the above code inside a function def get_award_title ( award_node ): return award_node . select_one ( 'h3' ) . text [ ___ ] . strip () def get_award_year ( award_node ): return int ( award_node . select_one ( 'h3' ) . text [ ___ ]) Make a list of titles for all awards In [54]: #original code: list_awards = [] for award_node in award_nodes : list_awards . append ( get_award_title ( ___ )) list_awards [: 50 ] How can we make this into a oneliner? We can use list comprehension l = [ f ( x ) for x in some_list ] which is equivalent to l = [] for x in some_list : element = f ( x ) l . append ( element ) List comprehensions are explained in the slides from us linked above. In [34]: # (TODO) use list comprehension to get a list of titles [ get_award_title ( ___ ) for award_node in award_nodes ] The recipients Check the html from above and note that the prize title is in the h6 a selector. In [35]: award_node . select ( 'h6 a' ) How do we handle there being more than one? In [36]: [ node . text for node in award_node . select ( 'h6 a' )] Let's encapsulate this process into a function and make it into a function. In [37]: def get_recipients ( award_node ): return [ node . text for node in award_node . select ( 'h6 a' )] We'll leave them as a list for now, to return to this later. This is how you would get the links: (Relevant for the homework) In [38]: [ state_node . get ( \"href\" ) for state_node in award_node . select ( 'h6 a' )] The prize \"motivation\" How would you get the 'motivation'/reason of the prize from the following award_node ? In [39]: award_node = award_nodes [ 200 ] award_node In [40]: print ( award_node . select ( 'p' )[ 0 ] . text ); Putting everything into functions: In [41]: def get_award_motivation ( award_node ): award_node = award_node . select_one ( 'p' ) if not award_node : #0, [], None, and {} all default to False in a python conditional statement. return None return award_node . text Let's create a Pandas dataframe Next, we parse the collected data and create a pandas.DataFrame . A DataFrame is like a table, where each row corresponds to a data entry and each column corresponds to a feature. Once we have a DataFrame, we can easily export it to our disk in CSV, JSON, or other formats. The easiest way to create a DataFrame is to build a list of dictionaries. Dictionaries are a pre-requisite for this lab. Refer to the slides from us here for a better understanding. Each entry (dict) in the list is a data point, where keys are column names in the table. Let's see it in action. In [1]: awards = [] for award_node in soup . select ( '.by_year' ): recipients = get_recipients ( award_node ) # Initialize the dictionary award = {} #{key: value} # Call `get_award_title` to get the title of award_node award [ 'title' ] = get_award_title ( award_node ) # Call `get_award_title` to get the year of award_node award [ 'year' ] = get_award_year ( award_node ) # Call `get_recipients` to get the list of recipients of award_node award [ 'recipients' ] = recipients # Count number of recipients using the built-in `len()` function award [ 'num_recipients' ] = len ( recipients ) # (TODO) call `get_award_motivation` to get the motivation of award_node award [ 'motivation' ] = get_award_motivation ( award_node ) awards . append ( award ) awards [ 0 : 2 ] In [2]: # (TODO) convert the list of dictionaries to a pandas DataFrame df_awards_raw = pd . DataFrame ( awards ) In [55]: df_awards_raw To export the data to a local CSV file, let's used the .to_csv() method. After you run the follwing code, you can find a scraped_awards.csv in the same directory with this notebook. You can open the notebook using Microsoft Excel or Numbers, but make sure you are using the UTF-8 codec. In [45]: df_awards_raw . to_csv ( 'scraped_awards.csv' ) Some quick EDA. In [46]: df_awards_raw . info () In [47]: df_awards_raw . year . min () What is going on with the recipients column? In [48]: df_awards_raw . head () Visualizing Number of Recipients by Year Finally, we visualize the number of recipients for each Nobel Prize by year. Don't worry about the syntax for the moment, you'll get used to it in future exercise. In [49]: titles = set ( df_awards_raw . title ) fig = plt . figure ( figsize = ( 20 , 44 ), dpi = 100 ) axes = fig . subplots ( len ( titles ), 1 ) for title , ax in zip ( titles , axes ): # (TODO) select entries whose titles match `title` plot_df = df_awards_raw [ df_awards_raw . title == title ] # (TODO) plot the selected entries using bar-plot, where x-axis is year and y-axis is number of recipeints ax . bar ( ___ , ___ , color = \"#97CFC4\" ) ax . set_title ( ___ ) ax . set_xlabel ( ___ ) ax . set_ylabel ( ___ ) In [50]: # `counter` is used to save the number of nobel prize winners every year counter = {} for year in range ( min ( df_awards_raw . year ), max ( df_awards_raw . year ) + 1 ): # (TODO) compute total number of recipients that year count = df_awards_raw [ df_awards_raw . year == year ] . num_recipients . sum () counter [ year ] = count fig = plt . figure ( figsize = ( 20 , 6 ), dpi = 100 ) ax = fig . add_subplot ( 1 , 1 , 1 ) # (TODO) make another bar-plot, where x-axis is year and y-axis is total number of recipeints ax . bar ( ___ , ___ , color = \"#97CFC4\" ) ax . set_title ( 'Total Amount of Nobel Prize' ) ax . set_xlabel ( 'year' ) ax . set_ylabel ( '#Recipients' ); End of Normal Lab Optional Further Readings Here are a couple resources that he referenced early in his course that helped solidify my understanding of data science. 50 Years of Data Science by Dave Donoho (2017) Tidy data by Hadley Wickam (2014)","tags":"labs","url":"labs/lab1/notebook-1/"},{"title":"Lecture 1: Introduction to CS109A","text":"Slides Lecture 1 : Introduction to CS109A (PDF) Exercises Lecture 1: Exercise: The Data Science Process [Notebook]","tags":"lectures","url":"lectures/lecture01/"},{"title":"Lecture 1: Introduction to CS109A","text":"session1_scaffold CS109A Introduction to Data Science Lecture 1, Exercise 1: The Data Science Process Harvard University Fall 2021 Instructors : Pavlos Protopapas and Natesh Pillai Title : Exercise: The Data Science Process Description : The aim of this exercise is to understand all the steps involved in a Data Science setting. Data Description: Hubway was metro-Boston's public bike share program, with more than 1600 bikes at 160+ stations across the Greater Boston area. Hubway was owned by four municipalities in the area. By 2016, Hubway operated 185 stations and 1750 bicycles, with 5 million rides since launching in 2011. In April 2017, Hubway held a Data Visualization Challenge at the Microsoft NERD Center in Cambridge, releasing 5 years of trip data. Instructions: Read the data files hubway_stations.csv and hubway_trips.csv into separate pandas dataframes. Get a quick understanding of the columns present in the data and their types. Remove all the data points with null values in any one (or more) of the columns. Create a new column age that gives the age of the rider using their birth date. Perform relevant EDA to answer the questions asked on the scaffold. Create a simple linear model to predict the number of checkouts based on the distance of the bikes from the centre of the city. Visualize the prediction against the data. Hints: pd.read_csv(filename) Returns a pandas dataframe containing the data and labels from the file data pd.describe() Generates descriptive statistics of the dataframe. pd.dropna() Removes missing values from the dataframe. It removes either the columns or rows based on the axis parameter. In [1]: # Import necessary libraries import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from helper import get_distance from math import radians , cos , sin , asin , sqrt from sklearn.linear_model import LinearRegression % matplotlib inline In [3]: # Read the data from the file \"hubway_stations.csv\" stations = pd . read_csv ( \"hubway_stations.csv\" ) # Read the data from the file \"hubway_trips.csv\" trips = pd . read_csv ( \"hubway_trips.csv\" ) In [4]: # Take a quick look at the stations data stations . head () Out[4]: id terminal station municipal lat lng status 0 3 B32006 Colleges of the Fenway Boston 42.340021 -71.100812 Existing 1 4 C32000 Tremont St. at Berkeley St. Boston 42.345392 -71.069616 Existing 2 5 B32012 Northeastern U / North Parking Lot Boston 42.341814 -71.090179 Existing 3 6 D32000 Cambridge St. at Joy St. Boston 42.361285 -71.065140 Existing 4 7 A32000 Fan Pier Boston 42.353412 -71.044624 Existing In [5]: # Take a quick look at the trips data trips . head () Out[5]: Unnamed: 0 hubway_id duration start_date strt_statn end_date end_statn zip_code birth_date gender 0 426015 482077 675 8/18/2012 19:48:00 8.0 8/18/2012 20:00:00 8.0 '02134 1983.0 Male 1 193080 220612 204 4/26/2012 18:14:00 31.0 4/26/2012 18:17:00 64.0 '02210 1953.0 Male 2 530051 598721 888 9/23/2012 09:26:00 39.0 9/23/2012 09:41:00 39.0 '02118 1985.0 Male 3 484594 547645 526 9/8/2012 12:55:00 88.0 9/8/2012 13:04:00 72.0 '02139 1985.0 Male 4 291265 332163 554 6/21/2012 18:53:00 47.0 6/21/2012 19:02:00 54.0 '02113 1986.0 Female UNDERSTANDING THE DATA It is important to completely understand all the information provided in the data. The first step for this is to take a closer look at all the columns and understand their types. In [6]: # Getting the data type of each column in the stations data stations . dtypes Out[6]: id int64 terminal object station object municipal object lat float64 lng float64 status object dtype: object In [7]: # Getting the data type of each column in the trips data trips . dtypes Out[7]: Unnamed: 0 int64 hubway_id int64 duration int64 start_date object strt_statn float64 end_date object end_statn float64 zip_code object birth_date float64 gender object dtype: object ⏸ Based on the datatypes, do you see any possible issues? In [11]: ### edTest(test_chow0) ### # Submit the questions as a string below. Separate each question by an eroteme (question mark) answer0 = 'Yes' In [15]: # Getting some statistical information from the stations data stations . describe () Out[15]: id lat lng count 142.000000 142.000000 142.000000 mean 74.323944 42.354820 -71.089087 std 41.389098 0.020089 0.027111 min 3.000000 42.309467 -71.146452 25% 39.250000 42.341652 -71.113183 50% 74.500000 42.353373 -71.089191 75% 109.750000 42.366265 -71.065210 max 145.000000 42.404490 -71.035705 In [14]: # Getting some statistical information from the trips data trips . describe () Out[14]: Unnamed: 0 hubway_id duration strt_statn end_statn birth_date count 210239.000000 210239.000000 2.102390e+05 210239.000000 210239.000000 210239.000000 mean 283491.142771 321401.542806 7.794459e+02 36.727567 36.662261 1976.285594 std 153204.497985 173059.875974 1.349006e+04 18.592716 18.551934 11.002281 min 0.000000 8.000000 0.000000e+00 3.000000 3.000000 1932.000000 25% 153899.000000 174103.000000 3.460000e+02 22.000000 22.000000 1969.000000 50% 280081.000000 319856.000000 5.320000e+02 38.000000 38.000000 1979.000000 75% 414740.000000 469290.000000 8.280000e+02 50.000000 50.000000 1985.000000 max 549285.000000 620312.000000 5.351083e+06 98.000000 98.000000 1995.000000 ⏸ Based on your understanding of the data, what questions would you like to have answered? In [16]: ### edTest(test_chow1) ### # Submit the questions as a string below. Separate each question by an eroteme (question mark) answer1 = 'Most common station, Who take more trips (male or female)' DATA PRE-PROCESSING Let us clean the data before breaking it down further. There are many pre-processing techqniues which will be covered later in the course. In [17]: # Delete all the rows of the stations dataframe with null values # axis=0 indicates that the rows with null values are to be deleted stations . dropna ( axis = 0 , inplace = True ) In [18]: # Delete all the rows of the trips dataframe with null values trips . dropna ( axis = 0 , inplace = True ) In [19]: # Create a new column that gives the age of each rider age_col = 2021.0 - trips [ 'birth_date' ] . values # Add the age column to the trips dataframe trips [ 'age' ] = age_col # Drop the 'birth_date' column trips . drop ( 'birth_date' , axis = 1 , inplace = True ) EXPLORATORY DATA ANALYSIS (EDA) As you would have noticed, the information extracted above is not sufficient to answer most of the questions and is definitely not sufficient to ask relevant questions. Hence, we will need to perform additional data analysis. In [20]: # Find out if there is any relation between the predictors of the stations data sns . pairplot ( stations ); Out[20]: <seaborn.axisgrid.PairGrid at 0x7f7d665c8850> ⏸ Based on the plot above, do you notice any recognizable relationship between any of the columns? A. The latitude and longitude are directly proportional to each other. B. The latitude and longitude are inversely proportional to each other. C. It is random. There seems to be no relation between the latitude and longitude. In [25]: ### edTest(test_chow2) ### # Submit an answer choice as a string below (eg. if you choose option A, put 'A') answer2 = 'C' In [26]: # Get the unique number of male and female bike riders gender_counts = np . unique ( trips [ 'gender' ] . values , return_counts = True ) # Plotting the genders of riders as a histogram fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 6 )) ax . bar ( range ( 2 ), width = 0.5 , height = gender_counts [ 1 ], color = [ '#e4a199' , 'green' ], alpha = 0.5 ) ax . set_xticks ([ 0 , 1 ]) ax . set_xticklabels ( gender_counts [ 0 ]) ax . set_title ( 'Users by Gender' ); Out[26]: Text(0.5, 1.0, 'Users by Gender') ⏸ Based on the plot above, who uses the bikes more, men or women? A. Women B. Men C. Can't say In [27]: ### edTest(test_chow3) ### # Submit an answer choice as a string below (eg. if you choose option A, put 'A') answer3 = 'B' In [28]: # Plotting the usage of bikes based on the ages of riders fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 6 )) age_counts = np . unique ( trips [ 'age' ], return_counts = True ) ax . bar ( age_counts [ 0 ], age_counts [ 1 ], align = 'center' , width = 0.4 , alpha = 0.6 ) ax . axvline ( x = np . mean ( age_col ), color = 'red' , label = 'average age' ) ax . axvline ( x = np . percentile ( age_col , 25 ), color = 'red' , linestyle = '--' , label = 'lower quartile' ) ax . axvline ( x = np . percentile ( age_col , 75 ), color = 'red' , linestyle = '--' , label = 'upper quartile' ) ax . set_xlim ([ 1 , 90 ]) ax . set_xlabel ( 'Age' ) ax . set_ylabel ( 'Number of Checkouts' ) ax . legend () ax . set_title ( 'Users by Age' ) plt . tight_layout () ⏸ Based on the plot above, who uses the bikes more, older or younger people? A. Older B. Younger In [36]: ### edTest(test_chow4) ### # Submit an answer choice as a string below (eg. if you choose option A, put 'A') answer4 = 'B' In [30]: # Get the hourwise bike checkouts check_out_hours = trips [ 'start_date' ] . apply ( lambda s : int ( s [ - 8 : - 6 ])) # Plotting the bike checkouts hourwise fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 5 )) check_out_counts = np . unique ( check_out_hours , return_counts = True ) ax . bar ( check_out_counts [ 0 ], check_out_counts [ 1 ], align = 'center' , width = 0.4 , alpha = 0.6 ) ax . set_xlim ([ - 1 , 24 ]) ax . set_xticks ( range ( 24 )) ax . set_xlabel ( 'Hour of Day' ) ax . set_ylabel ( 'Number of Checkouts' ) ax . set_title ( 'Time of Day vs Checkouts' ) plt . show () ⏸ Based on the plot above, when is the biggest rush hour? In [31]: ### edTest(test_chow5) ### # Submit the integer value below within the quotes answer5 = '17' MORE QUESTIONS? There are many questions that haven't been covered here. For what reasons are the bikes being used? Recreation, traffic or for health benfits? Is the usage more during the weekdays or weekends? Are people using bikes more in Boston or Cambridge? Feel free to add new code cells and find the answers. DATA MODELLING There are some questions that cannot be answered with simple graphing techniques. It requires combining different variables. How does user demographics impact the duration the bikes are being used? Or where they are being checked out? How does weather or traffic conditions impact bike usage? How do the characteristics of the station location affect the number of bikes being checked out? Let us try to answer the question: How does the distance from the center of the city affect the bike usage? In [32]: # Helper function within helper.py to compute the distance of the bike from the city center # It returns a dataframe that has the column of the checkout distance from the center counts_df = get_distance () # Take a quick look at the dataframe counts_df . head () Out[32]: id checkouts terminal station municipal lat lng status dist_to_center 0 3.0 1878 B32006 Colleges of the Fenway Boston 42.340021 -71.100812 Existing 2.335706 1 4.0 3376 C32000 Tremont St. at Berkeley St. Boston 42.345392 -71.069616 Existing 0.853095 2 5.0 1913 B32012 Northeastern U / North Parking Lot Boston 42.341814 -71.090179 Existing 1.802423 3 6.0 3616 D32000 Cambridge St. at Joy St. Boston 42.361285 -71.065140 Existing 0.467803 4 7.0 1384 A32000 Fan Pier Boston 42.353412 -71.044624 Existing 0.807582 In [33]: # Let us use a straight line y = ax + b to model the relation # between the number of checkouts and distance to the city center beta0 = 4394 beta1 = - 1175 y_pred = beta0 + beta1 * counts_df [ 'dist_to_center' ] . values In [34]: # Plotting the true data and the prediction fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 5 )) ax . scatter ( counts_df [ 'dist_to_center' ] . values , counts_df [ 'checkouts' ] . values , label = 'Data' , s = 70 , c = '#e4a199' ) ax . plot ( counts_df [ 'dist_to_center' ] . values , y_pred , c = 'blue' , alpha = 0.5 , linewidth = 2 , label = 'Prediction' ) ax . set_xlabel ( 'Distance to City Center (Miles)' ) ax . set_ylabel ( 'Number of Checkouts' ) ax . set_title ( 'Distance to City Center vs Checkouts' ); ax . legend (); Out[34]: <matplotlib.legend.Legend at 0x7f7d41a5c820> ⏸ Based on our \"linear\" model, what would most likely be the number of checkouts for a distance of 2.5 miles from the city center? A. 45000 B. 12530 C. 1450 D. 650 In [35]: ### edTest(test_chow6) ### # Submit an answer choice as a string below (eg. if you choose option A, put 'A') answer6 = 'C' In [ ]:","tags":"Lectures","url":"lectures/lecture01/notebook/"},{"title":"CS109a: Introduction to Data Science","text":"Fall 2021 Pavlos Protopapas and Natesh Pillai Welcome to CS109a/STAT121a/AC209a, also offered by the DCE as CSCI E-109a, Introduction to Data Science. This course is the first half of a one‐year course to data science. We will focus on the analysis of data to perform predictions using statistical and machine learning methods. Topics include data scraping, data management, data visualization, regression and classification methods, and deep neural networks. You will get ample practice through weekly homework assignments. The class material integrates the five key facets of an investigation using data: 1. data collection ‐ data wrangling, cleaning, and sampling to get a suitable data set 2. data management ‐ accessing data quickly and reliably 3. exploratory data analysis – generating hypotheses and building intuition 4. prediction or statistical learning 5. communication – summarizing results through visualization, stories, and interpretable summaries Only one of CS 109a, AC 209a, or Stat 121a can be taken for credit. Students who have previously taken CS 109, AC 209, or Stat 121 cannot take CS 109a, AC 209a, or Stat 121a for credit. Important Dates: Tuesday 9/8 - HW1 released Wednesday 9/8 - HW0 due at 11:59pm EST Helpline: cs109a2021@gmail.com Lectures: Mon & Wed 9:45-11am - SEC Room 1.321 Lab: Fri 9:45-11am - l114 Western Ave., Allston Room 2.111 Advanced Sections: Wed 12:45-2pm [starting 9/29] - SEC Room LL2.229 (See course schedule for dates) Office Hours: Current Schedule Here With More To Come Course material can be viewed in the public GitHub repository . Previous Material 2020 2019 2018 2017 2015 2014 . 2013","tags":"pages","url":"pages/cs109a-introduction-to-data-science/"},{"title":"Lab 05:","text":"Slides","tags":"labs","url":"labs/lab05/"},{"title":"S-Section 04: Regularization and Model Selection","text":"Jupyter Notebooks S-Section 4: Regularization and Model Selection","tags":"sections","url":"sections/section4/"}]}