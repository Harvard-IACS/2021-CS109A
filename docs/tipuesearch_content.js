var tipuesearch = {
    "pages": [{
        "title": "FAQ",
        "text": "General I'm unable to attend lectures in person. Can I take the class asynchronously? Lecture attendance is required. Non-DCE students should only register if they can attend in person. Does the individual HW mean I have to submit on my own but can I still work with my HW partner? An individual CS109A HW means you are supposed to work on your own, without any human intervention, assistance, or input. You are not allowed to work with partner. You are allowed to use OHs to ask for clarification questions, but we may not be able to answer all of your questions or help with your coding. You are allowed to use all of your materials from the course up to the HW, look at problem resolution online, and look at libraries documentation. Where should I send my questions? Use Ed for anying related to the course content or assignments. All other concerns should be sent to the course helpline: cs109a2021@gmail.com Auditing Can I audit this course? What are the rules for auditing? Yes, you are welcome to audit this course. Send an email to cs109a2021@gmail.com to request full Canvas access. All auditors must agree to abide by the following rules: All auditors are held to the same standard of academic honesty as our registered students. Please do not share homeworks or solutions with anyone. Violations will be reported to the Harvard Administrative Board. Auditors are not permitted to take the course for credit in the future. Auditors may not attend lectures or section during the live stream. Students are randomly assigned to small groups for coding exercises and mixing auditors and students in this way is not ideal. We are investigating methods that would allow auditors to join, but this is the current policy. Audiors should not submit HWs or participate in projects. Auditors should refrain from using any course and TF resources that are designed for our registered students like Ed, Jupyter Hub, and office hours. Auditors must have an active HUID number. Quizzes & Exercises When are quizzes and exercises due? Quizzes and exercises are due before the next lecture. I missed a quiz/exercise. Can I make it up? No. We will drop your lowest 25% of quizzes and your lowest 25% of exercises. This policy is to reduce stress and is in place so that missing a quiz or exercise on occasion should not affect your final grade.",
        "tags": "pages",
        "url": "pages/faq.html"
    }, {
        "title": "Preparation",
        "text": "In order to get the most out of CS109A, knowledge of multivariate calculus, probability theory, statistics, and some basic linear algebra (e.g., matrix operations, eigenvectors, etc.) is suggested. Below are some resources for self-assessment and review: Multivariate Calculus : multiple exams /w solutions Linear Algebra: multiple exams /w solutions 1 , 2 Probability: exams with solutions and problem sets with solutions Statistics: multiple pairs of exam questions and answers Q1 , A1 , Q2 , A2 , Q3 , A3 Here is a useful textbook for reviewing many of the above topics: Mathematics for Machine Learning Note: you can be successful in the course (assignments, quizzes, etc.) with the listed pre-requisites, but some of the material presented in lectures may be more easily understood with more background. This course dives right into core Machine Learning topics. For this, students are expected to be fluent in Python programming. You can familiarize yourself with Python by completing online tutorials. Additionally, there are free Basics Python courses that cover all necessary python topics. You are also expected to be able to manipulate data using pandas DataFrames, perform basic operations with numpy arrays, and make use of basic plotting functions (e.g. line chart, histogram, scatter plot, bar chart) from matplotlib. Python basics Throughout this course, we will be using Python as our primary programming language for exercises, labs, and homework. Thus, you must have basic Python programming knowledge. The following are the topics you need to cross off your checklist before the course begins: Variables, Datatypes, strings, file operations, Data structures such as lists, dictionaries, tuples and classes. Pandas Basics Most of the exercises you will encounter in this course exploit various datasets. Pandas is an open-source data analysis and manipulation tool, built on top of Python. In this course, we have provided the necessary support material and resources to work with Pandas. However, it is highly recommended that you get yourselves familiar with basic data manipulation using Pandas to ensure a smooth learning experience. Numpy Basics NumPy is a library for the Python programming language that provides support for large, multi-dimensional arrays and matrices and a large collection of high-level mathematical functions to operate on these data structures. Because of the extensive exercises provided in this course, it is important to use Numpy for efficient problem-solving to get identical results. Though this course aims to support individuals with no prior Numpy knowledge, you must go through the basics of this library to avoid any possible hiccups. Matplotlib Basics A large portion of this course uses different graphs and charts to explain topics and validate results. Matplotlib is a plotting library for Python. This library has been used to create all the graphs you will see throughout the course. Additionally, the exercises and homeworks are structured in a manner that integrates this library. Henceforth, it is highly recommended to get yourselves acquainted with Matplotlib Basics. For this course, we will be using Jupyter Notebooks. You can familiarize yourself with Jupyter notebooks by reading the following tutorials: A Beginner's Tutorial to Jupyter Notebooks Finally, we assume that students have a strong foundation in calculus, linear algebra, statistics, and probability. You should review these concepts before the course begins. Here is one useful resource: Mathematics for Machine Learning",
        "tags": "pages",
        "url": "pages/preparation.html"
    }, {
        "title": "Schedule",
        "text": "Date (Mon) Lecture (Mon) Lecture (Wed) Lab (Fri) Advanced Section (Wed) Assignment (R:Released Wed - D:Due Wed) 30-Aug No Lecture Lecture 1: Introduction to CS109A Lab 1: Data - formats| sources| & scraping 6-Sep No Lecture (Labor Day) Lecture 2: Introduction to PANDAS and EDA Lab 2: Pandas & EDA 2 R:HW1 - D:HW0 13-Sep Lecture 3: Introduction to Regression kNN and Linear Regression Lecture 4: Multi-linear and Polynomial Regression Lab 3: kNN & Linear Regression R:HW2 - D:HW1 20-Sep Lecture 5: Model Selection and Cross Validation Lecture 6: Regularization Ridge and Lasso Regression Lab 4: Multiple Regression & Polynomial Regression 27-Sep Lecture 7: Probability Lecture 8: Inference in Regression and Hypothesis Testing Lab 5: Estimation of Regulariztion Coeffs /w CV Advanced Section 1: Linear Algebra Primer R:HW3 - D:HW2 4-Oct Lecture 9: Missing Data & Imputation Lecture 10: Principal Component Analysis Lab 6: PCA Advanced Section 2: Hypothesis Testing 11-Oct No Lecture (Indigenous Peoples' Day) Lecture 11: Case Study Midterm Advanced Section 3: Math Foundations of PCA D: HW3 18-Oct Lecture 12: Visualization Lecture 13: Ethics Lab 7: Visualization R:HW4 25-Oct Lecture 14: Logistic Regression 1 Lecture 15: Logistic Regression 2 Lab 8: Classification Advanced Section 4: GLM R:HW5 - D:HW4 1-Nov Lecture 16: Decision Tree Lecture 17: Bagging Lab 9: Decision Trees 8-Nov Lecture 18: Random Forest Lecture 19: Boosting Lab 10: Random Forest Advanced Section 5: Stacking & Mixture of Experts R:HW6 - D:HW5 15-Nov Lecture 20: Model Interpretability Lecture 21: Experimental Design Lab 11: Model Interpretability & Ethics Advanced Section 6: Bandits (tentative) 22-Nov Lecture 22: NLP 1 No Lecture No Lab R:HW7 - D:HW6 29-Nov Lecture 23: NLP 2 Lecture 24: Final Review D:HW7 6-Dec Project Submission Deadline Reading Period 13-Dec Finals Week 20-Dec Projects: Final Showcase",
        "tags": "pages",
        "url": "pages/schedule.html"
    }, {
        "title": "Syllabus",
        "text": "TENTATIVE SYLLABUS SUBJECT TO CHANGE Introduction to Data Science (Fall 2021) CS 109a, AC 209a, Stat 121a, or CSCI E-109a Course Heads Pavlos Protopapas (SEAS) and Natesh Pillai (Statistics) Lectures: Mon & Wed 9:45am-11am - SEC Room 1.321 Labs: Friday 9:45am-11am - 114 Western Ave., Allston Room 2.111 Advanced Sections: Wed 12:45-2pm [starts 9/29] - SEC LL2.229 (see schedule for dates) Office Hours: Current Times With More To Come Prerequisites: You are expected to have programming experience at the level of CS 50 or above, and statistics knowledge at the level of Stat 100 or above (Stat 110 recommended). HW #0 is designed to test your knowledge on the prerequisites. Successful completion of this assignment will show that this course is suitable for you. HW #0 will not be graded but you are required to submit. Welcome to CS109a/STAT121a/AC209a, also offered by the DCE as CSCI E-109a, Introduction to Data Science. This course is the first half of a oneâ€year course in data science. The course focuses on the analysis of messy, real-life data to perform predictions using statistical and machine learning methods. Throughout the semester, our content continuously centers around five key facets: 1. data collection â€ data wrangling, cleaning, and sampling to get a suitable data set; 2. data management â€ accessing data quickly and reliably; 3. exploratory data analysis â€“ generating hypotheses and building intuition; 4. prediction or statistical learning; and 5. communication â€“ summarizing results through visualization, stories, and interpretable summaries. Only one of CS109a, AC209a, or STAT121a can be taken for credit. Students who have previously taken CS109, AC209, or STAT121 cannot take CS109A, AC 209A, or STAT121A for credit. Course Components The lectures, labs, and advanced sections will be recorded and accessed through the Zoom section on Canvas for Extension School students. Attendance is required for on campus students. Lectures The class meets for lectures twice a week for lectures (M & W). Attending and participating in lectures is a crucial component of learning the material presented in this course. What to expect A lecture will have the following pedagogy layout which will be repeated: Asynchronous pre-class exercises of approxmately 30 min. This will include, reading from the textbooks or other sources, watching videos to prepare you for the class. Approx. 10 minutes of Q&A regarding the pre-class exercises and/or review of homework and quiz questions. Live online instruction followed by a short Q/A session Hands-on exercises, on the ED platform. Sessions will help students develop the intuition for the core concepts, provide the necessary mathematical background, and provide guidance on technical details. Sessions will be accompanied by relevant examples to clarify key concepts and techniques. Labs Lab will be held every Friday at the same time and place as lectures. Labs guided hands-on coding challenges which will prepare students for successfully completing the homework assignments. Quizzes At the end of each lecture, there will be a short, graded quiz that will cover the pre-class and in-class material; there will be no AC209a content in the quizzes. The quizzes will be available until the next lecture. 25% of the quizzes will be dropped from your grade. Exercises Lectures will include one or more coding exercises focused on the newly introduced material; there will be no AC209a content in the exercises. The exercises are short enough to be completed during the time allotted in lecture but they will remain available until the beginning of the following lecture to accomodate those who cannot attend in real time. Your final grade will be calculated twice: one including exercise grades and one without. You will be given the higher of the two. In this way, exercises can only help your grade. Advanced Sections The course will include advanced sections for 209a students and will cover a different topic per week. These are 75-min lectures and will cover advanced topics like the mathematical underpinnings of the methods seen in lecture and hands-on exercises, along with extensions of those methods. The material covered in the advanced sections is required for all AC209a students. But all students are welcome and encouraged to attendadvanced sections. Note: Advanced sections are not held every week. Consult the course schedule for exact dates. Exams There will be a midterm exam on October 15th. Projects Students will work in groups of 2-4 to complete a final group project, due during the Exams period. See schedule for specific dates. Homework Assignments There will be 7 graded homework assignments. Some of them will be due one week after being assigned, and some will be due two weeks after being assigned. You have the option to work and submit in pairs for all the assignments except HW3 and HW6, which you will do individually. You will be working in Jupyter Notebooks, which you can run in your own environment or in the SEAS JupyterHub cloud. [Instructions for Setting up Your Environment] (coming soon) [Instructions for Using JupyterHub] (coming soon) On weeks with new assignments, the assignments will be released by Wednesday 3pm. Standard assignments are graded out of 5 points. AC209a students will have additional homework content for most assignments worth 1 point. Instructor Office Hours Natesh : (TBD) Pavlos : Monday 6:30-7:30 PM [IACS Office]; 7:30-8 PM [Online] Participation Students are expected to be actively engaged with the course. This includes: Attending and participating in lectures Making use of office hours Participating in the Ed discussion forum â€” both through asking thoughtful questions and by answering the questions of others Recommended Textbook An Introduction to Statistical Learning by James, Witten, Hastie, Tibshirani. The book is available here: Free electronic version : http://www-bcf.usc.edu/~gareth/ISL/ (Links to an external site). HOLLIS : http://link.springer.com.ezp-prod1.hul.harvard.edu/book/10.1007%2F978-1-4614-7138-7 Amazon: https://www.amazon.com/Introduction-Statistical-Learning-Applications-Statistics/dp/1461471370 (Links to an external site) . Course Policies Getting Help For questions about homework, course content, package installation, JupyterHub, and after you have tried to troubleshoot yourselves, the process to get help is: 1. Post the question in Ed and get a response from your peers. Note that in Ed questions are visible to everyone. The teaching staff monitors the posts. 2. Go to Office Hours ; this is the best way to get direct help. 3. For private matters send an email to the Helpline: cs109a2021@gmail.com . The Helpline is monitored by the teaching staff. 4. For personal and confidential matters send an email to the instructors . Collaboration Policy We expect you to adhere to the Harvard Honor Code at all times. Failure to adhere to the honor code and our policies may result in serious penalties, up to and including automatic failure in the course and reference to the ad board. If you work with a partner on an assignment make sure both parties solve all the problems. Do not divide and conquer. You are expected to be intellectually honest and give credit where credit is due. In particular: if you work with a fellow student but decide to submit individual assignments, include the name of each other in the designated area of the submission. if you work with a fellow student and want to submit the same assignment, you need to form a group prior to the submission. Details in the assignment. Remember, not all assignments will permit group submissions. you need to write your solutions entirely on your own or with your collaborator (e.g., not entirely from Google search results) you are welcome to take ideas from code presented in lecture or section, but you need to change it, adapt it to your style, and ultimately write your own. We do not want to see code copied verbatim from the above sources. if you use code found on the internet, books, or other sources you need to cite those sources. you should not view any written materials or code created by other students for the same assignment; you may not provide or make available solutions to individuals who take or may take this course in the future. if the assignment allows it you may use third-party libraries and example code, so long as the material is available to all students in the class and you give proper attribution. Do not remove any original copyright notices and headers. Late or Wrongly Submitted Assignments Each student is allowed up to 3 late days over the semester with at most 1 day applied to any single homework. Outside of these allotted late days, late homework will not be accepted unless there is a medical (if accompanied by a doctor's note) or other official University-excused reasons. There is no need to ask before using one of your late days. If you forgot to join a Group with your peer and are asking for the same grade we will accept this with no penalty up to HW3. For homeworks beyond that we feel that you should be familiar with the process of joining groups. After that there will be a penalty of -1 point for both members of the group provided the submission was on time. Grading Guidelines Homework will be graded based on: 1. How correct your code is (the Notebook cells should run, we are not troubleshooting code) 2. How you have interpreted the results â€” we want text not just code. It should be a report. 3. How well you present the results. The scale is 0 to 5 for each assignment. Re-grade Requests Our graders and instructors make every effort in grading accurately and in giving you a lot of feedback. If you discover that your answer to a homework problem was correct but it was marked as incorrect, send an email to the Helpline with a description of the error. Please do not submit regrade requests based on what you perceive is overly harsh grading , The points we take off are based on a grading rubric that is being applied uniformly to all submissions. If you decide to send a regrade request , send an email to the Helpline with subject line \"Regrade HW1: Grader=johnsmith\" replacing 'HW1' with the current assignment and 'johnsmith' with the name of the grader within 48 hours of the grade release . Communication from Staff to Students Class announcements will be through Ed . All homework and will be posted and submitted through Canvas . Quizzes are completed on Ed as well as all feedback forms. NOTE: make sure you adjust your account settings so you can receive emails from Canvas. Submitting an assignment Please consult [Homework Policies & Submission Instructions] (coming soon) Course Grade Your final score for the course will be computed using the following weights: Assignment Final Grade Weight Homework 0 1% Paired Homework (5) 35% (7% per HW) Individual Homework (2) 16% (8% per HW) Midterm 10% Quizzes 6% Exercises 6% Project 26% Total 100% Software We will be using Jupyter Notebooks, Python 3, and various python modules. You can access the notebook viewer either on your own machine by installing the Anaconda platform (Links to an external site) which includes Jupyter/IPython as well all packages that will be required for the course, or by using the SEAS JupyterHub from Canvas. Details in class. Auditing the Class If you would like to audit the class, please send an email to the Helpline indicating who you are and why you want to audit the class. You need a HUID to be included to Canvas. Please note that auditors may not submit assignments for grading or make use of other limited student resources such as office hours. Academic Integrity Ethical behavior is an important trait of a Data Scientist, from ethically handling data to attribution of code and work of others. Thus, in CS109A we give a strong emphasis to Academic Honesty. As a student your best guidelines are to be reasonable and fair. We encourage teamwork for problem sets, but you should not split the homework and you should work on all the problems together. For more detailed expectations, please refer to the Collaborations section above. Accommodations for Students with Disabilities Students needing academic adjustments or accommodations because of a documented disability must present their Faculty Letter from the Accessible Education Office (AEO) and speak with the professor by the end of the second week of the term, (fill in specific date). Failure to do so may result in the Course Head's inability to respond in a timely manner. All discussions will remain confidential, although Faculty are invited to contact AEO to discuss appropriate implementation. Diversity and Inclusion Statement Data Science and Computer Science have historically been representative of only a small sliver of the population. This is despite the contributions of a diverse group of early pioneers - see Ada Lovelace, Dorothy Vaughan, and Grace Hopper for just a few examples. As educators, we aim to build a diverse, inclusive, and representative community offering opportunities in data science to those who have been historically marginalized. We will encourage learning that advances ethical data science, exposes bias in the way data science is used, and advances research into fair and responsible data science. We need your help to create a learning environment that supports a diversity of thoughts, perspectives, and experiences, and honors your identities (including but not limited to race, gender, class, sexuality, religion, ability, etc.) To help accomplish this: If you have a name and/or set of pronouns that differ from those in your official Harvard records, please let us know! If you feel like your performance in the class is being impacted by your experiences outside of class, please do not hesitate to come and talk with us. We want to be a resource for you. Remember that you can also submit anonymous feedback (which will lead to us making a general announcement to the class, if necessary, to address your concerns). If you prefer to speak with someone outside of the course, you may find helpful resources at the Harvard Office of Diversity and Inclusion. We (like many people) are still learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please talk to us about it. As a participant in course discussions, you are expected to respect your classmates' diverse backgrounds and perspectives. Our course will discuss diversity, inclusion, and ethics in data science. Please contact us (in person or electronically) or submit anonymous feedback if you have any suggestions for how we can improve. Extension School Policies Accommodation Requests. Harvard Extension School is committed to providing an inclusive, accessible academic community for students with disabilities and chronic health conditions. The Accessibility Services Office (ASO) (https://extension.harvard.edu/for-students/support-and-services/accessibility-services/) offers accommodations and support to students with documented disabilities. If you have a need for accommodations or adjustments, contact Accessibility Services directly via email at accessibility@extension.harvard.edu or by phone at 617-998-9640. Academic Integrity. You are responsible for understanding Harvard Extension School policies on academic integrity (https://extension.harvard.edu/for-students/student-policies-conduct/academic-integrity/) and how to use sources responsibly. Stated most broadly, academic integrity means that all course work submitted, whether a draft or a final version of a paper, project, take-home exam, online exam, computer program, oral presentation, or lab report, must be your own words and ideas, or the sources must be clearly acknowledged. The potential outcomes for violations of academic integrity are serious and ordinarily include all of the following: required withdrawal (RQ), which means a failing grade in the course (with no refund), the suspension of registration privileges, and a notation on your transcript. Using sources responsibly (https://extension.harvard.edu/for-students/support-and-services/using-sources-effectively-and-responsibly/) is an essential part of your Harvard education. We provide additional information about our expectations regarding academic integrity on our website. We invite you to review that information and to check your understanding of academic citation rules by completing two free online 15-minute tutorials that are also available on our site. (The tutorials are anonymous open-learning tools.)",
        "tags": "pages",
        "url": "pages/syllabus.html"
    }, {
        "title": "Lecture 24: Review",
        "text": "Slides Lecture 24 : Review (PDF) Exercises",
        "tags": "lectures",
        "url": "lectures/lecture24/"
    }, {
        "title": "Lecture 23: Natural Language Processing",
        "text": "Slides Exercises Lecture 23: Exercise - NLP [Notebook] Lecture 23: Exercise - Movie Review Classifier Exercise [Notebook]",
        "tags": "lectures",
        "url": "lectures/lecture23/"
    }, {
        "title": "Lecture 23: Natural Language Processing",
        "text": "NLP Natural Language Processing (NLP) Harvard University Fall 2021 Instructors : PP, NP What is NLP ? NLP draws from many disciplines, including computer science and computational linguistics. NLP is a branch of artificial intelligence that allows computers to interpret, analyze and manipulate human language. NLP is about developing applications and services that can understand human languages. A little bit of the NLP history When was the first the first time that humans asked themselves: \"Can machines understand the human language?\", or \"Can machines think?\" 1930: French-Armenian Georges Artsrouni and a Russian Petr Troyanskii - Developing ways to create automatic translators. WWII: First attempt of using NLP (Germans) to transfer secret messages between them. 1946: The machine \"Colossus\" was created by Britain for decrypting the secret code generated by Tunny (code name given to Enigma, by the British). Alan Turing was part of this team. 1950: Alan Turing publishes Computing Machinery and Intelligence introducing the \"Turing test\", also known as the \"imitation game\" 1957: Noam Chomsky introduced the syntactic structures. Later: Charles Hockett found out several drawbacks to Chomsky's approach. 1960's: Terry Winograd, at the MIT, developed SHRDLU, the first NLP computer program that was able to do different tasks such as remembering names, etc. 1969: Roger Schank introduced the concept of tokens that provide a better grasp of the meaning of a sentence 80's: The field of Machine Learning appears, and there is when algorithms start to be created for the computers to have a better understanding of the human language. NLP Applications Do we use any technology that uses NLP algorithms? If so, what are these technologies? How often do we use them? Part-of-speech tagging Named Entity Recognition (NER) Question answering Speech recognition Text-to-speech and speech-to-text Topic modeling Sentiment classification Language modeling Translation Information retrieval: Web searching algorithms that use keyword matching. Any examples? Maybe Google? Target Ads: Recommendations based on key words from social media. Have you search for shoes, laptops, flowers? Later you'll see some adds based on all those searchs. Translators: Google Translate, Bing Translator, Babylon Translator. Speech Recognition: Alexa, Siri, Hey Google, live captions. Text Summarization: Algorithms that allow getting a summary out of a text. Sentiment Analysis: Analysis done to reviews or posts from apps like Twitter, Yelp, Airbnb, Google reviews, etc, to understand human's feelings and emotions. NLP methods, techniques and key concepts: Tokenization Stopwords Stemming/Lemmatization Preprocessing steps Bag of words model Which libraries can we use? Due to the tremendous developing that python has had in the last years and the interest that has grown exponentially for the NLP topics, methods, techniques and models, there are many libraries that we can use on Python when working with text data. We'll list some of the most important and popular libraries: NLTK, spaCy, Core NLP, Pattern, Gensim, Polyglot, Text Blob, AllenNLP, Hugging Face Transformers, Flair. NLTK https://www.nltk.org/ Free and open source. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum. NLTK has been called \"a wonderful tool for teaching, and working in, computational linguistics using Python,\" and \"an amazing library to play with natural language.\" spaCy https://spacy.io/ spaCy is a library for advanced Natural Language Processing in Python and Cython. It comes with pre-trained pipelines and currently supports tokenization and training for 60+ languages. It features state-of-the-art speed and neural network models for tagging, parsing, named entity recognition, text classification and more, multi-task learning with pre-trained transformers. CoreNLP https://stanfordnlp.github.io/CoreNLP/ CoreNLP is your one stop shop for natural language processing in Java! CoreNLP enables users to derive linguistic annotations for text, including token and sentence boundaries, parts of speech, named entities, numeric and time values, dependency and constituency parses, coreference, sentiment, quote attributions, and relations. CoreNLP currently supports 8 languages: Arabic, Chinese, English, French, German, Hungarian, Italian, and Spanish. Polyglot https://polyglot.readthedocs.io/en/latest/ Polyglot is a natural language pipeline that supports massive multilingual applications such as tokenization, language detection, part of speech tagging and sentiment analysis. Gensim https://radimrehurek.com/gensim/# Gensim is a free open-source Python library for representing documents as semantic vectors, as efficiently (computer-wise) and painlessly (human-wise) as possible. It is designed to process raw, unstructured digital texts (\"plain text\") using unsupervised machine learning algorithms. Getting the data we're going to use ready. NLTK's twitter corpus contains a sample of 20k Tweets retrieved from the Twitter Streaming API. In [1]: # Libraries to help with reading and manipulating data import numpy as np import pandas as pd # libraries for visualizations import seaborn as sns import matplotlib.pyplot as plt # Removes the limit for the number of displayed columns pd . set_option ( \"display.max_columns\" , None ) # Sets the limit for the number of displayed rows pd . set_option ( \"display.max_rows\" , 200 ) import re import string from sklearn import metrics from sklearn.model_selection import train_test_split from sklearn.metrics import confusion_matrix from sklearn.naive_bayes import MultinomialNB from sklearn.feature_extraction.text import CountVectorizer from sklearn.feature_extraction.text import TfidfVectorizer # Library to suppress warnings or deprecation notes import warnings warnings . filterwarnings ( \"ignore\" ) In [2]: # You'll need to install NLTK if you don't have it already ! pip install nltk Requirement already satisfied: nltk in /home/chris/.local/miniconda3/envs/cs109a/lib/python3.9/site-packages (3.6.5) Requirement already satisfied: joblib in /home/chris/.local/miniconda3/envs/cs109a/lib/python3.9/site-packages (from nltk) (1.0.1) Requirement already satisfied: tqdm in /home/chris/.local/miniconda3/envs/cs109a/lib/python3.9/site-packages (from nltk) (4.62.3) Requirement already satisfied: click in /home/chris/.local/miniconda3/envs/cs109a/lib/python3.9/site-packages (from nltk) (8.0.1) Requirement already satisfied: regex>=2021.8.3 in /home/chris/.local/miniconda3/envs/cs109a/lib/python3.9/site-packages (from nltk) (2021.11.10) In [3]: # Let's use the NLTK library import nltk from nltk.corpus import twitter_samples # Download nltk data nltk . download ( \"twitter_samples\" ) [nltk_data] Downloading package twitter_samples to [nltk_data] /home/chris/nltk_data... [nltk_data] Package twitter_samples is already up-to-date! Out[3]: True Where are the files that we're downloading? By running nltk.download('twitter_samples') , we are downloading the twitter samples in json files. We can get their specific location and we'll find these files in our computers anytime.** In [4]: twitter_samples Out[4]: <TwitterCorpusReader in '/home/chris/nltk_data/corpora/twitter_samples'> In [5]: twitter_samples Out[5]: <TwitterCorpusReader in '/home/chris/nltk_data/corpora/twitter_samples'> How many JSON files do exist in the corpus twitter_samples and which ones are they? Use the twitter_samples.fileids() method: In [6]: twitter_samples . fileids () Out[6]: ['negative_tweets.json', 'positive_tweets.json', 'tweets.20150430-223406.json'] Checking that we're getting positive and negative tweets from twitter_samples In [7]: # Checking the sixth element: print ( \"Positive : \" , twitter_samples . strings ( \"positive_tweets.json\" )[ 5 ], \" \\n \" ) print ( \"Negative : \" , twitter_samples . strings ( \"negative_tweets.json\" )[ 5 ]) Positive : @BhaktisBanter @PallaviRuhail This one is irresistible :) #FlipkartFashionFriday http://t.co/EbZ0L2VENM Negative : oh god, my babies' faces :( https://t.co/9fcwGvaki0 We can divide apply the string to both files with the objective of converting them into a lists In [8]: positive_tw = twitter_samples . strings ( \"positive_tweets.json\" ) negative_tw = twitter_samples . strings ( \"negative_tweets.json\" ) Double checking the data type for positive_tw and negative_tw In [9]: print ( type ( positive_tw )) print ( type ( negative_tw )) <class 'list'> <class 'list'> Checking tweets in the position #6 from both lists In [10]: positive_tw [ 5 ] Out[10]: '@BhaktisBanter @PallaviRuhail This one is irresistible :)\\n#FlipkartFashionFriday http://t.co/EbZ0L2VENM' In [11]: negative_tw [ 5 ] Out[11]: \"oh god, my babies' faces :( https://t.co/9fcwGvaki0\" Since we've checked that we have now two lists, we can get the amount of positive and negative tweets that we have available for our analysis In [12]: print ( \"Positive tweets: \" , len ( positive_tw )) print ( \"Negative tweets: \" , len ( negative_tw )) Positive tweets: 5000 Negative tweets: 5000 Adding the tweets to new DataFrames using Pandas Creating one dataframe for positive tweets and one for negative tweets. In [13]: positive_tweets = pd . DataFrame ( positive_tw , columns = [ \"tweets\" ]) positive_tweets Out[13]: tweets 0 #FollowFriday @France_Inte @PKuchly57 @Milipol... 1 @Lamb2ja Hey James! How odd :/ Please call our... 2 @DespiteOfficial we had a listen last night :)... 3 @97sides CONGRATS :) 4 yeaaaah yippppy!!! my accnt verified rqst has... ... ... 4995 @chriswiggin3 Chris, that's great to hear :) D... 4996 @RachelLiskeard Thanks for the shout-out :) It... 4997 @side556 Hey! :) Long time no talk... 4998 @staybubbly69 as Matt would say. WELCOME TO AD... 4999 @DanielOConnel18 you could say he will have eg... 5000 rows Ã— 1 columns In [14]: negative_tweets = pd . DataFrame ( negative_tw , columns = [ \"tweets\" ]) negative_tweets Out[14]: tweets 0 hopeless for tmr :( 1 Everything in the kids section of IKEA is so c... 2 @Hegelbon That heart sliding into the waste ba... 3 \"@ketchBurning: I hate Japanese call him \"bani... 4 Dang starting next week I have \"work\" :( ... ... 4995 I wanna change my avi but uSanele :( 4996 MY PUPPY BROKE HER FOOT :( 4997 where's all the jaebum baby pictures :(( 4998 But but Mr Ahmad Maslan cooks too :( https://t... 4999 @eawoman As a Hull supporter I am expecting a ... 5000 rows Ã— 1 columns We will merge the positive and negative tweets into one dataset to handle the data in a better and simpler way. We'll add tags for each kind of tweet. Positive tweets: pos and negative tweets: neg . Steps: Create a new column to identify both, positive and negative tweets. Call this new column sentiment . Do this for both DataFrames. In [15]: positive_tweets [ \"sentiment\" ] = 1 positive_tweets [ \"sent_descr\" ] = \"positive\" negative_tweets [ \"sentiment\" ] = 0 negative_tweets [ \"sent_descr\" ] = \"negative\" positive_tweets . head () Out[15]: tweets sentiment sent_descr 0 #FollowFriday @France_Inte @PKuchly57 @Milipol... 1 positive 1 @Lamb2ja Hey James! How odd :/ Please call our... 1 positive 2 @DespiteOfficial we had a listen last night :)... 1 positive 3 @97sides CONGRATS :) 1 positive 4 yeaaaah yippppy!!! my accnt verified rqst has... 1 positive How do the positive tweets look like? In [16]: positive_tweets . head () Out[16]: tweets sentiment sent_descr 0 #FollowFriday @France_Inte @PKuchly57 @Milipol... 1 positive 1 @Lamb2ja Hey James! How odd :/ Please call our... 1 positive 2 @DespiteOfficial we had a listen last night :)... 1 positive 3 @97sides CONGRATS :) 1 positive 4 yeaaaah yippppy!!! my accnt verified rqst has... 1 positive How do the negative tweets look like? In [17]: negative_tweets . head () Out[17]: tweets sentiment sent_descr 0 hopeless for tmr :( 0 negative 1 Everything in the kids section of IKEA is so c... 0 negative 2 @Hegelbon That heart sliding into the waste ba... 0 negative 3 \"@ketchBurning: I hate Japanese call him \"bani... 0 negative 4 Dang starting next week I have \"work\" :( 0 negative Merging the DataFrames to have both, positive and negative tweets in one DataFrame. Steps: Create a copy from the positive tweets to a new DataFrame that it's going to be called tweets_df Add the negative tweets at the end of tweets_df Check that all the 10 thousand tweets are mixed together, positive and negative. In [18]: # Creating the copy of the Positive tweets and adding it to a new DataFrame tweets_df = positive_tweets . copy () tweets_df Out[18]: tweets sentiment sent_descr 0 #FollowFriday @France_Inte @PKuchly57 @Milipol... 1 positive 1 @Lamb2ja Hey James! How odd :/ Please call our... 1 positive 2 @DespiteOfficial we had a listen last night :)... 1 positive 3 @97sides CONGRATS :) 1 positive 4 yeaaaah yippppy!!! my accnt verified rqst has... 1 positive ... ... ... ... 4995 @chriswiggin3 Chris, that's great to hear :) D... 1 positive 4996 @RachelLiskeard Thanks for the shout-out :) It... 1 positive 4997 @side556 Hey! :) Long time no talk... 1 positive 4998 @staybubbly69 as Matt would say. WELCOME TO AD... 1 positive 4999 @DanielOConnel18 you could say he will have eg... 1 positive 5000 rows Ã— 3 columns In [19]: # Adding the negative tweets to our new DataFrame \"tweets\". tweets_df = tweets_df . append ( negative_tweets , ignore_index = True ) tweets_df Out[19]: tweets sentiment sent_descr 0 #FollowFriday @France_Inte @PKuchly57 @Milipol... 1 positive 1 @Lamb2ja Hey James! How odd :/ Please call our... 1 positive 2 @DespiteOfficial we had a listen last night :)... 1 positive 3 @97sides CONGRATS :) 1 positive 4 yeaaaah yippppy!!! my accnt verified rqst has... 1 positive ... ... ... ... 9995 I wanna change my avi but uSanele :( 0 negative 9996 MY PUPPY BROKE HER FOOT :( 0 negative 9997 where's all the jaebum baby pictures :(( 0 negative 9998 But but Mr Ahmad Maslan cooks too :( https://t... 0 negative 9999 @eawoman As a Hull supporter I am expecting a ... 0 negative 10000 rows Ã— 3 columns In [20]: # Resetting the index tweets_df . reset_index ( drop = True , inplace = True ) tweets_df Out[20]: tweets sentiment sent_descr 0 #FollowFriday @France_Inte @PKuchly57 @Milipol... 1 positive 1 @Lamb2ja Hey James! How odd :/ Please call our... 1 positive 2 @DespiteOfficial we had a listen last night :)... 1 positive 3 @97sides CONGRATS :) 1 positive 4 yeaaaah yippppy!!! my accnt verified rqst has... 1 positive ... ... ... ... 9995 I wanna change my avi but uSanele :( 0 negative 9996 MY PUPPY BROKE HER FOOT :( 0 negative 9997 where's all the jaebum baby pictures :(( 0 negative 9998 But but Mr Ahmad Maslan cooks too :( https://t... 0 negative 9999 @eawoman As a Hull supporter I am expecting a ... 0 negative 10000 rows Ã— 3 columns In [21]: # Let's visualize and verify that our data is consistent. plt . figure ( figsize = ( 7 , 3.5 )) sns . set_theme ( style = \"white\" , palette = \"nipy_spectral\" ) sns . countplot ( data = tweets_df , x = \"sent_descr\" , hue = \"sent_descr\" ) Out[21]: <AxesSubplot:xlabel='sent_descr', ylabel='count'> In [22]: tweets_df = tweets_df . drop ([ \"sent_descr\" ], axis = 1 ) Preprocessing Text processing is an essential part of performing data analytics or modeling on string data. Unlike numerical and even categorical variables, text data can't be easily structured in a table format and has its own very unique and rather complex set of rules that it follows. Engaging in text processing allows us to move onto more difficult tasks which are unique to dealing with text What is text processing? Text processing is the practice of manipulating text data in order to make it more amenable to analysis and modeling. There are a whole host of powerful libraries dedicated to this, including: string and str.methods Regular expressions Natural language processing libraries such as nltk . gensim , and spaCy Preprocessing Tasks We have several preprocessing tasks when analyzing text: Tokenize Remove Stop Words Clean special characters in text Stemming/Lemmatization Cleaning text A good practice is to standardize the text. For an easier text manipulation we will convert any string to lowercase. We will remove special characters and any strings that are not going to be needed for further analysis. String module Cleaning the tweets before going though any other text manipulation is helpful. For these first steps we will use some of the methods that the module String has. To learn more about the String methods click here. Some of the String Methods: capitalize(): Converts the first character to upper case casefold(): Converts string into lower case center(): Returns a centered string count(): Returns the number of times a specified value occurs in a string endswith(): Returns true if the string ends with the specified value find(): Searches the string for a specified value and returns the position of where it was found isalnum() Returns True if all characters in the string are alphanumeric isalpha() Returns True if all characters in the string are in the alphabet isascii() Returns True if all characters in the string are ascii characters isdecimal() Returns True if all characters in the string are decimals isdigit() Returns True if all characters in the string are digits isidentifier() Returns True if the string is an identifier islower() Returns True if all characters in the string are lower case isnumeric() Returns True if all characters in the string are numeric isprintable() Returns True if all characters in the string are printable isspace() Returns True if all characters in the string are whitespaces istitle() Returns True if the string follows the rules of a title isupper() Returns True if all characters in the string are upper case lower() Converts a string into lower case lstrip() Returns a left trim version of the string replace() Returns a string where a specified value is replaced with a specified value rsplit() Splits the string at the specified separator, and returns a list rstrip() Returns a right trim version of the string split() Splits the string at the specified separator, and returns a list splitlines() Splits the string at line breaks and returns a list startswith() Returns true if the string starts with the specified value strip() Returns a trimmed version of the string swapcase() Swaps cases, lower case becomes upper case and vice versa title() Converts the first character of each word to upper case translate() Returns a translated string upper() Converts a string into upper case zfill() Fills the string with a specified number of 0 values at the beginning In [23]: # Before we start, let's create a copy of our data so we can compare all the changes later. initial_df = tweets_df . copy () Converting any uppercase string to lowercase. In [24]: def lowercase_func ( tweets ): return tweets . lower () tweets_df [ \"tweets\" ] = tweets_df [ \"tweets\" ] . apply ( lambda x : lowercase_func ( x )) tweets_df Out[24]: tweets sentiment 0 #followfriday @france_inte @pkuchly57 @milipol... 1 1 @lamb2ja hey james! how odd :/ please call our... 1 2 @despiteofficial we had a listen last night :)... 1 3 @97sides congrats :) 1 4 yeaaaah yippppy!!! my accnt verified rqst has... 1 ... ... ... 9995 i wanna change my avi but usanele :( 0 9996 my puppy broke her foot :( 0 9997 where's all the jaebum baby pictures :(( 0 9998 but but mr ahmad maslan cooks too :( https://t... 0 9999 @eawoman as a hull supporter i am expecting a ... 0 10000 rows Ã— 2 columns Removing URL's using the find() method from the String : Python string method find() determines if string str occurs in string, or in a substring of string if starting index beg and ending index end are given. We will search for all the tweets that contain \"http\". Once we've identified them, we will remove the URL's In [25]: def find_url_in_tweets ( tweets ): return tweets . find ( \"http\" ) tweets_df [ \"find_url\" ] = tweets_df [ \"tweets\" ] . apply ( lambda m : find_url_in_tweets ( m )) tweets_df Out[25]: tweets sentiment find_url 0 #followfriday @france_inte @pkuchly57 @milipol... 1 -1 1 @lamb2ja hey james! how odd :/ please call our... 1 -1 2 @despiteofficial we had a listen last night :)... 1 -1 3 @97sides congrats :) 1 -1 4 yeaaaah yippppy!!! my accnt verified rqst has... 1 -1 ... ... ... ... 9995 i wanna change my avi but usanele :( 0 -1 9996 my puppy broke her foot :( 0 -1 9997 where's all the jaebum baby pictures :(( 0 -1 9998 but but mr ahmad maslan cooks too :( https://t... 0 37 9999 @eawoman as a hull supporter i am expecting a ... 0 -1 10000 rows Ã— 3 columns In [26]: with_url = range ( 140 ) match_url = tweets_df [ tweets_df [ \"find_url\" ] . isin ( with_url )] match_url . head ( 10 ) Out[26]: tweets sentiment find_url 5 @bhaktisbanter @pallaviruhail this one is irre... 1 81 6 we don't like to keep our lovely customers wai... 1 104 11 who wouldn't love these big....juicy....selfie... 1 53 12 @mish23615351 follow @jnlazts &amp; http://t.... 1 37 14 great new opportunity for junior triathletes a... 1 107 19 hello i need to know something can u fm me on ... 1 80 27 spiritual ritual festival (nÃ©pal)\\nbeginning o... 1 105 30 my kik - hatessuce32429 #kik #kikme #lgbt #tin... 1 74 35 @jamiefigsxx follow @jnlazts &amp; http://t.c... 1 36 43 i added a video to a @youtube playlist http://... 1 39 Reviewing the tweets that include URL's In [27]: # Looking at the datapoint with index 0 to confirm that it has an URL. match_url [ \"tweets\" ] . iloc [ 0 ] Out[27]: '@bhaktisbanter @pallaviruhail this one is irresistible :)\\n#flipkartfashionfriday http://t.co/ebz0l2venm' Removing URL's from tweets. In [28]: def remove_urls ( tweets ): url_pattern = re . compile ( r \"http[s]?://\\S+|www\\.\\S+\" ) return url_pattern . sub ( r \"\" , tweets ) tweets_df [ \"tweets\" ] = tweets_df [ \"tweets\" ] . apply ( remove_urls ) tweets_df . head ( 10 ) Out[28]: tweets sentiment find_url 0 #followfriday @france_inte @pkuchly57 @milipol... 1 -1 1 @lamb2ja hey james! how odd :/ please call our... 1 -1 2 @despiteofficial we had a listen last night :)... 1 -1 3 @97sides congrats :) 1 -1 4 yeaaaah yippppy!!! my accnt verified rqst has... 1 -1 5 @bhaktisbanter @pallaviruhail this one is irre... 1 81 6 we don't like to keep our lovely customers wai... 1 104 7 @impatientraider on second thought, there's ju... 1 -1 8 jgh , but we have to go to bayan :d bye 1 -1 9 as an act of mischievousness, am calling the e... 1 -1 In [29]: tweets_df [ \"tweets\" ] . iloc [ 5 ] Out[29]: '@bhaktisbanter @pallaviruhail this one is irresistible :)\\n#flipkartfashionfriday ' In [30]: # Since we won't need anymore the column find_url, we will drop it. tweets_df = tweets_df . drop ([ \"find_url\" ], axis = 1 ) Replacing emojis and emoticons with words In [32]: # We will use the library emot, which is open source. # we'll need version 2.1 specifically ! pip install emot == 2 .1 Requirement already satisfied: emot==2.1 in /home/chris/.local/miniconda3/envs/cs109a/lib/python3.9/site-packages (2.1) In [33]: from emot.emo_unicode import UNICODE_EMO , EMOTICONS In [34]: UNICODE_EMO [ 'ðŸ˜‚' ] . replace ( \",\" , \" \" ) . replace ( \":\" , \" \" ) . split () #UNICODE_EMO['ðŸ˜‚'] Out[34]: ['face_with_tears_of_joy'] In [35]: EMOTICONS Out[35]: {':â€‘\\\\)': 'Happy face or smiley', ':\\\\)': 'Happy face or smiley', ':-\\\\]': 'Happy face or smiley', ':\\\\]': 'Happy face or smiley', ':-3': 'Happy face smiley', ':3': 'Happy face smiley', ':->': 'Happy face smiley', ':>': 'Happy face smiley', '8-\\\\)': 'Happy face smiley', ':o\\\\)': 'Happy face smiley', ':-\\\\}': 'Happy face smiley', ':\\\\}': 'Happy face smiley', ':-\\\\)': 'Happy face smiley', ':c\\\\)': 'Happy face smiley', ':\\\\&#94;\\\\)': 'Happy face smiley', '=\\\\]': 'Happy face smiley', '=\\\\)': 'Happy face smiley', ':â€‘D': 'Laughing, big grin or laugh with glasses', ':D': 'Laughing, big grin or laugh with glasses', '8â€‘D': 'Laughing, big grin or laugh with glasses', '8D': 'Laughing, big grin or laugh with glasses', 'Xâ€‘D': 'Laughing, big grin or laugh with glasses', 'XD': 'Laughing, big grin or laugh with glasses', '=D': 'Laughing, big grin or laugh with glasses', '=3': 'Laughing, big grin or laugh with glasses', 'B\\\\&#94;D': 'Laughing, big grin or laugh with glasses', ':-\\\\)\\\\)': 'Very happy', ':â€‘\\\\(': 'Frown, sad, andry or pouting', ':-\\\\(': 'Frown, sad, andry or pouting', ':\\\\(': 'Frown, sad, andry or pouting', ':â€‘c': 'Frown, sad, andry or pouting', ':c': 'Frown, sad, andry or pouting', ':â€‘<': 'Frown, sad, andry or pouting', ':<': 'Frown, sad, andry or pouting', ':â€‘\\\\[': 'Frown, sad, andry or pouting', ':\\\\[': 'Frown, sad, andry or pouting', ':-\\\\|\\\\|': 'Frown, sad, andry or pouting', '>:\\\\[': 'Frown, sad, andry or pouting', ':\\\\{': 'Frown, sad, andry or pouting', ':@': 'Frown, sad, andry or pouting', '>:\\\\(': 'Frown, sad, andry or pouting', \":'â€‘\\\\(\": 'Crying', \":'\\\\(\": 'Crying', \":'â€‘\\\\)\": 'Tears of happiness', \":'\\\\)\": 'Tears of happiness', \"Dâ€‘':\": 'Horror', 'D:<': 'Disgust', 'D:': 'Sadness', 'D8': 'Great dismay', 'D;': 'Great dismay', 'D=': 'Great dismay', 'DX': 'Great dismay', ':â€‘O': 'Surprise', ':O': 'Surprise', ':â€‘o': 'Surprise', ':o': 'Surprise', ':-0': 'Shock', '8â€‘0': 'Yawn', '>:O': 'Yawn', ':-\\\\*': 'Kiss', ':\\\\*': 'Kiss', ':X': 'Kiss', ';â€‘\\\\)': 'Wink or smirk', ';\\\\)': 'Wink or smirk', '\\\\*-\\\\)': 'Wink or smirk', '\\\\*\\\\)': 'Wink or smirk', ';â€‘\\\\]': 'Wink or smirk', ';\\\\]': 'Wink or smirk', ';\\\\&#94;\\\\)': 'Wink or smirk', ':â€‘,': 'Wink or smirk', ';D': 'Wink or smirk', ':â€‘P': 'Tongue sticking out, cheeky, playful or blowing a raspberry', ':P': 'Tongue sticking out, cheeky, playful or blowing a raspberry', 'Xâ€‘P': 'Tongue sticking out, cheeky, playful or blowing a raspberry', 'XP': 'Tongue sticking out, cheeky, playful or blowing a raspberry', ':â€‘Ãž': 'Tongue sticking out, cheeky, playful or blowing a raspberry', ':Ãž': 'Tongue sticking out, cheeky, playful or blowing a raspberry', ':b': 'Tongue sticking out, cheeky, playful or blowing a raspberry', 'd:': 'Tongue sticking out, cheeky, playful or blowing a raspberry', '=p': 'Tongue sticking out, cheeky, playful or blowing a raspberry', '>:P': 'Tongue sticking out, cheeky, playful or blowing a raspberry', ':â€‘/': 'Skeptical, annoyed, undecided, uneasy or hesitant', ':/': 'Skeptical, annoyed, undecided, uneasy or hesitant', ':-[.]': 'Skeptical, annoyed, undecided, uneasy or hesitant', '>:[(\\\\\\\\)]': 'Skeptical, annoyed, undecided, uneasy or hesitant', '>:/': 'Skeptical, annoyed, undecided, uneasy or hesitant', ':[(\\\\\\\\)]': 'Skeptical, annoyed, undecided, uneasy or hesitant', '=/': 'Skeptical, annoyed, undecided, uneasy or hesitant', '=[(\\\\\\\\)]': 'Skeptical, annoyed, undecided, uneasy or hesitant', ':L': 'Skeptical, annoyed, undecided, uneasy or hesitant', '=L': 'Skeptical, annoyed, undecided, uneasy or hesitant', ':S': 'Skeptical, annoyed, undecided, uneasy or hesitant', ':â€‘\\\\|': 'Straight face', ':\\\\|': 'Straight face', ':$': 'Embarrassed or blushing', ':â€‘x': 'Sealed lips or wearing braces or tongue-tied', ':x': 'Sealed lips or wearing braces or tongue-tied', ':â€‘#': 'Sealed lips or wearing braces or tongue-tied', ':#': 'Sealed lips or wearing braces or tongue-tied', ':â€‘&': 'Sealed lips or wearing braces or tongue-tied', ':&': 'Sealed lips or wearing braces or tongue-tied', 'O:â€‘\\\\)': 'Angel, saint or innocent', 'O:\\\\)': 'Angel, saint or innocent', '0:â€‘3': 'Angel, saint or innocent', '0:3': 'Angel, saint or innocent', '0:â€‘\\\\)': 'Angel, saint or innocent', '0:\\\\)': 'Angel, saint or innocent', ':â€‘b': 'Tongue sticking out, cheeky, playful or blowing a raspberry', '0;\\\\&#94;\\\\)': 'Angel, saint or innocent', '>:â€‘\\\\)': 'Evil or devilish', '>:\\\\)': 'Evil or devilish', '\\\\}:â€‘\\\\)': 'Evil or devilish', '\\\\}:\\\\)': 'Evil or devilish', '3:â€‘\\\\)': 'Evil or devilish', '3:\\\\)': 'Evil or devilish', '>;\\\\)': 'Evil or devilish', '\\\\|;â€‘\\\\)': 'Cool', '\\\\|â€‘O': 'Bored', ':â€‘J': 'Tongue-in-cheek', '#â€‘\\\\)': 'Party all night', '%â€‘\\\\)': 'Drunk or confused', '%\\\\)': 'Drunk or confused', ':-###..': 'Being sick', ':###..': 'Being sick', '<:â€‘\\\\|': 'Dump', '\\\\(>_<\\\\)': 'Troubled', '\\\\(>_<\\\\)>': 'Troubled', \"\\\\(';'\\\\)\": 'Baby', '\\\\(\\\\&#94;\\\\&#94;>``': 'Nervous or Embarrassed or Troubled or Shy or Sweat drop', '\\\\(\\\\&#94;_\\\\&#94;;\\\\)': 'Nervous or Embarrassed or Troubled or Shy or Sweat drop', '\\\\(-_-;\\\\)': 'Nervous or Embarrassed or Troubled or Shy or Sweat drop', '\\\\(~_~;\\\\) \\\\(ãƒ»\\\\.ãƒ»;\\\\)': 'Nervous or Embarrassed or Troubled or Shy or Sweat drop', '\\\\(-_-\\\\)zzz': 'Sleeping', '\\\\(\\\\&#94;_-\\\\)': 'Wink', '\\\\(\\\\(\\\\+_\\\\+\\\\)\\\\)': 'Confused', '\\\\(\\\\+o\\\\+\\\\)': 'Confused', '\\\\(o\\\\|o\\\\)': 'Ultraman', '\\\\&#94;_\\\\&#94;': 'Joyful', '\\\\(\\\\&#94;_\\\\&#94;\\\\)/': 'Joyful', '\\\\(\\\\&#94;O\\\\&#94;\\\\)ï¼': 'Joyful', '\\\\(\\\\&#94;o\\\\&#94;\\\\)ï¼': 'Joyful', '\\\\(__\\\\)': 'Kowtow as a sign of respect, or dogeza for apology', '_\\\\(\\\\._\\\\.\\\\)_': 'Kowtow as a sign of respect, or dogeza for apology', '<\\\\(_ _\\\\)>': 'Kowtow as a sign of respect, or dogeza for apology', '<m\\\\(__\\\\)m>': 'Kowtow as a sign of respect, or dogeza for apology', 'm\\\\(__\\\\)m': 'Kowtow as a sign of respect, or dogeza for apology', 'm\\\\(_ _\\\\)m': 'Kowtow as a sign of respect, or dogeza for apology', \"\\\\('_'\\\\)\": 'Sad or Crying', '\\\\(/_;\\\\)': 'Sad or Crying', '\\\\(T_T\\\\) \\\\(;_;\\\\)': 'Sad or Crying', '\\\\(;_;': 'Sad of Crying', '\\\\(;_:\\\\)': 'Sad or Crying', '\\\\(;O;\\\\)': 'Sad or Crying', '\\\\(:_;\\\\)': 'Sad or Crying', '\\\\(ToT\\\\)': 'Sad or Crying', ';_;': 'Sad or Crying', ';-;': 'Sad or Crying', ';n;': 'Sad or Crying', ';;': 'Sad or Crying', 'Q\\\\.Q': 'Sad or Crying', 'T\\\\.T': 'Sad or Crying', 'QQ': 'Sad or Crying', 'Q_Q': 'Sad or Crying', '\\\\(-\\\\.-\\\\)': 'Shame', '\\\\(-_-\\\\)': 'Shame', '\\\\(ä¸€ä¸€\\\\)': 'Shame', '\\\\(ï¼›ä¸€_ä¸€\\\\)': 'Shame', '\\\\(=_=\\\\)': 'Tired', '\\\\(=\\\\&#94;\\\\Â·\\\\&#94;=\\\\)': 'cat', '\\\\(=\\\\&#94;\\\\Â·\\\\Â·\\\\&#94;=\\\\)': 'cat', '=_\\\\&#94;=\\t': 'cat', '\\\\(\\\\.\\\\.\\\\)': 'Looking down', '\\\\(\\\\._\\\\.\\\\)': 'Looking down', '\\\\&#94;m\\\\&#94;': 'Giggling with hand covering mouth', '\\\\(\\\\ãƒ»\\\\ãƒ»?': 'Confusion', '\\\\(?_?\\\\)': 'Confusion', '>\\\\&#94;_\\\\&#94;<': 'Normal Laugh', '<\\\\&#94;!\\\\&#94;>': 'Normal Laugh', '\\\\&#94;/\\\\&#94;': 'Normal Laugh', '\\\\ï¼ˆ\\\\*\\\\&#94;_\\\\&#94;\\\\*ï¼‰': 'Normal Laugh', '\\\\(\\\\&#94;<\\\\&#94;\\\\) \\\\(\\\\&#94;\\\\.\\\\&#94;\\\\)': 'Normal Laugh', '\\\\(&#94;\\\\&#94;\\\\)': 'Normal Laugh', '\\\\(\\\\&#94;\\\\.\\\\&#94;\\\\)': 'Normal Laugh', '\\\\(\\\\&#94;_\\\\&#94;\\\\.\\\\)': 'Normal Laugh', '\\\\(\\\\&#94;_\\\\&#94;\\\\)': 'Normal Laugh', '\\\\(\\\\&#94;\\\\&#94;\\\\)': 'Normal Laugh', '\\\\(\\\\&#94;J\\\\&#94;\\\\)': 'Normal Laugh', '\\\\(\\\\*\\\\&#94;\\\\.\\\\&#94;\\\\*\\\\)': 'Normal Laugh', '\\\\(\\\\&#94;â€”\\\\&#94;\\\\ï¼‰': 'Normal Laugh', '\\\\(#\\\\&#94;\\\\.\\\\&#94;#\\\\)': 'Normal Laugh', '\\\\ï¼ˆ\\\\&#94;â€”\\\\&#94;\\\\ï¼‰': 'Waving', '\\\\(;_;\\\\)/~~~': 'Waving', '\\\\(\\\\&#94;\\\\.\\\\&#94;\\\\)/~~~': 'Waving', '\\\\(-_-\\\\)/~~~ \\\\($\\\\Â·\\\\Â·\\\\)/~~~': 'Waving', '\\\\(T_T\\\\)/~~~': 'Waving', '\\\\(ToT\\\\)/~~~': 'Waving', '\\\\(\\\\*\\\\&#94;0\\\\&#94;\\\\*\\\\)': 'Excited', '\\\\(\\\\*_\\\\*\\\\)': 'Amazed', '\\\\(\\\\*_\\\\*;': 'Amazed', '\\\\(\\\\+_\\\\+\\\\) \\\\(@_@\\\\)': 'Amazed', '\\\\(\\\\*\\\\&#94;\\\\&#94;\\\\)v': 'Laughing,Cheerful', '\\\\(\\\\&#94;_\\\\&#94;\\\\)v': 'Laughing,Cheerful', '\\\\(\\\\(d[-_-]b\\\\)\\\\)': 'Headphones,Listening to music', '\\\\(-\"-\\\\)': 'Worried', '\\\\(ãƒ¼ãƒ¼;\\\\)': 'Worried', '\\\\(\\\\&#94;0_0\\\\&#94;\\\\)': 'Eyeglasses', '\\\\(\\\\ï¼¾ï½–\\\\ï¼¾\\\\)': 'Happy', '\\\\(\\\\ï¼¾ï½•\\\\ï¼¾\\\\)': 'Happy', '\\\\(\\\\&#94;\\\\)o\\\\(\\\\&#94;\\\\)': 'Happy', '\\\\(\\\\&#94;O\\\\&#94;\\\\)': 'Happy', '\\\\(\\\\&#94;o\\\\&#94;\\\\)': 'Happy', '\\\\)\\\\&#94;o\\\\&#94;\\\\(': 'Happy', ':O o_O': 'Surprised', 'o_0': 'Surprised', 'o\\\\.O': 'Surpised', '\\\\(o\\\\.o\\\\)': 'Surprised', 'oO': 'Surprised', '\\\\(\\\\*ï¿£mï¿£\\\\)': 'Dissatisfied', '\\\\(â€˜A`\\\\)': 'Snubbed or Deflated'} Converting emojis into words. In [36]: def convert_emojis ( text ): for emot in UNICODE_EMO : text = text . replace ( emot , \" \" . join ( UNICODE_EMO [ emot ] . replace ( \",\" , \" \" ) . replace ( \":\" , \" \" ) . split ()), ) return text # Example: emotions = \"âœ¨ðŸŽ‰ðŸ˜ðŸ‘Œ ðŸ¤© ðŸ˜‚ðŸ˜Ž\" convert_emojis ( emotions ) Out[36]: 'sparklesparty_poppersmiling_face_with_heart-eyesOK_hand ðŸ¤© face_with_tears_of_joysmiling_face_with_sunglasses' Convert emoticons into words. In [37]: def convert_emoticons ( text ): for emot in EMOTICONS : text = re . sub ( u \"(\" + emot + \")\" , \" \" . join ( EMOTICONS [ emot ] . replace ( \",\" , \"\" ) . split ()), text ) return text In [38]: # Example: faces = \":) =) XD :D :( :D :3 &#94;_&#94;\" convert_emoticons ( faces ) Out[38]: 'Happy face or smiley Happy face smiley Laughing big grin or laugh with glasses Laughing big grin or laugh with glasses Frown sad andry or pouting Laughing big grin or laugh with glasses Happy face smiley Joyful' Replacing emojis and emoticons from the tweets. In [39]: tweets_df [ \"clean_tweets\" ] = tweets_df [ \"tweets\" ] . apply ( convert_emoticons ) tweets_df [ \"clean_tweets\" ] = tweets_df [ \"clean_tweets\" ] . apply ( convert_emojis ) tweets_df . head ( 10 ) Out[39]: tweets sentiment clean_tweets 0 #followfriday @france_inte @pkuchly57 @milipol... 1 #followfriday @france_inte @pkuchly57 @milipol... 1 @lamb2ja hey james! how odd :/ please call our... 1 @lamb2ja hey james! how odd Skeptical annoyed ... 2 @despiteofficial we had a listen last night :)... 1 @despiteofficial we had a listen last night Ha... 3 @97sides congrats :) 1 @97sides congrats Happy face or smiley 4 yeaaaah yippppy!!! my accnt verified rqst has... 1 yeaaaah yippppy!!! my accnt verified rqst has... 5 @bhaktisbanter @pallaviruhail this one is irre... 1 @bhaktisbanter @pallaviruhail this one is irre... 6 we don't like to keep our lovely customers wai... 1 we don't like to keep our lovely customers wai... 7 @impatientraider on second thought, there's ju... 1 @impatientraider on second thought, there's ju... 8 jgh , but we have to go to bayan :d bye 1 jgh , but we have to go to bayan :d bye 9 as an act of mischievousness, am calling the e... 1 as an act of mischievousness, am calling the e... Removing mentions(@). For example @mariacamila2000000 In [40]: def remove_mentions ( tweets ): mention_pattern = re . compile ( r \"(@[A-Za-z0-9]+)|[_]|([&#94;0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\" ) return mention_pattern . sub ( r \"\" , tweets ) tweets_df [ \"clean_tweets\" ] = tweets_df [ \"clean_tweets\" ] . apply ( remove_mentions ) tweets_df . head ( 5 ) Out[40]: tweets sentiment clean_tweets 0 #followfriday @france_inte @pkuchly57 @milipol... 1 followfriday inte paris for being top engaged... 1 @lamb2ja hey james! how odd :/ please call our... 1 hey james how odd Skeptical annoyed undecided... 2 @despiteofficial we had a listen last night :)... 1 we had a listen last night Happy face or smil... 3 @97sides congrats :) 1 congrats Happy face or smiley 4 yeaaaah yippppy!!! my accnt verified rqst has... 1 yeaaaah yippppy my accnt verified rqst has su... Removing any noise that might be left: Special characters In [41]: def remove_noise ( tweet ): tweet = re . sub ( \"(@[A-Za-z0â€“9_]+)\" , \"\" , tweet ) tweet = \"\" . join ([ char if char not in string . punctuation else \" \" for char in tweet ]) tweet = re . sub ( \" +\" , \" \" , tweet ) tweet = re . sub ( \"[0â€“9]+\" , \"\" , tweet ) tweet = re . sub ( \"[&#94;A-Za-z0â€“9_. ]+\" , \"\" , tweet ) return tweet tweets_df [ \"clean_tweets\" ] = tweets_df [ \"clean_tweets\" ] . apply ( lambda x : remove_noise ( x )) # Applying the lowercase method again given that emojis and emoticons were converted, and the dictionary that includes all # of these emojis and emoticons have the definitions with the first letter in uppercase. tweets_df [ \"clean_tweets\" ] = tweets_df [ \"clean_tweets\" ] . apply ( lambda x : lowercase_func ( x )) tweets_df Out[41]: tweets sentiment clean_tweets 0 #followfriday @france_inte @pkuchly57 @milipol... 1 followfriday inte paris for being top engaged ... 1 @lamb2ja hey james! how odd :/ please call our... 1 hey james how odd skeptical annoyed undecided... 2 @despiteofficial we had a listen last night :)... 1 we had a listen last night happy face or smil... 3 @97sides congrats :) 1 congrats happy face or smiley 4 yeaaaah yippppy!!! my accnt verified rqst has... 1 yeaaaah yippppy my accnt verified rqst has suc... ... ... ... ... 9995 i wanna change my avi but usanele :( 0 i wanna change my avi but usanele frown sad an... 9996 my puppy broke her foot :( 0 my puppy broke her foot frown sad andry or pou... 9997 where's all the jaebum baby pictures :(( 0 wheres all the jaebum baby pictures frown sad ... 9998 but but mr ahmad maslan cooks too :( 0 but but mr ahmad maslan cooks too frown sad an... 9999 @eawoman as a hull supporter i am expecting a ... 0 as a hull supporter i am expecting a misserab... 10000 rows Ã— 3 columns Stopwords In [42]: from nltk.tokenize import sent_tokenize , word_tokenize from nltk.corpus import stopwords nltk . download ( 'stopwords' ) [nltk_data] Downloading package stopwords to /home/chris/nltk_data... [nltk_data] Package stopwords is already up-to-date! Out[42]: True What are stopwords? What are the languages available? In [43]: print ( \"English: \\n \" , stopwords . words ( \"english\" )) print ( \" \\n Spanish: \\n \" , stopwords . words ( \"spanish\" )) print ( \" \\n Russian: \\n \" , stopwords . words ( \"russian\" )) English: ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] Spanish: ['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo', 'como', 'mÃ¡s', 'pero', 'sus', 'le', 'ya', 'o', 'este', 'sÃ­', 'porque', 'esta', 'entre', 'cuando', 'muy', 'sin', 'sobre', 'tambiÃ©n', 'me', 'hasta', 'hay', 'donde', 'quien', 'desde', 'todo', 'nos', 'durante', 'todos', 'uno', 'les', 'ni', 'contra', 'otros', 'ese', 'eso', 'ante', 'ellos', 'e', 'esto', 'mÃ­', 'antes', 'algunos', 'quÃ©', 'unos', 'yo', 'otro', 'otras', 'otra', 'Ã©l', 'tanto', 'esa', 'estos', 'mucho', 'quienes', 'nada', 'muchos', 'cual', 'poco', 'ella', 'estar', 'estas', 'algunas', 'algo', 'nosotros', 'mi', 'mis', 'tÃº', 'te', 'ti', 'tu', 'tus', 'ellas', 'nosotras', 'vosotros', 'vosotras', 'os', 'mÃ­o', 'mÃ­a', 'mÃ­os', 'mÃ­as', 'tuyo', 'tuya', 'tuyos', 'tuyas', 'suyo', 'suya', 'suyos', 'suyas', 'nuestro', 'nuestra', 'nuestros', 'nuestras', 'vuestro', 'vuestra', 'vuestros', 'vuestras', 'esos', 'esas', 'estoy', 'estÃ¡s', 'estÃ¡', 'estamos', 'estÃ¡is', 'estÃ¡n', 'estÃ©', 'estÃ©s', 'estemos', 'estÃ©is', 'estÃ©n', 'estarÃ©', 'estarÃ¡s', 'estarÃ¡', 'estaremos', 'estarÃ©is', 'estarÃ¡n', 'estarÃ­a', 'estarÃ­as', 'estarÃ­amos', 'estarÃ­ais', 'estarÃ­an', 'estaba', 'estabas', 'estÃ¡bamos', 'estabais', 'estaban', 'estuve', 'estuviste', 'estuvo', 'estuvimos', 'estuvisteis', 'estuvieron', 'estuviera', 'estuvieras', 'estuviÃ©ramos', 'estuvierais', 'estuvieran', 'estuviese', 'estuvieses', 'estuviÃ©semos', 'estuvieseis', 'estuviesen', 'estando', 'estado', 'estada', 'estados', 'estadas', 'estad', 'he', 'has', 'ha', 'hemos', 'habÃ©is', 'han', 'haya', 'hayas', 'hayamos', 'hayÃ¡is', 'hayan', 'habrÃ©', 'habrÃ¡s', 'habrÃ¡', 'habremos', 'habrÃ©is', 'habrÃ¡n', 'habrÃ­a', 'habrÃ­as', 'habrÃ­amos', 'habrÃ­ais', 'habrÃ­an', 'habÃ­a', 'habÃ­as', 'habÃ­amos', 'habÃ­ais', 'habÃ­an', 'hube', 'hubiste', 'hubo', 'hubimos', 'hubisteis', 'hubieron', 'hubiera', 'hubieras', 'hubiÃ©ramos', 'hubierais', 'hubieran', 'hubiese', 'hubieses', 'hubiÃ©semos', 'hubieseis', 'hubiesen', 'habiendo', 'habido', 'habida', 'habidos', 'habidas', 'soy', 'eres', 'es', 'somos', 'sois', 'son', 'sea', 'seas', 'seamos', 'seÃ¡is', 'sean', 'serÃ©', 'serÃ¡s', 'serÃ¡', 'seremos', 'serÃ©is', 'serÃ¡n', 'serÃ­a', 'serÃ­as', 'serÃ­amos', 'serÃ­ais', 'serÃ­an', 'era', 'eras', 'Ã©ramos', 'erais', 'eran', 'fui', 'fuiste', 'fue', 'fuimos', 'fuisteis', 'fueron', 'fuera', 'fueras', 'fuÃ©ramos', 'fuerais', 'fueran', 'fuese', 'fueses', 'fuÃ©semos', 'fueseis', 'fuesen', 'sintiendo', 'sentido', 'sentida', 'sentidos', 'sentidas', 'siente', 'sentid', 'tengo', 'tienes', 'tiene', 'tenemos', 'tenÃ©is', 'tienen', 'tenga', 'tengas', 'tengamos', 'tengÃ¡is', 'tengan', 'tendrÃ©', 'tendrÃ¡s', 'tendrÃ¡', 'tendremos', 'tendrÃ©is', 'tendrÃ¡n', 'tendrÃ­a', 'tendrÃ­as', 'tendrÃ­amos', 'tendrÃ­ais', 'tendrÃ­an', 'tenÃ­a', 'tenÃ­as', 'tenÃ­amos', 'tenÃ­ais', 'tenÃ­an', 'tuve', 'tuviste', 'tuvo', 'tuvimos', 'tuvisteis', 'tuvieron', 'tuviera', 'tuvieras', 'tuviÃ©ramos', 'tuvierais', 'tuvieran', 'tuviese', 'tuvieses', 'tuviÃ©semos', 'tuvieseis', 'tuviesen', 'teniendo', 'tenido', 'tenida', 'tenidos', 'tenidas', 'tened'] Russian: ['Ð¸', 'Ð²', 'Ð²Ð¾', 'Ð½Ðµ', 'Ñ‡Ñ‚Ð¾', 'Ð¾Ð½', 'Ð½Ð°', 'Ñ', 'Ñ', 'ÑÐ¾', 'ÐºÐ°Ðº', 'Ð°', 'Ñ‚Ð¾', 'Ð²ÑÐµ', 'Ð¾Ð½Ð°', 'Ñ‚Ð°Ðº', 'ÐµÐ³Ð¾', 'Ð½Ð¾', 'Ð´Ð°', 'Ñ‚Ñ‹', 'Ðº', 'Ñƒ', 'Ð¶Ðµ', 'Ð²Ñ‹', 'Ð·Ð°', 'Ð±Ñ‹', 'Ð¿Ð¾', 'Ñ‚Ð¾Ð»ÑŒÐºÐ¾', 'ÐµÐµ', 'Ð¼Ð½Ðµ', 'Ð±Ñ‹Ð»Ð¾', 'Ð²Ð¾Ñ‚', 'Ð¾Ñ‚', 'Ð¼ÐµÐ½Ñ', 'ÐµÑ‰Ðµ', 'Ð½ÐµÑ‚', 'Ð¾', 'Ð¸Ð·', 'ÐµÐ¼Ñƒ', 'Ñ‚ÐµÐ¿ÐµÑ€ÑŒ', 'ÐºÐ¾Ð³Ð´Ð°', 'Ð´Ð°Ð¶Ðµ', 'Ð½Ñƒ', 'Ð²Ð´Ñ€ÑƒÐ³', 'Ð»Ð¸', 'ÐµÑÐ»Ð¸', 'ÑƒÐ¶Ðµ', 'Ð¸Ð»Ð¸', 'Ð½Ð¸', 'Ð±Ñ‹Ñ‚ÑŒ', 'Ð±Ñ‹Ð»', 'Ð½ÐµÐ³Ð¾', 'Ð´Ð¾', 'Ð²Ð°Ñ', 'Ð½Ð¸Ð±ÑƒÐ´ÑŒ', 'Ð¾Ð¿ÑÑ‚ÑŒ', 'ÑƒÐ¶', 'Ð²Ð°Ð¼', 'Ð²ÐµÐ´ÑŒ', 'Ñ‚Ð°Ð¼', 'Ð¿Ð¾Ñ‚Ð¾Ð¼', 'ÑÐµÐ±Ñ', 'Ð½Ð¸Ñ‡ÐµÐ³Ð¾', 'ÐµÐ¹', 'Ð¼Ð¾Ð¶ÐµÑ‚', 'Ð¾Ð½Ð¸', 'Ñ‚ÑƒÑ‚', 'Ð³Ð´Ðµ', 'ÐµÑÑ‚ÑŒ', 'Ð½Ð°Ð´Ð¾', 'Ð½ÐµÐ¹', 'Ð´Ð»Ñ', 'Ð¼Ñ‹', 'Ñ‚ÐµÐ±Ñ', 'Ð¸Ñ…', 'Ñ‡ÐµÐ¼', 'Ð±Ñ‹Ð»Ð°', 'ÑÐ°Ð¼', 'Ñ‡Ñ‚Ð¾Ð±', 'Ð±ÐµÐ·', 'Ð±ÑƒÐ´Ñ‚Ð¾', 'Ñ‡ÐµÐ³Ð¾', 'Ñ€Ð°Ð·', 'Ñ‚Ð¾Ð¶Ðµ', 'ÑÐµÐ±Ðµ', 'Ð¿Ð¾Ð´', 'Ð±ÑƒÐ´ÐµÑ‚', 'Ð¶', 'Ñ‚Ð¾Ð³Ð´Ð°', 'ÐºÑ‚Ð¾', 'ÑÑ‚Ð¾Ñ‚', 'Ñ‚Ð¾Ð³Ð¾', 'Ð¿Ð¾Ñ‚Ð¾Ð¼Ñƒ', 'ÑÑ‚Ð¾Ð³Ð¾', 'ÐºÐ°ÐºÐ¾Ð¹', 'ÑÐ¾Ð²ÑÐµÐ¼', 'Ð½Ð¸Ð¼', 'Ð·Ð´ÐµÑÑŒ', 'ÑÑ‚Ð¾Ð¼', 'Ð¾Ð´Ð¸Ð½', 'Ð¿Ð¾Ñ‡Ñ‚Ð¸', 'Ð¼Ð¾Ð¹', 'Ñ‚ÐµÐ¼', 'Ñ‡Ñ‚Ð¾Ð±Ñ‹', 'Ð½ÐµÐµ', 'ÑÐµÐ¹Ñ‡Ð°Ñ', 'Ð±Ñ‹Ð»Ð¸', 'ÐºÑƒÐ´Ð°', 'Ð·Ð°Ñ‡ÐµÐ¼', 'Ð²ÑÐµÑ…', 'Ð½Ð¸ÐºÐ¾Ð³Ð´Ð°', 'Ð¼Ð¾Ð¶Ð½Ð¾', 'Ð¿Ñ€Ð¸', 'Ð½Ð°ÐºÐ¾Ð½ÐµÑ†', 'Ð´Ð²Ð°', 'Ð¾Ð±', 'Ð´Ñ€ÑƒÐ³Ð¾Ð¹', 'Ñ…Ð¾Ñ‚ÑŒ', 'Ð¿Ð¾ÑÐ»Ðµ', 'Ð½Ð°Ð´', 'Ð±Ð¾Ð»ÑŒÑˆÐµ', 'Ñ‚Ð¾Ñ‚', 'Ñ‡ÐµÑ€ÐµÐ·', 'ÑÑ‚Ð¸', 'Ð½Ð°Ñ', 'Ð¿Ñ€Ð¾', 'Ð²ÑÐµÐ³Ð¾', 'Ð½Ð¸Ñ…', 'ÐºÐ°ÐºÐ°Ñ', 'Ð¼Ð½Ð¾Ð³Ð¾', 'Ñ€Ð°Ð·Ð²Ðµ', 'Ñ‚Ñ€Ð¸', 'ÑÑ‚Ñƒ', 'Ð¼Ð¾Ñ', 'Ð²Ð¿Ñ€Ð¾Ñ‡ÐµÐ¼', 'Ñ…Ð¾Ñ€Ð¾ÑˆÐ¾', 'ÑÐ²Ð¾ÑŽ', 'ÑÑ‚Ð¾Ð¹', 'Ð¿ÐµÑ€ÐµÐ´', 'Ð¸Ð½Ð¾Ð³Ð´Ð°', 'Ð»ÑƒÑ‡ÑˆÐµ', 'Ñ‡ÑƒÑ‚ÑŒ', 'Ñ‚Ð¾Ð¼', 'Ð½ÐµÐ»ÑŒÐ·Ñ', 'Ñ‚Ð°ÐºÐ¾Ð¹', 'Ð¸Ð¼', 'Ð±Ð¾Ð»ÐµÐµ', 'Ð²ÑÐµÐ³Ð´Ð°', 'ÐºÐ¾Ð½ÐµÑ‡Ð½Ð¾', 'Ð²ÑÑŽ', 'Ð¼ÐµÐ¶Ð´Ñƒ'] Selecting stop words Given that we are aiming to perform a Sentiment Analysis, we don't want to remove the negative stopwords because it could impact our detection of any negative sentiment. In [44]: # We use NLTK - Loading stop words and removing negative stop words from the list stop_words = stopwords . words ( \"english\" ) keep_these_words = [ \"don\" , \"don't\" , \"ain\" , \"aren\" , \"aren't\" , \"couldn\" , \"couldn't\" , \"didn\" , \"didn't\" , \"doesn\" , \"doesn't\" , \"hadn\" , \"hadn't\" , \"hasn\" , \"hasn't\" , \"haven\" , \"haven't\" , \"isn\" , \"isn't\" , \"ma\" , \"mightn\" , \"mightn't\" , \"mustn\" , \"mustn't\" , \"needn\" , \"needn't\" , \"shan\" , \"shan't\" , \"no\" , \"nor\" , \"not\" , \"shouldn\" , \"shouldn't\" , \"wasn\" , \"wasn't\" , \"weren\" , \"weren't\" , \"won\" , \"won't\" , \"wouldn\" , \"wouldn't\" , ] our_stop_words = stop_words for word in keep_these_words : our_stop_words . remove ( word ) print ( our_stop_words ) ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y'] Before removing the stop words from our tweets, let's review what is Tokenization. Tokenization Tokenizers divide strings into lists of substrings. Tokenization consists of dividing a piece of text into smaller pieces. We can divide paragraph into sentences, sentence into words or word into characters. How do we understand the meaning of a sentence? We read each word, interpret its meaning, and read the next word until we find an end point. This is the reason why tokenization exists. If we want to create a model, the model might need all the words that make up the sentence separately. If instead on a sentence we have a paragraph, then we need to get all the sentences and out of all these sentences, we need to get the words. At that point we can move forward to perform any kind of prediction. What is Tokenization? String tokenization is a process where a string is broken into several parts or tokens. Why is important? It is important because before doing a text analysis we to identify the words that constitute a string of characters. It's also important because we can identify the different type of words after obtaining the tokens. NLTK has different tokenize methods that can be applied to strings according to the desire output. sentences, words, characters. sent_tokenize(): Splits strings into sentences according to punctuation rules that have been already pre trained. regexp_tokenize(): Splits strings into substrings using regular expressions. word_tokenize(): Find words and punctuation in a string 1. Splitting a string into sentences using sent_tokenize() In [45]: # Let's review an example before applying it to our DataFrames. first_paragraph = \"\"\"1. The Imitation Game \\n I propose to consider the question, \"\"Can machines think?\"\" This should begin with definitions of the meaning of the terms \"\"machine\"\" and \"\"think.\"\" The definitions might be framed so as to reflect so far as possible the normal use of the words, but this attitude is dangerous, If the meaning of the words \"\"machine\"\" and \"\"think\"\" are to be found by examining how they are commonly used it is difficult to escape the conclusion that the meaning and the answer to the question, \"\"Can machines think?\"\" is to be sought in a statistical survey such as a Gallup poll. But this is absurd. Instead of attempting such a definition I shall replace the question by another, which is closely related to it and is expressed in relatively unambiguous words.\" \"\"\" first_paragraph Out[45]: '1. The Imitation Game\\nI propose to consider the question, \"\"Can machines think?\"\" This should begin \\nwith definitions of the meaning of the terms \"\"machine\"\" and \"\"think.\"\" The definitions might be framed so as to reflect so \\nfar as possible the normal use of the words, but this attitude is dangerous, If the meaning of the words \"\"machine\"\" and \\n\"\"think\"\" are to be found by examining how they are commonly used it is difficult to escape the conclusion that the meaning \\nand the answer to the question, \"\"Can machines think?\"\" is to be sought in a statistical survey such as a Gallup poll. But \\nthis is absurd. Instead of attempting such a definition I shall replace the question by another, which is closely related \\nto it and is expressed in relatively unambiguous words.\" ' In [46]: nltk . download ( 'punkt' ) from nltk.tokenize import sent_tokenize sentences = sent_tokenize ( first_paragraph ) sentences [nltk_data] Downloading package punkt to /home/chris/nltk_data... [nltk_data] Package punkt is already up-to-date! Out[46]: ['1.', 'The Imitation Game\\nI propose to consider the question, \"\"Can machines think?\"\"', 'This should begin \\nwith definitions of the meaning of the terms \"\"machine\"\" and \"\"think.\"\"', 'The definitions might be framed so as to reflect so \\nfar as possible the normal use of the words, but this attitude is dangerous, If the meaning of the words \"\"machine\"\" and \\n\"\"think\"\" are to be found by examining how they are commonly used it is difficult to escape the conclusion that the meaning \\nand the answer to the question, \"\"Can machines think?\"\"', 'is to be sought in a statistical survey such as a Gallup poll.', 'But \\nthis is absurd.', 'Instead of attempting such a definition I shall replace the question by another, which is closely related \\nto it and is expressed in relatively unambiguous words.\"'] In [47]: # We can check how many sentences we got from the first text we have used for this example: print ( \"The number of sentences is\" , len ( sentences )) The number of sentences is 7 2. Splitting strings to substrings using regexp_tokenize() We'll use regexp_tokenize() to get substrings and words using a pattern Using a pattern to tokenize on whitespaces. The pattern we are adding in the next cell code pattern='\\s+' means: \\s white space character + one or more That means that the patter pattern='\\s+' is splitting each string into tokens using one specific pattern: One or more white spaces. The patterns can be chosen from the regular expressions. Click here to learn more. We select gaps=True : True if this tokenizer's pattern should be used to find separators between tokens; False if this tokenizer's pattern should be used to find the tokens themselves. In [48]: from nltk.tokenize import regexp_tokenize words_regex = regexp_tokenize ( first_paragraph , pattern = \"\\s+\" , gaps = True ) print ( words_regex ) ['1.', 'The', 'Imitation', 'Game', 'I', 'propose', 'to', 'consider', 'the', 'question,', '\"\"Can', 'machines', 'think?\"\"', 'This', 'should', 'begin', 'with', 'definitions', 'of', 'the', 'meaning', 'of', 'the', 'terms', '\"\"machine\"\"', 'and', '\"\"think.\"\"', 'The', 'definitions', 'might', 'be', 'framed', 'so', 'as', 'to', 'reflect', 'so', 'far', 'as', 'possible', 'the', 'normal', 'use', 'of', 'the', 'words,', 'but', 'this', 'attitude', 'is', 'dangerous,', 'If', 'the', 'meaning', 'of', 'the', 'words', '\"\"machine\"\"', 'and', '\"\"think\"\"', 'are', 'to', 'be', 'found', 'by', 'examining', 'how', 'they', 'are', 'commonly', 'used', 'it', 'is', 'difficult', 'to', 'escape', 'the', 'conclusion', 'that', 'the', 'meaning', 'and', 'the', 'answer', 'to', 'the', 'question,', '\"\"Can', 'machines', 'think?\"\"', 'is', 'to', 'be', 'sought', 'in', 'a', 'statistical', 'survey', 'such', 'as', 'a', 'Gallup', 'poll.', 'But', 'this', 'is', 'absurd.', 'Instead', 'of', 'attempting', 'such', 'a', 'definition', 'I', 'shall', 'replace', 'the', 'question', 'by', 'another,', 'which', 'is', 'closely', 'related', 'to', 'it', 'and', 'is', 'expressed', 'in', 'relatively', 'unambiguous', 'words.\"'] In [49]: # How many substrings did we get? print ( \"The number of substrings is\" , len ( words_regex )) The number of substrings is 133 3. Splitting a string into words and punctuation using word_tokenize() In [50]: from nltk.tokenize import word_tokenize words_tokens = word_tokenize ( first_paragraph ) print ( words_tokens ) ['1', '.', 'The', 'Imitation', 'Game', 'I', 'propose', 'to', 'consider', 'the', 'question', ',', '``', \"''\", 'Can', 'machines', 'think', '?', \"''\", \"''\", 'This', 'should', 'begin', 'with', 'definitions', 'of', 'the', 'meaning', 'of', 'the', 'terms', '``', \"''\", 'machine', \"''\", \"''\", 'and', '``', \"''\", 'think', '.', \"''\", \"''\", 'The', 'definitions', 'might', 'be', 'framed', 'so', 'as', 'to', 'reflect', 'so', 'far', 'as', 'possible', 'the', 'normal', 'use', 'of', 'the', 'words', ',', 'but', 'this', 'attitude', 'is', 'dangerous', ',', 'If', 'the', 'meaning', 'of', 'the', 'words', '``', \"''\", 'machine', \"''\", \"''\", 'and', \"''\", \"''\", 'think', \"''\", \"''\", 'are', 'to', 'be', 'found', 'by', 'examining', 'how', 'they', 'are', 'commonly', 'used', 'it', 'is', 'difficult', 'to', 'escape', 'the', 'conclusion', 'that', 'the', 'meaning', 'and', 'the', 'answer', 'to', 'the', 'question', ',', '``', \"''\", 'Can', 'machines', 'think', '?', \"''\", \"''\", 'is', 'to', 'be', 'sought', 'in', 'a', 'statistical', 'survey', 'such', 'as', 'a', 'Gallup', 'poll', '.', 'But', 'this', 'is', 'absurd', '.', 'Instead', 'of', 'attempting', 'such', 'a', 'definition', 'I', 'shall', 'replace', 'the', 'question', 'by', 'another', ',', 'which', 'is', 'closely', 'related', 'to', 'it', 'and', 'is', 'expressed', 'in', 'relatively', 'unambiguous', 'words', '.', \"''\"] In [51]: print ( \"The number of substrings is\" , len ( words_tokens )) The number of substrings is 170 In [52]: from nltk.tokenize import TweetTokenizer twk = TweetTokenizer () tweet_tokens = twk . tokenize ( first_paragraph ) print ( tweet_tokens ) ['1', '.', 'The', 'Imitation', 'Game', 'I', 'propose', 'to', 'consider', 'the', 'question', ',', '\"', '\"', 'Can', 'machines', 'think', '?', '\"', '\"', 'This', 'should', 'begin', 'with', 'definitions', 'of', 'the', 'meaning', 'of', 'the', 'terms', '\"', '\"', 'machine', '\"', '\"', 'and', '\"', '\"', 'think', '.', '\"', '\"', 'The', 'definitions', 'might', 'be', 'framed', 'so', 'as', 'to', 'reflect', 'so', 'far', 'as', 'possible', 'the', 'normal', 'use', 'of', 'the', 'words', ',', 'but', 'this', 'attitude', 'is', 'dangerous', ',', 'If', 'the', 'meaning', 'of', 'the', 'words', '\"', '\"', 'machine', '\"', '\"', 'and', '\"', '\"', 'think', '\"', '\"', 'are', 'to', 'be', 'found', 'by', 'examining', 'how', 'they', 'are', 'commonly', 'used', 'it', 'is', 'difficult', 'to', 'escape', 'the', 'conclusion', 'that', 'the', 'meaning', 'and', 'the', 'answer', 'to', 'the', 'question', ',', '\"', '\"', 'Can', 'machines', 'think', '?', '\"', '\"', 'is', 'to', 'be', 'sought', 'in', 'a', 'statistical', 'survey', 'such', 'as', 'a', 'Gallup', 'poll', '.', 'But', 'this', 'is', 'absurd', '.', 'Instead', 'of', 'attempting', 'such', 'a', 'definition', 'I', 'shall', 'replace', 'the', 'question', 'by', 'another', ',', 'which', 'is', 'closely', 'related', 'to', 'it', 'and', 'is', 'expressed', 'in', 'relatively', 'unambiguous', 'words', '.', '\"'] In [53]: print ( \"The number of substrings is\" , len ( tweet_tokens )) The number of substrings is 170 Let's try the tokenizers for our tweets. Which one would be better for the following text processing steps? Is the combination of some characters useful for analyzing tweets? In [54]: tweets_df [ \"tweets\" ] . apply ( lambda x : sent_tokenize ( x )) Out[54]: 0 [#followfriday @france_inte @pkuchly57 @milipo... 1 [@lamb2ja hey james!, how odd :/ please call o... 2 [@despiteofficial we had a listen last night :... 3 [@97sides congrats :)] 4 [yeaaaah yippppy!!!, my accnt verified rqst ha... ... 9995 [i wanna change my avi but usanele :(] 9996 [my puppy broke her foot :(] 9997 [where's all the jaebum baby pictures :((] 9998 [but but mr ahmad maslan cooks too :(] 9999 [@eawoman as a hull supporter i am expecting a... Name: tweets, Length: 10000, dtype: object In [55]: tweets_df [ \"tweets\" ] . apply ( lambda x : regexp_tokenize ( x , pattern = \"\\s+\" , gaps = True )) Out[55]: 0 [#followfriday, @france_inte, @pkuchly57, @mil... 1 [@lamb2ja, hey, james!, how, odd, :/, please, ... 2 [@despiteofficial, we, had, a, listen, last, n... 3 [@97sides, congrats, :)] 4 [yeaaaah, yippppy!!!, my, accnt, verified, rqs... ... 9995 [i, wanna, change, my, avi, but, usanele, :(] 9996 [my, puppy, broke, her, foot, :(] 9997 [where's, all, the, jaebum, baby, pictures, :((] 9998 [but, but, mr, ahmad, maslan, cooks, too, :(] 9999 [@eawoman, as, a, hull, supporter, i, am, expe... Name: tweets, Length: 10000, dtype: object In [56]: tweets_df [ \"tweets\" ] . apply ( lambda x : word_tokenize ( x )) Out[56]: 0 [#, followfriday, @, france_inte, @, pkuchly57... 1 [@, lamb2ja, hey, james, !, how, odd, :, /, pl... 2 [@, despiteofficial, we, had, a, listen, last,... 3 [@, 97sides, congrats, :, )] 4 [yeaaaah, yippppy, !, !, !, my, accnt, verifie... ... 9995 [i, wan, na, change, my, avi, but, usanele, :, (] 9996 [my, puppy, broke, her, foot, :, (] 9997 [where, 's, all, the, jaebum, baby, pictures, ... 9998 [but, but, mr, ahmad, maslan, cooks, too, :, (] 9999 [@, eawoman, as, a, hull, supporter, i, am, ex... Name: tweets, Length: 10000, dtype: object Before getting the tokens from our tweets, we will proceed to remove the stop words . In [57]: from nltk.tokenize import word_tokenize def remove_stop_words ( tweet ): tokens = word_tokenize ( tweet ) tweet_with_no_stop_words = [ token for token in tokens if not token in our_stop_words ] reformed_tweet = \" \" . join ( tweet_with_no_stop_words ) return reformed_tweet tweets_df [ \"clean_tweets\" ] = tweets_df [ \"clean_tweets\" ] . apply ( lambda x : remove_stop_words ( x ) ) # Saving clean tweets tweets_df [ \"tweets\" ] = tweets_df [ \"clean_tweets\" ] tweets_df = tweets_df . drop ([ \"clean_tweets\" ], axis = 1 ) tweets_df Out[57]: tweets sentiment 0 followfriday inte paris top engaged members co... 1 1 hey james odd skeptical annoyed undecided unea... 1 2 listen last night happy face smiley bleed amaz... 1 3 congrats happy face smiley 1 4 yeaaaah yippppy accnt verified rqst succeed go... 1 ... ... ... 9995 wan na change avi usanele frown sad andry pouting 0 9996 puppy broke foot frown sad andry pouting 0 9997 wheres jaebum baby pictures frown sad andry po... 0 9998 mr ahmad maslan cooks frown sad andry pouting 0 9999 hull supporter expecting misserable weeks frow... 0 10000 rows Ã— 2 columns To serve our purpose, we would like to keep some combination of characters as they can reference emojis and therefore, they can reference emotions. This is why instead of using word_tokenize() we will use regexp_tokenize() . Let's create an additional column in our DataFrame tweets_df to track the different process we are going to perform. In [58]: tweets_df [ \"tweets_token\" ] = tweets_df [ \"tweets\" ] . apply ( lambda x : regexp_tokenize ( x , pattern = \"\\s+\" , gaps = True ) ) tweets_df Out[58]: tweets sentiment tweets_token 0 followfriday inte paris top engaged members co... 1 [followfriday, inte, paris, top, engaged, memb... 1 hey james odd skeptical annoyed undecided unea... 1 [hey, james, odd, skeptical, annoyed, undecide... 2 listen last night happy face smiley bleed amaz... 1 [listen, last, night, happy, face, smiley, ble... 3 congrats happy face smiley 1 [congrats, happy, face, smiley] 4 yeaaaah yippppy accnt verified rqst succeed go... 1 [yeaaaah, yippppy, accnt, verified, rqst, succ... ... ... ... ... 9995 wan na change avi usanele frown sad andry pouting 0 [wan, na, change, avi, usanele, frown, sad, an... 9996 puppy broke foot frown sad andry pouting 0 [puppy, broke, foot, frown, sad, andry, pouting] 9997 wheres jaebum baby pictures frown sad andry po... 0 [wheres, jaebum, baby, pictures, frown, sad, a... 9998 mr ahmad maslan cooks frown sad andry pouting 0 [mr, ahmad, maslan, cooks, frown, sad, andry, ... 9999 hull supporter expecting misserable weeks frow... 0 [hull, supporter, expecting, misserable, weeks... 10000 rows Ã— 3 columns Let's check how many tokens do we get from each tweet after the tokenization. In [59]: tweets_df [ \"tweets_length\" ] = tweets_df [ \"tweets_token\" ] . apply ( lambda x : len ( x )) tweets_df Out[59]: tweets sentiment tweets_token tweets_length 0 followfriday inte paris top engaged members co... 1 [followfriday, inte, paris, top, engaged, memb... 11 1 hey james odd skeptical annoyed undecided unea... 1 [hey, james, odd, skeptical, annoyed, undecide... 19 2 listen last night happy face smiley bleed amaz... 1 [listen, last, night, happy, face, smiley, ble... 10 3 congrats happy face smiley 1 [congrats, happy, face, smiley] 4 4 yeaaaah yippppy accnt verified rqst succeed go... 1 [yeaaaah, yippppy, accnt, verified, rqst, succ... 16 ... ... ... ... ... 9995 wan na change avi usanele frown sad andry pouting 0 [wan, na, change, avi, usanele, frown, sad, an... 9 9996 puppy broke foot frown sad andry pouting 0 [puppy, broke, foot, frown, sad, andry, pouting] 7 9997 wheres jaebum baby pictures frown sad andry po... 0 [wheres, jaebum, baby, pictures, frown, sad, a... 8 9998 mr ahmad maslan cooks frown sad andry pouting 0 [mr, ahmad, maslan, cooks, frown, sad, andry, ... 8 9999 hull supporter expecting misserable weeks frow... 0 [hull, supporter, expecting, misserable, weeks... 9 10000 rows Ã— 4 columns After getting the length for each tweet, we can check the distribution for the tweet's length. In [60]: plt . figure ( figsize = ( 13 , 2 )) sns . boxplot ( data = tweets_df , x = \"tweets_length\" , palette = \"Reds_r\" ) Out[60]: <AxesSubplot:xlabel='tweets_length'> In [61]: plt . figure ( figsize = ( 12 , 4 )) sns . set_theme ( style = \"white\" ) sns . histplot ( tweets_df , x = \"tweets_length\" , color = \"black\" ) Out[61]: <AxesSubplot:xlabel='tweets_length', ylabel='Count'> In [62]: print ( \"Q25=\" , tweets_df [ \"tweets_length\" ] . quantile ( q = 0.25 )) print ( \"Q50=\" , tweets_df [ \"tweets_length\" ] . quantile ( q = 0.5 )) print ( \"Q75=\" , tweets_df [ \"tweets_length\" ] . quantile ( q = 0.75 )) Q25= 6.0 Q50= 8.0 Q75= 12.0 In [63]: tweets_df . tweets_length . describe () Out[63]: count 10000.00000 mean 9.15850 std 4.05199 min 0.00000 25% 6.00000 50% 8.00000 75% 12.00000 max 40.00000 Name: tweets_length, dtype: float64 In [64]: # Dropping the column tweets_length given that we will not need it for the next steps of our analysis tweets_df = tweets_df . drop ([ \"tweets_length\" ], axis = 1 ) Using Collections library The Collections module implements high-performance container datatypes (beyond the built-in types list, dict and tuple) and contains many useful data structures that you can use to store information in memory. Counter() A Counter is a container that tracks how many times equivalent values are added. It can be used to implement the same algorithms for which other languages commonly use bag or multi-set data structures In [65]: from collections import Counter tokens = Counter () tweets_df [ \"tweets_token\" ] . apply ( tokens . update ) # Printing the count of the tokens tokens_df = ( pd . DataFrame . from_dict ( tokens , orient = \"index\" ) . reset_index () . rename ( columns = { \"index\" : \"token\" , 0 : \"count\" }) ) tokens_df . sort_values ( by = \"count\" , ascending = False ) Out[65]: token count 1520 sad 5272 3880 andry 5147 3881 pouting 5037 3879 frown 4981 8 happy 4498 ... ... ... 2324 accumulated 1 7212 smileyplz 1 7211 tweeeps 1 7210 afterznoon 1 13227 misserable 1 13228 rows Ã— 2 columns In [66]: plot_tokens_top_30 = tokens_df . sort_values ( by = \"count\" , ascending = False ) . head ( 30 ) plt . figure ( figsize = ( 15 , 6 )) sns . set_theme ( style = \"whitegrid\" ) ax = sns . barplot ( x = \"token\" , y = \"count\" , data = plot_tokens_top_30 ) _ = plt . xticks ( rotation = 60 ) Stemming/Lemmatization - Stemming Stemming is the process of removing prefixes and suffixes from words so that they are reduced to simpler forms which are called stems. In [67]: from nltk import PorterStemmer sentence = [ \"I\" , \"was\" , \"thinking\" , \"about\" , \"geometry\" , \"while\" , \"I\" , \"was\" , \"enjoying\" , \"the\" , \"best\" , \"month\" , \"of\" , \"winter\" , ] porterStemmer = PorterStemmer () print ( \" Sentence: \\n \" , sentence , \" \\n\\n \" , \"Sentence with stemming: \\n \" , \" \" . join ([ porterStemmer . stem ( word ) for word in sentence ]), \" \\n \" , ) Sentence: ['I', 'was', 'thinking', 'about', 'geometry', 'while', 'I', 'was', 'enjoying', 'the', 'best', 'month', 'of', 'winter'] Sentence with stemming: i wa think about geometri while i wa enjoy the best month of winter In [68]: porterStemmer = PorterStemmer () tweets_df [ \"t_stem\" ] = tweets_df [ \"tweets_token\" ] . apply ( lambda lst : [ porterStemmer . stem ( word ) for word in lst ] ) tweets_df Out[68]: tweets sentiment tweets_token t_stem 0 followfriday inte paris top engaged members co... 1 [followfriday, inte, paris, top, engaged, memb... [followfriday, int, pari, top, engag, member, ... 1 hey james odd skeptical annoyed undecided unea... 1 [hey, james, odd, skeptical, annoyed, undecide... [hey, jame, odd, skeptic, annoy, undecid, unea... 2 listen last night happy face smiley bleed amaz... 1 [listen, last, night, happy, face, smiley, ble... [listen, last, night, happi, face, smiley, ble... 3 congrats happy face smiley 1 [congrats, happy, face, smiley] [congrat, happi, face, smiley] 4 yeaaaah yippppy accnt verified rqst succeed go... 1 [yeaaaah, yippppy, accnt, verified, rqst, succ... [yeaaaah, yippppi, accnt, verifi, rqst, succee... ... ... ... ... ... 9995 wan na change avi usanele frown sad andry pouting 0 [wan, na, change, avi, usanele, frown, sad, an... [wan, na, chang, avi, usanel, frown, sad, andr... 9996 puppy broke foot frown sad andry pouting 0 [puppy, broke, foot, frown, sad, andry, pouting] [puppi, broke, foot, frown, sad, andri, pout] 9997 wheres jaebum baby pictures frown sad andry po... 0 [wheres, jaebum, baby, pictures, frown, sad, a... [where, jaebum, babi, pictur, frown, sad, andr... 9998 mr ahmad maslan cooks frown sad andry pouting 0 [mr, ahmad, maslan, cooks, frown, sad, andry, ... [mr, ahmad, maslan, cook, frown, sad, andri, p... 9999 hull supporter expecting misserable weeks frow... 0 [hull, supporter, expecting, misserable, weeks... [hull, support, expect, misser, week, frown, s... 10000 rows Ã— 4 columns - Lemmatization Lemmatization reduce words to their root form. In lemmatization, the speech part of a word must be determined first and the normalization rules will be different for different parts of the speech, whereas, the stemmer operates on a single word without knowledge of the context, and therefore cannot discriminate between words that have different meanings depending on part of the speech. Generating tags for each token in the text to Normalize sentences and to lemmatize each word using the tag The function lemmatize_sentence gets the position tag of each token of a tweet. For example, if the tag starts with NN, the token is assigned as a noun. In [69]: nltk . download ( 'wordnet' ) from nltk.stem import WordNetLemmatizer sentence = [ \"I\" , \"was\" , \"thinking\" , \"about\" , \"geometry\" , \"while\" , \"I\" , \"was\" , \"enjoying\" , \"the\" , \"best\" , \"month\" , \"of\" , \"winter\" , ] lemmatizer = WordNetLemmatizer () print ( \" Sentence: \\n \" , sentence , \" \\n\\n \" , \"Sentence with lemmatizing: \\n \" , \" \" . join ([ lemmatizer . lemmatize ( word ) for word in sentence ]), \" \\n \" , ) [nltk_data] Downloading package wordnet to /home/chris/nltk_data... [nltk_data] Package wordnet is already up-to-date! Sentence: ['I', 'was', 'thinking', 'about', 'geometry', 'while', 'I', 'was', 'enjoying', 'the', 'best', 'month', 'of', 'winter'] Sentence with lemmatizing: I wa thinking about geometry while I wa enjoying the best month of winter Tagging using pos_tag A \"tag\" is a case-sensitive string that specifies some property of a token, such as its part of speech. Tagged tokens are encoded as tuples (tag, token). https://www.nltk.org/api/nltk.tag.html Common tags: CC: conjunction, coordinating IN: preposition or conjunction, subordinating JJ: adjective or numeral, ordinal NN: noun, common, singular or mass PRP: pronoun, personal SYM: symbol VB: verb, base form In [70]: # Run this cell to learn more about the tags # nltk.help.upenn_tagset() In [71]: nltk . download ( 'averaged_perceptron_tagger' ) from nltk.tag import pos_tag sentence = [ \"I\" , \"was\" , \"thinking\" , \"about\" , \"geometry\" , \"while\" , \"I\" , \"was\" , \"enjoying\" , \"the\" , \"best\" , \"month\" , \"of\" , \"winter\" , ] nltk . pos_tag ( sentence ) [nltk_data] Downloading package averaged_perceptron_tagger to [nltk_data] /home/chris/nltk_data... [nltk_data] Package averaged_perceptron_tagger is already up-to- [nltk_data] date! Out[71]: [('I', 'PRP'), ('was', 'VBD'), ('thinking', 'VBG'), ('about', 'IN'), ('geometry', 'NN'), ('while', 'IN'), ('I', 'PRP'), ('was', 'VBD'), ('enjoying', 'VBG'), ('the', 'DT'), ('best', 'JJS'), ('month', 'NN'), ('of', 'IN'), ('winter', 'NN')] Getting the lemmas using tweets_tokens. In [72]: lmtzr = WordNetLemmatizer () tweets_df [ \"t_lemma\" ] = tweets_df [ \"tweets_token\" ] . apply ( lambda lst : [ lmtzr . lemmatize ( word ) for word in lst ] ) tweets_df . tail () Out[72]: tweets sentiment tweets_token t_stem t_lemma 9995 wan na change avi usanele frown sad andry pouting 0 [wan, na, change, avi, usanele, frown, sad, an... [wan, na, chang, avi, usanel, frown, sad, andr... [wan, na, change, avi, usanele, frown, sad, an... 9996 puppy broke foot frown sad andry pouting 0 [puppy, broke, foot, frown, sad, andry, pouting] [puppi, broke, foot, frown, sad, andri, pout] [puppy, broke, foot, frown, sad, andry, pouting] 9997 wheres jaebum baby pictures frown sad andry po... 0 [wheres, jaebum, baby, pictures, frown, sad, a... [where, jaebum, babi, pictur, frown, sad, andr... [wheres, jaebum, baby, picture, frown, sad, an... 9998 mr ahmad maslan cooks frown sad andry pouting 0 [mr, ahmad, maslan, cooks, frown, sad, andry, ... [mr, ahmad, maslan, cook, frown, sad, andri, p... [mr, ahmad, maslan, cook, frown, sad, andry, p... 9999 hull supporter expecting misserable weeks frow... 0 [hull, supporter, expecting, misserable, weeks... [hull, support, expect, misser, week, frown, s... [hull, supporter, expecting, misserable, week,... In [73]: lemmas = Counter () tweets_df [ \"t_lemma\" ] . apply ( lemmas . update ) lemmas_df = ( pd . DataFrame . from_dict ( lemmas , orient = \"index\" ) . reset_index () . rename ( columns = { \"index\" : \"lemma\" , 0 : \"count\" }) ) lemmas_df . sort_values ( by = \"count\" , ascending = False ) Out[73]: lemma count 1475 sad 5272 3699 andry 5147 3700 pouting 5037 3698 frown 4981 8 happy 4498 ... ... ... 6009 forklift 1 6008 todos 1 6006 bom 1 6005 nb 1 12387 misserable 1 12388 rows Ã— 2 columns In [74]: plot_lemmas_top_30 = lemmas_df . sort_values ( by = \"count\" , ascending = False ) . head ( 30 ) plt . figure ( figsize = ( 15 , 6 )) sns . set_theme ( style = \"whitegrid\" ) ax = sns . barplot ( x = \"lemma\" , y = \"count\" , data = plot_lemmas_top_30 ) _ = plt . xticks ( rotation = 60 ) Getting the taggs using tweets_tokens. In [75]: from nltk import pos_tag tweets_df [ \"t_postag\" ] = tweets_df [ \"tweets\" ] . apply ( lambda v : nltk . pos_tag ( nltk . word_tokenize ( v )) ) tweets_df . head () Out[75]: tweets sentiment tweets_token t_stem t_lemma t_postag 0 followfriday inte paris top engaged members co... 1 [followfriday, inte, paris, top, engaged, memb... [followfriday, int, pari, top, engag, member, ... [followfriday, inte, paris, top, engaged, memb... [(followfriday, JJ), (inte, JJ), (paris, NN), ... 1 hey james odd skeptical annoyed undecided unea... 1 [hey, james, odd, skeptical, annoyed, undecide... [hey, jame, odd, skeptic, annoy, undecid, unea... [hey, james, odd, skeptical, annoyed, undecide... [(hey, NN), (james, NNS), (odd, JJ), (skeptica... 2 listen last night happy face smiley bleed amaz... 1 [listen, last, night, happy, face, smiley, ble... [listen, last, night, happi, face, smiley, ble... [listen, last, night, happy, face, smiley, ble... [(listen, VBN), (last, JJ), (night, NN), (happ... 3 congrats happy face smiley 1 [congrats, happy, face, smiley] [congrat, happi, face, smiley] [congrats, happy, face, smiley] [(congrats, NNS), (happy, JJ), (face, NN), (sm... 4 yeaaaah yippppy accnt verified rqst succeed go... 1 [yeaaaah, yippppy, accnt, verified, rqst, succ... [yeaaaah, yippppi, accnt, verifi, rqst, succee... [yeaaaah, yippppy, accnt, verified, rqst, succ... [(yeaaaah, NN), (yippppy, JJ), (accnt, NN), (v... In [76]: tags = Counter () tweets_df [ \"t_postag\" ] . apply ( tags . update ) tags_df = ( pd . DataFrame . from_dict ( tags , orient = \"index\" ) . reset_index () . rename ( columns = { \"index\" : \"tag\" , 0 : \"count\" }) ) tags_df . sort_values ( by = \"count\" , ascending = False ) Out[76]: tag count 1733 (sad, JJ) 5208 4610 (andry, NN) 5061 8 (happy, JJ) 4495 9 (face, NN) 4450 10 (smiley, NN) 3978 ... ... ... 7844 (overdue, JJ) 1 7845 (mice, NN) 1 7846 (womandancingwomandancingwomandancingsa, NN) 1 7847 (joys, NNS) 1 16918 (misserable, JJ) 1 16919 rows Ã— 2 columns In [77]: plot_tags_top_30 = tags_df . sort_values ( by = \"count\" , ascending = False ) . head ( 30 ) plt . figure ( figsize = ( 15 , 6 )) sns . set_theme ( style = \"whitegrid\" ) ax = sns . barplot ( x = \"tag\" , y = \"count\" , data = plot_tags_top_30 ) _ = plt . xticks ( rotation = 60 ) In [78]: from nltk.corpus import wordnet from nltk.stem import WordNetLemmatizer lemmatizer = WordNetLemmatizer () def get_wordnet_pos ( treebank_tag ): if treebank_tag . startswith ( \"J\" ): return wordnet . ADJ elif treebank_tag . startswith ( \"V\" ): return wordnet . VERB elif treebank_tag . startswith ( \"N\" ): return wordnet . NOUN elif treebank_tag . startswith ( \"R\" ): return wordnet . ADV else : return None tweets_df [ \"lemma_sentence\" ] = tweets_df [ \"t_postag\" ] . transform ( lambda value : \" \" . join ( [ lemmatizer . lemmatize ( a [ 0 ], pos = get_wordnet_pos ( a [ 1 ])) if get_wordnet_pos ( a [ 1 ]) else a [ 0 ] for a in value ] ) ) tweets_df Out[78]: tweets sentiment tweets_token t_stem t_lemma t_postag lemma_sentence 0 followfriday inte paris top engaged members co... 1 [followfriday, inte, paris, top, engaged, memb... [followfriday, int, pari, top, engag, member, ... [followfriday, inte, paris, top, engaged, memb... [(followfriday, JJ), (inte, JJ), (paris, NN), ... followfriday inte paris top engage member comm... 1 hey james odd skeptical annoyed undecided unea... 1 [hey, james, odd, skeptical, annoyed, undecide... [hey, jame, odd, skeptic, annoy, undecid, unea... [hey, james, odd, skeptical, annoyed, undecide... [(hey, NN), (james, NNS), (odd, JJ), (skeptica... hey james odd skeptical annoyed undecided unea... 2 listen last night happy face smiley bleed amaz... 1 [listen, last, night, happy, face, smiley, ble... [listen, last, night, happi, face, smiley, ble... [listen, last, night, happy, face, smiley, ble... [(listen, VBN), (last, JJ), (night, NN), (happ... listen last night happy face smiley bleed amaz... 3 congrats happy face smiley 1 [congrats, happy, face, smiley] [congrat, happi, face, smiley] [congrats, happy, face, smiley] [(congrats, NNS), (happy, JJ), (face, NN), (sm... congrats happy face smiley 4 yeaaaah yippppy accnt verified rqst succeed go... 1 [yeaaaah, yippppy, accnt, verified, rqst, succ... [yeaaaah, yippppi, accnt, verifi, rqst, succee... [yeaaaah, yippppy, accnt, verified, rqst, succ... [(yeaaaah, NN), (yippppy, JJ), (accnt, NN), (v... yeaaaah yippppy accnt verify rqst succeed get ... ... ... ... ... ... ... ... ... 9995 wan na change avi usanele frown sad andry pouting 0 [wan, na, change, avi, usanele, frown, sad, an... [wan, na, chang, avi, usanel, frown, sad, andr... [wan, na, change, avi, usanele, frown, sad, an... [(wan, NN), (na, TO), (change, VB), (avi, RB),... wan na change avi usanele frown sad andry pouting 9996 puppy broke foot frown sad andry pouting 0 [puppy, broke, foot, frown, sad, andry, pouting] [puppi, broke, foot, frown, sad, andri, pout] [puppy, broke, foot, frown, sad, andry, pouting] [(puppy, JJ), (broke, VBD), (foot, JJ), (frown... puppy break foot frown sad andry pouting 9997 wheres jaebum baby pictures frown sad andry po... 0 [wheres, jaebum, baby, pictures, frown, sad, a... [where, jaebum, babi, pictur, frown, sad, andr... [wheres, jaebum, baby, picture, frown, sad, an... [(wheres, NNS), (jaebum, VBP), (baby, NN), (pi... wheres jaebum baby picture frown sad andry pou... 9998 mr ahmad maslan cooks frown sad andry pouting 0 [mr, ahmad, maslan, cooks, frown, sad, andry, ... [mr, ahmad, maslan, cook, frown, sad, andri, p... [mr, ahmad, maslan, cook, frown, sad, andry, p... [(mr, NN), (ahmad, NN), (maslan, NN), (cooks, ... mr ahmad maslan cook frown sad andry pouting 9999 hull supporter expecting misserable weeks frow... 0 [hull, supporter, expecting, misserable, weeks... [hull, support, expect, misser, week, frown, s... [hull, supporter, expecting, misserable, week,... [(hull, NN), (supporter, NN), (expecting, VBG)... hull supporter expect misserable week frown sa... 10000 rows Ã— 7 columns Double checking the differences between Stemming and Lemmatization In [79]: Sentence_from_Turing = \"The idea behind digital computers may be explained by saying that these machines are intended to carry out any operations which could be done by a human computer.\" tokens = nltk . word_tokenize ( Sentence_from_Turing ) stemmer = PorterStemmer () lemmatizer = WordNetLemmatizer () # Stem and lemmatize print ( \" {0:30}{1:30}{2:30} \" . format ( \"Word\" , \"Stemmer\" , \"Lemma \\n \" )) for word in tokens : print ( \" {0:30}{1:30}{2:30} \" . format ( word , stemmer . stem ( word ), lemmatizer . lemmatize ( word ) ) ) Word Stemmer Lemma The the The idea idea idea behind behind behind digital digit digital computers comput computer may may may be be be explained explain explained by by by saying say saying that that that these these these machines machin machine are are are intended intend intended to to to carry carri carry out out out any ani any operations oper operation which which which could could could be be be done done done by by by a a a human human human computer comput computer . . . Bag of words model This model allows us to extract features from the text by converting the text into a matrix of occurrence of words. We will take our tweets that have been already processed, and the sentiment (1: Positive, 0: Negative). Then, we will proceed to create a list with the tweets and finally we will be able to use Countvectorizer . https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html Countvectorizer is a method to convert text to numerical data: It converts a collection of text documents to a matrix of token counts. In [80]: df = tweets_df [[ \"tweets\" , \"sentiment\" ]] df Out[80]: tweets sentiment 0 followfriday inte paris top engaged members co... 1 1 hey james odd skeptical annoyed undecided unea... 1 2 listen last night happy face smiley bleed amaz... 1 3 congrats happy face smiley 1 4 yeaaaah yippppy accnt verified rqst succeed go... 1 ... ... ... 9995 wan na change avi usanele frown sad andry pouting 0 9996 puppy broke foot frown sad andry pouting 0 9997 wheres jaebum baby pictures frown sad andry po... 0 9998 mr ahmad maslan cooks frown sad andry pouting 0 9999 hull supporter expecting misserable weeks frow... 0 10000 rows Ã— 2 columns In [81]: tweets_list = list ( tweets_df [ \"tweets\" ] . values ) sentiment = tweets_df [ \"sentiment\" ] . values In [82]: tweets_list [: 10 ] Out[82]: ['followfriday inte paris top engaged members community week happy face smiley', 'hey james odd skeptical annoyed undecided uneasy hesitant please call contact centre able assist happy face smiley many thanks', 'listen last night happy face smiley bleed amazing track scotland', 'congrats happy face smiley', 'yeaaaah yippppy accnt verified rqst succeed got blue tick mark fb profile happy face smiley days', 'one irresistible happy face smileyflipkartfashionfriday', 'dont like keep lovely customers waiting long hope enjoy happy friday lwwf happy face smiley', 'second thought theres not enough time dd happy face smiley new shorts entering system sheep must buying', 'jgh go bayan bye', 'act mischievousness calling etl layer inhouse warehousing app katamariwell name implies p'] In [83]: # define max features max_features = 100 # each instance will have 10 features cv = CountVectorizer ( max_features = max_features , ngram_range = ( 1 , 2 )) In [84]: matrix_tweets_cv = cv . fit_transform ( tweets_list ) # Getting the words all_words = cv . get_feature_names () # visualizing the first 50 words print ( \"First 100 words: \\n \" , all_words [ 0 : 100 ]) First 100 words: ['always', 'amp', 'andry', 'andry pouting', 'back', 'birthday', 'cant', 'come', 'could', 'day', 'didnt', 'dont', 'even', 'face', 'face smiley', 'face smileyconfusion', 'feel', 'follow', 'follow back', 'followed', 'friday', 'frown', 'frown sad', 'fun', 'get', 'go', 'going', 'good', 'got', 'great', 'guys', 'happy', 'happy face', 'hey', 'hi', 'home', 'hope', 'ill', 'im', 'ive', 'kik', 'know', 'last', 'let', 'like', 'lol', 'looking', 'lot', 'love', 'lt', 'make', 'miss', 'morning', 'much', 'na', 'need', 'never', 'new', 'next', 'nice', 'night', 'no', 'not', 'oh', 'one', 'people', 'please', 'pouting', 'really', 'right', 'sad', 'sad andry', 'see', 'sleep', 'smiley', 'smileyconfusion', 'snapchat', 'someone', 'soon', 'sorry', 'still', 'thank', 'thanks', 'thats', 'think', 'time', 'today', 'us', 'via', 'wan', 'wan na', 'want', 'week', 'weekend', 'well', 'wish', 'work', 'would', 'yes', 'youre'] In [85]: wordfreq = cv . vocabulary_ wordfreq Out[85]: {'week': 92, 'happy': 31, 'face': 13, 'smiley': 74, 'happy face': 32, 'face smiley': 14, 'hey': 33, 'please': 66, 'thanks': 82, 'last': 42, 'night': 60, 'got': 28, 'one': 64, 'dont': 11, 'like': 44, 'hope': 36, 'friday': 20, 'not': 62, 'time': 85, 'new': 57, 'go': 25, 'love': 48, 'follow': 17, 'amp': 1, 'back': 4, 'follow back': 18, 'know': 41, 'great': 29, 'get': 24, 'today': 86, 'hi': 34, 'smileyconfusion': 75, 'face smileyconfusion': 15, 'need': 55, 'ive': 39, 'fun': 23, 'lol': 45, 'thank': 81, 'us': 87, 'well': 94, 'kik': 40, 'come': 7, 'lot': 47, 'see': 72, 'lt': 49, 'oh': 63, 'looking': 46, 'next': 58, 'feel': 16, 'never': 56, 'good': 27, 'im': 38, 'going': 26, 'would': 97, 'no': 61, 'someone': 77, 'yes': 98, 'cant': 6, 'still': 80, 'day': 9, 'via': 88, 'let': 43, 'youre': 99, 'weekend': 93, 'make': 50, 'really': 68, 'sleep': 73, 'birthday': 5, 'want': 91, 'miss': 51, 'morning': 52, 'thats': 83, 'ill': 37, 'right': 69, 'even': 12, 'home': 35, 'soon': 78, 'wan': 89, 'could': 8, 'always': 0, 'na': 54, 'wan na': 90, 'nice': 59, 'guys': 30, 'much': 53, 'work': 96, 'didnt': 10, 'people': 65, 'snapchat': 76, 'think': 84, 'wish': 95, 'sorry': 79, 'sad': 70, 'followed': 19, 'frown': 21, 'andry': 2, 'pouting': 67, 'frown sad': 22, 'sad andry': 71, 'andry pouting': 3} In [86]: matrix_tweets_cv . toarray ()[ 1 ] Out[86]: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) In [87]: print ( cv . get_feature_names ()) ['always', 'amp', 'andry', 'andry pouting', 'back', 'birthday', 'cant', 'come', 'could', 'day', 'didnt', 'dont', 'even', 'face', 'face smiley', 'face smileyconfusion', 'feel', 'follow', 'follow back', 'followed', 'friday', 'frown', 'frown sad', 'fun', 'get', 'go', 'going', 'good', 'got', 'great', 'guys', 'happy', 'happy face', 'hey', 'hi', 'home', 'hope', 'ill', 'im', 'ive', 'kik', 'know', 'last', 'let', 'like', 'lol', 'looking', 'lot', 'love', 'lt', 'make', 'miss', 'morning', 'much', 'na', 'need', 'never', 'new', 'next', 'nice', 'night', 'no', 'not', 'oh', 'one', 'people', 'please', 'pouting', 'really', 'right', 'sad', 'sad andry', 'see', 'sleep', 'smiley', 'smileyconfusion', 'snapchat', 'someone', 'soon', 'sorry', 'still', 'thank', 'thanks', 'thats', 'think', 'time', 'today', 'us', 'via', 'wan', 'wan na', 'want', 'week', 'weekend', 'well', 'wish', 'work', 'would', 'yes', 'youre'] In [88]: len ( cv . get_feature_names ()) Out[88]: 100 Preparing the data for Modeling In [89]: X = matrix_tweets_cv . toarray () y = sentiment X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.3 , random_state = 1 ) In [90]: print ( X_train . shape ) print ( X_test . shape ) (7000, 100) (3000, 100) In [91]: # Model Generation Using Multinomial Naive Bayes clf = MultinomialNB () . fit ( X_train , y_train ) predicted = clf . predict ( X_test ) print ( \"MultinomialNB Accuracy:\" , metrics . accuracy_score ( y_test , predicted )) MultinomialNB Accuracy: 0.9643333333333334 Can we improve the bag of words model? The main issue is that different sentences can yield same/similar vectors. TF-IDF allows for a simple mathematical way of defining word \"importance\". This allows for a smarter document vector. TF-IDF (term frequencyâ€“inverse document frequency) Term frequencyâ€“inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. Term frequency: This summarizes how often a word appears within a documents. Inverse document frequency: This downscales words that appear a lot across documents in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus. It is calculated as: $$ TFIDF_{i,j} = TF_{i,j} \\times \\log \\frac{N}{DF_i} $$ where $TF_{i,j}$ is the total number of occurences of $i$ in $j$, $DF_i$ is the total number of documents containing $i$ and $N$ is the total number of documents. In [92]: tf = TfidfVectorizer () text_tf = tf . fit_transform ( df [ \"tweets\" ]) In [93]: X_train , X_test , y_train , y_test = train_test_split ( text_tf , df [ \"sentiment\" ], test_size = 0.3 , random_state = 123 ) In [94]: # getting the words all_words = tf . get_feature_names () # visualize first 50 words print ( \"First 50 words: \" , all_words [ 0 : 50 ]) First 50 words: ['aa', 'aaaaaaaaaaa', 'aaaaaaaaaaaaa', 'aaaaaaaaaaaah', 'aaaaaand', 'aaaaages', 'aaaaahhhhhhhh', 'aaaahh', 'aaaahhh', 'aaahhh', 'aah', 'aahhhh', 'aaj', 'aameenall', 'aameenhappy', 'aamir', 'aapke', 'aaron', 'aarwwi', 'aasramany', 'aayegi', 'ab', 'abandoned', 'abandoning', 'abbeytaylor', 'abby', 'abbycan', 'abbymill', 'abes', 'abhi', 'abi', 'abit', 'abla', 'able', 'abligaverins', 'aboard', 'abouty', 'abp', 'abroad', 'abrupt', 'abs', 'abscess', 'absent', 'absolute', 'absolutely', 'abstinence', 'abt', 'abu', 'abudhabi', 'abusive'] In [96]: print ( tf . vocabulary_ ) {'followfriday': 4009, 'inte': 5513, 'paris': 8309, 'top': 11824, 'engaged': 3417, 'members': 7087, 'community': 2211, 'week': 12617, 'happy': 4792, 'face': 3658, 'smiley': 10448, 'hey': 4971, 'james': 5682, 'odd': 7957, 'skeptical': 10364, 'annoyed': 454, 'undecided': 12140, 'uneasy': 12149, 'hesitant': 4967, 'please': 8675, 'call': 1676, 'contact': 2298, 'centre': 1818, 'able': 33, 'assist': 652, 'many': 6891, 'thanks': 11525, 'listen': 6494, 'last': 6281, 'night': 7750, 'bleed': 1264, 'amazing': 361, 'track': 11872, 'scotland': 9909, 'congrats': 2268, 'yeaaaah': 13014, 'yippppy': 13071, 'accnt': 63, 'verified': 12347, 'rqst': 9690, 'succeed': 11097, 'got': 4502, 'blue': 1300, 'tick': 11681, 'mark': 6915, 'fb': 3791, 'profile': 9010, 'days': 2677, 'one': 8056, 'irresistible': 5577, 'smileyflipkartfashionfriday': 10497, 'dont': 3070, 'like': 6440, 'keep': 5991, 'lovely': 6657, 'customers': 2560, 'waiting': 12498, 'long': 6583, 'hope': 5099, 'enjoy': 3427, 'friday': 4112, 'lwwf': 6739, 'second': 9950, 'thought': 11632, 'theres': 11576, 'not': 7852, 'enough': 3432, 'time': 11700, 'dd': 2690, 'new': 7684, 'shorts': 10223, 'entering': 3439, 'system': 11289, 'sheep': 10150, 'must': 7495, 'buying': 1636, 'jgh': 5762, 'go': 4432, 'bayan': 960, 'bye': 1643, 'act': 89, 'mischievousness': 7240, 'calling': 1679, 'etl': 3517, 'layer': 6315, 'inhouse': 5456, 'warehousing': 12537, 'app': 525, 'katamariwell': 5959, 'name': 7563, 'implies': 5364, 'gouv': 4509, 'influencers': 5440, 'wouldnt': 12893, 'love': 6646, 'bigjuicyselfies': 1172, 'follow': 4004, 'amp': 390, 'back': 800, 'perfect': 8457, 'already': 335, 'know': 6142, 'whats': 12685, 'great': 4551, 'opportunity': 8105, 'junior': 5892, 'triathletes': 11933, 'aged': 190, 'gatorade': 4274, 'series': 10033, 'get': 4331, 'entries': 3453, 'laying': 6316, 'greetings': 4565, 'card': 1733, 'range': 9227, 'print': 8969, 'today': 11754, 'job': 5791, 'friends': 4131, 'lunch': 6720, 'yummmm': 13145, 'smileynostalgia': 10531, 'tbs': 11402, 'ku': 6180, 'id': 5283, 'conflict': 2255, 'help': 4941, 'heres': 4960, 'screenshot': 9921, 'working': 12861, 'hi': 4976, 'liv': 6509, 'smileyconfusion': 10465, 'hello': 4937, 'need': 7640, 'something': 10689, 'fm': 3985, 'twitter': 12050, 'sure': 11195, 'thing': 11593, 'dm': 3013, 'fle': 3949, 'followers': 4008, 'ive': 5640, 'heard': 4888, 'four': 4072, 'seasons': 9944, 'pretty': 8945, 'dope': 3081, 'penthouse': 8444, 'obvs': 7946, 'gobigorgohomehave': 4439, 'fun': 4192, 'yall': 12991, 'yeah': 13015, 'suppose': 11187, 'lol': 6561, 'chat': 1901, 'bit': 1221, 'youth': 13117, 'opportunities': 8104, 'gtgt': 4622, 'nailpolishmediumskintonekissmark': 7550, 'smileyconfusionconfusionconfusion': 10468, 'havent': 4847, 'seen': 9972, 'years': 13020, 'arrow': 602, 'thank': 11519, 'rest': 9493, 'goes': 4446, 'quickly': 9167, 'bed': 1032, 'music': 7485, 'fix': 3930, 'dream': 3128, 'spiritual': 10826, 'ritual': 9588, 'festival': 3841, 'npalconfusionbeginning': 7890, 'lineup': 6474, 'smileyit': 10510, 'left': 6354, 'yconfusionsee': 13010, 'sarah': 9825, 'send': 10007, 'us': 12257, 'email': 3354, 'bitsycom': 1233, 'well': 12643, 'asap': 616, 'lols': 6570, 'kik': 6051, 'hatessuce': 4838, 'kikme': 6056, 'lgbt': 6405, 'tinder': 11711, 'nsfw': 7894, 'akua': 271, 'cumshot': 2536, 'come': 2185, 'house': 5154, 'smileyconfusionconfusionconfusionconfusion': 10469, 'nsnsupplements': 7895, 'effective': 3285, 'press': 8936, 'release': 9402, 'distribution': 2991, 'results': 9501, 'link': 6477, 'removed': 9428, 'pressrelease': 8937, 'newsdistribution': 7695, 'bam': 871, 'bestfriend': 1120, 'loves': 6665, 'lot': 6621, 'see': 9961, 'warsaw': 12547, 'lt': 6689, 'everyone': 3546, 'watch': 12557, 'documentary': 3027, 'earthlings': 3244, 'youtube': 13122, 'supports': 11186, 'buuuuuuuut': 1633, 'oh': 7983, 'looking': 6594, 'forward': 4062, 'visiting': 12427, 'next': 7702, 'letsgetmessy': 6394, 'jo': 5790, 'makes': 6830, 'feel': 3810, 'better': 1133, 'never': 7682, 'nor': 7841, 'anyone': 491, 'kpop': 6168, 'flesh': 3951, 'good': 4465, 'girl': 4382, 'best': 1117, 'wishes': 12788, 'kimimi': 6075, 'reason': 9303, 'epic': 3466, 'soundtrack': 10759, 'shout': 10235, 'added': 110, 'video': 12380, 'playlist': 8668, 'im': 5339, 'twitch': 12047, 'going': 4451, 'league': 6332, 'would': 12892, 'dear': 2703, 'jordan': 5826, 'ana': 400, 'okay': 8000, 'fake': 3709, 'gameplays': 4239, 'wink': 12773, 'smirk': 10585, 'haha': 4696, 'kidding': 6044, 'stuff': 11062, 'aygur': 772, 'exactly': 3564, 'product': 9005, 'line': 6471, 'etsy': 3519, 'shop': 10212, 'check': 1913, 'boxroomcrafts': 1410, 'vacation': 12294, 'rechargeable': 9320, 'normally': 7845, 'comes': 2188, 'charger': 1886, 'buy': 1634, 'espana': 3498, 'menthe': 7106, 'inter': 5521, 'shes': 10165, 'asleep': 637, 'no': 7793, 'talk': 11332, 'sooo': 10716, 'someone': 10686, 'text': 11496, 'yes': 13046, 'bet': 1127, 'hell': 4934, 'fit': 3920, 'hearing': 4890, 'speech': 10805, 'pity': 8623, 'green': 4562, 'gardens': 4264, 'midnight': 7175, 'sun': 11143, 'beautiful': 1014, 'canals': 1705, 'dasvidaniya': 2655, 'till': 11698, 'visithappy': 12426, 'scouting': 9915, 'sg': 10091, 'future': 4209, 'wlan': 12804, 'pros': 9046, 'conference': 2247, 'asia': 625, 'change': 1862, 'lollipop': 6566, 'nez': 7705, 'agnezmo': 199, 'big': 1168, 'oley': 8025, 'mama': 6847, 'stand': 10892, 'stronger': 11039, 'kjkkndr': 6121, 'god': 4440, 'misty': 7261, 'baby': 789, 'cute': 2562, 'woohoo': 12842, 'cant': 1721, 'wait': 12494, 'signed': 10290, 'yet': 13059, 'still': 10975, 'thinking': 11599, 'mka': 7276, 'liam': 6407, 'access': 59, 'welcome': 12637, 'stats': 10932, 'day': 2671, 'arrived': 601, 'follower': 4007, 'unfollowers': 12160, 'via': 12360, 'shouldnt': 10232, 'surprisedyou': 11202, 'figure': 3865, 'fr': 4077, 'happybirthdayemilybett': 4793, 'wishing': 12789, 'beautifulsweettalentedamazing': 1018, 'plans': 8650, 'drain': 3118, 'smileyembarrassed': 10492, 'blushing': 1307, 'fahupdates': 3699, 'ta': 11290, 'timezones': 11705, 'parents': 8307, 'proud': 9051, 'least': 6340, 'maybe': 6990, 'sometimes': 10692, 'grades': 4521, 'al': 272, 'grande': 4531, 'manilabro': 6877, 'chosen': 1998, 'let': 6390, 'youre': 13110, 'aroundat': 595, 'side': 10275, 'world': 12873, 'eh': 3295, 'take': 11315, 'care': 1739, 'laney': 6255, 'finally': 3884, 'fucking': 4176, 'weekend': 12618, 'real': 9286, 'joined': 5806, 'hushedcallwithfraydoe': 5243, 'gift': 4360, 'fraydoe': 4087, 'yeahhh': 13017, 'make': 6827, 'smileyconfusionconfusion': 10467, 'hushedpinwithsammy': 5244, 'event': 3534, 'might': 7177, 'luv': 6731, 'really': 9298, 'appreciate': 544, 'share': 10123, 'around': 594, 'wow': 12897, 'tom': 11780, 'gym': 4677, 'monday': 7329, 'likes': 6451, 'invite': 5553, 'join': 5805, 'scope': 9902, 'influencer': 5439, 'nudes': 7900, 'sleep': 10395, 'smileyconfusionconfusionconfusionconfusionconfusionconfusionconfusionconfusion': 10473, 'birthday': 1208, 'jp': 5847, 'hideo': 4982, 'want': 12529, 'tshirts': 11978, 'cool': 2346, 'haw': 4850, 'phela': 8514, 'mom': 7315, 'obviously': 7945, 'prince': 8965, 'charming': 1897, 'stage': 10883, 'luck': 6701, 'das': 2651, 'tylers': 12067, 'hipster': 5022, 'glasses': 4411, 'marty': 6933, 'glad': 4406, 'joining': 5807, 'done': 3057, 'afternoon': 174, 'lets': 6392, 'read': 9279, 'kahfi': 5914, 'finish': 3903, 'ohmyg': 7992, 'yaya': 13004, 'dub': 3174, 'stalk': 10886, 'ig': 5306, 'gondooo': 4462, 'moo': 7348, 'tologooo': 11779, 'become': 1030, 'details': 2844, 'robertkelly': 9616, 'spaniel': 10777, 'nelson': 7660, 'zzz': 13207, 'xx': 12966, 'physiotherapy': 8552, 'hashtag': 4826, 'custom': 2558, 'flexedbiceps': 3954, 'monica': 7332, 'miss': 7244, 'sounds': 10758, 'morning': 7365, 'thats': 11542, 'takes': 11320, 'definitely': 2751, 'try': 11972, 'tonight': 11804, 'took': 11815, 'advice': 149, 'treviso': 11929, 'concert': 2240, 'city': 2044, 'country': 2402, 'ill': 5328, 'start': 10911, 'fine': 3894, 'gorgeous': 4498, 'friend': 4127, 'xohappy': 12955, 'oven': 8198, 'roasted': 9610, 'garlic': 4265, 'olive': 8026, 'oil': 7995, 'dried': 3143, 'tomatoes': 11783, 'basil': 935, 'century': 1822, 'tuna': 12003, 'business': 1618, 'thebunkerjl': 11548, 'right': 9560, 'atchya': 660, 'doesnt': 3031, 'even': 3532, 'almost': 328, 'chance': 1857, 'arvsgt': 615, 'president': 8935, 'cheers': 1923, 'po': 8712, 'ice': 5273, 'cream': 2463, 'agree': 204, 'hehehehe': 4927, 'point': 8722, 'stay': 10935, 'home': 5068, 'lafontpresse': 6227, 'soon': 10712, 'promise': 9022, 'web': 12601, 'whatsapp': 12686, 'volta': 12447, 'funcionar': 4193, 'com': 2180, 'iphone': 5562, 'jailbroken': 5675, 'plan': 8640, 'watching': 12561, 'laterconfusion': 6290, 'mins': 7228, 'leia': 6369, 'appears': 533, 'hologram': 5065, 'rd': 9266, 'message': 7129, 'obi': 7938, 'wan': 12526, 'sits': 10343, 'luke': 6714, 'ucl': 12086, 'arsenal': 604, 'small': 10433, 'team': 11417, 'passing': 8339, 'locomotive': 6552, 'dewsbury': 2862, 'railway': 9195, 'station': 10930, 'dewconfusion': 2861, 'west': 12659, 'yorkshireconfusion': 13094, 'smh': 10441, 'live': 6510, 'strange': 11010, 'imagine': 5344, 'megan': 7059, 'masaantodaymasaantodaya': 6941, 'shweta': 10260, 'tripathi': 11944, 'masaantodaymasaantodayhappy': 6942, 'kurtas': 6191, 'half': 4742, 'number': 7910, 'wsalelove': 12920, 'ah': 207, 'larry': 6275, 'anyway': 498, 'kinda': 6081, 'gooood': 4492, 'hunyo': 5234, 'life': 6425, 'enn': 3430, 'surely': 11196, 'could': 2391, 'warmup': 12542, 'coming': 2196, 'th': 11510, 'bath': 947, 'dum': 3193, 'andar': 410, 'ram': 9212, 'sampath': 9800, 'sona': 10697, 'mohapatra': 7309, 'samantha': 9791, 'edwards': 3279, 'mein': 7069, 'tulane': 11996, 'razi': 9262, 'wah': 12486, 'josh': 5832, 'always': 350, 'smile': 10444, 'picture': 8566, 'timing': 11706, 'giveitup': 4398, 'given': 4399, 'gas': 4269, 'subsidy': 11092, 'initiative': 5462, 'proposed': 9044, 'feeling': 3814, 'delighted': 2769, 'missed': 7246, 'yesterday': 13055, 'lmaoo': 6532, 'songs': 10703, 'ever': 3538, 'shall': 10109, 'little': 6507, 'throwback': 11652, 'yconfusion': 13009, 'outlying': 8187, 'islands': 5596, 'cheung': 1944, 'chau': 1905, 'mui': 7460, 'wototally': 12890, 'different': 2904, 'kfckitchentours': 6019, 'kitchen': 6110, 'clean': 2070, 'totally': 11846, 'amazed': 359, 'india': 5410, 'cusp': 2555, 'testing': 11492, 'waters': 12564, 'rewarding': 9529, 'arummzz': 614, 'drive': 3150, 'ptraveling': 9072, 'traveler': 11913, 'yogyakarta': 13084, 'jeep': 5733, 'indonesia': 5426, 'instamood': 5502, 'na': 7529, 'skype': 10382, 'may': 6989, 'look': 6591, 'nice': 7730, 'friendly': 4130, 'pretend': 8940, 'film': 3877, 'congratulations': 2271, 'winner': 12775, 'cheesydelights': 1925, 'contest': 2310, 'smileyplease': 10537, 'address': 121, 'guys': 4665, 'pc': 8392, 'seeing': 9965, 'marketing': 6917, 'wont': 12837, 'regret': 9374, 'hours': 5152, 'leave': 6341, 'without': 12793, 'delays': 2761, 'actually': 100, 'easy': 3249, 'guess': 4636, 'train': 11888, 'wd': 12583, 'hour': 5150, 'shifting': 10170, 'engine': 3419, 'etc': 3511, 'sunburn': 11144, 'peeling': 8423, 'gratis': 4546, 'visit': 12424, 'blog': 1284, 'razorstan': 9263, 'huge': 5193, 'warm': 12540, 'complete': 2226, 'triangle': 11932, 'northern': 7847, 'ireland': 5570, 'sights': 10286, 'intlagency': 5538, 'smthng': 10600, 'ulolwanna': 12108, 'hug': 5192, 'uxoxolove': 12290, 'uu': 12284, 'jaann': 5654, 'kiss': 6102, 'confusion': 2259, 'topnewfollowers': 11828, 'connect': 2274, 'wonderful': 12830, 'made': 6773, 'fluffy': 3980, 'inside': 5485, 'pirouette': 8610, 'moose': 7358, 'trip': 11943, 'philly': 8521, 'december': 2720, 'dude': 3182, 'question': 9159, 'flawed': 3947, 'pain': 8245, 'negate': 7648, 'strength': 11022, 'went': 12653, 'solo': 10673, 'moves': 7400, 'werent': 12655, 'fav': 3777, 'nirvana': 7774, 'song': 10701, 'smells': 10438, 'teen': 11448, 'spirit': 10825, 'rip': 9571, 'amy': 398, 'winehouse': 12769, 'esigroup': 3495, 'couple': 2408, 'tomhiddleston': 11788, 'elizabetholsen': 3337, 'yaytheylookgreat': 13006, 'goodnight': 4476, 'vid': 12378, 'wake': 12500, 'gon': 4460, 'shoot': 10209, 'itty': 5635, 'bitty': 1237, 'teenie': 11451, 'bikini': 1177, 'much': 7443, 'gets': 4333, 'together': 11770, 'ending': 3407, 'xfiles': 12947, 'content': 2308, 'surprised': 11201, 'rain': 9196, 'fabulous': 3656, 'fantastic': 3751, 'work': 12853, 'jb': 5721, 'forever': 4041, 'belieber': 1069, 'entreprises': 3452, 'hear': 4887, 'nighty': 7757, 'bugs': 1568, 'bite': 1228, 'bracelet': 1420, 'idea': 5286, 'foundry': 4071, 'game': 4234, 'sense': 10015, 'didnt': 2889, 'pic': 8556, 'effing': 3288, 'phone': 8528, 'woot': 12847, 'gta': 4615, 'derek': 2816, 'using': 12272, 'parkshare': 8314, 'gloucestershire': 4422, 'aaaahhh': 8, 'man': 6857, 'traffic': 11880, 'stress': 11023, 'reliever': 9409, 'howre': 5163, 'arbeloa': 563, 'turning': 12014, 'daysomg': 2679, 'difference': 2902, 'say': 9859, 'europe': 3525, 'rise': 9580, 'find': 3889, 'hard': 4802, 'believe': 1071, 'uncountable': 12136, 'coz': 2432, 'unlimited': 12185, 'course': 2414, 'teampositive': 11426, 'aldub': 286, 'hotbeveragehotbeveragehotbeverage': 5135, 'rita': 9586, 'info': 5443, 'wed': 12609, 'way': 12573, 'boy': 1411, 'gifts': 4362, 'secem': 9949, 'nyc': 7928, 'true': 11964, 'samar': 9792, 'sethi': 10053, 'high': 4985, 'lennox': 6377, 'exe': 3580, 'skeem': 10361, 'saam': 9739, 'people': 8446, 'pumpchkin': 9093, 'pump': 9092, 'lishielou': 6491, 'polite': 8734, 'izzat': 5650, 'wese': 12657, 'trust': 11969, 'khawateen': 6031, 'sath': 9836, 'selfies': 9992, 'mana': 6858, 'kar': 5937, 'deya': 2864, 'evening': 3533, 'sorted': 10741, 'caneeee': 1717, 'smart': 10434, 'hair': 4726, 'tbh': 11398, 'jacob': 5664, 'mampg': 6855, 'upgradehappy': 12222, 'tee': 11447, 'family': 3726, 'reading': 9282, 'talking': 11336, 'person': 8472, 'two': 12051, 'conversations': 2332, 'smileyconfusionconfusionconfusionconfusionconfusionconfusionconfusion': 10472, 'online': 8063, 'mclaren': 7015, 'fridayfeeling': 4117, 'tgif': 11503, 'square': 10861, 'enix': 3425, 'bissmillah': 1220, 'ya': 12974, 'allah': 306, 'training': 11892, 'socent': 10643, 'startups': 10920, 'drop': 3156, 'arnd': 591, 'town': 11862, 'basically': 933, 'piss': 8612, 'cup': 2538, 'test': 11489, 'also': 339, 'terrible': 11486, 'complicated': 2230, 'discussions': 2965, 'snapchat': 10609, 'lynettelowe': 6749, 'kikmenow': 6059, 'snapme': 10613, 'hot': 5131, 'amazon': 365, 'kikmeguys': 6058, 'shift': 10169, 'smileyconfusionconfusionconfusionconfusionconfusion': 10470, 'ely': 3352, 'definately': 2746, 'growing': 4600, 'sport': 10841, 'rt': 9694, 'rakyat': 9210, 'writing': 12911, 'since': 10312, 'mentioned': 7108, 'fly': 3982, 'fishing': 3918, 'getting': 4336, 'follows': 4016, 'promoted': 9028, 'posts': 8796, 'cyber': 2575, 'stalked': 10887, 'ourdaughtersourpride': 8177, 'mypapamypride': 7522, 'papa': 8292, 'coach': 2123, 'positive': 8782, 'kha': 6023, 'mention': 7107, 'atleast': 671, 'pigglywiggly': 8579, 'mango': 6870, 'lassis': 6280, 'montys': 7347, 'marvellous': 6935, 'though': 11631, 'suspect': 11215, 'meant': 7030, 'hrs': 5174, 'touch': 11850, 'kepler': 6003, 'chalna': 1850, 'hai': 4718, 'mohamad': 7308, 'thankyou': 11530, 'hazel': 4862, 'food': 4020, 'market': 6916, 'brooklyn': 1508, 'pta': 9069, 'awake': 732, 'okayy': 8003, 'awww': 760, 'ha': 4682, 'surprise': 11200, 'doc': 3022, 'yorkshire': 13093, 'splendid': 10827, 'spam': 10774, 'folder': 3998, 'won': 12827, 'amount': 389, 'travel': 11912, 'nigeria': 7746, 'claim': 2050, 'rted': 9696, 'legs': 6366, 'hurt': 5237, 'bad': 820, 'smileyconfusionconfusionconfusionconfusionconfusionconfusionconfusionconfusionconfusionconfusionconfusionconfusionconfusionconfusionconfusionconfusionconfusion': 10477, 'lopez': 6604, 'mine': 7211, 'saturday': 9840, 'thaaanks': 11513, 'puhon': 9086, 'happinessss': 4790, 'tnc': 11748, 'prior': 8974, 'notification': 7866, 'used': 12264, 'probably': 8988, 'funny': 4200, 'fat': 3771, 'cos': 2376, 'ate': 661, 'yuna': 13152, 'tameside': 11346, 'google': 4486, 'account': 67, 'scousers': 9914, 'everything': 3551, 'zoe': 13194, 'mate': 6967, 'literally': 6502, 'theyre': 11588, 'sameee': 9794, 'edgar': 3262, 'updating': 12218, 'log': 6554, 'bring': 1485, 'abes': 28, 'meet': 7050, 'sighs': 10283, 'dreamily': 3131, 'pout': 8810, 'eyes': 3644, 'quacketyquack': 9142, 'sunshinehoran': 11160, 'tweet': 12028, 'happened': 4782, 'phil': 8517, 'em': 3353, 'del': 2758, 'rodders': 9632, 'else': 3346, 'play': 8659, 'newest': 7691, 'gamejam': 4238, 'irish': 5572, 'literature': 6503, 'inaccessible': 5380, 'flipkartfashionfriday': 3962, 'xriya': 12961, 'kareenas': 5945, 'fans': 3746, 'knowing': 6145, 'playing': 8667, 'brain': 1427, 'dots': 3095, 'braindots': 1429, 'fair': 3706, 'rush': 9724, 'either': 3309, 'brandi': 1438, 'selfie': 9990, 'carnival': 1755, 'men': 7095, 'put': 9124, 'masks': 6948, 'xavier': 12938, 'forneret': 4055, 'jennifer': 5743, 'site': 10340, 'free': 4094, 'ball': 863, 'pool': 8744, 'coins': 2145, 'edit': 3264, 'trish': 11948, 'loved': 6650, 'heartsuit': 4902, 'gratefulness': 4545, 'happiness': 4789, 'post': 8788, 'three': 11644, 'things': 11594, 'grateful': 4544, 'comments': 2199, 'wakeup': 12502, 'beside': 1112, 'dirtydirty': 2940, 'sex': 10070, 'lmaooo': 6533, 'facewithsteamfromnosefacewithsteamfromnose': 3671, 'louis': 6637, 'hes': 4966, 'throw': 11651, 'cause': 1796, 'inspire': 5491, 'think': 11597, 'ff': 3848, 'twoofs': 12055, 'gr': 4514, 'wkend': 12800, 'smileyhounds': 10507, 'wish': 12786, 'year': 13019, 'kind': 6080, 'exhausted': 3585, 'liked': 6441, 'words': 12851, 'cheltenham': 1933, 'area': 572, 'lass': 6279, 'kale': 5921, 'crisps': 2491, 'ruin': 9706, 'open': 8087, 'worldwide': 12876, 'fuck': 4170, 'outta': 8193, 'sfvbeta': 10089, 'looks': 6595, 'vantastic': 12318, 'xcylin': 12943, 'bundle': 1589, 'shows': 10251, 'internet': 5532, 'price': 8956, 'realisticly': 9291, 'paying': 8379, 'nethappy': 7675, 'education': 3277, 'powerful': 8883, 'weapon': 12588, 'use': 12263, 'worldnelson': 12874, 'mandela': 6867, 'recent': 9316, 'jampk': 5686, 'dand': 2625, 'chenab': 1936, 'flows': 3976, 'pakistanincredibleindia': 8260, 'teenchoice': 11450, 'choiceinternationalartist': 1980, 'superjunior': 11174, 'caught': 1793, 'first': 3915, 'salmon': 9784, 'checking': 1915, 'enjoyed': 3428, 'superblend': 11167, 'following': 4010, 'youd': 13098, 'project': 9017, 'youthorguk': 13121, 'awesome': 741, 'stream': 11017, 'artist': 608, 'alma': 326, 'mater': 6969, 'highschooldays': 4990, 'clientvisit': 2090, 'faith': 3707, 'christian': 2003, 'school': 9892, 'lizaminnelli': 6524, 'upcoming': 12214, 'uk': 12103, 'appearances': 531, 'smilingfacewithopenmouthsmilingeyessmilingfacewithopenmouthsmilingeyesreally': 10580, 'single': 10317, 'listening': 6496, 'hills': 5006, 'every': 3543, 'beats': 1011, 'wrong': 12915, 'ready': 9284, 'natural': 7604, 'pefumery': 8426, 'workshop': 12870, 'neals': 7629, 'yard': 12997, 'covent': 2423, 'garden': 4262, 'tomorrow': 11794, 'fback': 3792, 'indo': 5425, 'harmos': 4813, 'americano': 376, 'bala': 855, 'remember': 9418, 'aww': 758, 'heads': 4879, 'saw': 9858, 'dark': 2643, 'plz': 8708, 'handshome': 4771, 'juga': 5865, 'hurray': 5235, 'meeting': 7051, 'hate': 4833, 'decide': 2722, 'save': 9851, 'list': 6492, 'show': 10241, 'hiya': 5035, 'exec': 3581, 'loryngoodchambercouk': 6611, 'photos': 8539, 'beforewhen': 1044, 'thx': 11677, 'support': 11181, 'chinas': 1967, 'girlthat': 4390, 'homosexualbut': 5081, 'himher': 5009, 'dhyungbot': 2875, 'give': 4393, 'fam': 3723, 'mind': 7208, 'timetunnel': 11704, 'quite': 9172, 'radio': 9189, 'set': 10052, 'heart': 4891, 'hiii': 4996, 'jack': 5658, 'ily': 5336, 'sparklessparklessparkles': 10783, 'played': 8661, 'dominoes': 3052, 'pub': 9074, 'heated': 4907, 'probhappy': 8989, 'sorry': 10736, 'hastily': 4830, 'typed': 12071, 'bank': 895, 'came': 1691, 'screenshotting': 9923, 'pakistanis': 8261, 'poot': 8755, 'points': 8724, 'dreamteam': 3133, 'gooo': 4489, 'bailey': 843, 'pbbgold': 8387, 'drank': 3123, 'old': 8017, 'gotten': 4507, 'smileyconfusionconfusionconfusionconfusionconfusionconfusionconfusionconfusionconfusionconfusionconfusionconfusionconfusionconfusion': 10476, 'welsh': 12648, 'wales': 12510, 'yippee': 13069, 'heartdecorationheartdecoration': 4898, 'bro': 1494, 'lord': 6606, 'sweet': 11245, 'michael': 7164, 'ure': 12245, 'smirkconfusion': 10586, 'bigot': 1173, 'usually': 12276, 'front': 4149, 'squats': 10863, 'dobar': 3021, 'dan': 2618, 'smileythe': 10556, 'brand': 1437, 'heavies': 4913, 'musicology': 7490, 'spend': 10810, 'marathon': 6900, 'iflix': 5303, 'head': 4869, 'officially': 7976, 'graduated': 4523, 'cry': 2512, 'yep': 13042, 'expert': 3609, 'bisexuality': 1216, 'minal': 7206, 'aidzin': 227, 'yo': 13079, 'pi': 8554, 'cook': 2340, 'book': 1354, 'dinner': 2925, 'tough': 11852, 'choice': 1979, 'others': 8162, 'chill': 1959, 'smu': 10601, 'oval': 8197, 'dcoz': 2689, 'basketball': 938, 'player': 8662, 'whahahaha': 12676, 'soamazing': 10635, 'moment': 7317, 'onto': 8069, 'girlmy': 4388, 'wardrobe': 12535, 'smileyconfusionwsalelove': 10489, 'user': 12267, 'farouk': 3763, 'teamred': 11427, 'apparently': 527, 'member': 7086, 'hopefully': 5102, 'depends': 2807, 'greatly': 4557, 'appreciated': 545, 'changes': 1864, 'design': 2826, 'ahhh': 215, 'cinepambata': 2035, 'mechanics': 7035, 'official': 7975, 'entry': 3455, 'form': 4052, 'downloaded': 3106, 'sali': 9782, 'ur': 12243, 'moms': 7326, 'swishers': 11264, 'copping': 2355, 'ducktails': 3180, 'surreal': 11204, 'exposure': 3624, 'sotw': 10749, 'jingly': 5777, 'jangly': 5689, 'loveliness': 6656, 'halesowen': 4741, 'blackcountryfair': 1244, 'street': 11021, 'assessments': 644, 'mental': 7103, 'body': 1326, 'ooze': 8085, 'appeal': 528, 'amassiveoverdoseofships': 356, 'latest': 6293, 'wayenjoy': 12575, 'sitehappy': 10341, 'isis': 5592, 'chan': 1856, 'tamp': 11349, 'noted': 7857, 'pkwalasawaal': 8634, 'gemma': 4300, 'orleans': 8146, 'fever': 3845, 'geskenya': 4329, 'obamainkenya': 7937, 'magicalkenya': 6797, 'smileygreatkenya': 10501, 'allgoodthingske': 309, 'anime': 442, 'umaru': 12117, 'singer': 10315, 'dayan': 2672, 'ships': 10183, 'order': 8126, 'room': 9655, 'car': 1728, 'gone': 4463, 'streaming': 11019, 'suan': 11074, 'hahaha': 4700, 'story': 11006, 'related': 9391, 'labeled': 6208, 'worst': 12884, 'batch': 946, 'principal': 8967, 'due': 3186, 'olover': 8033, 'march': 6902, 'starting': 10916, 'wooftastic': 12841, 'received': 9314, 'necessary': 7637, 'rn': 9603, 'whatever': 12681, 'hat': 4832, 'isonly': 5602, 'success': 11098, 'abstinence': 45, 'amberr': 367, 'wtf': 12923, 'moments': 7320, 'thrown': 11655, 'middle': 7171, 'finished': 3904, 'ended': 3406, 'needs': 7645, 'repeated': 9442, 'relentlessly': 9406, 'approximately': 552, 'oldschool': 8023, 'runescape': 9718, 'daaay': 2586, 'jummamubarik': 5881, 'frnds': 4145, 'stayblessed': 10936, 'blessed': 1270, 'pussycats': 9123, 'main': 6818, 'launched': 6303, 'pretoria': 8942, 'carei': 1746, 'carefahrinahmad': 1742, 'tengkuaaronshah': 11471, 'eksperimencinta': 3313, 'tykksin': 12064, 'videosta': 12386, 'sub': 11077, 'special': 10795, 'paysafecard': 8385, 'giveaway': 4394, 'lue': 6711, 'descconfusion': 2820, 'months': 7344, 'septiceye': 10028, 'hoodie': 5096, 'eeeep': 3282, 'yay': 13003, 'sohappyrightnow': 10665, 'mmmmmm': 7282, 'azzsets': 776, 'babe': 783, 'feedback': 3808, 'gained': 4228, 'value': 12308, 'peaceful': 8405, 'nights': 7756, 'refreshed': 9363, 'dreams': 3132, 'manthan': 6884, 'tuned': 12006, 'freshness': 4108, 'mother': 7381, 'determination': 2846, 'smileymaxfreshmove': 10524, 'girls': 4389, 'loneliest': 6579, 'tattoo': 11378, 'changed': 1863, 'smileywish': 10565, 'fridayand': 4113, 'magnificent': 6804, 'heartsuitheartsuit': 4903, 'achieved': 78, 'rashmi': 9242, 'dedication': 2733, 'inspiration': 5490, 'happyfriday': 4795, 'charlied': 1895, 'rayner': 9259, 'nearly': 7633, 'retweeted': 9513, 'updated': 12216, 'sign': 10287, 'alerts': 289, 'da': 2584, 'dang': 2627, 'rad': 9188, 'fanart': 3734, 'boots': 1371, 'tp': 11869, 'massive': 6956, 'niamh': 7725, 'fennell': 3833, 'journalism': 5836, 'graduate': 4522, 'lands': 6253, 'copying': 2358, 'pasting': 8347, 'tweets': 12035, 'crystal': 2522, 'yesss': 13051, 'ariana': 580, 'selena': 9985, 'gomez': 4459, 'tomlinson': 11789, 'payne': 8381, 'caradelevingne': 1730, 'tulipi': 11997, 'trade': 11876, 'tired': 11721, 'nope': 7840, 'apply': 540, 'iamca': 5268, 'found': 4068, 'mw': 7511, 'aftie': 179, 'goodmorning': 4473, 'whiteant': 12713, 'prokabaddi': 9019, 'koel': 6156, 'mallick': 6843, 'recites': 9326, 'national': 7600, 'anthem': 478, 'dayhappy': 2674, 'yournaturalleaders': 13112, 'youngnaturalleadersmonjuly': 13108, 'cumbria': 2535, 'flockstars': 3969, 'thurjuly': 11669, 'itv': 5637, 'goodnightsleeptight': 4477, 'haveagoodday': 4846, 'leg': 6356, 'september': 10027, 'perhaps': 8466, 'bb': 964, 'promote': 9027, 'full': 4186, 'album': 282, 'fully': 4190, 'intend': 5518, 'write': 12908, 'designs': 2829, 'possible': 8785, 'attack': 677, 'gtd': 4617, 'birds': 1203, 'teamadmicro': 11418, 'fridaydownpour': 4115, 'clears': 2078, 'rohit': 9642, 'queen': 9153, 'smileyotwolgrandtrailer': 10534, 'inspired': 5492, 'sheer': 10152, 'fact': 3688, 'obama': 7936, 'beat': 1004, 'innumerable': 5474, 'odds': 7958, 'ni': 7720, 'shauri': 10139, 'yako': 12990, 'memotohaters': 7094, 'sunday': 11146, 'lots': 6623, 'pampering': 8275, 'twas': 12021, 'cabincrew': 1650, 'interview': 5537, 'langkawi': 6257, 'st': 10874, 'august': 696, 'fulfil': 4184, 'fantasies': 3750, 'backhandindexpointingright': 810, 'backhandindexpointingleftsparklingheart': 809, 'cer': 1824, 'extwelebs': 3636, 'pr': 8891, 'apartment': 509, 'makeover': 6828, 'ideas': 5288, 'factory': 3690, 'living': 6521, 'brilliantly': 1484, 'happyyyyyy': 4798, 'birthdaaaaayyyy': 1207, 'kill': 6064, 'interested': 5526, 'internship': 5533, 'program': 9011, 'sadly': 9754, 'career': 1740, 'page': 8241, 'issue': 5606, 'sad': 9747, 'overwhelmingly': 8213, 'games': 4242, 'aha': 208, 'seeinghappy': 9966, 'beauts': 1020, 'clifton': 2091, 'win': 12760, 'deo': 2801, 'faaaaaabulous': 3651, 'freebiefriday': 4095, 'aluminiumfree': 347, 'stayfresh': 10938, 'john': 5800, 'worries': 12881, 'navigate': 7607, 'arsalan': 603, 'thnks': 11615, 'progrmr': 9016, 'pm': 8709, 'quit': 9171, 'figures': 3867, 'hardly': 4807, 'surprising': 11203, 'roses': 9666, 'emotive': 3382, 'poetry': 8721, 'followed': 4006, 'frequentflyer': 4103, 'end': 3404, 'breaking': 1456, 'apologizing': 522, 'kb': 5979, 'mumbai': 7468, 'londondairy': 6575, 'icecream': 5274, 'experience': 3604, 'urs': 12253, 'past': 8344, 'covers': 2428, 'sins': 10323, 'excited': 3572, 'tears': 11433, 'xxxxxxx': 12973, 'jim': 5773, 'chuckle': 2017, 'shopping': 10214, 'agreed': 205, 'cake': 1662, 'doh': 3038, 'subscribers': 11089, 'needed': 7642, 'reached': 9275, 'scriptninja': 9928, 'dry': 3166, 'scorching': 9905, 'summer': 11137, 'peters': 8492, 'younger': 13106, 'woman': 12823, 'stamina': 10890, 'tomgt': 11787, 'arent': 575, 'expect': 3596, 'anything': 495, 'less': 6385, 'tweeties': 12030, 'kochar': 6154, 'fab': 3652, 'gt': 4614, 'karabom': 5938, 'loners': 6582, 'introducing': 5542, 'word': 12848, 'vs': 12460, 'altering': 341, 'understanding': 12145, 'wordconfusion': 12849, 'spreading': 10852, 'problem': 8990, 'supa': 11164, 'dupa': 3205, 'near': 7630, 'dartmoor': 2650, 'gold': 4454, 'colour': 2176, 'marisa': 6913, 'mikky': 7183, 'ok': 7998, 'someday': 10684, 'girly': 4391, 'dii': 2916, 'forget': 4044, 'sis': 10333, 'smf': 10440, 'ft': 4166, 'japanese': 5696, 'import': 5365, 'kitty': 6118, 'matching': 6965, 'stationary': 10931, 'drawing': 3125, 'close': 2103, 'broken': 1502, 'connecting': 2276, 'specialise': 10796, 'thermal': 11578, 'imaging': 5345, 'surveys': 11209, 'connected': 2275, 'planning': 8648, 'south': 10764, 'korea': 6164, 'facts': 3691, 'planned': 8646, 'scamper': 9873, 'slept': 10406, 'alarm': 276, 'aint': 233, 'mad': 6769, 'chweina': 2024, 'xd': 12944, 'jotzh': 5834, 'waste': 12553, 'place': 8635, 'completely': 2228, 'cover': 2424, 'worth': 12886, 'bringing': 1486, 'coat': 2125, 'beforehand': 1043, 'tho': 11618, 'foh': 3994, 'outside': 8191, 'smileyconfusionconfusionconfusionconfusionconfusionconfusion': 10471, 'holiday': 5060, 'menacing': 7096, 'jojo': 5810, 'accepted': 58, 'savs': 9857, 'guyshope': 4666, 'updates': 12217, 'smileybyeadmin': 10460, 'lukris': 6717, 'faceblowingakiss': 3659, 'momma': 7322, 'bear': 998, 'redheart': 9349, 'redid': 9355, 'grade': 4520, 'vball': 12325, 'atm': 672, 'retweets': 9516, 'building': 1570, 'packing': 8235, 'suitcase': 11124, 'remove': 9427, 'hangcopying': 4774, 'translation': 11904, 'dostoevskys': 3092, 'notes': 7858, 'ubu': 12081, 'voucher': 12456, 'bugatti': 1565, 'potz': 8806, 'bra': 1418, 'knows': 6147, 'yummy': 13147, 'ala': 273, 'bdayt': 989, 'season': 9942, 'mnwreeen': 7285, 'jazz': 5720, 'truck': 11961, 'speak': 10790, 'pbevent': 8389, 'hq': 5170, 'add': 109, 'yoona': 13089, 'hairpin': 4728, 'otp': 8165, 'collection': 2163, 'mastership': 6961, 'honey': 5087, 'paindo': 8246, 'await': 729, 'report': 9453, 'manny': 6880, 'asshole': 646, 'brijresidency': 1480, 'stories': 11002, 'structure': 11043, 'units': 12177, 'encompasses': 3400, 'bhk': 1156, 'flats': 3942, 'honored': 5092, 'currys': 2547, 'clash': 2060, 'milano': 7184, 'okhand': 8006, 'followback': 4005, 'legit': 6364, 'loser': 6614, 'sit': 10339, 'gassed': 4270, 'dead': 2696, 'starsquad': 10910, 'whitemediumstar': 12714, 'mok': 7313, 'awwww': 761, 'cfc': 1834, 'bajwa': 849, 'news': 7694, 'manhappy': 6871, 'utc': 12278, 'flumekaytranadaalunageorge': 3981, 'tickets': 11683, 'kms': 6131, 'certainty': 1831, 'smileyhope': 10505, 'solve': 10678, 'fasteroncomingfistat': 3770, 'hurry': 5236, 'totem': 11848, 'somewhere': 10693, 'later': 6288, 'alice': 297, 'click': 2084, 'checkout': 1916, 'dogs': 3037, 'cat': 1781, 'goodwynsgoodies': 4481, 'ugh': 12093, 'fade': 3692, 'moaning': 7287, 'leeds': 6351, 'jozi': 5846, 'wasnt': 12551, 'fifth': 3859, 'available': 716, 'tix': 11735, 'pa': 8227, 'ba': 777, 'ng': 7708, 'atl': 668, 'coldplay': 2151, 'favorites': 3784, 'scientist': 9897, 'yellow': 13036, 'atlas': 670, 'yein': 13029, 'receiving': 9315, 'selos': 10000, 'jabongatpumaurbanstampedeansgt': 5656, 'timely': 11702, 'arrival': 599, 'waiter': 12496, 'bill': 1182, 'sircustomer': 10327, 'kitchend': 6111, 'title': 11730, 'says': 9865, 'pocket': 8715, 'wripped': 12906, 'jeans': 5728, 'rob': 9611, 'connie': 2278, 'ship': 10179, 'crew': 2482, 'staff': 10881, 'sweetan': 11246, 'ask': 629, 'filming': 3879, 'edmce': 3273, 'mum': 7467, 'begging': 1046, 'sopranos': 10728, 'ukraine': 12105, 'olly': 8030, 'disneyarts': 2978, 'talented': 11330, 'elmoprinssi': 3344, 'salsa': 9786, 'dancing': 2624, 'tell': 11461, 'truth': 11971, 'smileyhappy': 10502, 'pls': 8693, 'interesting': 5527, 'nd': 7623, 'blogiversary': 1288, 'reviews': 9525, 'cutie': 2565, 'bohol': 1330, 'ayeayecaptain': 769, 'briliant': 1481, 'conversation': 2330, 'key': 6015, 'annual': 457, 'productive': 9006, 'far': 3755, 'spinning': 10822, 'voices': 12444, 'yeheyy': 13028, 'pinya': 8602, 'whoooah': 12722, 'trance': 11897, 'lover': 6661, 'favorite': 3782, 'subject': 11082, 'physics': 8551, 'heading': 4874, 'stopped': 10996, 'indian': 5411, 'matter': 6978, 'jungle': 5890, 'accommodate': 64, 'shouting': 10236, 'secret': 9953, 'behind': 1057, 'thishappy': 11609, 'sandroforceo': 9811, 'ceomonth': 1823, 'swag': 11227, 'turn': 12011, 'mia': 7157, 'flow': 3973, 'workinprogress': 12864, 'choosing': 1988, 'colours': 2179, 'smileyjust': 10513, 'landreth': 6252, 'finnigan': 3909, 'loyal': 6680, 'royal': 9686, 'fotoset': 4065, 'reusful': 9519, 'seems': 9971, 'somebody': 10683, 'smileyuneo': 10560, 'selling': 9998, 'deni': 2792, 'uneo': 12150, 'young': 13105, 'understand': 12143, 'muntu': 7476, 'another': 464, 'gem': 4299, 'falcos': 3713, 'supersmash': 11178, 'hotnsexy': 5143, 'friskyfriday': 4141, 'beach': 992, 'tiring': 11723, 'movie': 7401, 'crop': 2497, 'pics': 8564, 'nash': 7588, 'tissues': 11726, 'chocolate': 1976, 'tea': 11411, 'hannibal': 4779, 'episode': 3468, 'hotbed': 5134, 'bush': 1614, 'classicassures': 2065, 'sosaree': 10745, 'thrilled': 11647, 'international': 5531, 'assignment': 649, 'aerial': 154, 'camera': 1692, 'operatorswales': 8093, 'boom': 1362, 'hong': 5089, 'kong': 6162, 'ferry': 3838, 'central': 1816, 'girlfriends': 4385, 'afterwork': 177, 'drink': 3145, 'dj': 3005, 'restoconfusion': 9496, 'drinkt': 3149, 'koffie': 6157, 'stargate': 10905, 'atlantis': 669, 'muaahhh': 7434, 'ohh': 7988, 'hii': 4995, 'seenoevilmonkey': 9973, 'di': 2876, 'nagsend': 7544, 'yung': 13153, 'ko': 6152, 'ulit': 12106, 'nells': 7659, 'smileypartypopperballoon': 10535, 'ugly': 12097, 'leggete': 6362, 'qui': 9163, 'per': 8453, 'la': 6206, 'smileymar': 10523, 'encouraged': 3403, 'employer': 3384, 'board': 1317, 'stuck': 11053, 'poster': 8791, 'sticker': 10969, 'sponsor': 10838, 'prize': 8983, 'tablet': 11294, 'liampayne': 6408, 'lemon': 6374, 'milo': 7202, 'aurini': 700, 'juicebro': 5868, 'pillars': 8585, 'respective': 9483, 'communities': 2210, 'boii': 1332, 'smashingbook': 10435, 'bible': 1164, 'sick': 10271, 'lamo': 6245, 'fangirl': 3742, 'platonic': 8657, 'science': 9896, 'morningits': 7369, 'cooke': 2341, 'supporting': 11184, 'helping': 4946, 'residents': 9476, 'servicewithasmile': 10047, 'sparkles': 10782, 'fams': 3732, 'bloodline': 1294, 'huskies': 5245, 'obituary': 7939, 'advert': 148, 'goofingaround': 4484, 'madness': 6780, 'bollywood': 1335, 'dah': 2599, 'move': 7397, 'nothing': 7862, 'bitterness': 1235, 'anger': 431, 'hatred': 4841, 'towards': 11859, 'pure': 9110, 'indifference': 5420, 'suite': 11126, 'zach': 13161, 'cody': 2139, 'bartram': 927, 'delivered': 2775, 'ac': 51, 'achieving': 80, 'excellence': 3568, 'producer': 9004, 'boggling': 1327, 'fatiguing': 3774, 'baareeq': 781, 'sharing': 10129, 'gamedev': 4237, 'hobby': 5046, 'sa': 9735, 'tweeniefox': 12025, 'accessories': 60, 'products': 9007, 'tamang': 11343, 'hinala': 5011, 'niam': 7724, 'selfieeeee': 9991, 'especially': 3499, 'aling': 301, 'swim': 11259, 'perfection': 8458, 'bout': 1402, 'goodbye': 4469, 'feminists': 3830, 'fought': 4067, 'snobby': 10624, 'bitch': 1222, 'caroline': 1758, 'mighty': 7178, 'fire': 3910, 'threw': 11645, 'making': 6833, 'hbd': 4863, 'teamjkt': 11423, 'follback': 4002, 'jogging': 5799, 'tudachoppa': 11991, 'pro': 8985, 'remote': 9426, 'traveling': 11914, 'newly': 7692, 'listed': 6493, 'ebay': 3254, 'store': 10999, 'disneyinfinity': 2979, 'starwars': 10922, 'characters': 1881, 'preorder': 8920, 'starter': 10914, 'pack': 8232, 'hit': 5028, 'snap': 10608, 'homies': 5079, 'bought': 1399, 'skins': 10372, 'bday': 987, 'chant': 1870, 'jai': 5673, 'italy': 5612, 'times': 11703, 'fast': 3768, 'heeeeyyy': 4921, 'fan': 3733, 'woah': 12810, 'review': 9523, 'phew': 8516, 'overwhelmed': 8211, 'babysmilingfacewithsmilingeyessmilingfacewithsmilingeyessmilingfacewithsmilingeyes': 794, 'whenever': 12696, 'hugs': 5198, 'ang': 422, 'kisses': 6104, 'philippines': 8520, 'entered': 3438, 'package': 8233, 'enter': 3437, 'works': 12869, 'reynaldoz': 9534, 'bruise': 1528, 'rib': 9541, 'grinningfacegrinningfacewithsmilingeyesfacewithtearsofjoysmilingfacewithopenmouthsmilingfacewithopenmouthsmilingeyessmilingfacewithopenmouthcoldsweatwinkingface': 4584, 'worry': 12882, 'tombraider': 11785, 'hype': 5256, 'thejuiceinthemix': 11552, 'rela': 9388, 'low': 6675, 'priority': 8975, 'match': 6964, 'harry': 4816, 'bc': 980, 'opportune': 8103, 'collapse': 2157, 'chaotic': 1874, 'cosas': 2377, 'note': 7856, 'alliteration': 312, 'oppayaa': 8102, 'hows': 5164, 'natgeo': 7593, 'lick': 6420, 'elbow': 3318, 'interestinghappy': 5528, 'emu': 3392, 'stoked': 10982, 'woke': 12816, 'facewithtearsofjoyfacewithtearsofjoy': 3676, 'peoples': 8448, 'approval': 549, 'gods': 4445, 'jisung': 5780, 'kid': 6043, 'sunshine': 11159, 'mm': 7279, 'nicola': 7742, 'brighten': 1478, 'helen': 4932, 'dreyaniks': 3141, 'brian': 1467, 'shipped': 10180, 'weeks': 12626, 'australia': 706, 'ol': 8015, 'bones': 1347, 'creaking': 2462, 'abuti': 50, 'tweetlandlol': 12033, 'android': 416, 'xmas': 12953, 'skyblock': 10379, 'standing': 10897, 'bcause': 981, 'die': 2890, 'sympathy': 11279, 'laugh': 6297, 'nana': 7571, 'xo': 12954, 'unnieeeehappy': 12195, 'nuka': 7907, 'penacova': 8432, 'djset': 3008, 'edm': 3272, 'kizomba': 6120, 'latinhouse': 6295, 'housemusic': 5156, 'portugal': 8778, 'wild': 12750, 'ride': 9554, 'anytime': 497, 'taste': 11370, 'yer': 13045, 'mtn': 7430, 'maganda': 6790, 'short': 10217, 'mistress': 7260, 'saphire': 9822, 'gingi': 4378, 'busy': 1620, 'instagramif': 5499, 'among': 388, 'yetfeel': 13060, 'coconut': 2136, 'sambal': 9793, 'mussels': 7494, 'recipe': 9322, 'kalin': 5922, 'mixcloud': 7267, 'sarcasm': 9831, 'chelsea': 1932, 'useless': 12266, 'thursday': 11671, 'hang': 4773, 'hehe': 4925, 'said': 9765, 'benson': 1098, 'facebook': 3663, 'solid': 10671, 'bella': 1073, 'facewithstuckouttonguewinkingeyeso': 3674, 'maryhicks': 6937, 'kikmeboys': 6057, 'photooftheday': 8538, 'musicbiz': 7489, 'sheskindahot': 10166, 'fleekile': 3950, 'mbalula': 6999, 'repeat': 9441, 'africa': 166, 'mexican': 7147, 'feelings': 3816, 'thatll': 11541, 'scarred': 9878, 'office': 7973, 'donut': 3071, 'foiegras': 3995, 'lovers': 6663, 'despite': 2835, 'weather': 12598, 'wedding': 12610, 'tony': 11811, 'stark': 10908, 'incredible': 5406, 'pictures': 8567, 'poems': 8720, 'bubbling': 1545, 'dale': 2603, 'billion': 1184, 'magical': 6796, 'op': 8086, 'cast': 1777, 'vote': 12452, 'elections': 3324, 'jcreport': 5723, 'lfc': 6403, 'piggin': 8578, 'peace': 8403, 'botanical': 1389, 'soaps': 10637, 'late': 6285, 'upload': 12226, 'freshly': 4107, 'doneweeks': 3059, 'healed': 4881, 'mskittenns': 7422, 'exciting': 3574, 'tobibro': 11751, 'isp': 5603, 'steel': 10946, 'wednesday': 12613, 'likehappy': 6445, 'swear': 11234, 'met': 7139, 'earlier': 3232, 'smiling': 10570, 'cam': 1687, 'loudlycryingfaceredheart': 6635, 'except': 3570, 'mashaallah': 6943, 'french': 4100, 'wwat': 12932, 'france': 4083, 'videos': 12385, 'pen': 8431, 'fixed': 3931, 'yaaaaaaaay': 12976, 'reviewing': 9524, 'beiruting': 1061, 'coffee': 2141, 'panda': 8279, 'eonnie': 3462, 'teeth': 11454, 'favourite': 3786, 'soda': 10653, 'fuller': 4187, 'shit': 10188, 'healthy': 4884, 'beatingheart': 1005, 'rettweet': 9508, 'mvg': 7509, 'valuable': 12307, 'girlfriendconfusion': 4384, 'staying': 10939, 'madrid': 6784, 'starts': 10917, 'ohayoung': 7986, 'sore': 10730, 'bergerac': 1106, 'chants': 1872, 'individuals': 5424, 'excellent': 3569, 'farhappy': 3760, 'adam': 107, 'beachs': 994, 'movies': 7404, 'showsim': 10252, 'suicide': 11120, 'squad': 10860, 'fond': 4018, 'christopher': 2008, 'initially': 5461, 'cocky': 2133, 'prove': 9053, 'attitudes': 685, 'improving': 5373, 'suggest': 11118, 'dates': 2659, 'indeed': 5408, 'happyshappy': 4796, 'intelligent': 5517, 'strong': 11038, 'cs': 2523, 'certain': 1829, 'exams': 3567, 'forgot': 4048, 'homebased': 5069, 'knee': 6133, 'sale': 9780, 'fleur': 3952, 'dress': 3136, 'readystockhijabmartidr': 9285, 'idr': 5297, 'tompolo': 11797, 'aimed': 230, 'stopping': 10997, 'buyers': 1635, 'disappoint': 2947, 'paper': 8293, 'towns': 11863, 'slacking': 10385, 'cracking': 2442, 'particularly': 8320, 'striking': 11030, 'mam': 6846, 'tastes': 11372, 'feytyaz': 3847, 'instant': 5503, 'stiffening': 10972, 'interest': 5525, 'rickyfebs': 9549, 'secrets': 9954, 'grindea': 4579, 'courier': 2413, 'crypts': 2520, 'possibly': 8786, 'arma': 588, 'record': 9333, 'gosh': 4500, 'limbo': 6462, 'retweeting': 9515, 'orchard': 8123, 'art': 605, 'super': 11165, 'karachi': 5939, 'ka': 5905, 'venice': 12338, 'rains': 9200, 'javaid': 5708, 'several': 10066, 'parts': 8328, 'witness': 12794, 'accumulated': 70, 'water': 12562, 'maroon': 6921, 'cocktail': 2129, 'dresses': 3138, 'mididresses': 7173, 'quick': 9164, 'vein': 12335, 'autofocus': 709, 'manual': 6885, 'shooting': 10210, 'veins': 12336, 'crackle': 2443, 'glaze': 4412, 'layout': 6317, 'bomb': 1337, 'socialwebsite': 10648, 'pake': 8256, 'joim': 5804, 'feed': 3807, 'troops': 11952, 'brands': 1441, 'wanting': 12531, 'beauty': 1021, 'mail': 6817, 'ladolcevitainluxembourgcom': 6220, 'prrequest': 9059, 'journorequest': 5840, 'themadstork': 11555, 'shaun': 10138, 'wants': 12533, 'bot': 1388, 'chloe': 1974, 'watched': 12560, 'actress': 96, 'dms': 3017, 'away': 738, 'xoxo': 12956, 'wicked': 12736, 'hola': 5053, 'juan': 5855, 'sending': 10008, 'houston': 5159, 'tx': 12059, 'jenni': 5742, 'stumble': 11064, 'upon': 12229, 'probnice': 8994, 'choker': 1983, 'btw': 1540, 'seouljins': 10022, 'photoset': 8540, 'sadomasochistsparadise': 9758, 'wynter': 12936, 'bottoms': 1398, 'outtake': 8194, 'sadomasochists': 9757, 'paradise': 8300, 'cuties': 2566, 'ty': 12062, 'bby': 977, 'clip': 2098, 'losing': 6617, 'cypher': 2581, 'amen': 373, 'kusuma': 6192, 'planted': 8652, 'allowed': 318, 'corner': 2363, 'addicted': 111, 'itgreat': 5620, 'gurl': 4660, 'suck': 11100, 'editing': 3268, 'turns': 12015, 'owe': 8216, 'daniel': 2632, 'apehappy': 513, 'saarhave': 9740, 'ahead': 213, 'verse': 12353, 'butterfly': 1626, 'part': 8316, 'bonus': 1349, 'mehappy': 7062, 'filled': 3875, 'laughter': 6301, 'sos': 10744, 'yummmyyy': 13146, 'eating': 3251, 'dosa': 3086, 'easier': 3246, 'unless': 12183, 'achi': 77, 'youuu': 13126, 'bawi': 958, 'ako': 265, 'queenesther': 9154, 'sharp': 10134, 'yess': 13050, 'wonder': 12828, 'poldi': 8730, 'cimbom': 2032, 'media': 7039, 'buddy': 1553, 'sigh': 10282, 'bruhhh': 1527, 'daddy': 2592, 'communal': 2207, 'knowledge': 6146, 'attention': 683, 'questions': 9160, 'tb': 11395, 'credit': 2471, 'departments': 2804, 'anz': 503, 'extreme': 3633, 'offshoring': 7980, 'absolutely': 44, 'classic': 2064, 'smileygottolovebanks': 10500, 'yup': 13156, 'inshaaallah': 5482, 'dua': 3172, 'thru': 11656, 'hatreds': 4842, 'aameenall': 13, 'coca': 2127, 'cola': 2146, 'fanta': 3749, 'pepsi': 8452, 'sprite': 10859, 'alls': 322, 'sweeeeeeeety': 11244, 'welcometweet': 12640, 'psygustokita': 9067, 'gaming': 4246, 'setup': 10060, 'wet': 12661, 'feet': 3819, 'carpet': 1760, 'toworry': 11865, 'judgmental': 5861, 'hypocritical': 5261, 'narcissist': 7579, 'jumpsuit': 5888, 'bt': 1535, 'denimshappy': 2794, 'verge': 12346, 'owl': 8217, 'break': 1453, 'constant': 2289, 'welcomed': 12638, 'anairline': 401, 'run': 9716, 'sia': 10264, 'patel': 8350, 'counting': 2399, 'brilliant': 1483, 'parent': 8306, 'teacher': 11414, 'finding': 3892, 'started': 10913, 'comparative': 2215, 'religion': 9410, 'rant': 9231, 'student': 11055, 'benchers': 1089, 'porsche': 8773, 'paddock': 8238, 'budapestgp': 1551, 'johnyherbert': 5803, 'rolling': 9647, 'porschesupercup': 8774, 'waking': 12504, 'koyal': 6167, 'melodiesan': 7081, 'unexpected': 12153, 'create': 2467, 'memories': 7091, 'students': 11056, 'eps': 3471, 'catch': 1784, 'wirh': 12782, 'arc': 564, 'wolves': 12821, 'birthdays': 1213, 'fulfill': 4185, 'desireameen': 2830, 'kca': 5982, 'votejktid': 12454, 'helpinggroupdms': 4947, 'quote': 9175, 'weird': 12633, 'dp': 3111, 'wife': 12742, 'kids': 6048, 'poor': 8751, 'chicks': 1952, 'guide': 4641, 'zonzofox': 13197, 'bhaiya': 1149, 'brother': 1513, 'lucky': 6705, 'dog': 3033, 'patty': 8367, 'elaborate': 3316, 'kuching': 6183, 'rate': 9249, 'merdeka': 7119, 'palace': 8265, 'hotel': 5137, 'suites': 11127, 'plusmiles': 8706, 'service': 10045, 'hahahaa': 4701, 'marm': 6919, 'mean': 7023, 'nex': 7701, 'safe': 9762, 'gwd': 4672, 'okok': 8013, 'idiot': 5292, 'chaerin': 1837, 'unnie': 12194, 'viable': 12361, 'alternative': 342, 'nowadays': 7875, 'pass': 8336, 'ip': 5560, 'tombow': 11784, 'abt': 46, 'friyay': 4143, 'xxx': 12968, 'smug': 10602, 'marrickville': 6923, 'public': 9077, 'ten': 11468, 'ago': 200, 'eighteen': 3305, 'auvssscr': 713, 'ncaaseason': 7620, 'kills': 6070, 'slow': 10424, 'trying': 11973, 'popsicles': 8762, 'soft': 10660, 'melt': 7084, 'mouth': 7395, 'thankyouuu': 11533, 'dianna': 2882, 'ngga': 7712, 'usah': 12261, 'dipikirin': 2927, 'elah': 3317, 'easily': 3247, 'whos': 12724, 'entp': 3449, 'killin': 6068, 'meme': 7089, 'worthy': 12888, 'shot': 10225, 'emon': 3376, 'decent': 2721, 'outdoor': 8181, 'rave': 9254, 'pisses': 8615, 'smileyive': 10512, 'services': 10046, 'dv': 3215, 'aku': 270, 'bakal': 850, 'liat': 6412, 'kak': 5919, 'merry': 7124, 'tv': 12020, 'outfit': 8182, 'hashtaggt': 4827, 'fashionfriday': 3767, 'uploading': 12228, 'wanted': 12530, 'anglenelson': 434, 'cheap': 1906, 'mymonsoonstory': 7521, 'tree': 11920, 'skin': 10368, 'clearing': 2076, 'lotion': 6622, 'moisturizing': 7311, 'important': 5366, 'monsoon': 7336, 'acker': 83, 'whoop': 12723, 'romantic': 9651, 'valencia': 12302, 'weekendenjoy': 12619, 'daaru': 2589, 'party': 8329, 'chaddi': 1836, 'bros': 1511, 'beginning': 1049, 'means': 7029, 'wonderfulgreat': 12831, 'closely': 2105, 'trimmed': 11941, 'pubes': 9076, 'jaylin': 5716, 'es': 3488, 'mi': 7156, 'tio': 11715, 'sinaloa': 10311, 'arre': 597, 'stylish': 11073, 'trendy': 11927, 'kim': 6072, 'north': 7846, 'fabfriday': 3653, 'facetime': 3668, 'calum': 1685, 'constantly': 2290, 'messaging': 7132, 'announce': 452, 'filbarbarian': 3871, 'beer': 1036, 'talks': 11339, 'arms': 590, 'testicle': 11491, 'light': 6435, 'social': 10644, 'katerina': 5963, 'maniataki': 6874, 'ahh': 214, 'alright': 337, 'worthwhile': 12887, 'judging': 5860, 'tech': 11441, 'windows': 12764, 'asking': 634, 'stupid': 11067, 'loving': 6674, 'plugin': 8703, 'bass': 940, 'slap': 10387, 'doors': 3080, 'vipgeneralconfusion': 12413, 'seats': 9947, 'early': 3235, 'actshappy': 98, 'london': 6573, 'babyhappy': 792, 'toptravelcentar': 11832, 'ttctop': 11983, 'lux': 6732, 'luxurytravel': 6737, 'beograd': 1102, 'srbija': 10866, 'putovanja': 9126, 'wendy': 12652, 'provide': 9056, 'fresh': 4105, 'drainage': 3119, 'homebound': 5070, 'hahahays': 4712, 'yeeeeah': 13022, 'moar': 7288, 'kittehs': 6115, 'incoming': 5400, 'towers': 11861, 'yippeee': 13070, 'scrummy': 9931, 'recipes': 9323, 'bio': 1194, 'moved': 7398, 'mcpe': 7018, 'vainglory': 12300, 'driver': 3151, 'lilydale': 6460, 'fss': 4165, 'raise': 9202, 'magicalmysterytour': 6798, 'chek': 1928, 'rules': 9711, 'weebly': 12614, 'donetsk': 3058, 'smirkconfusionconfusion': 10587, 'earth': 3242, 'ros': 9662, 'followingif': 4012, 'printed': 8970, 'personalised': 8475, 'wrap': 12903, 'cardsstationery': 1738, 'adrian': 137, 'parcel': 8304, 'tuesday': 11992, 'pris': 8976, 'wz': 12937, 'pattern': 8366, 'cuttingmaking': 2570, 'buttonhole': 1628, 'finishing': 3905, 'designer': 2828, 'famous': 3731, 'clients': 2089, 'keeps': 5995, 'alive': 303, 'trial': 11931, 'spm': 10830, 'dinooo': 2926, 'cardio': 1736, 'steak': 10940, 'cue': 2531, 'photo': 8532, 'laptop': 6271, 'putting': 9128, 'guinea': 4644, 'pigs': 8580, 'smileylt': 10522, 'bestfriendsfriends': 1123, 'salamat': 9779, 'mga': 7152, 'naggreet': 7541, 'guise': 4646, 'godbless': 4441, 'crush': 2510, 'apple': 537, 'eye': 3638, 'ga': 4220, 'models': 7302, 'deserve': 2823, 'charles': 1892, 'workhard': 12858, 'instagram': 5498, 'ys': 13133, 'aiden': 226, 'forritwah': 4056, 'teabread': 11412, 'baconbutter': 819, 'jay': 5712, 'afang': 156, 'soup': 10760, 'semo': 10006, 'brb': 1449, 'forced': 4034, 'lbloggers': 6322, 'pbloggers': 8390, 'bookreview': 1358, 'toddlerlife': 11764, 'tato': 11376, 'bulat': 1572, 'discussing': 2964, 'shared': 10124, 'suggestion': 11119, 'concerned': 2239, 'snake': 10605, 'performed': 8463, 'con': 2235, 'todayyyyy': 11762, 'max': 6987, 'gaza': 4282, 'retweet': 9512, 'delsigthis': 2779, 'logging': 6556, 'bbb': 965, 'tried': 11938, 'peacefully': 8406, 'legal': 6358, 'age': 189, 'shant': 10119, 'ditching': 2998, 'toryhappy': 11841, 'bajrangibhaijaanhighestweek': 847, 'sokay': 10668, 'andythank': 420, 'youand': 13097, 'return': 9509, 'tuitutil': 11994, 'bud': 1550, 'ass': 641, 'learn': 6335, 'takeaways': 11316, 'instead': 5504, 'hr': 5171, 'genial': 4312, 'competition': 2221, 'naruto': 7585, 'yosh': 13095, 'procrastinating': 9001, 'plus': 8704, 'sorting': 10742, 'boll': 1333, 'telling': 11463, 'kfc': 6018, 'itunes': 5636, 'dedicatedfan': 2731, 'purpleheartpurpleheartpurpleheartpurpleheart': 9114, 'daft': 2596, 'teething': 11455, 'troubles': 11957, 'huxley': 5247, 'basket': 937, 'ben': 1086, 'sent': 10018, 'gamer': 4240, 'trainfollow': 11891, 'meretweet': 7121, 'thisfollow': 11607, 'retweetedgain': 9514, 'active': 93, 'distance': 2987, 'propose': 9043, 'suitable': 11123, 'comment': 2198, 'alternatives': 344, 'final': 3881, 'tgifhappy': 11504, 'stockholm': 10979, 'zack': 13163, 'destroying': 2841, 'heel': 4922, 'claws': 2069, 'moviewill': 7405, 'blonde': 1291, 'box': 1408, 'cheerios': 1922, 'seeds': 9963, 'problems': 8993, 'cutest': 2563, 'ffback': 3849, 'cooder': 2339, 'spotify': 10845, 'weve': 12664, 'vc': 12326, 'careers': 1741, 'tgp': 11508, 'races': 9182, 'average': 719, 'joes': 5798, 'bluejays': 1302, 'vinylbear': 12410, 'pals': 8269, 'furbabys': 4203, 'luffs': 6712, 'mega': 7056, 'retail': 9503, 'launch': 6302, 'whsmith': 12726, 'ps': 9062, 'keeping': 5993, 'purpleheart': 9113, 'shannon': 10118, 'closing': 2111, 'redecorate': 9346, 'bob': 1321, 'ellie': 3340, 'mairi': 6821, 'workouts': 12867, 'impairs': 5360, 'uggghhhh': 12092, 'dam': 2606, 'dun': 3198, 'eczema': 3259, 'sufferers': 11111, 'umji': 12119, 'ndee': 7625, 'bargainsuk': 914, 'pleasure': 8685, 'publilius': 9079, 'syrus': 11288, 'fear': 3800, 'death': 2706, 'dreaded': 3127, 'fell': 3824, 'fuk': 4183, 'unblock': 12134, 'youll': 13102, 'manually': 6886, 'tweak': 12022, 'php': 8544, 'fall': 3715, 'oomf': 8076, 'pippa': 8605, 'hschool': 5176, 'bus': 1612, 'uses': 12270, 'cardi': 1735, 'everyday': 3545, 'everytime': 3554, 'laughingsmiling': 6300, 'hk': 5036, 'whyd': 12728, 'acorn': 86, 'originally': 8144, 'apart': 507, 'cpuconfusion': 2437, 'considerably': 2284, 'advanced': 144, 'onair': 8052, 'bay': 959, 'hold': 5055, 'river': 9589, 'rescue': 9469, 'mutt': 7504, 'wondered': 12829, 'confirm': 2252, 'delivery': 2776, 'sanjaya': 9817, 'switched': 11266, 'lap': 6268, 'optimized': 8112, 'lu': 6697, 'straight': 11009, 'omg': 8044, 'tweetofthedecade': 12034, 'class': 2062, 'happiest': 4787, 'fridays': 4124, 'bbmme': 971, 'pindfea': 8592, 'bbm': 969, 'pin': 8590, 'bbmpin': 972, 'addmeonbbm': 119, 'addme': 118, 'todays': 11758, 'normal': 7844, 'menu': 7110, 'married': 6924, 'glenn': 4416, 'decker': 2728, 'height': 4929, 'sculptors': 9932, 'pillar': 8584, 'storegood': 11000, 'released': 9403, 'ti': 11679, 'dota': 3094, 'image': 5342, 'mentions': 7109, 'nudge': 7901, 'spot': 10844, 'tasty': 11373, 'plant': 8651, 'hilly': 5007, 'cycle': 2576, 'england': 3421, 'scotlandismassive': 9910, 'gen': 4302, 'vikk': 12398, 'fna': 3989, 'mombasa': 7316, 'feels': 3817, 'tukutanemombasa': 11995, 'reasonstovisitmombasa': 9305, 'karibumombasa': 5948, 'hanbin': 4763, 'dseems': 3167, 'giving': 4401, 'banner': 897, 'certainly': 1830, 'goosnight': 4496, 'kindly': 6086, 'familiar': 3724, 'jealous': 5727, 'sound': 10755, 'hitting': 5033, 'tent': 11475, 'yea': 13013, 'cozy': 2433, 'chilled': 1960, 'phenomenal': 8515, 'collab': 2153, 'gave': 4276, 'birth': 1206, 'facewithtearsofjoy': 3675, 'behaves': 1054, 'monster': 7337, 'doesntconfusion': 3032, 'spree': 10853, 'tank': 11356, 'outstanding': 8192, 'donation': 3056, 'contestkiduniya': 2311, 'mfundo': 7151, 'balboa': 857, 'oche': 7953, 'hun': 5220, 'inner': 5469, 'nerd': 7669, 'tamed': 11345, 'insidious': 5486, 'logic': 6557, 'math': 6973, 'channel': 1868, 'continues': 2316, 'doubt': 3097, 'itconfusion': 5615, 'subs': 11088, 'teoh': 11478, 'forgiven': 4047, 'wonderfuls': 12832, 'mannerfuls': 6879, 'yhoooooo': 13065, 'ngi': 7713, 'mood': 7349, 'pushing': 9121, 'limits': 6466, 'obakeng': 7935, 'stop': 10993, 'goats': 4438, 'alhamdullilah': 296, 'talkhappy': 11335, 'pebble': 8415, 'engrossed': 3424, 'bing': 1191, 'screaming': 9919, 'whole': 12720, 'wide': 12738, 'globeshowingamericasglobeshowingamericasredheartredheartredheart': 4419, 'anguishedface': 437, 'wat': 12556, 'muahhh': 7436, 'smileymay': 10525, 'bless': 1269, 'pausetime': 8371, 'cars': 1766, 'drifting': 3144, 'loose': 6601, 'scopidf': 9903, 'pay': 8374, 'campaign': 1697, 'kickstarter': 6042, 'article': 606, 'absolute': 43, 'jenna': 5740, 'bellybutton': 1078, 'innie': 5470, 'outie': 8185, 'sam': 9790, 'delish': 2773, 'ranges': 9228, 'supermarkets': 11175, 'joselito': 5830, 'freya': 4109, 'nth': 7898, 'latepost': 6287, 'lupet': 6725, 'mo': 7286, 'eric': 3477, 'askaman': 630, 'helpful': 4944, 'alternatively': 343, 'webz': 12608, 'oops': 8083, 'theyll': 11587, 'realise': 9287, 'anymore': 488, 'carmel': 1753, 'kko': 6124, 'decisions': 2726, 'nicapapa': 7729, 'matt': 6976, 'probs': 8995, 'honestly': 5084, 'explain': 3614, 'relationship': 9393, 'pick': 8558, 'employers': 3385, 'wellhappy': 12645, 'tessnzachcute': 11488, 'paperboys': 8294, 'papers': 8295, 'honest': 5083, 'reassured': 9306, 'personal': 8474, 'guysss': 4668, 'happen': 4780, 'mubank': 7437, 'dongwoos': 3064, 'bright': 1476, 'tommorow': 11791, 'newyork': 7700, 'magic': 6795, 'lollll': 6568, 'twinx': 12044, 'path': 8352, 'firmansyahbl': 3914, 'usual': 12275, 'procedure': 8996, 'grim': 4577, 'fandango': 3739, 'ordinary': 8130, 'extraordinary': 3629, 'bos': 1383, 'birmingham': 1205, 'oracle': 8117, 'samosas': 9799, 'fireball': 3911, 'shoe': 10204, 'serves': 10044, 'sushi': 11214, 'shoeshi': 10206, 'flat': 3941, 'lymond': 6748, 'philippa': 8519, 'novel': 7873, 'tara': 11361, 'aur': 699, 'han': 4760, 'imran': 5374, 'khan': 6026, 'agaaaain': 182, 'sir': 10326, 'doli': 3041, 'siregar': 10328, 'ninh': 7770, 'size': 10352, 'gems': 4301, 'geekiest': 4292, 'geeks': 4293, 'wallets': 12521, 'request': 9464, 'persons': 8479, 'covering': 2427, 'rally': 9211, 'rotate': 9672, 'sucks': 11103, 'directionhappy': 2935, 'driving': 3154, 'eek': 3283, 'red': 9344, 'suitcasebeijing': 11125, 'china': 1966, 'meni': 7099, 'tebrik': 11440, 'etdi': 3512, 'smileyconfusioncongratulations': 10482, 'forevergrowingheart': 4043, 'rod': 9631, 'tame': 11344, 'embracing': 3360, 'actor': 95, 'aplomb': 517, 'foreveralone': 4042, 'jobs': 5794, 'mysummer': 7525, 'hahahaha': 4705, 'wear': 12590, 'uniform': 12171, 'evil': 3558, 'owwwwchoo': 8225, 'chweet': 2023, 'shorthaired': 10220, 'oscar': 8153, 'realized': 9295, 'harmony': 4812, 'judge': 5859, 'denerivery': 2791, 'kiksexting': 6062, 'kikkomansabor': 6055, 'killer': 6066, 'henessydiaries': 4955, 'journey': 5837, 'band': 879, 'convo': 2337, 'vault': 12322, 'expanding': 3595, 'vinny': 12408, 'money': 7330, 'hahahahaha': 4706, 'cents': 1821, 'repay': 9440, 'debt': 2718, 'smileyi': 10508, 'kirmizi': 6099, 'evet': 3556, 'wifi': 12744, 'lifestyle': 6430, 'qatarday': 9138, 'sunwithfacesunwithfacesunwithface': 11162, 'innovate': 5472, 'talked': 11334, 'volunteering': 12450, 'saran': 9828, 'drama': 3122, 'genre': 4314, 'romance': 9650, 'comedyhappy': 2187, 'leanneriner': 6334, 'porno': 8770, 'weloveyounamjoon': 12647, 'homey': 5077, 'kenya': 6002, 'emotional': 3379, 'roller': 9646, 'coaster': 2124, 'aspect': 639, 'najam': 7554, 'sethis': 10054, 'confession': 2248, 'ad': 105, 'pricelessantique': 8958, 'takesonetoknowone': 11321, 'extra': 3628, 'ucount': 12087, 'ji': 5768, 'books': 1359, 'turkish': 12010, 'knew': 6135, 'crap': 2451, 'burn': 1604, 'airways': 247, 'trusting': 11970, 'airline': 241, 'deliver': 2774, 'sexy': 10080, 'backhandindexpointingleftbeatingheart': 805, 'yello': 13035, 'race': 9181, 'gail': 4226, 'yael': 12986, 'lesson': 6387, 'en': 3395, 'mis': 7239, 'manos': 6881, 'holding': 5057, 'hands': 4770, 'managers': 6863, 'prettiest': 8944, 'readers': 9281, 'rose': 9664, 'dnt': 3018, 'minho': 7218, 'ideal': 5287, 'type': 12070, 'weekly': 12625, 'idol': 5294, 'pose': 8779, 'gtlt': 4628, 'shortlist': 10221, 'dominion': 3050, 'picnic': 8563, 'tmrw': 11746, 'nobody': 7795, 'jummamubarak': 5880, 'shower': 10245, 'shalwarkameez': 10110, 'itter': 5634, 'offer': 7968, 'jummaprayer': 5882, 'aus': 701, 'af': 155, 'display': 2982, 'enabled': 3397, 'company': 2214, 'peeps': 8425, 'tweeps': 12026, 'folow': 4017, 'grace': 4517, 'ohhh': 7989, 'teaser': 11436, 'airecs': 237, 'acid': 81, 'mouse': 7394, 'ep': 3463, 'including': 5399, 'robin': 9618, 'rough': 9676, 'control': 2325, 'remixes': 9425, 'encourage': 3402, 'changing': 1865, 'oe': 7962, 'rts': 9697, 'faves': 3779, 'toss': 11843, 'ladies': 6219, 'asmita': 638, 'welcomesheep': 12639, 'library': 6416, 'stores': 11001, 'mr': 7412, 'climbed': 2095, 'cuddle': 2527, 'jilla': 5770, 'opening': 8091, 'headlining': 4876, 'jumma': 5878, 'mubarik': 7439, 'spent': 10812, 'total': 11845, 'congratz': 2272, 'bitches': 1223, 'contribution': 2323, 'yuppiieeehappy': 13157, 'smileyalienthoughthappyalien': 10450, 'crowd': 2503, 'loudest': 6627, 'garydid': 4268, 'particular': 8319, 'attraction': 686, 'date': 2658, 'supprt': 11190, 'zyrophobia': 13205, 'savage': 9849, 'eshop': 3494, 'cleanse': 2073, 'scam': 9872, 'ridden': 9553, 'vyapam': 12471, 'renamedwhat': 9430, 'tongue': 11801, 'sticking': 10970, 'cheeky': 1919, 'playful': 8665, 'blowing': 1297, 'raspberryd': 9244, 'pleased': 8676, 'waves': 12571, 'couch': 2386, 'eight': 3304, 'wondering': 12833, 'dodged': 3028, 'explanation': 3616, 'proving': 9057, 'grow': 4599, 'bag': 835, 'sanza': 9821, 'truman': 11968, 'places': 8637, 'coloured': 2178, 'aggregates': 195, 'tend': 11469, 'yaa': 12975, 'slr': 10428, 'som': 10681, 'honoured': 5093, 'hehehe': 4926, 'view': 12391, 'geary': 4289, 'elsehappy': 3347, 'explorers': 3619, 'bird': 1202, 'wayanadan': 12574, 'forests': 4040, 'smileywayanadphoto': 10562, 'srijith': 10867, 'vvisit': 12469, 'website': 12605, 'whispered': 12709, 'lying': 6745, 'pokemon': 8726, 'flipkartfashionfridayhey': 3963, 'dazzle': 2683, 'urself': 12254, 'double': 3096, 'flared': 3938, 'black': 1242, 'forideas': 4050, 'views': 12395, 'browse': 1522, 'bored': 1376, 'becoming': 1031, 'female': 3828, 'tour': 11853, 'manager': 6862, 'delve': 2782, 'muchhh': 7447, 'tmr': 11745, 'breakfast': 1455, 'holidays': 5061, 'gl': 4403, 'tonights': 11807, 'count': 2396, 'smileyconfusionconfusionhappy': 10480, 'litey': 6504, 'manuella': 6888, 'maine': 6819, 'abhi': 29, 'tak': 11314, 'ye': 13012, 'nhi': 7717, 'dekhi': 2757, 'promos': 9026, 'se': 9936, 'welcoming': 12641, 'xpax': 12958, 'lisa': 6489, 'aboard': 35, 'institution': 5505, 'nc': 7619, 'guy': 4664, 'cheese': 1924, 'overload': 8204, 'pizza': 8630, 'mcfloat': 7010, 'fudge': 4179, 'sandae': 9806, 'munchkins': 7474, 'granny': 4536, 'baller': 865, 'lil': 6454, 'faded': 3693, 'chain': 1839, 'smileywhat': 10563, 'everybody': 3544, 'ought': 8175, 'events': 3535, 'eventsorg': 3536, 'sort': 10739, 'champions': 1855, 'letter': 6397, 'saying': 9864, 'approving': 551, 'unique': 12173, 'affaraid': 158, 'dearslim': 2705, 'role': 9643, 'billy': 1188, 'character': 1880, 'labs': 6212, 'ovh': 8214, 'maxi': 6988, 'bunch': 1588, 'acc': 55, 'sprit': 10858, 'yous': 13114, 'til': 11696, 'severe': 10067, 'hammies': 4756, 'smileyyep': 10568, 'livefreedom': 6512, 'pistolsgood': 8616, 'unlocked': 12187, 'bemeapp': 1084, 'thumbs': 11664, 'beme': 1083, 'figured': 3866, 'bemecode': 1085, 'proudtobeme': 9052, 'round': 9677, 'au': 688, 'calm': 1683, 'kepo': 6005, 'names': 7568, 'dominguez': 3049, 'spence': 10809, 'luckily': 6704, 'clearly': 2077, 'heiyo': 4931, 'dudaftiehappy': 3181, 'smileybreaktym': 10458, 'fatal': 3772, 'dangerous': 2628, 'terms': 11485, 'health': 4882, 'outraged': 8190, 'muna': 7472, 'magstart': 6808, 'smileysalute': 10546, 'thq': 11637, 'continous': 2314, 'jayamravi': 5714, 'thalaivar': 11518, 'brohappy': 1500, 'pt': 9068, 'heiya': 4930, 'grab': 4515, 'manutdnews': 6890, 'av': 714, 'gd': 4285, 'wknd': 12801, 'ear': 3230, 'meconfusion': 7038, 'roll': 9644, 'yday': 13011, 'hxh': 5252, 'besides': 1113, 'vids': 12388, 'badass': 821, 'killua': 6071, 'scenes': 9882, 'suffering': 11112, 'appreciating': 546, 'feeding': 3809, 'jabongatpumaurbanstampede': 5655, 'noticing': 7865, 'efforts': 3290, 'unappreciated': 12132, 'gracious': 4519, 'nailedit': 7549, 'ourdisneyinfinity': 8178, 'mary': 6936, 'jillmill': 5772, 'webcam': 12603, 'teens': 11452, 'elfindelmundo': 3329, 'sexi': 10075, 'mainly': 6820, 'mt': 7426, 'favour': 3785, 'dancetastic': 2623, 'satyajit': 9843, 'rays': 9261, 'porosh': 8772, 'pathor': 8354, 'situationgoldbugs': 10347, 'wine': 12767, 'bottles': 1396, 'spill': 10817, 'jazmin': 5719, 'bonilla': 1348, 'stars': 10909, 'hollywood': 5064, 'rofl': 9636, 'shades': 10097, 'grey': 4571, 'netsec': 7678, 'edition': 3269, 'ne': 7628, 'kev': 6012, 'apology': 523, 'fangirled': 3743, 'cruuzzy': 2511, 'sisters': 10338, 'told': 11775, 'misses': 7248, 'youn': 13104, 'unlisted': 12186, 'hickey': 4979, 'dads': 2593, 'hocking': 5047, 'mamma': 6853, 'human': 5213, 'beings': 1060, 'mere': 7120, 'brainu': 1431, 'holistic': 5062, 'cosmovisiontoo': 2378, 'narrowminded': 7583, 'isnt': 5598, 'charging': 1889, 'cess': 1833, 'alix': 304, 'tweeting': 12032, 'quan': 9149, 'tips': 11719, 'naaahhh': 7532, 'duh': 3188, 'emesh': 3367, 'hilarious': 5001, 'kath': 5964, 'kia': 6036, 'tango': 11354, 'tracerequest': 11870, 'homie': 5078, 'dassy': 2653, 'fwm': 4214, 'selamat': 9979, 'sorelike': 10731, 'model': 7301, 'nichola': 7734, 'malta': 6844, 'ticket': 11682, 'gto': 4630, 'tomorrowland': 11795, 'incall': 5390, 'smileywhy': 10564, 'babyfaceblowingakissfaceblowingakissfaceblowingakiss': 790, 'shobs': 10199, 'incomplete': 5402, 'smileyconfusionfriends': 10483, 'barkada': 916, 'silverstone': 10299, 'pull': 9089, 'asked': 631, 'bookstore': 1361, 'lately': 6286, 'ganna': 4255, 'tmazur': 11742, 'confusionconfusion': 2260, 'hillary': 5005, 'clinton': 2097, 'court': 2418, 'notice': 7863, 'slice': 10408, 'lifeso': 6429, 'hidden': 4980, 'untapped': 12207, 'taking': 11323, 'mca': 7005, 'gettin': 4335, 'hella': 4935, 'wana': 12527, 'bandz': 886, 'asaphappy': 617, 'donington': 3065, 'park': 8311, 'hoping': 5109, 'drives': 3153, 'merci': 7117, 'bien': 1166, 'mon': 7327, 'amie': 379, 'vacationhappy': 12295, 'pitbull': 8618, 'fri': 4110, 'annyeong': 458, 'oppa': 8099, 'joyful': 5843, 'indonesian': 5429, 'elf': 3328, 'mcgee': 7013, 'flight': 3959, 'bf': 1141, 'jennyjean': 5744, 'kikchat': 6052, 'sabadodeganarseguidores': 9742, 'sexysasunday': 10085, 'giveawaydms': 4395, 'marseille': 6926, 'ganda': 4249, 'frontalot': 4150, 'wilsey': 12756, 'director': 2936, 'skypeconfusion': 10383, 'fnaf': 3990, 'steam': 10943, 'mustafa': 7497, 'syedraza': 11277, 'assured': 656, 'current': 2544, 'goin': 4450, 'sweety': 11252, 'hugsx': 5199, 'bubbles': 1544, 'strongest': 11040, 'spots': 10846, 'barnstaplebideford': 922, 'abit': 31, 'road': 9605, 'roblox': 9622, 'credits': 2472, 'rocroglodyysbro': 9630, 'hiring': 5024, 'ree': 9357, 'aspetti': 640, 'boring': 1379, 'chicken': 1949, 'chips': 1972, 'cupboards': 2539, 'empty': 3388, 'jamie': 5684, 'ian': 5269, 'latin': 6294, 'beauties': 1013, 'asian': 626, 'version': 12354, 'fave': 3778, 'vaing': 12299, 'kikgirl': 6053, 'orgasm': 8139, 'phonesex': 8531, 'spacers': 10769, 'felicity': 3823, 'smoak': 10593, 'redheartglassesheartwitharrow': 9352, 'children': 1957, 'psychopaths': 9066, 'spoile': 10834, 'dimples': 2919, 'contemplating': 2305, 'indie': 5416, 'route': 9679, 'jsl': 5851, 'gotcha': 4503, 'kina': 6078, 'donna': 3066, 'smileyso': 10551, 'reachability': 9274, 'bug': 1564, 'therep': 11574, 'tipped': 11717, 'jk': 5783, 'bitter': 1234, 'airs': 246, 'naggy': 7542, 'anal': 402, 'child': 1955, 'phappy': 8510, 'vidcon': 12379, 'anxious': 485, 'shaking': 10108, 'dad': 2590, 'tipping': 11718, 'smoke': 10594, 'butgood': 1622, 'white': 12712, 'grandpa': 4534, 'prolly': 9020, 'backhandindexpointingleftfire': 807, 'yeomans': 13041, 'stashed': 10925, 'closerchasing': 2107, 'specs': 10802, 'chasing': 1900, 'savesso': 9854, 'yesthe': 13058, 'walls': 12523, 'ears': 3240, 'angel': 425, 'mochamichelle': 7296, 'iphne': 5561, 'simply': 10307, 'bi': 1159, 'prob': 8987, 'tamaki': 11342, 'background': 803, 'backgrounds': 804, 'sealed': 9940, 'lips': 6485, 'wearing': 12592, 'braces': 1422, 'tonguetiedlt': 11802, 'maggie': 6794, 'afraid': 165, 'mull': 7463, 'limited': 6465, 'nilhappy': 7766, 'glasgow': 4409, 'netball': 7672, 'thistles': 11612, 'thistlelove': 11611, 'effect': 3284, 'minecraft': 7213, 'peopleits': 8447, 'drew': 3140, 'plshappy': 8695, 'delicious': 2768, 'stadium': 10880, 'muddled': 7452, 'racket': 9187, 'isolating': 5601, 'fas': 3764, 'pune': 9099, 'participate': 8317, 'icecreammaster': 5275, 'group': 4595, 'channels': 1869, 'letitgrow': 6391, 'huhu': 5203, 'shet': 10167, 'desk': 2831, 'oo': 8073, 'orz': 8149, 'stash': 10924, 'problemmmmmmm': 8992, 'jaebumjb': 5670, 'english': 3423, 'yeeaayy': 13021, 'alhamdulillah': 295, 'mosthappy': 7378, 'smilingfacewithsmilingeyes': 10582, 'amin': 382, 'weed': 12615, 'definition': 2752, 'crowdfunding': 2504, 'goal': 4434, 'walk': 12512, 'hellooooooooo': 4939, 'selections': 9983, 'lynne': 6750, 'buffer': 1562, 'button': 1627, 'composer': 2232, 'emails': 3356, 'fridayfun': 4119, 'nonfilipina': 7816, 'ejayster': 3310, 'united': 12175, 'states': 10929, 'le': 6325, 'ie': 5299, 'stan': 10891, 'lee': 6350, 'redheartredheart': 9353, 'discovery': 2961, 'cousin': 2420, 'keplerb': 6004, 'yrs': 13132, 'teleportation': 11459, 'arrive': 600, 'shahid': 10103, 'afridi': 167, 'tou': 11849, 'mahnor': 6814, 'baloch': 869, 'nikki': 7764, 'flowers': 3974, 'blackfly': 1245, 'courgette': 2412, 'affect': 159, 'fruit': 4156, 'italian': 5611, 'netfilx': 7673, 'ma': 6753, 'knees': 6134, 'diva': 2999, 'dst': 3168, 'unmarriedchecks': 12192, 'fingertheres': 3902, 'rock': 9626, 'wiellys': 12741, 'paul': 8369, 'barcode': 910, 'charlotte': 1896, 'rated': 9250, 'thtas': 11660, 'aviation': 722, 'spotters': 10848, 'trailblazerhonors': 11887, 'meganh': 7060, 'murray': 7479, 'liverpool': 6514, 'labour': 6210, 'leader': 6327, 'jeremyleader': 5749, 'jezwecan': 5760, 'timeguess': 11701, 'itmorning': 5626, 'krane': 6169, 'alot': 332, 'thathappy': 11538, 'agayhippiehippy': 188, 'force': 4033, 'exercise': 3583, 'ginger': 4376, 'teach': 11413, 'awareness': 737, 'portsmouth': 8777, 'linking': 6479, 'sonal': 10699, 'hungryhappy': 5230, 'hmmm': 5038, 'collin': 2167, 'pedanthappy': 8418, 'kit': 6108, 'thobut': 11619, 'katkennedy': 5969, 'ack': 82, 'hih': 4993, 'choir': 1981, 'rosidbinr': 9668, 'frown': 4152, 'andry': 418, 'pouting': 8811, 'duke': 3189, 'earl': 3231, 'tau': 11380, 'awak': 731, 'orayt': 8121, 'srj': 10868, 'knw': 6151, 'block': 1278, 'dikha': 2917, 'reh': 9378, 'adolf': 132, 'hitler': 5029, 'obstacles': 7943, 'exist': 3587, 'surrendered': 11206, 'roger': 9638, 'terrific': 11487, 'advaddict': 142, 'jimin': 5774, 'gifted': 4361, 'notanapology': 7853, 'map': 6893, 'googles': 4488, 'informed': 5447, 'dependencies': 2806, 'motherfucking': 7383, 'matters': 6980, 'davids': 2668, 'damn': 2612, 'month': 7341, 'college': 2166, 'steroids': 10961, 'alansmithpart': 275, 'publication': 9078, 'servus': 10048, 'bonasio': 1342, 'doidos': 3039, 'task': 11368, 'delegate': 2762, 'aaahhh': 9, 'jentop': 5746, 'information': 5446, 'virgin': 12416, 'volunteer': 12449, 'nonmapbox': 7817, 'maps': 6896, 'restricted': 9499, 'mapbox': 6894, 'basemaps': 931, 'contractually': 2319, 'speaking': 10793, 'colom': 2170, 'researchers': 9470, 'toes': 11767, 'seafood': 9938, 'weltum': 12649, 'teh': 11456, 'dety': 2849, 'huh': 5201, 'sings': 10320, 'annoying': 455, 'katmtan': 5970, 'swan': 11229, 'fandom': 3740, 'blurry': 1305, 'besok': 1115, 'urgently': 12246, 'within': 12792, 'currently': 2545, 'dorset': 3085, 'goddess': 4443, 'blast': 1259, 'shitfaced': 10190, 'souls': 10753, 'donating': 3055, 'singing': 10316, 'disney': 2977, 'doug': 3098, 'wittysrk': 12796, 'counted': 2398, 'bnte': 1313, 'hain': 4724, 'shiiiitt': 10173, 'case': 1771, 'moods': 7352, 'rm': 9600, 'negooo': 7650, 'male': 6838, 'sister': 10336, 'smileyhave': 10503, 'madeline': 6774, 'nuns': 7913, 'moualdi': 7391, 'mornin': 7364, 'yapsters': 12995, 'tenmiles': 11472, 'ply': 8707, 'copy': 2357, 'icon': 5280, 'alchemists': 284, 'goodhappy': 4470, 'dayz': 2681, 'preview': 8950, 'thug': 11662, 'lmao': 6531, 'sharethelove': 10126, 'petpuk': 8495, 'highvalue': 4991, 'halsey': 4753, 'anniversary': 450, 'folks': 4000, 'bae': 829, 'acts': 97, 'smileyheartwitharrow': 10504, 'replies': 9450, 'ferries': 3837, 'complain': 2222, 'rude': 9703, 'bonding': 1344, 'niggs': 7749, 'smileyconfusionbonakid': 10466, 'readingres': 9283, 'wordoftheweek': 12850, 'wotw': 12891, 'est': 3504, 'earn': 3238, 'sleepthe': 10402, 'whatevs': 12682, 'jess': 5753, 'surry': 11208, 'botany': 1390, 'awwwww': 762, 'gel': 4297, 'alison': 302, 'lsa': 6685, 'response': 9488, 'fron': 4148, 'debbie': 2715, 'carol': 1757, 'patients': 8359, 'discharge': 2955, 'lounge': 6642, 'walmart': 12524, 'mahiyaa': 6812, 'jasmine': 5701, 'heena': 4924, 'balance': 856, 'studies': 11057, 'hayley': 4857, 'shoulder': 10231, 'pads': 8240, 'mounted': 7393, 'inquisitor': 5476, 'smileycosplay': 10490, 'cosplayprogress': 2380, 'mike': 7182, 'hundal': 5221, 'dunno': 3201, 'housing': 5158, 'insecurity': 5480, 'nhs': 7719, 'devolution': 2860, 'patriotism': 8363, 'halla': 4747, 'ark': 586, 'jiyeons': 5782, 'buzz': 1640, 'videoburnt': 12381, 'mist': 7255, 'opi': 8094, 'avoplex': 726, 'nail': 7548, 'cuticle': 2564, 'replenishing': 9448, 'ml': 7277, 'heels': 4923, 'elbows': 3319, 'serious': 10035, 'submission': 11086, 'youve': 13130, 'lb': 6321, 'cherish': 1939, 'flipping': 3966, 'learnt': 6338, 'backflip': 802, 'djumpgiants': 3010, 'foampit': 3992, 'usa': 12258, 'pamer': 8271, 'thks': 11614, 'actuallythough': 103, 'arts': 611, 'crafts': 2446, 'session': 10050, 'mehtab': 7068, 'aunty': 698, 'landed': 6250, 'gc': 4284, 'yeeew': 13025, 'pre': 8902, 'lan': 6248, 'yeey': 13026, 'strangely': 11011, 'arrange': 596, 'doodle': 3075, 'star': 10902, 'comic': 2194, 'summoner': 11141, 'none': 7814, 'persongesturingno': 8477, 'lycra': 6742, 'vincent': 12402, 'couldnt': 2392, 'roy': 9685, 'bg': 1146, 'imgcircle': 5346, 'crops': 2498, 'headshelp': 4880, 'font': 4019, 'dthanks': 3170, 'deathofgrass': 2710, 'loan': 6543, 'lawnmower': 6312, 'popularor': 8765, 'charismatic': 1890, 'manhe': 6872, 'thrive': 11648, 'economy': 3258, 'doesn': 3030, 'bursts': 1611, 'georgie': 4324, 'million': 7199, 'fls': 3978, 'kindest': 6083, 'sometime': 10691, 'iceland': 5276, 'crazy': 2459, 'landscapes': 6254, 'yok': 13085, 'lah': 6234, 'concordia': 2243, 'visited': 12425, 'reunited': 9518, 'xxxibmchll': 12969, 'sea': 9937, 'prettier': 8943, 'imitatia': 5348, 'michelle': 7166, 'comeback': 2186, 'gross': 4591, 'treat': 11917, 'equal': 3472, 'injustice': 5467, 'feminism': 3829, 'ineedfeminismbecause': 5431, 'forgotten': 4049, 'recommend': 9329, 'redhead': 9348, 'wacky': 12477, 'rather': 9251, 'bests': 1125, 'worsts': 12885, 'toowaytoliveahappylife': 11821, 'hoxton': 5165, 'holborn': 5054, 'tournament': 11856, 'equalityact': 3473, 'nohope': 7804, 'karen': 5946, 'wagging': 12484, 'bum': 1581, 'wwoooo': 12935, 'nite': 7777, 'drawings': 3126, 'barrett': 923, 'laiten': 6240, 'arond': 593, 'considering': 2285, 'mature': 6982, 'childrens': 1958, 'journeyps': 5838, 'foam': 3991, 'ladys': 6225, 'mob': 7289, 'riverdd': 9591, 'false': 3721, 'bulletin': 1577, 'folk': 3999, 'spring': 10856, 'fiesta': 3857, 'noise': 7806, 'awuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuu': 757, 'aich': 225, 'sept': 10026, 'rudramadevi': 9704, 'anushka': 483, 'gunashekar': 4652, 'harryxhood': 4818, 'upsetting': 12237, 'ooh': 8075, 'humanist': 5214, 'reader': 9280, 'magazine': 6791, 'username': 12268, 'rape': 9233, 'csrracing': 2524, 'lack': 6216, 'hygiene': 5255, 'tose': 11842, 'clothes': 2112, 'temperature': 11465, 'lives': 6515, 'idols': 5295, 'planet': 8643, 'braving': 1445, 'ryan': 9731, 'tidy': 11687, 'hagergang': 4694, 'tshirt': 11977, 'chanhun': 1867, 'individual': 5423, 'photoshoot': 8542, 'afterall': 171, 'sadkaay': 9752, 'tharkness': 11537, 'peak': 8408, 'orestboy': 8133, 'heatwave': 4911, 'lowered': 6678, 'standards': 10894, 'recruited': 9340, 'doom': 3076, 'nasty': 7589, 'affiliated': 160, 'gthappy': 4626, 'situated': 10345, 'hall': 4746, 'ted': 11445, 'pixgram': 8629, 'creative': 2470, 'slideshow': 10409, 'download': 3104, 'unamusedfacenewmoonface': 12129, 'tentatively': 11476, 'nibbling': 7727, 'ivyi': 5642, 'muchmore': 7450, 'sho': 10198, 'superpowers': 11177, 'obsessed': 7941, 'oth': 8161, 'third': 11601, 'growingheart': 4601, 'thanked': 11520, 'smileyngarepfollbackdarinabilahjkt': 10530, 'applies': 539, 'sunglasses': 11149, 'jackie': 5660, 'sunnies': 11153, 'style': 11071, 'jlo': 5785, 'jlovers': 5786, 'turkey': 12009, 'goodafternoon': 4466, 'collage': 2155, 'treats': 11919, 'furry': 4205, 'bruce': 1525, 'kunoriforceo': 6187, 'bhavya': 1153, 'aayegi': 20, 'timming': 11707, 'wiw': 12798, 'bips': 1200, 'zareen': 13169, 'daisy': 2602, 'bcoz': 982, 'karte': 5953, 'mak': 6825, 'lega': 6357, 'branding': 1439, 'smileypsygustokita': 10540, 'spag': 10772, 'boat': 1318, 'outboarding': 8179, 'toowink': 11823, 'spell': 10807, 'reboardinghappy': 9308, 'firingoffboarding': 3912, 'chetwynd': 1943, 'sn': 10603, 'dg': 2866, 'signs': 10292, 'trades': 11877, 'bnf': 1311, 'tonighttheres': 11808, 'jasonamprob': 5704, 'feb': 3805, 'tourconfusion': 11854, 'victoriasecret': 12375, 'finland': 3906, 'helsinki': 4951, 'airport': 243, 'plane': 8641, 'beyond': 1139, 'onting': 8068, 'tiis': 11695, 'lng': 6539, 'yan': 12992, 'ull': 12107, 'steve': 10963, 'bell': 1072, 'prescott': 8927, 'leadership': 6328, 'cartoon': 1769, 'upside': 12238, 'statementhappy': 10928, 'smileyselamathariraya': 10547, 'running': 9719, 'lovesummertime': 6667, 'dumont': 3196, 'videoconfusion': 12382, 'jax': 5711, 'jones': 5821, 'awesomeee': 742, 'geoff': 4320, 'update': 12215, 'kelly': 5997, 'amazingly': 364, 'talanted': 11328, 'brings': 1488, 'droppedlooks': 3158, 'confusionconfusionconfusionconfusionconfusion': 2261, 'sitting': 10344, 'vsco': 12461, 'thankies': 11523, 'wweddingblog': 12934, 'ltbtw': 6690, 'hash': 4823, 'tag': 11305, 'difimeetanalien': 2907, 'yaaay': 12980, 'bff': 1142, 'players': 8663, 'section': 9956, 'follbaaack': 4001, 'cools': 2349, 'az': 774, 'cauliflower': 1795, 'attempt': 680, 'prinsesa': 8968, 'yaaah': 12979, 'families': 3725, 'law': 6309, 'toy': 11867, 'sonaaaaaaa': 10698, 'beautifull': 1016, 'ribs': 9543, 'josephines': 5831, 'mirror': 7237, 'cretaperfectme': 2480, 'cretaperfectsuv': 2481, 'creta': 2479, 'loads': 6542, 'hearts': 4900, 'telecoms': 11458, 'judy': 5864, 'superb': 11166, 'slightly': 10412, 'rakna': 9209, 'ew': 3561, 'whose': 12725, 'fifa': 3858, 'survive': 11211, 'px': 9133, 'dishoom': 2974, 'rajnigandha': 9206, 'minju': 7224, 'rapper': 9235, 'lead': 6326, 'vocal': 12439, 'yujin': 13140, 'visual': 12430, 'maknae': 6835, 'jane': 5688, 'general': 4306, 'hah': 4695, 'hawk': 4853, 'greatest': 4553, 'history': 5027, 'along': 331, 'talkback': 11333, 'process': 8998, 'listing': 6499, 'feature': 3802, 'mostly': 7379, 'cinemas': 2034, 'roads': 9606, 'shops': 10215, 'faceblowingakissfaceblowingakiss': 3660, 'defend': 2742, 'fashion': 3766, 'atrocity': 675, 'pandimensional': 8280, 'manifestations': 6875, 'argos': 577, 'ring': 9568, 'smileynad': 10528, 'kimikoo': 6074, 'plezzzzloudlycryingface': 8687, 'hungry': 5229, 'asthma': 657, 'inhaler': 5455, 'breathe': 1461, 'smileysmilingfacewithsmilingeyes': 10550, 'goodluck': 4472, 'hunger': 5227, 'mockingjay': 7299, 'teasers': 11437, 'thehungergames': 11551, 'adored': 136, 'reina': 9384, 'smileysorry': 10552, 'sounding': 10757, 'felt': 3826, 'crying': 2514, 'blogged': 1285, 'excuse': 3578, 'attenders': 682, 'mvhr': 7510, 'whn': 12717, 'dhappy': 2871, 'andre': 413, 'mamayang': 6851, 'rinku': 9570, 'powys': 8886, 'shropshire': 10255, 'border': 1373, 'cameron': 1694, 'schools': 9893, 'san': 9804, 'diego': 2893, 'jump': 5883, 'source': 10762, 'appeasement': 534, 'aj': 255, 'situation': 10346, 'action': 91, 'grunt': 4611, 'favouriting': 3788, 'sc': 9870, 'antichrist': 479, 'ju': 5854, 'lindseyy': 6470, 'reply': 9451, 'osullivan': 8157, 'halfway': 4744, 'ex': 3563, 'postive': 8794, 'opinions': 8096, 'avi': 721, 'lovelies': 6655, 'dare': 2639, 'corridor': 2372, 'peoplewithbunnyearspartying': 8450, 'hmmmm': 5040, 'ollymnh': 8031, 'neither': 7658, 'partypopperpartypopperpartypopper': 8332, 'rundown': 9717, 'yah': 12987, 'leviboard': 6400, 'kleperb': 6127, 'gtdlt': 4619, 'proves': 9055, 'impeccable': 5361, 'ridabot': 9551, 'setokido': 10055, 'shoulda': 10229, 'hippo': 5021, 'ones': 8060, 'materialistic': 6970, 'showpo': 10250, 'coughcough': 2388, 'kairoline': 5918, 'fshoppe': 4164, 'backed': 801, 'thankshappy': 11526, 'smileysmilingface': 10549, 'makesme': 6831, 'bytes': 1646, 'santorini': 9819, 'escape': 3490, 'beatport': 1008, 'oncomingfistlightskintone': 8054, 'trmdhesitant': 11950, 'manuel': 6887, 'valls': 12306, 'king': 6088, 'seven': 10064, 'kingdoms': 6090, 'andals': 408, 'taught': 11381, 'hide': 4981, 'privacy': 8979, 'wise': 12785, 'ways': 12579, 'natsuki': 7603, 'often': 7982, 'writes': 12910, 'catchy': 1786, 'coughed': 2389, 'neil': 7656, 'emirates': 3370, 'brill': 1482, 'urquhart': 12252, 'castle': 1780, 'helps': 4950, 'simple': 10306, 'generally': 4307, 'glass': 4410, 'shatter': 10136, 'contrast': 2321, 'educampakl': 3276, 'leaving': 6344, 'rotorua': 9674, 'pehly': 8429, 'phir': 8524, 'somi': 10694, 'burfday': 1599, 'smileymy': 10527, 'solved': 10679, 'university': 12180, 'santo': 9818, 'tomas': 11781, 'swati': 11232, 'worked': 12854, 'norhing': 7843, 'dialogue': 2879, 'lines': 6473, 'chainsaw': 1841, 'shake': 10106, 'amusement': 397, 'awe': 739, 'daily': 2600, 'protect': 9048, 'pop': 8756, 'ish': 5586, 'fahad': 3697, 'bhai': 1148, 'iqrar': 5566, 'waseem': 12548, 'abroad': 38, 'rotation': 9673, 'survived': 11212, 'moviee': 7402, 'chef': 1927, 'grogolconfusion': 4590, 'sejeng': 9977, 'singles': 10319, 'dating': 2660, 'longdistance': 6584, 'rhys': 9540, 'pwrfl': 9132, 'department': 2803, 'benefit': 1094, 'elses': 3348, 'soo': 10710, 'enterprison': 3441, 'schoolsoutforsummer': 9894, 'fellow': 3825, 'juggle': 5866, 'purrthos': 9116, 'cathos': 1788, 'catamis': 1783, 'fourfiveseconds': 4073, 'deaf': 2699, 'drugs': 3164, 'alcohol': 285, 'apexis': 514, 'meth': 7142, 'champagne': 1854, 'fc': 3795, 'streamer': 11018, 'juices': 5869, 'flowing': 3975, 'facewithtearsofjoyfacewithtearsofjoythank': 3680, 'correct': 2371, 'portrait': 8776, 'izumi': 5649, 'fugiwara': 4180, 'clonmel': 2102, 'refreshing': 9364, 'vibrant': 12367, 'estimated': 3508, 'server': 10042, 'quiet': 9168, 'yey': 13061, 'cenatic': 1814, 'inshaallah': 5483, 'wil': 12749, 'dourthe': 3101, 'trendno': 11925, 'smileyakshaymostlovedsuperstarever': 10449, 'indirecting': 5422, 'askurban': 636, 'lyka': 6747, 'helped': 4943, 'includes': 5398, 'mobilegames': 7293, 'ios': 5558, 'nap': 7577, 'aff': 157, 'uname': 12127, 'jonginuh': 5825, 'billie': 1183, 'arm': 587, 'forecast': 4037, 'soothing': 10724, 'fantasy': 3753, 'vii': 12396, 'sweetheart': 11249, 'krsna': 6173, 'shoes': 10205, 'sooooooo': 10719, 'freaking': 4091, 'original': 8143, 'zayn': 13171, 'grant': 4538, 'fucker': 4173, 'dies': 2897, 'pet': 8488, 'illustration': 5333, 'wohooyou': 12815, 'gleam': 4413, 'painting': 8251, 'deal': 2700, 'stops': 10998, 'raining': 9198, 'prime': 8963, 'minister': 7223, 'org': 8134, 'sunjam': 11150, 'cough': 2387, 'industry': 5430, 'present': 8931, 'practicing': 8893, 'proactive': 8986, 'environment': 3458, 'nearlyconfusion': 7634, 'unreal': 12201, 'boysawesome': 1415, 'zaine': 13164, 'zac': 13160, 'isaac': 5581, 'smileyoss': 10533, 'remembered': 9419, 'frank': 4085, 'iero': 5300, 'phase': 8513, 'david': 2667, 'beginner': 1048, 'shining': 10176, 'sunflowers': 11147, 'sunny': 11154, 'favourites': 3787, 'tommarow': 11790, 'ranked': 9229, 'birthdaymonth': 1212, 'vianey': 12362, 'aka': 260, 'bffconfusion': 1143, 'bffs': 1144, 'julyth': 5877, 'birthdaygirl': 1210, 'andrew': 415, 'keys': 6017, 'otwol': 8169, 'awhile': 750, 'gotwolgrandtrailer': 4508, 'alltime': 323, 'julia': 5873, 'robert': 9615, 'awwhh': 759, 'bulldog': 1576, 'unfortunate': 12164, 'born': 1380, 'caring': 1750, 'fightstickfriday': 3864, 'uzumce': 12292, 'eat': 3250, 'extravagant': 3631, 'extravaganthappy': 3632, 'smileyconfusionw': 10487, 'tearout': 11432, 'selektion': 9984, 'yoot': 13090, 'fingers': 3899, 'crossed': 2500, 'deserved': 2824, 'buzzing': 1641, 'gudday': 4634, 'dave': 2666, 'haileyhelps': 4723, 'nug': 7905, 'floydian': 3977, 'eid': 3301, 'mubarak': 7438, 'brotheeeeerrrr': 1512, 'adventure': 145, 'tokyo': 11774, 'kansai': 5932, 'follbackhappy': 4003, 'lifevideo': 6432, 'uppe': 12232, 'om': 8036, 'minuter': 7233, 'detailed': 2843, 'data': 2657, 'soooo': 10717, 'jesus': 5757, 'amsterdam': 395, 'perform': 8460, 'drinks': 3148, 'nextweek': 7703, 'sends': 10009, 'booty': 1372, 'bcuz': 985, 'step': 10953, 'option': 8113, 'stable': 10877, 'sturdy': 11069, 'lukkkee': 6716, 'heheok': 4928, 'daam': 2588, 'nowsee': 7884, 'againensoi': 185, 'daykeep': 2675, 'smiles': 10446, 'tc': 11404, 'madam': 6771, 'siddi': 10273, 'unknown': 12182, 'roomie': 9656, 'gn': 4430, 'ltlt': 6692, 'articles': 607, 'gf': 4339, 'gives': 4400, 'consent': 2281, 'mister': 7258, 'supportive': 11185, 'vine': 12403, 'vineswith': 12407, 'peyton': 8500, 'enjoying': 3429, 'nagato': 7539, 'yukichan': 13142, 'shoushitsu': 10234, 'blocked': 1280, 'archdbanterbury': 566, 'experttradesmen': 3611, 'banter': 899, 'quiz': 9174, 'tradetalk': 11878, 'floofs': 3970, 'muahah': 7435, 'anticipation': 480, 'jds': 5725, 'laro': 6274, 'tayo': 11394, 'answers': 471, 'ht': 5181, 'smileyangelica': 10454, 'anghel': 432, 'confusionconfusionotwolgrandtrailer': 2262, 'aa': 0, 'kkk': 6123, 'regan': 9368, 'plays': 8669, 'macbook': 6762, 'rehearse': 9381, 'youthcelebrate': 13119, 'muted': 7502, 'july': 5876, 'gohf': 4448, 'invited': 5554, 'smileybelfast': 10456, 'vegetarian': 12332, 'shell': 10158, 'steven': 10964, 'gooday': 4468, 'answer': 468, 'door': 3079, 'tells': 11464, 'oshieer': 8155, 'smileyrealreviews': 10541, 'happycustomers': 4794, 'realoshi': 9299, 'dealsuthaonotebachao': 2701, 'bigger': 1170, 'dime': 2918, 'uhuh': 12102, 'musicalnotemusicalnotemusicalnotemusicalnote': 7487, 'code': 2138, 'pleasant': 8673, 'onboard': 8053, 'raheel': 9192, 'flyhigh': 3983, 'phones': 8530, 'bothered': 1393, 'everette': 3540, 'taylor': 11392, 'smileywsalelove': 10567, 'peachyloans': 8407, 'fridayfreebie': 4118, 'noeli': 7799, 'noe': 7798, 'tattoos': 11379, 'yisss': 13072, 'bindingofissac': 1190, 'xboxone': 12940, 'console': 2288, 'justin': 5901, 'gladly': 4407, 'son': 10696, 'morocco': 7372, 'peru': 8482, 'nxt': 7926, 'bps': 1417, 'resort': 9479, 'havuuuuuloveyouheartsuituuuuuuuuhappy': 4849, 'possitve': 8787, 'hopeyou': 5107, 'sweetie': 11250, 'xxxx': 12970, 'throwbackfriday': 11653, 'christen': 2002, 'ki': 6035, 'yaad': 12981, 'gayi': 4280, 'opossum': 8098, 'bushes': 1615, 'belated': 1064, 'yeahh': 13016, 'kuffar': 6184, 'computers': 2234, 'cell': 1812, 'spread': 10851, 'diarrhea': 2884, 'immigrant': 5355, 'lice': 6417, 'goictived': 4449, 'tagsforlikes': 11308, 'trapmusic': 11910, 'hotmusicdelocos': 5142, 'wellgot': 12644, 'planed': 8642, 'weekendthanks': 12623, 'kinickers': 6093, 'lifehappy': 6427, 'smileyyou': 10569, 'shady': 10101, 'management': 6861, 'reservation': 9472, 'tkts': 11737, 'likewise': 6452, 'nightenjoy': 7751, 'overgeneralization': 8203, 'ikr': 5324, 'smilingfacewithhearteyes': 10576, 'consumerism': 2296, 'soul': 10750, 'swimming': 11261, 'riding': 9559, 'recently': 9317, 'fics': 3852, 'ouch': 8173, 'slip': 10413, 'disc': 2953, 'todaybring': 11755, 'thw': 11675, 'chutes': 2021, 'chalut': 1853, 'minute': 7232, 'replay': 9447, 'iplayer': 5563, 'unneeded': 12193, 'megamoh': 7058, 'power': 8882, 'tools': 11816, 'zealand': 13182, 'lavro': 6308, 'wakes': 12501, 'pile': 8582, 'dump': 3197, 'couscous': 2419, 'links': 6480, 'womens': 12826, 'fiction': 3853, 'wahahaah': 12487, 'ynggk': 13078, 'orhan': 8140, 'pamuk': 8276, 'hero': 4964, 'canopy': 1719, 'maple': 6895, 'leaves': 6342, 'syrup': 11287, 'farm': 3761, 'stephanie': 10955, 'sparklingheartsparklinghearthappy': 10785, 'congrtaualtions': 2273, 'noticed': 7864, 'phileas': 8518, 'club': 2114, 'inc': 5388, 'photographs': 8536, 'phonegraphs': 8529, 'srsly': 10870, 'pwincess': 9131, 'ripaaaaa': 9572, 'foodie': 4022, 'banate': 877, 'uhb': 12099, 'ray': 9258, 'deptnot': 2813, 'hospital': 5126, 'grt': 4608, 'features': 3803, 'infographic': 5444, 'oclock': 7954, 'habit': 4688, 'thankyoudfor': 11531, 'roadtrip': 9607, 'ifc': 5301, 'bombs': 1339, 'whip': 12707, 'lilsisbro': 6458, 'preordered': 8921, 'pixars': 8624, 'steelbook': 10947, 'hmm': 5037, 'thishope': 11610, 'bufetowa': 1560, 'pegell': 8428, 'lemess': 6373, 'nconfusion': 7622, 'kyle': 6198, 'paypal': 8383, 'confirmation': 2253, 'oct': 7955, 'tud': 11990, 'jst': 5852, 'addictive': 114, 'humphrey': 5218, 'yell': 13032, 'erm': 3482, 'breach': 1450, 'superheroes': 11173, 'scififan': 9898, 'lawson': 6313, 'lemons': 6375, 'yogurt': 13083, 'pot': 8797, 'discovered': 2960, 'liquorice': 6488, 'pud': 9082, 'salute': 9789, 'moving': 7406, 'cajun': 1661, 'spiced': 10813, 'yum': 13144, 'smileycajunchicken': 10461, 'enadavex': 3398, 'congratulation': 2270, 'infinite': 5433, 'fighting': 3862, 'bildideen': 1181, 'gern': 4327, 'deluxe': 2781, 'cikaaasmilingfacewish': 2030, 'ohappy': 7985, 'maaf': 6756, 'telat': 11457, 'ngucapinnya': 7714, 'maaaaaay': 6755, 'dayweek': 2680, 'viparita': 12412, 'karani': 5941, 'legsupthewallconfusion': 6367, 'unwind': 12212, 'robbed': 9612, 'coco': 2134, 'comfy': 2193, 'jalulu': 5680, 'continue': 2315, 'rosh': 9667, 'travels': 11916, 'glaconfusion': 4405, 'airportconfusion': 244, 'avail': 715, 'dwanna': 3219, 'presents': 8934, 'suit': 11122, 'pallavi': 8268, 'rizzy': 9594, 'nairobi': 7552, 'hrdstellobama': 5172, 'regional': 9372, 'civil': 2047, 'society': 10650, 'rights': 9563, 'region': 9371, 'globewe': 4420, 'hajur': 4736, 'yayy': 13007, 'mustve': 7501, 'nerve': 7670, 'prelims': 8914, 'exam': 3565, 'costacc': 2382, 'nwb': 7925, 'walking': 12516, 'shud': 10256, 'begin': 1047, 'wisconsin': 12783, 'cold': 2147, 'lasted': 6282, 'tatiii': 11375, 'hmu': 5042, 'picked': 8559, 'cala': 1667, 'brush': 1532, 'ego': 3293, 'wherever': 12701, 'sooooooooo': 10722, 'interaction': 5522, 'offers': 7972, 'dongsaeng': 3062, 'chorong': 1994, 'friendship': 4133, 'ffs': 3850, 'impressivewhat': 5370, 'dragon': 3117, 'duck': 3178, 'mix': 7266, 'cheetah': 1926, 'wagga': 12483, 'coursework': 2417, 'lorna': 6610, 'scan': 9874, 'smileyeye': 10495, 'canvas': 1724, 'padpainting': 8239, 'iqbal': 5565, 'ima': 5340, 'hon': 5082, 'furniture': 4204, 'grinningface': 4582, 'budha': 1557, 'aja': 256, 'besi': 1111, 'chati': 1904, 'phulani': 8548, 'swasa': 11231, 'bahari': 839, 'jiba': 5769, 'gujarat': 4649, 'distrubed': 2993, 'otherwise': 8163, 'cr': 2438, 'inspite': 5494, 'highest': 4987, 'holder': 5056, 'threatens': 11642, 'basis': 936, 'vr': 12459, 'pamper': 8273, 'cuts': 2568, 'angelo': 428, 'quezon': 9162, 'cityconfusion': 2045, 'sweatpants': 11239, 'breath': 1460, 'bond': 1343, 'tripping': 11946, 'farbridges': 3758, 'segalakatakata': 9975, 'nixus': 7780, 'begun': 1051, 'flint': 3960, 'shortcake': 10218, 'separately': 10025, 'attacked': 678, 'criticised': 2493, 'gesture': 4330, 'pedal': 8417, 'stroke': 11035, 'attentive': 684, 'osoro': 8156, 'caro': 1756, 'youhappy': 13101, 'deposit': 2810, 'secure': 9957, 'shock': 10200, 'spilled': 10818, 'coffe': 2140, 'caketenerina': 1666, 'auguri': 695, 'iso': 5599, 'certification': 1832, 'itme': 5625, 'paralyzed': 8302, 'anxiety': 484, 'sadnessme': 9756, 'kai': 5916, 'direction': 2934, 'advocaten': 152, 'itd': 5616, 'blogs': 1289, 'development': 2855, 'spain': 10773, 'tests': 11493, 'def': 2740, 'bantime': 900, 'failban': 3701, 'awkward': 751, 'abs': 40, 'laughing': 6299, 'galing': 4231, 'founder': 4070, 'loveyaaaah': 6671, 'specialist': 10797, 'breathing': 1462, 'aw': 728, 'babyyy': 797, 'voice': 12443, 'amazes': 360, 'djstruthmate': 3009, 'recap': 9312, 'featuring': 3804, 'urock': 12251, 'siwonie': 10348, 'flickr': 3957, 'tack': 11295, 'zephbot': 13187, 'hhahahahaha': 4974, 'blew': 1273, 'upp': 12230, 'entire': 3445, 'las': 6277, 'vegas': 12329, 'strip': 11031, 'youuuuuuu': 13129, 'hahahahahhaha': 4709, 'callies': 1678, 'puppy': 9106, 'owner': 8221, 'callinganimalabusehotlineasap': 1680, 'gorefiend': 4497, 'mythic': 7526, 'reminder': 9421, 'blacksmallsquarebea': 1247, 'miller': 7195, 'lockscreenblacksmallsquarertfav': 6551, 'itmbf': 5624, 'smileykeesh': 10514, 'yesterdays': 13057, 'groupie': 4596, 'bebe': 1023, 'sizams': 10351, 'kurta': 6190, 'color': 2172, 'invoices': 5555, 'kanina': 5931, 'pong': 8742, 'umaga': 12115, 'browser': 1523, 'typically': 12073, 'pleasse': 8684, 'leeteuk': 6353, 'oppamiss': 8101, 'pearl': 8411, 'thusi': 11674, 'pouring': 8809, 'milk': 7191, 'tgv': 11509, 'austerlitz': 704, 'blois': 1290, 'miles': 7187, 'chateau': 1903, 'des': 2818, 'marais': 6899, 'taxi': 11387, 'curry': 2546, 'noms': 7811, 'enji': 3426, 'haters': 4836, 'purchase': 9108, 'speciallymarked': 10798, 'custard': 2557, 'sms': 10599, 'onpack': 8065, 'instructions': 5507, 'tiles': 11697, 'downstairs': 3108, 'nibbles': 7726, 'greek': 4559, 'ruins': 9709, 'petra': 8496, 'shadowplaylouis': 10099, 'mutual': 7505, 'cuz': 2572, 'liveonstreamate': 6513, 'lani': 6263, 'grazing': 4550, 'pride': 8961, 'bristolart': 1491, 'sleeping': 10396, 'inapp': 5384, 'ensure': 3435, 'item': 5617, 'screw': 9924, 'amber': 366, 'popculturefan': 8758, 'steps': 10958, 'hpc': 5169, 'smileyconfusionwip': 10488, 'sws': 11271, 'newsround': 7698, 'hounds': 5149, 'ada': 106, 'racist': 9186, 'called': 1677, 'hulk': 5209, 'tight': 11694, 'prayers': 8898, 'pardon': 8305, 'phl': 8525, 'abu': 47, 'dhabi': 2868, 'blessing': 1271, 'hihihi': 4994, 'teamjanuaryclaims': 11422, 'godonna': 4444, 'msg': 7419, 'bowwowchicawowwow': 1407, 'settle': 10059, 'air': 235, 'dkt': 3011, 'porch': 8768, 'uber': 12079, 'mobile': 7291, 'application': 538, 'familyhappy': 3728, 'smileysending': 10548, 'giggles': 4366, 'smilingfacewithopenmouth': 10578, 'buns': 1593, 'delights': 2771, 'bare': 911, 'winds': 12766, 'kahlil': 5915, 'gibran': 4354, 'flash': 3940, 'boon': 1364, 'stiff': 10971, 'upper': 12233, 'lip': 6484, 'britain': 1492, 'smileylatmon': 10515, 'thatjust': 11540, 'endeavour': 3405, 'anne': 449, 'joy': 5841, 'experiences': 3606, 'os': 8150, 'exploits': 3617, 'ign': 5310, 'pubcast': 9075, 'tengaman': 11470, 'celebratio': 1809, 'letsprocrastinate': 6396, 'jh': 5763, 'heuertz': 4970, 'determined': 2847, 'women': 12825, 'cycling': 2578, 'install': 5500, 'glorify': 4421, 'infirmities': 5436, 'realizing': 9296, 'silly': 10298, 'realize': 9294, 'suave': 11075, 'gentlemen': 4316, 'monthly': 7342, 'mileage': 7186, 'target': 11364, 'samsung': 9803, 'quality': 9147, 'ey': 3637, 'beth': 1130, 'gangster': 4254, 'somethingi': 10690, 'athenas': 664, 'fancy': 3738, 'nowhappy': 7878, 'wellington': 12646, 'momenti': 7318, 'rich': 9545, 'tina': 11710, 'peeled': 8422, 'christina': 2004, 'newsletter': 7696, 'zy': 13204, 'olur': 8034, 'shreds': 10254, 'flawless': 3948, 'remix': 9424, 'reactions': 9278, 'draw': 3124, 'fanbase': 3735, 'vanda': 12312, 'hayli': 4858, 'edwin': 3280, 'elvena': 3350, 'emc': 3361, 'rubber': 9700, 'ducks': 3179, 'swearwords': 11235, 'infection': 5432, 'msr': 7425, 'wrote': 12917, 'deadthank': 2698, 'christophe': 2007, 'gans': 4256, 'films': 3880, 'brotherhood': 1514, 'wolf': 12817, 'pills': 8587, 'nocturnal': 7796, 'rrp': 9691, 'jah': 5671, 'wobble': 12811, 'retard': 9507, 'notifications': 7867, 'checkup': 1917, 'puns': 9101, 'elite': 3336, 'camillus': 1695, 'pleaseee': 8677, 'hand': 4764, 'spare': 10780, 'tyre': 12076, 'boot': 1370, 'realised': 9288, 'joking': 5815, 'msgs': 7421, 'ahahah': 211, 'shame': 10112, 'abandoning': 23, 'disagree': 2945, 'nowhere': 7879, 'contradict': 2320, 'continuously': 2317, 'chaos': 1873, 'contained': 2302, 'cranium': 2450, 'sneakers': 10616, 'nike': 7759, 'nikeoriginal': 7761, 'nikeindonesia': 7760, 'pierojogger': 8576, 'skoy': 10375, 'winter': 12778, 'falklands': 3714, 'jamielee': 5685, 'escn': 3491, 'congraaats': 2265, 'hooh': 5097, 'chrome': 2010, 'remah': 9412, 'morningand': 7366, 'stormsgtgt': 11004, 'thunderstorms': 11668, 'circuscircus': 2040, 'omgg': 8045, 'thankie': 11522, 'tdy': 11409, 'peter': 8491, 'powers': 8885, 'expelled': 3602, 'boughy': 1400, 'kernel': 6007, 'slepthappy': 10407, 'paralysis': 8301, 'lizahappy': 6523, 'smileyfaceblowingakiss': 10496, 'mocha': 7295, 'lolhook': 6564, 'vampire': 12310, 'diaries': 2883, 'muchbest': 7444, 'tequila': 11479, 'twice': 12040, 'thanq': 11534, 'goodwill': 4480, 'vandr': 12314, 'ash': 619, 'meister': 7071, 'debatable': 2712, 'tourney': 11858, 'seed': 9962, 'solar': 10669, 'shown': 10249, 'ek': 3311, 'rivera': 9590, 'tacos': 11298, 'mexico': 7148, 'taco': 11297, 'viva': 12432, 'mxico': 7516, 'burger': 1600, 'fardo': 3759, 'thebestangkapuso': 11547, 'siscon': 10334, 'lighter': 6437, 'tooth': 11819, 'bacon': 818, 'patient': 8357, 'korean': 6165, 'netizen': 7677, 'crueler': 2507, 'clerance': 2080, 'elephants': 3326, 'marula': 6934, 'tdif': 11408, 'shoutouts': 10238, 'shortly': 10222, 'itsamarvelthingmarvel': 5632, 'japans': 5697, 'homework': 5076, 'marco': 6903, 'fruits': 4157, 'herbs': 4958, 'parks': 8313, 'self': 9988, 'esteem': 3507, 'xiii': 12951, 'patience': 8356, 'sobtian': 10641, 'donutbethers': 3072, 'coworker': 2430, 'minutes': 7234, 'deathly': 2709, 'hallows': 4749, 'supernatural': 11176, 'consultanthappy': 2294, 'himachal': 5008, 'universe': 12179, 'ashamed': 620, 'charge': 1884, 'inform': 5445, 'downloading': 3107, 'wheredoistart': 12698, 'moviemarathon': 7403, 'conversational': 2331, 'skills': 10367, 'shadows': 10100, 'owned': 8220, 'pair': 8252, 'typical': 12072, 'itll': 5623, 'cortezsuperstars': 2374, 'haigh': 4719, 'llp': 6530, 'consult': 2293, 'tthanks': 11984, 'colin': 2152, 'internships': 5534, 'luxuous': 6735, 'hotels': 5139, 'tarryn': 11366, 'humans': 5215, 'goodness': 4475, 'hbdme': 4864, 'aoki': 504, 'yeeeeyyy': 13024, 'barsostay': 925, 'malese': 6840, 'blakey': 1252, 'responses': 9489, 'independent': 5409, 'sums': 11142, 'debacle': 2711, 'perfectly': 8459, 'longer': 6586, 'themhappy': 11557, 'amyjackson': 399, 'omegle': 8041, 'countrymusic': 2403, 'five': 3927, 'freddys': 4093, 'demo': 2788, 'pumped': 9094, 'fanboy': 3736, 'thegrandad': 11550, 'impression': 5369, 'meenakshi': 7049, 'grand': 4528, 'sidni': 10279, 'remarriage': 9414, 'occasion': 7948, 'completion': 2229, 'quotes': 9176, 'languageshappy': 6262, 'java': 5707, 'phps': 8546, 'notion': 7869, 'references': 9359, 'equally': 3474, 'confusing': 2258, 'edge': 3263, 'ohioans': 7991, 'stick': 10968, 'hourhappy': 5151, 'doctor': 3024, 'herehappy': 4959, 'doctorhappy': 3026, 'ambre': 368, 'suzan': 11224, 'offline': 7978, 'thesims': 11581, 'guides': 4642, 'covered': 2426, 'madonnamademtv': 6783, 'cmon': 2117, 'madonnafamily': 6781, 'madonnafans': 6782, 'beverly': 1137, 'martin': 6932, 'mb': 6997, 'meaningless': 7028, 'common': 2206, 'celebrate': 1806, 'fails': 3703, 'paint': 8249, 'muertosatfringe': 7454, 'gtagiveaway': 4616, 'emulation': 3393, 'brought': 1517, 'omeglegrinningfacewithsmilingeyesyou': 8042, 'sharapova': 10122, 'broes': 1498, 'enemies': 3413, 'ithappy': 5621, 'relax': 9396, 'ou': 8171, 'painted': 8250, 'nails': 7551, 'pink': 8596, 'sarahmae': 9826, 'cc': 1800, 'cares': 1747, 'meoooowwwwwolf': 7111, 'barkkkkkkkiiideee': 920, 'bark': 915, 'routine': 9681, 'aleks': 287, 'awh': 747, 'upgrading': 12224, 'kumpul': 6186, 'cantik': 1723, 'ganteng': 4257, 'kresna': 6170, 'jelly': 5736, 'simon': 10304, 'sonsconfusion': 10708, 'lesley': 6383, 'davies': 2669, 'blood': 1293, 'panties': 8289, 'lion': 6482, 'artworkbylie': 613, 'judo': 5863, 'mornings': 7370, 'presentation': 8932, 'daredevil': 2640, 'episodes': 3470, 'despondently': 2836, 'rewatch': 9530, 'smileyconfusionhappy': 10484, 'welcomahave': 12636, 'favor': 3780, 'tridon': 11937, 'master': 6958, 'nim': 7767, 'therere': 11575, 'kebun': 5986, 'adorable': 134, 'ubud': 12082, 'ladyposse': 6224, 'xoxoxo': 12957, 'sneak': 10615, 'peek': 8421, 'items': 5618, 'collectionstay': 2165, 'inbox': 5387, 'orders': 8129, 'prices': 8959, 'bn': 1309, 'happyweekend': 4797, 'therealgolden': 11570, 'girlfriendsmya': 4386, 'londoncity': 6574, 'pplhappy': 8889, 'posting': 8793, 'closest': 2109, 'njoy': 7784, 'followingg': 4011, 'private': 8980, 'pusher': 9119, 'pri': 8955, 'freak': 4088, 'stunning': 11066, 'wooohooo': 12845, 'cuss': 2556, 'teenage': 11449, 'ace': 73, 'sauce': 9844, 'livi': 6519, 'fowles': 4074, 'oliviafowles': 8028, 'burnout': 1608, 'djohnforceo': 3007, 'blush': 1306, 'tgirl': 11506, 'matthew': 6981, 'provoking': 9058, 'indiankulture': 5413, 'oppose': 8106, 'babesa': 787, 'biker': 1175, 'dis': 2942, 'lyk': 6746, 'gud': 4633, 'weight': 12631, 'bcus': 984, 'rubbish': 9701, 'veggie': 12333, 'learned': 6336, 'steph': 10954, 'nj': 7781, 'allowing': 319, 'exploring': 3620, 'einstein': 3306, 'banana': 875, 'listenablecohesive': 6495, 'russell': 9725, 'bannon': 898, 'sourceso': 10763, 'gossip': 4501, 'alex': 291, 'heswifi': 4969, 'wub': 12929, 'cerbchan': 1825, 'jarraaaaa': 5700, 'morrrrning': 7373, 'snoozed': 10626, 'clickscothanks': 2086, 'gay': 4278, 'lesbian': 6382, 'rigid': 9565, 'theocratic': 11565, 'wing': 12770, 'fundamentalist': 4196, 'islamist': 5594, 'brianaaaa': 1468, 'brianazabrocki': 1469, 'sky': 10378, 'novak': 7872, 'fantastichappy': 3752, 'batb': 945, 'clap': 2054, 'whilst': 12705, 'hugging': 5195, 'aki': 264, 'thencerest': 11561, 'indiemusic': 5418, 'mahzo': 6816, 'sexyjudy': 10084, 'pussy': 9122, 'sexo': 10076, 'humidity': 5216, 'promotional': 9031, 'cakei': 1664, 'seemed': 9970, 'sloppy': 10416, 'seconds': 9952, 'stock': 10978, 'marmite': 6920, 'ecki': 3257, 'nic': 7728, 'taft': 11304, 'finalists': 3883, 'lottery': 6625, 'awards': 735, 'usagi': 12260, 'loooooove': 6599, 'wowwww': 12899, 'redheartblueheartgreenhearttwoheartsredheartblueheartgreenhearttwohearts': 9350, 'lepas': 6381, 'sembuh': 10003, 'sibuk': 10269, 'balik': 861, 'kins': 6097, 'gotham': 4505, 'sunnyday': 11155, 'texting': 11498, 'dudesdudettes': 3185, 'costs': 2383, 'flippin': 3965, 'fortune': 4059, 'divinediscontentthe': 3002, 'amnotness': 387, 'autofollow': 710, 'teamfollowback': 11420, 'geer': 4294, 'bat': 943, 'mz': 7528, 'yang': 12993, 'deennya': 2735, 'jehwan': 5735, 'julie': 5875, 'thanksyou': 11529, 'calumsos': 1686, 'lukesos': 6715, 'ashtonsos': 623, 'chelny': 1931, 'datz': 2662, 'jeremy': 5748, 'fmt': 3988, 'dat': 2656, 'heartbeat': 4893, 'voxy': 12458, 'clutching': 2115, 'turtle': 12016, 'sons': 10707, 'andree': 414, 'angels': 430, 'confused': 2256, 'besteverdoctorwhoepisode': 1119, 'relevant': 9407, 'smileyconfusionconfusionconfusionconfusionconfusionconfusionconfusionconfusionconfusion': 10474, 'puke': 9087, 'proper': 9037, 'subliminal': 11085, 'messages': 7131, 'eatmeat': 3252, 'brewproject': 1465, 'lovenafianna': 6658, 'lewis': 6402, 'clock': 2100, 'happens': 4785, 'coughing': 2390, 'muslim': 7492, 'prophets': 9041, 'ishe': 5588, 'mistake': 7256, 'understood': 12147, 'stophappy': 10995, 'politicians': 8736, 'argue': 578, 'intellect': 5515, 'recommendation': 9330, 'shiva': 10196, 'joyfulmusic': 5844, 'mp': 7409, 'apps': 553, 'summers': 11139, 'standrews': 10898, 'fish': 3917, 'sandcastle': 9808, 'ewok': 3562, 'nate': 7592, 'brawl': 1447, 'rear': 9301, 'naked': 7559, 'choke': 1982, 'coffees': 2142, 'heck': 4916, 'gun': 4651, 'associate': 653, 'guns': 4657, 'um': 12114, 'endowmentsi': 3410, 'responseshappy': 9490, 'options': 8114, 'sinister': 10322, 'ai': 224, 'sikandar': 10294, 'pti': 9071, 'standwdik': 10900, 'westandwithik': 12660, 'starbucks': 10903, 'logo': 6559, 'stands': 10899, 'obsession': 7942, 'addiction': 113, 'renew': 9431, 'lose': 6613, 'charity': 1891, 'min': 7205, 'hokies': 5052, 'biz': 1238, 'posted': 8790, 'recruiting': 9341, 'non': 7812, 'america': 374, 'california': 1674, 'mistgameplay': 7259, 'iloveyou': 5334, 'vex': 12359, 'designed': 2827, 'igers': 5308, 'smileyleicaq': 10517, 'leica': 6370, 'dudeeeeeee': 3183, 'lists': 6500, 'persona': 8473, 'yepp': 13043, 'muchsmilingfacewithsmilingeyessmilingfacewithsmilingeyes': 7451, 'greg': 4566, 'useful': 12265, 'posey': 8780, 'miami': 7159, 'jamesyammouni': 5683, 'jakubowski': 5678, 'sparks': 10786, 'hollands': 5063, 'brando': 1440, 'breakdown': 1454, 'materials': 6971, 'thorin': 11626, 'hunt': 5233, 'mummy': 7470, 'kitney': 6113, 'choroo': 1995, 'nahi': 7546, 'musings': 7491, 'aztec': 775, 'princess': 8966, 'rainy': 9201, 'kingfisher': 6091, 'overtones': 8209, 'relaxed': 9397, 'facesor': 3667, 'chinua': 1968, 'achebe': 76, 'intellectual': 5516, 'issues': 5608, 'liquid': 6487, 'melbournetrip': 7079, 'taxikitchen': 11388, 'nooow': 7839, 'published': 9080, 'ebook': 3256, 'selfpublishers': 9994, 'study': 11060, 'mcdo': 7007, 'everywhere': 3555, 'dreamer': 3130, 'tanishanonly': 11355, 'kindled': 6085, 'flame': 3936, 'convictions': 2334, 'bar': 905, 'reasons': 9304, 'repath': 9439, 'smileyrepath': 10544, 'adisconfusion': 127, 'stefanie': 10950, 'ms': 7416, 'lightbox': 6436, 'ran': 9218, 'incorrect': 5405, 'spelling': 10808, 'apologist': 520, 'vuly': 12467, 'emmy': 3373, 'khusro': 6034, 'batman': 950, 'pearson': 8413, 'reputation': 9461, 'nikkei': 7763, 'hmmmask': 5039, 'woodford': 12840, 'cousins': 2421, 'smileyredheartpurpleheartheartwitharrowkissmark': 10543, 'vscocam': 12462, 'vscoph': 12465, 'vscogood': 12464, 'vscophil': 12466, 'vscocousins': 12463, 'yaap': 12983, 'urwelc': 12256, 'neon': 7668, 'pants': 8290, 'haaa': 4684, 'turner': 12013, 'willing': 12752, 'auspost': 703, 'failed': 3702, 'openfollow': 8090, 'rp': 9688, 'eng': 3416, 'yjcosplay': 13073, 'cosplayers': 2379, 'luxembourg': 6733, 'bunnys': 1592, 'broadcast': 1495, 'needa': 7641, 'gals': 4233, 'bend': 1091, 'heaven': 4912, 'proposal': 9042, 'scores': 9908, 'goals': 4435, 'january': 5694, 'hanabutle': 4762, 'kikhorny': 6054, 'interracial': 5535, 'makeup': 6832, 'chu': 2015, 'weekends': 12621, 'punting': 9103, 'horseracing': 5122, 'horses': 5124, 'betting': 1134, 'horseracingtips': 5123, 'soulful': 10751, 'guitarsoul': 4648, 'cocoared': 2135, 'tyldesley': 12065, 'salut': 9788, 'brief': 1473, 'introduction': 5543, 'earliest': 3233, 'subcontinent': 11078, 'bfr': 1145, 'mauryashappy': 6985, 'jordanian': 5827, 'tenyai': 11477, 'hee': 4918, 'ss': 10873, 'semi': 10005, 'atp': 673, 'wimbledon': 12758, 'federer': 3806, 'nadal': 7534, 'monfilsmost': 7331, 'handsome': 4772, 'cilic': 2031, 'nisas': 7775, 'firm': 3913, 'diary': 2885, 'potentially': 8803, 'itsesha': 5633, 'chillin': 1961, 'lils': 6457, 'tails': 11311, 'kittens': 6117, 'actuallyfacewithtearsofjoygrinningfacewithsmilingeyesand': 102, 'assassin': 642, 'garret': 4266, 'baz': 963, 'pumps': 9097, 'leo': 6379, 'xst': 12963, 'standard': 10893, 'centrifugal': 1820, 'haired': 4727, 'eternity': 3514, 'forgive': 4046, 'kangin': 5929, 'kristin': 6172, 'backhandindexpointingleftgrowingheart': 808, 'cass': 1776, 'surajettan': 11194, 'kashi': 5955, 'ashwathy': 624, 'mommy': 7323, 'yessss': 13053, 'tirth': 11725, 'brambhatt': 1435, 'snooker': 10625, 'compensation': 2219, 'theoper': 11566, 'premiostumundo': 8916, 'beg': 1045, 'differ': 2901, 'philosophical': 8522, 'backwow': 817, 'graphic': 4539, 'level': 6399, 'smileylearn': 10516, 'thurs': 11670, 'aug': 693, 'excl': 3575, 'augconfusion': 694, 'tidying': 11688, 'raw': 9256, 'weenie': 12628, 'annoyingbaby': 456, 'included': 5397, 'lazy': 6319, 'cosy': 2384, 'clientamendseditfinalfinalfinalpdfthat': 2088, 'smileymorning': 10526, 'mauliate': 6984, 'ito': 5629, 'clicking': 2085, 'okkay': 8011, 'knock': 6139, 'soloists': 10674, 'ryu': 9734, 'saera': 9759, 'verify': 12348, 'pinkeu': 8597, 'angry': 435, 'animationconfusion': 440, 'aqui': 558, 'screencaps': 9920, 'jonghyun': 5824, 'seungyeon': 10063, 'cnblue': 2118, 'mbc': 7001, 'wgmpri': 12668, 'masa': 6938, 'entrepreneurship': 3451, 'empowered': 3386, 'limpopo': 6468, 'naito': 7553, 'picts': 8565, 'norapowel': 7842, 'hornykik': 5113, 'livesex': 6516, 'walked': 12513, 'pumpkins': 9096, 'smiled': 10445, 'looked': 6592, 'wot': 12889, 'downloadconfusion': 3105, 'tanks': 11357, 'reserve': 9474, 'thrice': 11646, 'wilts': 12757, 'patron': 8364, 'venture': 12340, 'deathcure': 2708, 'bottom': 1397, 'boob': 1351, 'blame': 1253, 'weekhappy': 12624, 'dine': 2921, 'modern': 7303, 'weatherly': 12599, 'bits': 1231, 'letting': 6398, 'grill': 4576, 'disk': 2975, 'nt': 7897, 'iirc': 5318, 'dalso': 2605, 'ux': 12289, 'refinements': 9360, 'zdps': 13181, 'justice': 5899, 'adventures': 146, 'daw': 2670, 'tine': 11712, 'gensan': 4315, 'ordered': 8127, 'frightlings': 4138, 'undead': 12139, 'plushes': 8705, 'cushions': 2554, 'xxxxxx': 12972, 'nba': 7617, 'mypark': 7523, 'chronicles': 2013, 'gryph': 4612, 'volume': 12448, 'illiamtan': 5331, 'decision': 2725, 'favorable': 3781, 'sides': 10277, 'ellen': 3339, 'degeneres': 2753, 'showconfusion': 10243, 'shirts': 10187, 'mints': 7230, 'rocking': 9628, 'superdry': 11171, 'berangkaat': 1105, 'lagiii': 6231, 'receive': 9313, 'siguro': 10293, 'un': 12124, 'kesa': 6009, 'lotsa': 6624, 'organisation': 8136, 'offices': 7974, 'andi': 412, 'mrdiddy': 7413, 'fingerscrossed': 3900, 'deep': 2736, 'htaccess': 5182, 'file': 3872, 'elbro': 3320, 'adf': 124, 'womad': 12822, 'gran': 4527, 'canaria': 1707, 'gig': 4364, 'twist': 12045, 'defined': 2748, 'teamnatural': 11425, 'huni': 5231, 'mafai': 6788, 'yayayayayits': 13005, 'yt': 13135, 'convention': 2329, 'barely': 913, 'computer': 2233, 'brighton': 1479, 'slayed': 10391, 'nickname': 7739, 'babygirl': 791, 'regarding': 9369, 'requests': 9465, 'momentsideas': 7321, 'serioushimmat': 10037, 'karain': 5940, 'baat': 782, 'smileyu': 10559, 'meri': 7122, 'debates': 2714, 'hoteemy': 5136, 'unclesfacewithtearsofjoy': 12135, 'pronounce': 9035, 'native': 7602, 'american': 375, 'proverb': 9054, 'lovable': 6645, 'yesha': 13049, 'montoya': 7346, 'eagerly': 3228, 'saves': 9853, 'jade': 5666, 'payment': 8380, 'processed': 8999, 'supreme': 11191, 'leon': 6380, 'statter': 10933, 'kojimaen': 6159, 'ks': 6177, 'concertd': 2241, 'randybis': 9225, 'physique': 8553, 'shaved': 10141, 'uncut': 12137, 'boi': 1331, 'cheapest': 1908, 'printing': 8973, 'regular': 9376, 'printer': 8971, 'countries': 2401, 'nz': 7933, 'printers': 8972, 'hiding': 4983, 'large': 6272, 'format': 4053, 'everyonehappy': 3548, 'replying': 9452, 'senior': 10012, 'raid': 9193, 'conserve': 2282, 'battery': 953, 'comfortable': 2191, 'swt': 11272, 'youtheres': 13120, 'rail': 9194, 'fridayd': 4114, 'reservationseu': 9473, 'localgaragederby': 6545, 'campus': 1702, 'subgames': 11080, 'faceit': 3664, 'snpcaht': 10630, 'hakhakhak': 4737, 'tt': 11982, 'kyungsoos': 6204, 'animated': 439, 'viewed': 12392, 'property': 9040, 'agent': 192, 'accurate': 71, 'description': 2822, 'theory': 11568, 'clear': 2074, 'surrender': 11205, 'marine': 6908, 'underwater': 12148, 'ocean': 7951, 'yvette': 13158, 'author': 708, 'mwf': 7515, 'programme': 9012, 'taal': 11291, 'lake': 6241, 'lovehappy': 6653, 'emt': 3390, 'scurri': 9933, 'agile': 197, 'shipping': 10182, 'solution': 10677, 'sme': 10436, 'omark': 8039, 'omar': 8038, 'biggest': 1171, 'losers': 6615, 'kamaal': 5924, 'amm': 385, 'busyearly': 1621, 'collectionhappy': 2164, 'hopehousekids': 5103, 'pitmantraining': 8620, 'walkersmithway': 12514, 'keepitlocal': 5994, 'sehun': 9976, 'seleaders': 9981, 'rossaholic': 9671, 'uneventful': 12151, 'sofa': 10659, 'surfing': 11197, 'cunt': 2537, 'unfollow': 12157, 'convos': 2338, 'rescoops': 9468, 'multiracial': 7466, 'couples': 2409, 'fk': 3934, 'narrow': 7582, 'minded': 7209, 'assholes': 647, 'freakeh': 4089, 'warlock': 12539, 'dyeah': 3221, 'faithful': 3708, 'balloon': 867, 'gtmj': 4629, 'hurts': 5240, 'workout': 12866, 'madison': 6779, 'supposed': 11188, 'beonknockknock': 1103, 'toda': 11753, 'caughthappy': 1794, 'congraduation': 2267, 'gents': 4317, 'bitchface': 1225, 'limit': 6464, 'unamusedface': 12128, 'organic': 8135, 'everyonewere': 3550, 'york': 13091, 'nearest': 7632, 'lendal': 6376, 'memberspikami': 7088, 'captured': 1727, 'fulton': 4191, 'sheen': 10148, 'baloney': 870, 'unvarnished': 12211, 'lie': 6421, 'laid': 6238, 'thick': 11590, 'blarney': 1258, 'flattery': 3943, 'thin': 11592, 'sachin': 9746, 'unimportant': 12172, 'context': 2312, 'dampen': 2617, 'excitement': 3573, 'emma': 3372, 'yu': 13138, 'comparing': 2217, 'bellybuttons': 1079, 'rocket': 9627, 'raspberry': 9243, 'narendra': 7581, 'modi': 7304, 'aaaaaand': 4, 'teams': 11428, 'macauley': 6760, 'however': 5162, 'wheeeeeeeeeeeeeeeen': 12692, 'teamlh': 11424, 'heechul': 4919, 'toast': 11749, 'coffeeweekdays': 2143, 'kyohei': 6201, 'island': 5595, 'sail': 9766, 'faster': 3769, 'commercial': 2200, 'insurance': 5510, 'requirements': 9467, 'lookfortheo': 6593, 'cl': 2049, 'thou': 11630, 'homehappy': 5073, 'april': 554, 'flying': 3984, 'airforce': 240, 'clark': 2058, 'field': 3854, 'pampanga': 8272, 'festivalhappy': 3842, 'hotairballoon': 5132, 'yepproud': 13044, 'trollhappy': 11951, 'highvoltageyour': 4992, 'rocks': 9629, 'clappinghands': 2056, 'brows': 1521, 'oily': 7996, 'skinconfusion': 10369, 'maricarljanah': 6906, 'becauseyour': 1027, 'navy': 7608, 'battlefield': 956, 'degrees': 2756, 'fahrenheit': 3698, 'tia': 11680, 'cocktailglassfriedshrimpcocktailglassfriedshrimpfriedshrimpcocktailglassfriedshrimpcocktailglass': 2130, 'relaxinghappy': 9401, 'stool': 10990, 'topple': 11830, 'findyourfit': 3893, 'skaro': 10358, 'preferred': 8908, 'term': 11483, 'whomosexual': 12721, 'packed': 8234, 'stack': 10878, 'pandora': 8281, 'rings': 9569, 'attend': 681, 'digitalexeter': 2911, 'digitalmarketing': 2912, 'sociamedia': 10649, 'andy': 419, 'nb': 7616, 'bom': 1336, 'dia': 2877, 'todos': 11765, 'forklift': 4051, 'warehouse': 12536, 'workers': 12856, 'lsceens': 6686, 'extras': 3630, 'immature': 5352, 'gandhi': 4250, 'grassy': 4542, 'oliviajadexo': 8029, 'feetblog': 3820, 'infrared': 5449, 'heating': 4910, 'scribble': 9926, 'daughteryrs': 2664, 'oldporridge': 8021, 'fiend': 3856, 'boys': 1414, 'comp': 2212, 'vikings': 12397, 'tblast': 11400, 'np': 7889, 'havingi': 4848, 'taxes': 11386, 'bills': 1187, 'feets': 3821, 'oooooooooohh': 8079, 'petjam': 8494, 'toying': 11868, 'virtual': 12418, 'pounce': 8808, 'signing': 10291, 'benteke': 1100, 'agnes': 198, 'socialmediacouk': 10646, 'fruity': 4158, 'vodkas': 12442, 'sellyourcarinwords': 9999, 'chaloniklosellyourcarinwords': 1852, 'chaloniklo': 1851, 'pictwittercomjxzlbvo': 8568, 'kindle': 6084, 'paperwhites': 8297, 'laserlike': 6278, 'focus': 3993, 'ghost': 4349, 'tagsforlikesapp': 11309, 'instagood': 5497, 'tbt': 11403, 'trending': 11924, 'trueyou': 11966, 'smileynew': 10529, 'socket': 10651, 'spannersebay': 10779, 'patiently': 8358, 'sleepingface': 10397, 'pglcsgo': 8504, 'pearlyyy': 8412, 'favoriteotherwisei': 3783, 'crave': 2456, 'slower': 10425, 'sjw': 10357, 'cryd': 2513, 'cakehamper': 1663, 'baskets': 939, 'hated': 4834, 'glowing': 4424, 'yayyyy': 13008, 'mercedes': 7114, 'hood': 5094, 'badge': 823, 'hosted': 5129, 'drone': 3155, 'ignore': 5311, 'retaliate': 9506, 'bollinger': 1334, 'smileyplay': 10536, 'wheres': 12700, 'planningtiming': 8649, 'denmark': 2797, 'bushraaaaaa': 1616, 'whitey': 12715, 'culture': 2533, 'coursee': 2415, 'smileycheers': 10463, 'completed': 2227, 'intro': 5540, 'smileyexcited': 10494, 'graphicsgraphicdesign': 4540, 'videographer': 12384, 'youtuber': 13124, 'mistakes': 7257, 'space': 10768, 'fill': 3874, 'teds': 11446, 'bogus': 1328, 'hahahaaahowly': 4702, 'outmomma': 8188, 'afternon': 173, 'henry': 4956, 'whangarei': 12679, 'katie': 5967, 'nicholas': 7735, 'pauline': 8370, 'traffickers': 11882, 'daring': 2642, 'worse': 12883, 'hence': 4954, 'expression': 3625, 'maltby': 6845, 'handlettering': 4768, 'roof': 9654, 'tops': 11831, 'ease': 3245, 'sour': 10761, 'dough': 3099, 'bharathi': 1150, 'pirates': 8609, 'egypt': 3294, 'explains': 3615, 'hubby': 5188, 'mixed': 7269, 'sakin': 9772, 'six': 10349, 'christmas': 2006, 'wiganer': 12747, 'brave': 1444, 'avril': 727, 'njs': 7785, 'prosecco': 9047, 'pech': 8416, 'micro': 7167, 'bikinis': 1178, 'catspjsconfusion': 1792, 'gal': 4229, 'ahp': 222, 'lazyweekend': 6320, 'yuki': 13141, 'overdue': 8202, 'mice': 7162, 'womandancingwomandancingwomandancingsa': 12824, 'joys': 5845, 'lay': 6314, 'jurassic': 5895, 'twohearts': 12053, 'ding': 2923, 'nila': 7765, 'wave': 12570, 'lawbreaker': 6310, 'smileyhot': 10506, 'pairing': 8254, 'unfortunately': 12165, 'smell': 10437, 'cookie': 2342, 'shir': 10184, 'hale': 4740, 'cheshire': 1941, 'shots': 10226, 'decorating': 2729, 'lem': 6372, 'recs': 9342, 'ingat': 5453, 'din': 2920, 'mono': 7335, 'kathryn': 5966, 'jr': 5849, 'developed': 2853, 'properties': 9039, 'hsr': 5180, 'base': 929, 'major': 6824, 'sugarrush': 11115, 'knitting': 6136, 'earsconfusion': 3241, 'partly': 8325, 'binge': 1192, 'homegirl': 5072, 'nancy': 7573, 'fenja': 3832, 'chorri': 1996, 'aapke': 16, 'benchmarks': 1090, 'ke': 5983, 'hisaab': 5025, 'ho': 5045, 'gaya': 4279, 'islife': 5597, 'ofc': 7963, 'influenced': 5438, 'rtss': 9698, 'hwaiting': 5249, 'behiri': 1058, 'titanfall': 11728, 'xbox': 12939, 'saturdaythen': 9841, 'ultimate': 12110, 'experiment': 3608, 'gastronomy': 4271, 'newblogpost': 7687, 'foodiefridaysfoodie': 4023, 'yoghurt': 13082, 'pancakes': 8278, 'thechase': 11549, 'sabah': 9743, 'kapima': 5936, 'gelen': 4298, 'guzel': 4669, 'bir': 1201, 'hediye': 4917, 'thanx': 11535, 'revolvingheartsfaceblowingakiss': 9528, 'visas': 12420, 'parisa': 8310, 'epiphany': 3467, 'ndia': 7627, 'lit': 6501, 'emcon': 3363, 'swore': 11270, 'yaaaah': 12978, 'defenders': 2743, 'retailers': 9504, 'kianweareproud': 6038, 'distracted': 2989, 'dayofarch': 2676, 'admin': 129, 'bapu': 904, 'ivypowel': 5643, 'newmusic': 7693, 'sexchat': 10073, 'titles': 11731, 'tomato': 11782, 'pathways': 8355, 'balkan': 862, 'gypsy': 4679, 'mayhem': 6993, 'burek': 1598, 'meatconfusion': 7034, 'gibanica': 4353, 'pieconfusion': 8572, 'likely': 6447, 'surrey': 11207, 'afterwards': 176, 'breads': 1452, 'newchange': 7688, 'bleh': 1266, 'temporal': 11466, 'void': 12445, 'stem': 10952, 'sf': 10087, 'ykr': 13075, 'sparkymm': 10787, 'grsrockfishing': 4607, 'topwaternew': 11834, 'twitlonger': 12049, 'meso': 7127, 'jummah': 5879, 'allrecite': 321, 'durood': 3210, 'pak': 8255, 'canthank': 1722, 'lessons': 6388, 'lasts': 6284, 'cjradacomateada': 2048, 'suprised': 11192, 'debut': 2719, 'bebravetoday': 1026, 'fandoms': 3741, 'shippers': 10181, 'aside': 628, 'differences': 2903, 'housemates': 5155, 'pbbbigatingconcert': 8386, 'jedzjabka': 5731, 'pijjabka': 8581, 'polish': 8733, 'cider': 2027, 'chapman': 1876, 'mustread': 7500, 'cricket': 2484, 'state': 10926, 'origin': 8142, 'queries': 9156, 'abby': 25, 'sumedh': 11135, 'sunnah': 11152, 'familyconfusion': 3727, 'quad': 9143, 'biking': 1176, 'carrie': 1763, 'propriety': 9045, 'chronic': 2012, 'illness': 5332, 'coated': 2126, 'superday': 11170, 'chocolatey': 1978, 'yasu': 13001, 'oooooh': 8078, 'fucked': 4172, 'hallo': 4748, 'improvement': 5372, 'dylan': 3224, 'laurasharks': 6306, 'laura': 6305, 'patrice': 8362, 'keepin': 5992, 'alwayshappy': 352, 'mohr': 7310, 'wguest': 12669, 'oneal': 8057, 'rising': 9582, 'award': 734, 'tks': 11736, 'luas': 6698, 'stones': 10989, 'lbs': 6323, 'targets': 11365, 'quicker': 9165, 'heavy': 4914, 'diet': 2899, 'addy': 123, 'sosweet': 10747, 'nominiere': 7808, 'foxus': 4075, 'aquila': 559, 'und': 12138, 'hardcore': 4803, 'smileyconfusionrelievedface': 10485, 'ltffspecialacha': 6691, 'banda': 880, 'victoryhand': 12377, 'bhi': 1155, 'krta': 6174, 'haihandsome': 4721, 'facewithtearsofjoymust': 3681, 'beautifullycrafted': 1017, 'mockingbird': 7298, 'diploma': 2928, 'blend': 1267, 'benefits': 1095, 'numbero': 7911, 'lolz': 6571, 'ambrose': 369, 'gwinett': 4673, 'bierce': 1167, 'suffered': 11110, 'ravages': 9253, 'illadvised': 5330, 'marriages': 6922, 'virginity': 12417, 'staring': 10907, 'cynical': 2580, 'joke': 5811, 'yahuda': 12989, 'nosmet': 7851, 'pony': 8743, 'cuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuute': 2571, 'fing': 3897, 'vacant': 12293, 'smileywith': 10566, 'hauc': 4844, 'lovessss': 6666, 'hissing': 5026, 'overnight': 8206, 'cornish': 2367, 'allclear': 307, 'rainmr': 9199, 'runs': 9721, 'milesmrs': 7188, 'complains': 2224, 'raincoat': 9197, 'wethappy': 12662, 'measure': 7032, 'mans': 6882, 'wealth': 12587, 'invested': 5547, 'garbii': 4261, 'wimbledonreally': 12759, 'washed': 12549, 'refuel': 9365, 'dunedin': 3199, 'karvonen': 5954, 'kalle': 5923, 'wsalelovea': 12921, 'rakhi': 9208, 'photographer': 8535, 'carry': 1765, 'dragic': 3116, 'projects': 9018, 'represents': 9460, 'sloveniahappy': 10423, 'slovenia': 10422, 'fridge': 4125, 'cats': 1791, 'ludlow': 6710, 'selway': 10001, 'submitted': 11087, 'spanish': 10778, 'greet': 4563, 'subjects': 11084, 'raynexxx': 9260, 'oitnb': 7997, 'weekendsunshinerain': 12622, 'wind': 12762, 'prepared': 8924, 'conditions': 2246, 'msged': 7420, 'chiquitos': 1973, 'ohaha': 7984, 'delhi': 2767, 'webtogsawards': 12607, 'graces': 4518, 'sheffield': 10156, 'tramlines': 11896, 'tl': 11738, 'hacked': 4691, 'lads': 6221, 'beeeeepin': 1035, 'hilarioussuper': 5002, 'duper': 3206, 'awesomehappy': 743, 'facewithstuckouttonguewinkingeye': 3673, 'handle': 4766, 'critiquesthats': 2495, 'contectually': 2304, 'beliebers': 1070, 'winkingface': 12774, 'grinningfacewithsmilingeyes': 4586, 'ultor': 12112, 'mamaya': 6850, 'loiyals': 6560, 'para': 8298, 'smileytruthfulwordsof': 10558, 'beanatividadnknkkpagpapakumbaba': 997, 'smileybirthdaypresent': 10457, 'compliment': 2231, 'swerve': 11255, 'irrelephant': 5576, 'ifwhen': 5305, 'goodtime': 4479, 'scared': 9875, 'tryna': 11974, 'anonymous': 462, 'ryanhamilton': 9732, 'dipsatch': 2931, 'aunt': 697, 'dagga': 2598, 'wonders': 12834, 'trucker': 11962, 'burketeer': 1603, 'suspects': 11216, 'ids': 5298, 'smileystarted': 10553, 'linkhappy': 6478, 'promo': 9025, 'required': 9466, 'twine': 12042, 'dianes': 2881, 'sayhappybirthday': 9862, 'thanksss': 11527, 'randomly': 9221, 'booked': 1355, 'buckinghampalace': 1547, 'personality': 8476, 'chibi': 1946, 'maker': 6829, 'timog': 11708, 'otw': 8168, 'kami': 5925, 'feelinggood': 3815, 'demand': 2785, 'workno': 12865, 'naman': 7562, 'smileytara': 10554, 'barkin': 918, 'blends': 1268, 'beers': 1037, 'toohappy': 11814, 'yeap': 13018, 'onkey': 8062, 'editconfusion': 3265, 'umma': 12120, 'pervert': 8484, 'onyu': 8072, 'appa': 526, 'lucy': 6709, 'horriblesorry': 5117, 'monthshappy': 7345, 'builds': 1571, 'quantum': 9150, 'greater': 4552, 'blockchain': 1279, 'nowplaying': 7883, 'loftey': 6553, 'routte': 9682, 'assiamain': 648, 'mixconfusionone': 7268, 'jointit': 5809, 'futurereleases': 4210, 'scary': 9879, 'murder': 7477, 'mystery': 7524, 'commas': 2197, 'mrjs': 7414, 'hunny': 5232, 'emily': 3368, 'fani': 3744, 'lifeyoure': 6433, 'beautifulhappy': 1015, 'nathan': 7595, 'pete': 8490, 'meditation': 7044, 'puts': 9127, 'alumni': 348, 'mba': 6998, 'representatives': 9458, 'foto': 4064, 'whatisyourfashionality': 12683, 'lorenangel': 6607, 'kw': 6194, 'boatbound': 1319, 'luxury': 6736, 'yachting': 12985, 'masses': 6954, 'tellanoldjokeday': 11462, 'contributions': 2324, 'reqd': 9463, 'frees': 4098, 'speculationand': 10804, 'allows': 320, 'consistency': 2287, 'startup': 10918, 'tropics': 11956, 'startupph': 10919, 'based': 930, 'zodiac': 13193, 'rapunzel': 9237, 'imaginative': 5343, 'therver': 11580, 'bestoftheday': 1124, 'oralsex': 8118, 'bibboo': 1163, 'carly': 1752, 'happily': 4788, 'contract': 2318, 'ends': 3411, 'gtkw': 4627, 'matsubouzu': 6975, 'horse': 5121, 'sonic': 10704, 'startdgoodmorning': 10912, 'videogames': 12383, 'harana': 4800, 'sing': 10313, 'belfast': 1066, 'danny': 2635, 'rare': 9238, 'sponsorship': 10839, 'aswell': 659, 'gigi': 4367, 'pond': 8741, 'nick': 7737, 'austin': 705, 'weak': 12584, 'girlfriend': 4383, 'bravo': 1446, 'sayed': 9861, 'paste': 8345, 'iamamonster': 5267, 'rxthedailysurveyvotes': 9729, 'broke': 1501, 'asshappy': 645, 'roux': 9683, 'tweeted': 12029, 'walkin': 12515, 'audience': 689, 'pfb': 8501, 'shay': 10144, 'jute': 5903, 'walangmakakapigilsakin': 12507, 'lori': 6608, 'ehm': 3299, 'trick': 11935, 'baekhyun': 833, 'eyesmiles': 3646, 'borrow': 1382, 'knives': 6137, 'acha': 74, 'thek': 11553, 'widely': 12739, 'eventually': 3537, 'returning': 9510, 'reaapearing': 9271, 'yesbut': 13047, 'kno': 6138, 'whet': 12702, 'grattis': 4547, 'shorter': 10219, 'tweetin': 12031, 'errachiq': 3484, 'batool': 951, 'detail': 2842, 'inshallah': 5484, 'healthylifestyle': 4885, 'falla': 3716, 'skating': 10360, 'analyzing': 404, 'coursetalking': 2416, 'variety': 12320, 'twitching': 12048, 'insomnia': 5488, 'medication': 7041, 'opposite': 8107, 'saved': 9852, 'everlasting': 3541, 'archives': 568, 'spin': 10820, 'yoga': 13081, 'weights': 12632, 'sports': 10842, 'massage': 6952, 'ouchconfusionosteopath': 8174, 'trainers': 11890, 'nooooow': 7836, 'sharm': 10132, 'smileytrip': 10557, 'almasterbandconcert': 327, 'wallet': 12519, 'tbc': 11396, 'univesity': 12181, 'architecture': 567, 'random': 9220, 'typo': 12074, 'snark': 10614, 'lessions': 6386, 'drunk': 3165, 'bruuh': 1533, 'firsteurope': 3916, 'iove': 5559, 'presenting': 8933, 'corp': 2369, 'missing': 7250, 'according': 66, 'mne': 7284, 'pchelok': 8396, 'ja': 5653, 'carefully': 1744, 'sweetest': 11248, 'comet': 2189, 'ahah': 209, 'candy': 1716, 'axio': 766, 'reminds': 9423, 'rabbit': 9180, 'nutshell': 7918, 'taken': 11318, 'letshavecocktailsafternuclai': 6395, 'malik': 6842, 'umair': 12116, 'celebrity': 1811, 'canon': 1718, 'gangstay': 4253, 'grinding': 4580, 'thoracicbridge': 11625, 'passed': 8337, 'nonscripted': 7820, 'password': 8342, 'shoshannavassil': 10224, 'addmeonsnapchat': 120, 'dmme': 3016, 'mpoints': 7410, 'olddutch': 8018, 'workspv': 12872, 'soph': 10725, 'thoughts': 11634, 'subjective': 11083, 'painful': 8247, 'brou': 1516, 'cut': 2561, 'anotur': 465, 'liao': 6409, 'bomesince': 1341, 'ord': 8125, 'liaotake': 6410, 'lor': 6605, 'smileybrolol': 10459, 'sibei': 10268, 'xialan': 12950, 'thnx': 11617, 'malfunctioning': 6841, 'cleared': 2075, 'clowns': 2113, 'jokers': 5812, 'smileyits': 10511, 'emeraldeye': 3365, 'steele': 10948, 'nigth': 7758, 'estoy': 3509, 'escuchando': 3492, 'enemy': 3414, 'elsewhere': 3349, 'aroberts': 592, 'bipolar': 1199, 'hahahahahahahahahahahahahaha': 4708, 'dying': 3223, 'yoohoo': 13088, 'shattering': 10137, 'records': 9337, 'bajrangibhaijaanstorm': 848, 'superhappy': 11172, 'doll': 3042, 'energy': 3415, 'mdearhappy': 7019, 'alrd': 333, 'dhan': 2870, 'satguru': 9835, 'tera': 11480, 'aasramany': 19, 'pita': 8617, 'gkeep': 4402, 'keeo': 5990, 'guessing': 4638, 'darling': 2647, 'akarshan': 262, 'singledforeveralone': 10318, 'chic': 1947, 'sweetpea': 11251, 'gluten': 4426, 'cakes': 1665, 'pastry': 8348, 'workshopshappy': 12871, 'highfive': 4988, 'artsy': 612, 'verbal': 12344, 'beslagic': 1114, 'bth': 1536, 'kaaaa': 5906, 'oxford': 8226, 'campaigner': 1698, 'routes': 9680, 'wahoo': 12489, 'masaantoday': 6940, 'anchors': 407, 'seahappy': 9939, 'celebrating': 1808, 'partnership': 8327, 'robbenisland': 9613, 'cultural': 2532, 'whale': 12677, 'aquatic': 557, 'safari': 9761, 'garrus': 4267, 'liara': 6411, 'appointments': 543, 'burnley': 1607, 'dayshappy': 2678, 'represent': 9457, 'football': 4030, 'fmfamily': 3986, 'aamir': 15, 'difficult': 2906, 'medium': 7045, 'madhura': 6776, 'ampm': 391, 'adore': 135, 'nva': 7921, 'minuets': 7231, 'gamecity': 4235, 'headrest': 4878, 'pitted': 8622, 'spoken': 10836, 'advised': 150, 'paypoint': 8384, 'topped': 11829, 'deepthroat': 2738, 'truly': 11967, 'lateral': 6289, 'bee': 1034, 'upward': 12241, 'bound': 1401, 'movingonup': 7407, 'aitor': 254, 'sns': 10631, 'sharief': 10128, 'jawad': 5710, 'odj': 7960, 'presale': 8926, 'methings': 7144, 'betcha': 1129, 'dumb': 3194, 'butt': 1624, 'qualky': 9148, 'milf': 7190, 'likelike': 6446, 'sexysaturday': 10086, 'jubby': 5858, 'vw': 12470, 'umpffffff': 12123, 'ca': 1647, 'domg': 3048, 'nanti': 7575, 'difollow': 2908, 'raspberrytongue': 9245, 'stubborn': 11052, 'soaping': 10636, 'nothavingit': 7861, 'klee': 6126, 'butter': 1625, 'hemmings': 4952, 'heartdecoration': 4897, 'brad': 1423, 'congrads': 2266, 'accomplishment': 65, 'sees': 9974, 'texts': 11499, 'kfcroleplayers': 6020, 'tregaron': 11922, 'potential': 8802, 'boar': 1316, 'extremely': 3634, 'sweaty': 11240, 'glyon': 4427, 'beatingheartlitterinbinsignpeoplewithbunnyearspartying': 1006, 'initial': 5460, 'teespm': 11453, 'johnny': 5802, 'utube': 12283, 'respect': 9481, 'loss': 6618, 'combine': 2184, 'pigeon': 8577, 'contacting': 2300, 'ops': 8109, 'fingerscrossedhappy': 3901, 'photobomb': 8533, 'fitz': 3926, 'gim': 4371, 'smileyconfusionconfusionconfusionconfusionconfusionconfusionconfusionconfusionconfusionconfusionconfusionconfusionconfusionconfusionconfusionconfusionconfusionconfusionconfusionconfusionconfusionconfusionconfusionconfusionconfusion': 10478, 'hiim': 4997, 'definetly': 2749, 'exited': 3591, 'yorkhappy': 13092, 'mobilegame': 7292, 'bomdia': 1340, 'apotongue': 524, 'ultraviolet': 12113, 'jul': 5871, 'oooh': 8077, 'yawns': 13002, 'wags': 12485, 'tail': 11310, 'neo': 7666, 'ftw': 4167, 'prefer': 8906, 'maman': 6849, 'gibson': 4355, 'afterznoon': 178, 'tweeeps': 12024, 'smileyplz': 10538, 'abp': 37, 'kiya': 6119, 'smileyconfusionconfusionconfusionconfusionconfusionconfusionconfusionconfusionconfusionconfusionconfusion': 10475, 'van': 12311, 'olympic': 8035, 'facewithmedicalmask': 3670, 'classy': 2068, 'attached': 676, 'sorts': 10743, 'saving': 9856, 'equipment': 3475, 'bedshaving': 1033, 'shockconfusion': 10201, 'bobble': 1322, 'versionconfusion': 12355, 'storyline': 11007, 'anu': 482, 'priyabgt': 8982, 'dl': 3012, 'mh': 7154, 'patch': 8349, 'psp': 9063, 'mutuals': 7506, 'thanking': 11524, 'mayo': 6995, 'contribute': 2322, 'huffpost': 5191, 'tribute': 11934, 'heartshapedbox': 4901, 'magictrikband': 6801, 'magictrik': 6800, 'towering': 11860, 'roommatedo': 9658, 'smileylol': 10519, 'tami': 11347, 'sachi': 9745, 'bdk': 990, 'faran': 3756, 'ank': 443, 'purpose': 9115, 'struggling': 11048, 'bleuu': 1272, 'eagle': 3229, 'oceana': 7952, 'incredibly': 5407, 'idk': 5293, 'reaction': 9277, 'meds': 7046, 'oopsthats': 8084, 'fridayfauxpas': 4116, 'subtle': 11095, 'hints': 5017, 'tip': 11716, 'prims': 8964, 'algorithm': 294, 'iii': 5317, 'mezzer': 7150, 'wathappy': 12566, 'rosa': 9663, 'ann': 446, 'yvw': 13159, 'boost': 1369, 'unforgettable': 12163, 'humor': 5217, 'mums': 7471, 'hahahhaah': 4713, 'swe': 11233, 'sombrero': 10682, 'lost': 6620, 'helloooo': 4938, 'lesly': 6384, 'screenshots': 9922, 'spammer': 10775, 'proceed': 8997, 'entertaining': 3443, 'milestone': 7189, 'alljudith': 313, 'district': 2992, 'council': 2394, 'midares': 7169, 'gender': 4304, 'nightly': 7753, 'mgmt': 7153, 'ilysm': 5338, 'ppl': 8888, 'zen': 13185, 'neat': 7635, 'riders': 9556, 'saunders': 9846, 'fyi': 4218, 'dig': 2909, 'voting': 12455, 'learning': 6337, 'skill': 10366, 'blondhairedpersonmediumskintonealiendeciduoustree': 1292, 'facewithtearsofjoyfacewithtearsofjoyfacewithtearsofjoy': 3677, 'seem': 9969, 'suspicious': 11219, 'minds': 7210, 'calories': 1684, 'harder': 4804, 'smileyand': 10453, 'alexx': 293, 'finale': 3882, 'jessica': 5755, 'carina': 1749, 'francisco': 4084, 'teret': 11481, 'potassium': 8799, 'rehydrate': 9382, 'drinkitallup': 3147, 'thirstquencher': 11603, 'tapir': 11360, 'calf': 1668, 'adventurous': 147, 'mealtimes': 7022, 'uhc': 12100, 'scale': 9871, 'network': 7679, 'servers': 10043, 'areal': 574, 'lunchdinner': 6721, 'extremesports': 3635, 'quadbiking': 9144, 'bloggersrequired': 1286, 'bloggerswanted': 1287, 'brainer': 1430, 'investmentshould': 5551, 'mse': 7418, 'fund': 4195, 'realitybiter': 9293, 'nooooowwwww': 7837, 'dynamite': 3225, 'taylorkay': 11393, 'lile': 6456, 'tid': 11685, 'tmi': 11743, 'deploying': 2809, 'degree': 2755, 'jules': 5872, 'betty': 1136, 'hddcs': 4868, 'salman': 9783, 'eternal': 3513, 'pthht': 9070, 'spotting': 10849, 'weakness': 12586, 'anonadada': 461, 'tope': 11825, 'hikes': 4998, 'xxoo': 12967, 'finger': 3898, 'russia': 9726, 'fridayhappy': 4121, 'silverwashed': 10300, 'fritillary': 4142, 'moon': 7355, 'aps': 555, 'trash': 11911, 'wasting': 12555, 'clever': 2081, 'kevens': 6013, 'loz': 6681, 'popular': 8763, 'pastimes': 8346, 'ashramcalling': 622, 'returns': 9511, 'ontrack': 8070, 'smileyredheart': 10542, 'grl': 4587, 'showing': 10247, 'german': 4325, 'subtitles': 11094, 'pinter': 8601, 'morninggggg': 7368, 'dogface': 3034, 'limas': 6461, 'showed': 10244, 'smilingfacesmilingface': 10575, 'build': 1569, 'direct': 2933, 'awesomeo': 744, 'tune': 12005, 'multiple': 7465, 'engineer': 3420, 'cya': 2574, 'harrogate': 4815, 'jetting': 5759, 'suppliers': 11180, 'req': 9462, 'fridaylouge': 4123, 'hurtful': 5238, 'thstreetmusic': 11658, 'stevievie': 10965, 'hawaii': 4851, 'kick': 6039, 'olds': 8022, 'deeply': 2737, 'brown': 1518, 'johneclipsecouk': 5801, 'hahand': 4714, 'thousand': 11636, 'lies': 6424, 'newspaper': 7697, 'lew': 6401, 'bending': 1092, 'nah': 7545, 'beaut': 1012, 'fallout': 3719, 'technically': 11442, 'gunderson': 4655, 'europa': 3524, 'thoroughly': 11627, 'impressed': 5368, 'scripthappy': 9927, 'overtake': 8208, 'motorway': 7390, 'hahahaand': 4703, 'niteflirt': 7778, 'hbu': 4866, 'bowling': 1406, 'chris': 2000, 'ellis': 3341, 'buys': 1639, 'librarian': 6415, 'practice': 8892, 'niall': 7723, 'bornconfusion': 1381, 'ik': 5320, 'select': 9982, 'aln': 329, 'score': 9906, 'stydia': 11070, 'nawazuddin': 7610, 'siddiquebig': 10274, 'nomnomnom': 7809, 'dukefreebiefriday': 3190, 'lsk': 6688, 'kyungsoo': 6203, 'eonni': 3461, 'ahm': 217, 'insyaallah': 5511, 'ham': 4754, 'villa': 12399, 'brum': 1530, 'denis': 2795, 'vagina': 12297, 'rly': 9597, 'izzy': 5651, 'mitch': 7264, 'comate': 2181, 'minn': 7226, 'meiners': 7070, 'cameraclix': 1693, 'recentlywebsite': 9318, 'upgrades': 12223, 'coolingtowers': 2348, 'soonthanks': 10715, 'showinginterest': 10248, 'flipkartfashionfridaymulticolored': 3964, 'paired': 8253, 'wid': 12737, 'wedges': 12612, 'brightampcolorful': 1477, 'motivated': 7385, 'nnnnnnot': 7791, 'gfs': 4341, 'bluesidemenxix': 1304, 'shaikh': 10104, 'ardent': 570, 'goooood': 4494, 'mooorning': 7356, 'wuppertal': 12930, 'fridayfunday': 4120, 'resigned': 9477, 'chalkhill': 1847, 'celebration': 1810, 'midday': 7170, 'carter': 1768, 'holds': 5058, 'fika': 3869, 'remedies': 9416, 'tgifits': 11505, 'thankful': 11521, 'atrack': 674, 'promising': 9024, 'christ': 2001, 'badminton': 826, 'littluns': 6508, 'ikprideofpak': 5323, 'janjua': 5690, 'friendhappy': 4129, 'thinks': 11600, 'smileycheck': 10462, 'thinkhappy': 11598, 'smileynotice': 10532, 'pleasetwoheartsx': 8682, 'pimple': 8588, 'forehead': 4038, 'volcano': 12446, 'smileydone': 10491, 'mag': 6789, 'miryenda': 7238, 'dvn': 3216, 'technologys': 11444, 'possibilities': 8784, 'touchtoday': 11851, 'idownloader': 5296, 'snowball': 10629, 'expire': 3612, 'gb': 4283, 'loveu': 6668, 'morefuninthephilippines': 7361, 'lahos': 6237, 'caramoan': 1732, 'kareem': 5944, 'smileyfriday': 10498, 'recite': 9325, 'surah': 11193, 'kahaf': 5913, 'dogi': 3036, 'melanie': 7075, 'cases': 1772, 'bosch': 1384, 'washing': 12550, 'machine': 6766, 'refollow': 9361, 'preparing': 8925, 'naps': 7578, 'mesmilingfacewithsunglassespersontippinghandlightskintone': 7126, 'relapse': 9389, 'walletboy': 12520, 'prada': 8894, 'haigirl': 4720, 'thenboy': 11560, 'pprada': 8890, 'punjabiswillgetit': 9100, 'smilingface': 10572, 'indiie': 5421, 'hitter': 5032, 'soi': 10667, 'mass': 6951, 'shoud': 10228, 'remind': 9420, 'ughtmed': 12096, 'kissme': 6107, 'likeforfollow': 6443, 'overwhelming': 8212, 'groupmates': 4597, 'recitation': 9324, 'pointing': 8723, 'kyunke': 6205, 'aitchison': 253, 'curvy': 2552, 'montedoa': 7339, 'header': 4873, 'speaker': 10791, 'introduce': 5541, 'avoid': 723, 'smileytedthought': 10555, 'laboratory': 6209, 'idc': 5285, 'fuckin': 4175, 'wooo': 12844, 'neobytes': 7667, 'universal': 12178, 'pirated': 8608, 'files': 3873, 'takedown': 11317, 'confusionindirag': 2264, 'judiciary': 5862, 'committed': 2203, 'govt': 4511, 'policy': 8732, 'rbi': 9265, 'similar': 10303, 'progress': 9015, 'transfer': 11899, 'ggs': 4344, 'defenitely': 2744, 'nofx': 7801, 'friskyfiday': 4140, 'yipeeee': 13068, 'shed': 10146, 'incentive': 5391, 'veges': 12330, 'mariners': 6909, 'gz': 4681, 'named': 7564, 'rajeevhope': 9205, 'hvng': 5248, 'funfilled': 4198, 'fridayit': 4122, 'ws': 12919, 'realy': 9300, 'diff': 2900, 'kabirfell': 5907, 'dresdengermany': 3135, 'ltplease': 6694, 'thisconfusion': 11606, 'amazinglt': 363, 'congratulating': 2269, 'graduation': 4524, 'plot': 8691, 'sleeps': 10401, 'tdf': 11407, 'wineglasscocktailglasssunbicyclewineglasscocktailglasssunbicycle': 12768, 'minions': 7222, 'slot': 10417, 'wowlooking': 12898, 'isabella': 5582, 'okeyyyyyy': 8005, 'vdddd': 12327, 'smileyconfusionconfusionwink': 10481, 'smirkconfusionconfusionconfusion': 10588, 'selfeeee': 9989, 'insta': 5496, 'persongesturingokraisinghandsfacewithstuckouttonguesmilingfacewithopenmouthsmilingeyessmilingfacewithopenmouthsmilingeyesgrinningfacewithsmilingeyesfacewithtearsofjoyfacewithtearsofjoymonkeygrinningfacewithsmilingeyesfacewithstuckouttongueclosedeyesredheart': 8478, 'gugi': 4640, 'hhahhaaa': 4975, 'albums': 283, 'smileyenjoy': 10493, 'jeeze': 5734, 'teamcannibal': 11419, 'teamspacewhalingisthebest': 11429, 'earlynow': 3237, 'fitfa': 3921, 'identified': 5290, 'pharmacy': 8512, 'smileyverylaterealisation': 10561, 'iwishiknewbetter': 5645, 'satisfy': 9839, 'combination': 2183, 'turned': 12012, 'essaycheyetee': 3501, 'yas': 12998, 'valladolid': 12305, 'supposedly': 11189, 'thumbsup': 11665, 'sleigher': 10405, 'oforange': 7981, 'immediate': 5353, 'foxys': 4076, 'instrumental': 5509, 'alone': 330, 'goldcoast': 4455, 'thahane': 11515, 'lelomustfall': 6371, 'meal': 7021, 'narutoina': 7586, 'liker': 6450, 'smileylike': 10518, 'newdress': 7689, 'resist': 9478, 'fots': 4066, 'afzal': 180, 'troye': 11960, 'todayhappy': 11757, 'dtwitterfollowerswhatsuphappyfrieddaykeepsafealways': 3171, 'loveyeahemojasphere': 6672, 'switch': 11265, 'beautifulwinter': 1019, 'okthe': 8014, 'chatand': 1902, 'mewill': 7146, 'vanillaas': 12316, 'sidemen': 10276, 'yaaaaayyyyyy': 12977, 'friendaaaaaaa': 4128, 'shouts': 10239, 'adding': 116, 'bulbs': 1573, 'corn': 2362, 'tbps': 11401, 'delightful': 2770, 'burst': 1610, 'divine': 3000, 'wheelie': 12695, 'bins': 1193, 'ubericecream': 12080, 'messengerforaday': 7135, 'kylie': 6199, 'seasonal': 9943, 'toilet': 11772, 'breaks': 1457, 'ikaw': 5321, 'musta': 7496, 'cheatmate': 1912, 'kyuhyun': 6202, 'ghanton': 4348, 'decides': 2724, 'howeasyget': 5161, 'therein': 11573, 'majalah': 6822, 'thumbsupmediumlightskintonethumbsupmediumlightskintonethumbsupmediumlightskintonesmilingfacewithhearteyessmilingfacewithhearteyessmilingfacewithhearteyessmilingfacewithhearteyessmilingfacewithhearteyescherryblossomcherryblossomcherryblossomcherryblossomcherryblossom': 11666, 'singapore': 10314, 'heroes': 4965, 'gp': 4513, 'essay': 3500, 'alevel': 290, 'dominique': 3051, 'goood': 4490, 'earthling': 3243, 'hs': 5175, 'liking': 6453, 'smileybe': 10455, 'lamp': 6247, 'sites': 10342, 'afoot': 164, 'revamp': 9520, 'voted': 12453, 'brainchild': 1428, 'poem': 8719, 'aliyan': 305, 'confidence': 2250, 'confined': 2251, 'limitshappy': 6467, 'colorado': 2173, 'goodyear': 4482, 'tyres': 12077, 'upto': 12239, 'cashback': 1774, 'tampcs': 11350, 'applyconfusion': 541, 'yourewelcome': 13111, 'simpin': 10305, 'bk': 1240, 'sketchbook': 10365, 'lovewilde': 6669, 'colors': 2175, 'colorpencils': 2174, 'cray': 2458, 'imma': 5351, 'ob': 7934, 'kino': 6096, 'goning': 4464, 'thos': 11629, 'adult': 139, 'acting': 90, 'kardamena': 5943, 'beaches': 993, 'samos': 9798, 'greece': 4558, 'caesar': 1654, 'salad': 9777, 'tad': 11299, 'bland': 1257, 'respond': 9486, 'smileyily': 10509, 'okk': 8010, 'den': 2790, 'allover': 315, 'expecting': 3599, 'dedicated': 2730, 'freedom': 4097, 'hangout': 4777, 'whoever': 12719, 'tourist': 11855, 'kutiyapanti': 6193, 'professional': 9009, 'hits': 5031, 'boomshot': 1363, 'fuh': 4182, 'yeeeey': 13023, 'donot': 3069, 'lilac': 6455, 'expose': 3623, 'roast': 9609, 'lipstick': 6486, 'cran': 2449, 'vodka': 12441, 'prayrs': 8900, 'onemochaonelove': 8059, 'southpaw': 10765, 'musical': 7486, 'genius': 4313, 'stromae': 11037, 'redcircle': 9345, 'younow': 13109, 'jonah': 5818, 'jareddddd': 5698, 'dob': 3020, 'postcode': 8789, 'talkmobile': 11338, 'classes': 2063, 'huha': 5202, 'lucia': 6700, 'promotinghappy': 9029, 'transforms': 11902, 'sword': 11269, 'misread': 7243, 'forwarding': 4063, 'richardits': 9547, 'debate': 2713, 'ibiza': 5270, 'birthdaymoneyforjesusjuice': 1211, 'ytb': 13136, 'tutorials': 12019, 'constructive': 2292, 'criticism': 2494, 'ganesha': 4251, 'texture': 11500, 'photography': 8537, 'hinduism': 5016, 'hindugod': 5015, 'elephantgod': 3325, 'selfish': 9993, 'bboying': 973, 'cardgame': 1734, 'pixelart': 8625, 'gamedesign': 4236, 'indiedev': 5417, 'pixeldailies': 8627, 'plateaus': 8655, 'laguna': 6233, 'smileyconfusiontime': 10486, 'tha': 11511, 'bahot': 842, 'baje': 846, 'raat': 9179, 'liya': 6522, 'hath': 4839, 'bestbut': 1118, 'ghante': 4347, 'itna': 5627, 'bana': 874, 'paya': 8375, 'truei': 11965, 'tod': 11752, 'uta': 12277, 'manga': 6869, 'festive': 3843, 'jamuna': 5687, 'dubbed': 3176, 'swiftmas': 11258, 'ruth': 9728, 'trion': 11942, 'interactions': 5523, 'forums': 4061, 'raising': 9203, 'smileyrevolvinghearts': 10545, 'disgusting': 2970, 'commodore': 2205, 'programming': 9014, 'gtgtth': 4625, 'annabel': 448, 'bridge': 1472, 'quest': 9158, 'borderlands': 1374, 'wanderrook': 12528, 'organise': 8137, 'gain': 4227, 'gm': 4428, 'precious': 8903, 'lady': 6222, 'mizz': 7273, 'smirkkiss': 10590, 'aameenhappy': 14, 'smileylong': 10521, 'pakistanbleedgreenvictoryhandlightskintone': 8259, 'sophia': 10726, 'chicago': 1948, 'soexcited': 10658, 'honeymoon': 5088, 'lister': 6498, 'daesh': 2595, 'coord': 2352, 'fsa': 4163, 'estatic': 3506, 'whens': 12697, 'dusty': 3212, 'tunisia': 12007, 'ltyoutube': 6696, 'crd': 2461, 'classs': 2067, 'irritating': 5579, 'dcause': 2687, 'tone': 11798, 'apologies': 519, 'delay': 2759, 'prizes': 8984, 'fiverr': 3929, 'gina': 4373, 'soproud': 10729, 'pnefc': 8711, 'ahmd': 219, 'mrs': 7415, 'baines': 845, 'enought': 3433, 'luckanother': 6702, 'watering': 12563, 'hole': 5059, 'burgers': 1601, 'melbourneburgers': 7078, 'arianna': 582, 'esai': 3489, 'rotterdam': 9675, 'cheer': 1920, 'jordy': 5829, 'clasie': 2061, 'horny': 5112, 'salon': 9785, 'bleaches': 1262, 'olaplex': 8016, 'damage': 2607, 'teamwork': 11430, 'zitecofficestories': 13192, 'mandooff': 6868, 'mtv': 7432, 'colleague': 2160, 'eb': 3253, 'twould': 12056, 'tweetup': 12036, 'detective': 2845, 'jonathancreek': 5819, 'dvr': 3217, 'kat': 5958, 'rarer': 9240, 'okkkk': 8012, 'frends': 4101, 'milte': 7203, 'backhandindexpointingleftfaceblowingakiss': 806, 'mario': 6912, 'danielle': 2633, 'rewatches': 9532, 'priced': 8957, 'sige': 10280, 'punta': 9102, 'kayo': 5977, 'noooo': 7828, 'googled': 4487, 'prompt': 9033, 'existence': 3589, 'upgrade': 12220, 'tmobile': 11744, 'orange': 8120, 'ee': 3281, 'sales': 9781, 'viewing': 12394, 'teapot': 11431, 'cocktails': 2131, 'hotter': 5146, 'suits': 11128, 'smileylondoutrading': 10520, 'wip': 12779, 'tweaked': 12023, 'kal': 5920, 'macavity': 6761, 'wall': 12518, 'wayward': 12580, 'pines': 8593, 'laterhappy': 6291, 'muscles': 7481, 'ilikeit': 5327, 'belonging': 1080, 'watford': 12565, 'enterprise': 3440, 'cube': 2526, 'particpants': 8318, 'saudi': 9845, 'arabia': 561, 'approve': 550, 'recognized': 9328, 'bailona': 844, 'responsibilty': 9492, 'sunlight': 11151, 'tiger': 11692, 'elevate': 3327, 'position': 8781, 'promotion': 9030, 'insecure': 5478, 'ge': 4287, 'dangod': 2631, 'horror': 5120, 'bitchessss': 1224, 'shitty': 10194, 'squashed': 10862, 'commits': 2202, 'becca': 1028, 'delta': 2780, 'ives': 5641, 'nut': 7916, 'vlook': 12436, 'yun': 13151, 'joe': 5796, 'dirt': 2938, 'sharon': 10133, 'medicines': 7043, 'ttyl': 11987, 'gav': 4275, 'linda': 6469, 'answering': 470, 'sonically': 10705, 'tym': 12068, 'prepare': 8923, 'dieback': 2891, 'endit': 3408, 'minecon': 7212, 'sering': 10034, 'joerine': 5797, 'scene': 9881, 'smileyconfusionconfusionconfusionconfusionjoshane': 10479, 'tandem': 11353, 'ligao': 6434, 'albay': 279, 'smileyconcertbcyclnhs': 10464, 'sat': 9834, 'honorary': 5091, 'alacer': 274, 'skeloghost': 10363, 'infinity': 5435, 'halie': 4745, 'greathappy': 4555, 'redheartfaceblowingakiss': 9351, 'madadagdagan': 6770, 'bmc': 1308, 'waymoney': 12577, 'lifeand': 6426, 'ditched': 2997, 'embarrassing': 3359, 'entropy': 3454, 'evolution': 3559, 'giggle': 4365, 'loop': 6600, 'evaim': 3528, 'camden': 1690, 'uhh': 12101, 'scoups': 9913, 'jren': 5850, 'nuestd': 7903, 'lovelayyy': 6654, 'kidney': 6046, 'neuer': 7681, 'fixing': 3933, 'spray': 10850, 'outhappy': 8184, 'untangling': 12206, 'dtangle': 3169, 'patent': 8351, 'innovation': 5473, 'traine': 11889, 'falls': 3720, 'quirk': 9170, 'donnaestrydomgovzaconfusion': 3067, 'uni': 12169, 'talents': 11331, 'uff': 12089, 'karhy': 5947, 'thy': 11678, 'ap': 506, 'juaquin': 5856, 'vnzor': 12438, 'heyy': 4972, 'flavors': 3945, 'deserves': 2825, 'improve': 5371, 'tomorrows': 11796, 'thakyou': 11517, 'trees': 11921, 'beatriz': 1009, 'cancel': 1709, 'puff': 9084, 'egg': 3291, 'tarts': 11367, 'chai': 1838, 'mtr': 7431, 'alyssa': 354, 'rub': 9699, 'tummy': 12000, 'zelda': 13184, 'emailed': 3355, 'query': 9157, 'birthdaycake': 1209, 'jiva': 5781, 'tropicaldrinkcocktailglassclinkingbeermugs': 11954, 'mubbarak': 7441, 'deborah': 2717, 'coupon': 2410, 'colourdeb': 2177, 'purple': 9112, 'woooo': 12846, 'chippys': 1971, 'vessel': 12357, 'vintage': 12409, 'smilingfacewithhearteyessmilingfacewithsmilingeyes': 10577, 'puttinga': 9129, 'facemind': 3665, 'gooffeanottere': 4483, 'kiksex': 6061, 'mugshot': 7459, 'tokens': 11773, 'maritimenation': 6914, 'rhs': 9539, 'tatton': 11377, 'construction': 2291, 'planting': 8653, 'behave': 1053, 'jumpjulias': 5887, 'malema': 6839, 'frens': 4102, 'nuf': 7904, 'teasing': 11438, 'aliens': 299, 'closer': 2106, 'alien': 298, 'monitor': 7333, 'kimmy': 6076, 'planetbollywoodnews': 8644, 'epi': 3465, 'fbackhappy': 3793, 'airports': 245, 'tricky': 11936, 'ddont': 2692, 'beshak': 1110, 'chenoweth': 1938, 'oodles': 8074, 'hailey': 4722, 'milks': 7193, 'sxxx': 11274, 'runway': 9722, 'shares': 10125, 'gooodnight': 4491, 'iv': 5638, 'ris': 9579, 'katy': 5972, 'jaycy': 5715, 'jen': 5739, 'buds': 1558, 'faces': 3666, 'suffer': 11109, 'karaoke': 5942, 'ltsw': 6695, 'gooooooo': 4495, 'giant': 4352, 'careful': 1743, 'lineom': 6472, 'yanga': 12994, 'refuses': 9367, 'init': 5458, 'actual': 99, 'lolhappy': 6563, 'cconfusion': 1801, 'collagen': 2156, 'answin': 472, 'hopetowin': 5106, 'inventory': 5546, 'loveforfood': 6651, 'foodforthought': 4021, 'thoughtfortheday': 11633, 'carpe': 1759, 'diem': 2895, 'nath': 7594, 'ning': 7769, 'recommended': 9332, 'positivity': 8783, 'leads': 6330, 'wantmy': 12532, 'hahathat': 4715, 'nicebut': 7731, 'daughter': 2663, 'filling': 3876, 'although': 346, 'opinion': 8095, 'harm': 4810, 'stormy': 11005, 'upgraded': 12221, 'ans': 466, 'synced': 11282, 'device': 2856, 'scott': 9911, 'messing': 7137, 'studios': 11059, 'nylons': 7930, 'gvb': 4671, 'cd': 1802, 'mountaintitled': 7392, 'unto': 12208, 'theworldwouldchange': 11585, 'category': 1787, 'faking': 3711, 'mah': 6809, 'emotions': 3381, 'panel': 8282, 'iam': 5266, 'hopeful': 5101, 'counts': 2404, 'neenkin': 7647, 'successful': 11099, 'masterpiece': 6960, 'partypopper': 8331, 'debit': 2716, 'beagles': 995, 'feat': 3801, 'charlie': 1894, 'puthconfusion': 9125, 'wiz': 12799, 'khalifa': 6024, 'webb': 12602, 'tyler': 12066, 'svu': 11226, 'wears': 12593, 'darker': 2644, 'bernie': 1108, 'henrys': 4957, 'trap': 11909, 'fitness': 3923, 'tommy': 11792, 'discuss': 2963, 'nownow': 7882, 'vivians': 12434, 'performance': 8461, 'transparentglasseshappy': 11908, 'bitcoin': 1227, 'insight': 5487, 'rang': 9226, 'ping': 8594, 'masquerade': 6950, 'mask': 6947, 'zorroreturms': 13202, 'pk': 8633, 'hay': 4855, 'jacqueline': 5665, 'passion': 8340, 'ultimately': 12111, 'fullfledged': 4188, 'entrepreneur': 3450, 'bride': 1470, 'workplace': 12868, 'venue': 12341, 'lagos': 6232, 'luxord': 6734, 'kingdom': 6089, 'potatos': 8801, 'hundreds': 5224, 'aline': 300, 'cited': 2041, 'academic': 52, 'pokiri': 8727, 'nenokkadine': 7665, 'favs': 3790, 'akshay': 269, 'bharris': 1151, 'heritage': 4962, 'wood': 12838, 'beleaf': 1065, 'dx': 3220, 'spnfamily': 10832, 'spn': 10831, 'alwayskeepfighting': 353, 'jaredpadalecki': 5699, 'jensenackles': 5745, 'peasant': 8414, 'teami': 11421, 'ahahha': 212, 'reminders': 9422, 'distant': 2988, 'shoutout': 10237, 'adulthood': 140, 'hopeless': 5104, 'ikea': 5322, 'sliding': 10410, 'bani': 894, 'poutingme': 8841, 'babies': 788, 'neighbour': 7655, 'motors': 7388, 'hates': 4837, 'search': 9941, 'whyfrown': 12729, 'sialanfrown': 10265, 'athabasca': 663, 'glacier': 4404, 'jasper': 5705, 'jaspernationalpark': 5706, 'alberta': 281, 'explorealberta': 3618, 'mare': 6904, 'ivan': 5639, 'hahahah': 4704, 'replacements': 9446, 'relate': 9390, 'stupidi': 11068, 'grasp': 4541, 'basics': 934, 'digital': 2910, 'researching': 9471, 'lonely': 6580, 'wearyface': 12594, 'gameswatch': 4244, 'choreographing': 1993, 'raaaaayyyxo': 9178, 'cries': 2486, 'mingming': 7216, 'peed': 8420, 'newwine': 7699, 'dish': 2972, 'doushite': 3102, 'gundam': 4654, 'dollar': 3043, 'loudlycryingfaceloudlycryingfaceloudlycryingfaceloudlycryingfaceloudlycryingfaceloudlycryingface': 6633, 'weirdconfusion': 12634, 'pale': 8266, 'imitation': 5349, 'rash': 9241, 'absent': 42, 'sequel': 10030, 'ouucchhh': 8195, 'wisdom': 12784, 'frightening': 4137, 'pret': 8939, 'wkwkwjhope': 12803, 'verfied': 12345, 'hyemi': 5254, 'chains': 1840, 'sentirse': 10019, 'incompleta': 5401, 'thwarting': 11676, 'chances': 1859, 'zante': 13166, 'jallen': 5679, 'audraesar': 692, 'craaaazzyy': 2439, 'popped': 8760, 'helium': 4933, 'climatechange': 2093, 'californias': 1675, 'influential': 5441, 'pollution': 8739, 'watchdog': 12559, 'califor': 1673, 'uniteblue': 12174, 'tcot': 11405, 'elhaida': 3332, 'juries': 5896, 'televoting': 11460, 'idaho': 5284, 'restrict': 9498, 'droughtlinked': 3160, 'dieof': 2896, 'abrupt': 39, 'climate': 2092, 'doomed': 3077, 'mammoths': 6854, 'megafauna': 7057, 'australias': 707, 'dirtiest': 2939, 'considers': 2286, 'biomass': 1196, 'golf': 4457, 'ulti': 12109, 'prexy': 8953, 'kindergarten': 6082, 'sane': 9815, 'boss': 1387, 'hozier': 5167, 'soooooooner': 10720, 'respectlost': 9484, 'hypercholesteloremia': 5259, 'calibraska': 1671, 'shabbir': 10094, 'genuine': 4318, 'contenders': 2307, 'older': 8019, 'upset': 12235, 'aos': 505, 'encore': 3401, 'badthwin': 828, 'baymax': 961, 'mixers': 7270, 'wft': 12667, 'promotions': 9032, 'hub': 5186, 'loud': 6626, 'parasyte': 8303, 'alllllll': 314, 'zayniscomingbackonjuly': 13175, 'era': 3476, 'toofrown': 11812, 'nathanntwohearts': 7597, 'yousmilingfacewithsmilingeyes': 13116, 'dieididieieiei': 2894, 'ervin': 3487, 'netflix': 7674, 'offerbut': 7969, 'desperate': 2834, 'amargolonnard': 355, 'batalladelosgallos': 944, 'webcamsex': 12604, 'duty': 3214, 'uve': 12287, 'allisbae': 311, 'hopes': 5105, 'sorka': 10732, 'funeral': 4197, 'nonexistent': 7815, 'wowza': 12900, 'fah': 3696, 'boo': 1350, 'guitar': 4647, 'hyung': 5262, 'predict': 8904, 'sj': 10355, 'nominated': 7807, 'dudes': 3184, 'brace': 1419, 'frml': 4144, 'omgggg': 8047, 'falling': 3718, 'finals': 3885, 'whyyy': 12731, 'evan': 3529, 'opt': 8110, 'giveaways': 4396, 'muster': 7499, 'goods': 4478, 'merchs': 7116, 'drinking': 3146, 'savanna': 9850, 'straw': 11014, 'yester': 13054, 'birexus': 1204, 'strypes': 11049, 'consumed': 2295, 'werk': 12656, 'foreals': 4036, 'wesen': 12658, 'uwesiti': 12288, 'mj': 7274, 'facewithtearsofjoywearyface': 3683, 'bosen': 1385, 'benny': 1097, 'badly': 825, 'gtfrown': 4620, 'zaz': 13180, 'rehash': 9379, 'websites': 12606, 'mushroomed': 7484, 'piece': 8571, 'exception': 3571, 'reaches': 9276, 'vicky': 12371, 'hahah': 4699, 'nnnnn': 7790, 'ninasty': 7768, 'tsktsk': 11980, 'dick': 2886, 'kawaii': 5974, 'manly': 6878, 'youu': 13125, 'soehmtee': 10657, 'potato': 8800, 'fries': 4135, 'asf': 618, 'eish': 3308, 'knowi': 6144, 'mojo': 7312, 'shamen': 10113, 'mara': 6898, 'neh': 7653, 'association': 654, 'councillors': 2395, 'contacts': 2301, 'creating': 2468, 'sholong': 10208, 'reject': 9385, 'gee': 4290, 'don': 3053, 'gidi': 4357, 'pampered': 8274, 'ehn': 3300, 'arresting': 598, 'anybody': 487, 'disappears': 2946, 'dazed': 2682, 'survivedwell': 11213, 'bodies': 1324, 'fragments': 4081, 'wouldve': 12894, 'horrible': 5116, 'walao': 12508, 'kbs': 5980, 'djderek': 3006, 'gigs': 4368, 'legend': 6359, 'tragedy': 11884, 'flexedbicepslightskintonemonkey': 3955, 'strucked': 11042, 'belgium': 1067, 'fabian': 3654, 'delph': 2777, 'fallen': 3717, 'poutingind': 8833, 'draking': 3121, 'silently': 10296, 'eyy': 3648, 'mumma': 7469, 'poutingembarrassed': 8820, 'dedox': 2734, 'pray': 8897, 'appropriate': 548, 'across': 88, 'picwhat': 8569, 'hindi': 5014, 'disability': 2943, 'pension': 8440, 'ptsd': 9073, 'impossible': 5367, 'physically': 8550, 'financially': 3888, 'hungary': 5226, 'nooo': 7827, 'togheter': 11771, 'snsd': 10632, 'anna': 447, 'akana': 261, 'askip': 635, 'texistes': 11495, 'pas': 8334, 'moore': 7357, 'regions': 9373, 'broadcasts': 1496, 'kei': 5996, 'eyeing': 3641, 'kate': 5960, 'spade': 10771, 'pero': 8470, 'walang': 12506, 'bld': 1260, 'agessss': 194, 'corinehurleigh': 2361, 'snapchatme': 10610, 'sfs': 10088, 'zara': 13168, 'trousers': 11959, 'effects': 3286, 'itokay': 5630, 'luckely': 6703, 'freed': 4096, 'orcalove': 8122, 'result': 9500, 'patnicolexx': 8361, 'ems': 3389, 'craving': 2457, 'crumble': 2509, 'mcflurry': 7011, 'cable': 1651, 'venuss': 12342, 'concepts': 2237, 'tagal': 11306, 'awful': 745, 'appointment': 542, 'cereal': 1826, 'previous': 8951, 'users': 12269, 'twt': 12057, 'fansign': 3748, 'expensive': 3603, 'zzzzzzplease': 13209, 'stressed': 11024, 'deosnt': 2802, 'brolost': 1506, 'praying': 8899, 'lowest': 6679, 'tbhthey': 11399, 'tacky': 11296, 'charms': 1898, 'haaretz': 4685, 'israel': 5604, 'syria': 11285, 'develop': 2852, 'chemical': 1935, 'weapons': 12589, 'officials': 7977, 'wsj': 12922, 'rep': 9433, 'bts': 1538, 'wong': 12835, 'confiscated': 2254, 'hadnt': 4692, 'icepack': 5277, 'doses': 3089, 'killers': 6067, 'whimpers': 12706, 'awkwardraven': 752, 'senpai': 10014, 'buttsex': 1629, 'abla': 32, 'messiah': 7136, 'headline': 4875, 'brk': 1493, 'falsettos': 3722, 'zone': 13196, 'loveofmylife': 6659, 'aired': 238, 'killing': 6069, 'untruth': 12210, 'cross': 2499, 'ij': 5319, 'amazingcollisionsparklesdizzy': 362, 'lizzykay': 6526, 'soshi': 10746, 'buttt': 1630, 'poutingfrown': 8822, 'heyyyyy': 4973, 'yeols': 13040, 'danceeeeee': 2620, 'nemanja': 7661, 'vidic': 12387, 'romabut': 9649, 'linguist': 6475, 'dumbest': 3195, 'consider': 2283, 'crash': 2453, 'resources': 9480, 'vehicles': 12334, 'ayex': 771, 'lamon': 6246, 'scrolling': 9929, 'curved': 2550, 'cement': 1813, 'botconfusion': 1391, 'jmu': 5788, 'camps': 1700, 'tease': 11435, 'awuna': 755, 'mbulelo': 7004, 'crackling': 2444, 'plug': 8701, 'fuse': 4208, 'dammit': 2610, 'carlton': 1751, 'aflblueshawks': 163, 'alexs': 292, 'motorsport': 7389, 'discconfusion': 2954, 'thatinstead': 11539, 'seo': 10020, 'nls': 7788, 'christy': 2009, 'niece': 7744, 'bloody': 1295, 'sandwhich': 9812, 'buset': 1613, 'discrimination': 2962, 'pregnancy': 8911, 'foot': 4028, 'maternity': 6972, 'kicked': 6040, 'domesticviolence': 3047, 'domestic': 3046, 'violence': 12411, 'victims': 12372, 'vaw': 12324, 'stressful': 11025, 'government': 4510, 'sapiosexual': 9823, 'beta': 1128, 'hogan': 5049, 'scrubbed': 9930, 'wwe': 12933, 'irene': 5571, 'naa': 7530, 'sold': 10670, 'hmyking': 5043, 'valentine': 12303, 'et': 3510, 'rships': 9693, 'btwn': 1541, 'homobiphobic': 5080, 'disciplines': 2956, 'incl': 5395, 'european': 3526, 'langs': 6258, 'fresherstofinals': 4106, 'brokenheart': 1503, 'realistic': 9290, 'benzema': 1101, 'hahahahahaah': 4707, 'donno': 3068, 'language': 6260, 'russian': 9727, 'waaaaa': 12473, 'eidwithgrofers': 3303, 'boreddd': 1377, 'mug': 7456, 'tiddler': 11686, 'cards': 1737, 'clan': 2052, 'slots': 10418, 'pffff': 8502, 'bugbounty': 1566, 'selfxss': 9996, 'host': 5128, 'poisoning': 8725, 'execution': 3582, 'ktksbye': 6179, 'alert': 288, 'cancelled': 1711, 'petite': 8493, 'stefan': 10949, 'bv': 1642, 'cost': 2381, 'security': 9958, 'odoo': 7961, 'partner': 8326, 'effin': 3287, 'workingfrown': 12862, 'branch': 1436, 'concern': 2238, 'pod': 8716, 'talkkama': 11337, 'hawako': 4852, 'waa': 12472, 'kimaaaaaani': 6073, 'prisssss': 8978, 'baggage': 836, 'screwed': 9925, 'sugar': 11113, 'rarely': 9239, 'agh': 196, 'cutting': 2569, 'undercoverboss': 12141, 'delayed': 2760, 'ache': 75, 'wrists': 12907, 'ligo': 6439, 'dozen': 3110, 'killed': 6065, 'shark': 10130, 'heartache': 4892, 'zayniscomingback': 13174, 'sweden': 11241, 'elmhurst': 3343, 'etid': 3515, 'chillinwith': 1962, 'father': 3773, 'istanya': 5609, 'goingsupply': 4452, 'infrastructureteachersdoctorsnursesparamedicsampother': 5451, 'countless': 2400, 'thingscope': 11595, 'allgtfrown': 310, 'pasha': 8335, 'donate': 3054, 'pleas': 8672, 'arianator': 581, 'kendall': 5999, 'kylies': 6200, 'manila': 6876, 'cried': 2485, 'jeebus': 5732, 'reabsorbtion': 9272, 'abscess': 41, 'threatening': 11641, 'crown': 2505, 'oooooooouch': 8080, 'barney': 921, 'bes': 1109, 'problematic': 8991, 'mess': 7128, 'maa': 6754, 'entd': 3436, 'bangalore': 888, 'poutingat': 8814, 'luis': 6713, 'manzano': 6892, 'shaaaa': 10093, 'convener': 2328, 'smileypm': 10539, 'delete': 2764, 'mefrown': 7054, 'bombing': 1338, 'syriaand': 11286, 'allow': 316, 'airfields': 239, 'jets': 5758, 'poutingconfusion': 8818, 'jacks': 5661, 'spamming': 10776, 'smilingfacepurpleheart': 10574, 'mommys': 7324, 'overweight': 8210, 'sigeg': 10281, 'habhab': 4687, 'masud': 6962, 'kaha': 5911, 'kos': 6166, 'akong': 267, 'pala': 8264, 'dolphins': 3044, 'holy': 5067, 'anythin': 494, 'loverx': 6664, 'zayniscomingbackonjulyi': 13176, 'trend': 11923, 'againfrown': 186, 'beware': 1138, 'agonising': 202, 'modimo': 7306, 'tseba': 11976, 'wena': 12650, 'fela': 3822, 'jm': 5787, 'nowt': 7885, 'willy': 12755, 'vomit': 12451, 'bowl': 1405, 'devastating': 2851, 'titan': 11727, 'ae': 153, 'omgyoure': 8048, 'soooooooo': 10721, 'shiny': 10178, 'wavy': 12572, 'emo': 3374, 'germany': 4326, 'loudlycryingface': 6628, 'load': 6540, 'shedding': 10147, 'haa': 4683, 'bheyps': 1154, 'ayemso': 770, 'swelling': 11254, 'sm': 10431, 'wut': 12931, 'vicious': 12370, 'circle': 2038, 'simpson': 10308, 'spinorbinmusicxcodysimpson': 10824, 'gleek': 4414, 'poooo': 8746, 'bhs': 1158, 'pitchwars': 8619, 'chap': 1875, 'mines': 7214, 'transcript': 11898, 'apmas': 518, 'timw': 11709, 'accs': 69, 'vitamin': 12431, 'healthier': 4883, 'stretch': 11026, 'choose': 1987, 'blockjam': 1282, 'schedules': 9888, 'whacked': 12675, 'thelock': 11554, 'hotgirls': 5141, 'relationships': 9394, 'languages': 6261, 'ghantay': 4346, 'nai': 7547, 'rejection': 9387, 'deny': 2800, 'feelfrown': 3811, 'ruining': 9708, 'exit': 3590, 'gomen': 4458, 'grew': 4570, 'inch': 5392, 'suuuuuuuper': 11221, 'anyones': 492, 'inactive': 5383, 'orphan': 8147, 'whaaat': 12674, 'kaya': 5975, 'naaaaan': 7531, 'pausing': 8372, 'smileyam': 10451, 'inglewood': 5454, 'ummm': 12121, 'charcoal': 1883, 'midend': 7172, 'rodfanta': 9633, 'wasp': 12552, 'stings': 10977, 'averted': 720, 'exo': 3592, 'seeklyfrown': 9968, 'shocking': 10202, 'riptito': 9577, 'manbearpig': 6864, 'academics': 53, 'exclusive': 3577, 'unfair': 12154, 'esp': 3497, 'bleak': 1263, 'charts': 1899, 'pfft': 8503, 'polaroid': 8729, 'newbethvideo': 7685, 'xur': 12965, 'imy': 5378, 'heartsuitheartsuitxoxoxo': 4905, 'cerealtoo': 1827, 'stud': 11054, 'earrings': 3239, 'massages': 6953, 'hunde': 5222, 'yach': 12984, 'huvvft': 5246, 'zoo': 13198, 'fieldtrips': 3855, 'kingfutureboss': 6092, 'sizwe': 10354, 'assignments': 650, 'shifts': 10171, 'aaaaahhhhhhhh': 6, 'boredom': 1378, 'sinse': 10324, 'somehow': 10685, 'tiny': 11714, 'barbells': 906, 'owww': 8223, 'sparklingheart': 10784, 'looooong': 6597, 'apartement': 508, 'longe': 6585, 'litro': 6505, 'shepherd': 10161, 'lami': 6244, 'relaxfrown': 9399, 'lungomare': 6724, 'pesaro': 8486, 'giachietittiwedding': 4351, 'igersoftheday': 5309, 'summertime': 11140, 'nose': 7849, 'bruised': 1529, 'scarf': 9876, 'afford': 161, 'morningfrown': 7367, 'fry': 4162, 'woe': 12813, 'nigga': 7747, 'motn': 7387, 'lighting': 6438, 'meand': 7024, 'longeryet': 6587, 'ver': 12343, 'huhuhu': 5205, 'mlady': 7278, 'acads': 54, 'scheduled': 9887, 'nowwww': 7888, 'cops': 2356, 'window': 12763, 'hugh': 5196, 'pawing': 8373, 'muddy': 7453, 'marks': 6918, 'cupcakes': 2541, 'talaga': 11327, 'poppin': 8761, 'joc': 5795, 'playin': 8666, 'ferdelicious': 3834, 'subj': 11081, 'sobrang': 10639, 'zamn': 13165, 'afropunk': 168, 'fest': 3840, 'shithouse': 10191, 'dance': 2619, 'oppahug': 8100, 'handed': 4765, 'ladder': 6218, 'climb': 2094, 'wracking': 12902, 'booset': 1368, 'restarted': 9494, 'assassins': 643, 'creed': 2474, 'ii': 5316, 'heaps': 4886, 'gameswish': 4245, 'beginsighps': 1050, 'ankle': 444, 'stepped': 10957, 'puddle': 9083, 'slippers': 10414, 'eve': 3530, 'sararocs': 9829, 'hairs': 4729, 'faceblowingakisssparklingheart': 3662, 'anywayhedidanicejob': 499, 'disappointedface': 2949, 'local': 6544, 'cruise': 2508, 'wail': 12491, 'wheelchair': 12694, 'checked': 1914, 'fits': 3924, 'sbenu': 9869, 'sasin': 9833, 'poutingbut': 8817, 'anarchy': 405, 'candle': 1715, 'medicine': 7042, 'hoya': 5166, 'poutingno': 8845, 'aing': 232, 'hush': 5241, 'gurly': 4661, 'purty': 9117, 'closerthat': 2108, 'shiver': 10197, 'paid': 8244, 'properly': 9038, 'gol': 4453, 'pea': 8402, 'emotionally': 3380, 'mentally': 7105, 'tierd': 11690, 'thnkyouuuuuuuuu': 11616, 'highlights': 4989, 'climbing': 2096, 'courage': 2411, 'fishy': 3919, 'idek': 5289, 'apink': 515, 'bulet': 1574, 'gendut': 4305, 'noo': 7822, 'racing': 9185, 'hotwheels': 5148, 'itfrown': 5619, 'ahaha': 210, 'whyyyyyy': 12734, 'sucked': 11101, 'akon': 266, 'nightmare': 7754, 'mino': 7227, 'crazyyyy': 2460, 'thooo': 11624, 'zz': 13206, 'soundcheck': 10756, 'antagonistic': 477, 'obs': 7940, 'phantasy': 8509, 'issuei': 5607, 'morningsleepdeprivedtiredashelllookingaspoteven': 7371, 'kinaras': 6079, 'closed': 2104, 'poutingwe': 8871, 'awami': 733, 'gtgtgt': 4624, 'muca': 7442, 'niqqa': 7772, 'ajaydevgn': 258, 'pbcontestants': 8388, 'snakes': 10606, 'youfrown': 13099, 'aarwwi': 18, 'lmbo': 6534, 'dangit': 2629, 'emceeeee': 3362, 'ohmygod': 7993, 'scenario': 9880, 'tooooooo': 11818, 'baechyyy': 831, 'okayyyy': 8004, 'noone': 7826, 'drag': 3113, 'seriously': 10038, 'misundersranding': 7262, 'chal': 1845, 'raha': 9191, 'yhm': 13064, 'edsa': 3275, 'jasmingarrick': 5702, 'nakamaforever': 7558, 'unicefs': 12170, 'accounts': 68, 'fu': 4168, 'managed': 6860, 'stephen': 10956, 'appeared': 532, 'frustration': 4161, 'mapz': 6897, 'woza': 12901, 'entle': 3446, 'senight': 10011, 'amateur': 358, 'hotscratch': 5144, 'sell': 9997, 'socks': 10652, 'pesos': 8487, 'degrassi': 2754, 'bcz': 986, 'rejected': 9386, 'chem': 1934, 'onscreen': 8066, 'thoofscreenhe': 11623, 'ignores': 5313, 'herfrown': 4961, 'irony': 5575, 'rhisfor': 9538, 'camsex': 1703, 'gofrown': 4447, 'fxhelp': 4216, 'poopie': 8750, 'partisan': 8324, 'pipped': 8606, 'cupcake': 2540, 'glue': 4425, 'kuchar': 6182, 'ups': 12234, 'definite': 2750, 'anywaysconfusion': 501, 'lanzzz': 6265, 'samei': 9795, 'georgia': 4323, 'transmission': 11906, 'orang': 8119, 'suma': 11134, 'shouldeeerr': 10230, 'ages': 193, 'repack': 9434, 'groups': 4598, 'repacks': 9436, 'alwaysconfusion': 351, 'grown': 4605, 'dyed': 3222, 'rihanna': 9566, 'ginge': 4375, 'nikes': 7762, 'adidas': 125, 'procom': 9000, 'ifeelyou': 5302, 'todaythats': 11759, 'ratbaglater': 9248, 'semester': 10004, 'gin': 4372, 'poutingpurpleheart': 8854, 'gutted': 4663, 'reynold': 9535, 'desserts': 2838, 'village': 12400, 'cities': 2042, 'jhastie': 5766, 'uniting': 12176, 'oppressed': 8108, 'afghanistnwar': 162, 'tore': 11835, 'sunggyu': 11148, 'injure': 5463, 'plaster': 8654, 'rtd': 9695, 'welder': 12642, 'hogo': 5050, 'vishaya': 12421, 'adu': 138, 'bjp': 1239, 'madatte': 6772, 'anta': 474, 'vishwas': 12422, 'illa': 5329, 'wua': 12928, 'picky': 8562, 'swell': 11253, 'unfollowed': 12158, 'thenting': 11563, 'sexual': 10079, 'syncs': 11283, 'plugdj': 8702, 'suspemsion': 11217, 'cope': 2353, 'offroading': 7979, 'harvest': 4821, 'machinery': 6767, 'inapropriate': 5385, 'weave': 12600, 'investment': 5550, 'scottish': 9912, 'dire': 2932, 'nomoney': 7810, 'yungthot': 13154, 'nawf': 7612, 'sum': 11133, 'bechos': 1029, 'overly': 8205, 'lab': 6207, 'zap': 13167, 'distressing': 2990, 'cinema': 2033, 'louisianashooting': 6641, 'har': 4799, 'chum': 2018, 'ncc': 7621, 'ph': 8507, 'itong': 5631, 'shirt': 10186, 'thaaaat': 11512, 'cttoconfusion': 2525, 'expired': 3613, 'bis': 1214, 'confusionfrown': 2263, 'jfc': 5761, 'flies': 3958, 'bodo': 1325, 'amat': 357, 'yelaaaaaaa': 13031, 'sooooooooooo': 10723, 'dublin': 3177, 'potter': 8805, 'hugwant': 5200, 'smilewant': 10447, 'eyessince': 3647, 'comics': 2195, 'pining': 8595, 'keybind': 6016, 'warfare': 12538, 'controlls': 2326, 'diagnosed': 2878, 'wiv': 12797, 'scheuermanns': 9890, 'diseaseback': 2967, 'bone': 1345, 'rlyhurts': 9598, 'howdo': 5160, 'georgesampson': 4322, 'pe': 8401, 'presence': 8930, 'signal': 10288, 'calls': 1682, 'reckon': 9327, 'taunton': 11382, 'justiceforsandrabland': 5900, 'sandrabland': 9810, 'disturbing': 2994, 'happpy': 4791, 'justinbieber': 5902, 'gtdaianerufatolt': 4618, 'smileyamconfusion': 10452, 'delphy': 2778, 'injured': 5464, 'dom': 3045, 'techniques': 11443, 'mince': 7207, 'bangyongguk': 893, 'symphony': 11280, 'poutingneed': 8843, 'co': 2122, 'wth': 12926, 'schedule': 9886, 'aisyhhh': 251, 'eyed': 3640, 'bald': 858, 'seungchan': 10061, 'aigooo': 229, 'riri': 9578, 'vet': 12358, 'vaim': 12298, 'kept': 6006, 'luminous': 6719, 'treating': 11918, 'horribly': 5118, 'popularity': 8764, 'whyjust': 12730, 'stopbebeeeee': 10994, 'inaccuracies': 5381, 'jokes': 5813, 'inaccurate': 5382, 'worried': 12878, 'burning': 1606, 'tragic': 11885, 'toronto': 11837, 'stuart': 11051, 'partys': 8333, 'iyalaya': 5646, 'clockfrown': 2101, 'happier': 4786, 'hampm': 4757, 'ubusy': 12083, 'gymnastics': 4678, 'aahhhh': 11, 'noggin': 7802, 'bumped': 1585, 'feelslikeanidiot': 3818, 'pregnant': 8912, 'dearly': 2704, 'suk': 11129, 'buttttttt': 1631, 'pumpkin': 9095, 'scones': 9901, 'outnumbered': 8189, 'eris': 3480, 'geez': 4295, 'exolt': 3593, 'hive': 5034, 'vietnam': 12390, 'dunt': 3202, 'awwwwwwww': 764, 'sobbing': 10638, 'buff': 1561, 'toni': 11803, 'deactivated': 2695, 'shadys': 10102, 'isibaya': 5591, 'facewithcoldsweat': 3669, 'colder': 2149, 'sausage': 9847, 'adios': 126, 'messenger': 7134, 'shittier': 10193, 'leno': 6378, 'identity': 5291, 'crisis': 2490, 'roommates': 9660, 'knocked': 6140, 'nighter': 7752, 'flew': 3953, 'visits': 12428, 'wetherspoons': 12663, 'pubs': 9081, 'police': 8731, 'lisaherring': 6490, 'ebony': 3255, 'polka': 8738, 'loudlycryingfaceloudlycryingface': 6631, 'ndi': 7626, 'leftovers': 6355, 'grahampatrick': 4525, 'owns': 8222, 'ruined': 9707, 'walnut': 12525, 'whips': 12708, 'boah': 1315, 'mady': 6787, 'momentnot': 7319, 'aminormalyet': 384, 'poorly': 8754, 'pjs': 8632, 'groaning': 4588, 'stroya': 11041, 'nou': 7871, 'ken': 5998, 'saras': 9830, 'accident': 61, 'manage': 6859, 'describe': 2821, 'prydz': 9060, 'sisterinlaw': 10337, 'installed': 5501, 'seat': 9945, 'rearended': 9302, 'everyones': 3549, 'boobs': 1352, 'stairs': 10885, 'childhood': 1956, 'toothsensitivity': 11820, 'factor': 3689, 'shem': 10159, 'awell': 740, 'weekendofmadness': 12620, 'tropicaldrinkcocktailglasstropicaldrink': 11955, 'cb': 1798, 'threatened': 11640, 'dancer': 2621, 'noi': 7805, 'choregrapherbut': 1991, 'poutingwhyyyyyyyyyyyy': 8875, 'happening': 4784, 'existed': 3588, 'messaged': 7130, 'hasnt': 4829, 'replied': 9449, 'hoe': 5048, 'recording': 9335, 'xiu': 12952, 'nk': 7786, 'gi': 4350, 'uss': 12274, 'awwwwww': 763, 'elisse': 3335, 'ksoo': 6178, 'tat': 11374, 'rdbut': 9267, 'glen': 4415, 'yoult': 13103, 'cledera': 2079, 'rancho': 9219, 'imperial': 5363, 'de': 2693, 'silang': 10295, 'subdivision': 11079, 'center': 1815, 'canampt': 1706, 'evry': 3560, 'cornwall': 2368, 'veritably': 12349, 'penny': 8437, 'addictedtoanalsex': 112, 'sweetbj': 11247, 'blowjob': 1298, 'mhhh': 7155, 'sed': 9959, 'seungh': 10062, 'mee': 7047, 'envious': 3457, 'lovey': 6670, 'dovey': 3103, 'workin': 12860, 'schade': 9884, 'isco': 5583, 'penis': 8436, 'hushed': 5242, 'nationpraying': 7601, 'louisianalafayette': 6640, 'matteroftheheart': 6979, 'waduhpantes': 12479, 'suspended': 11218, 'smoking': 10595, 'cliche': 2083, 'youuuu': 13127, 'rma': 9601, 'jersey': 5751, 'texted': 11497, 'jaclintiler': 5663, 'likeforlike': 6444, 'hotfmnoaidilforariana': 5140, 'pleasefrown': 8680, 'fuckkk': 4177, 'sanum': 9820, 'poutingllaollao': 8837, 'dessert': 2837, 'foood': 4027, 'glares': 4408, 'vines': 12406, 'figuring': 3868, 'choreo': 1992, 'offensive': 7967, 'yousad': 13115, 'yeyy': 13062, 'hd': 4867, 'sux': 11222, 'nothaveld': 7860, 'mosquitoe': 7376, 'bites': 1229, 'kinky': 6095, 'hsould': 5179, 'justget': 5897, 'shuffle': 10257, 'buckling': 1548, 'springs': 10857, 'millz': 7200, 'askies': 633, 'millzxytongue': 7201, 'awusasho': 756, 'mbeki': 7002, 'unlucky': 12190, 'briefly': 1474, 'eggs': 3292, 'spotted': 10847, 'greatful': 4554, 'brooke': 1507, 'cracked': 2441, 'maverickgamerjuly': 6986, 'external': 3627, 'sd': 9935, 'airdroid': 236, 'supported': 11182, 'cramps': 2448, 'unstan': 12204, 'tay': 11390, 'ngeze': 7711, 'parties': 8322, 'cocktaily': 2132, 'loudlycryingfaceloudlycryingfaceloudlycryingface': 6632, 'ams': 393, 'crashing': 2455, 'airplane': 242, 'poutingumbrellawithraindrops': 8868, 'pens': 8439, 'spares': 10781, 'guests': 4639, 'barcelona': 908, 'bilbao': 1180, 'sharyl': 10135, 'shane': 10117, 'deactivate': 2694, 'giddy': 4356, 'zipper': 13191, 'repair': 9437, 'offering': 7971, 'cont': 2297, 'englandfrown': 3422, 'wore': 12852, 'tempted': 11467, 'oreos': 8131, 'networks': 7680, 'ain': 231, 'lolipop': 6565, 'kebab': 5984, 'klappertart': 6125, 'moodboster': 7350, 'unprepared': 12200, 'sry': 10872, 'dresscode': 3137, 'caitlintierney': 1660, 'stab': 10875, 'meh': 7061, 'wrocilam': 12914, 'loowwfrown': 6603, 'mariebear': 6907, 'recovered': 9339, 'wayne': 12578, 'lossstolen': 6619, 'accidentally': 62, 'damaged': 2608, 'devices': 2857, 'warranty': 12545, 'repairs': 9438, 'facewithtearsofjoyfacewithtearsofjoyfacewithtearsofjoyboym': 3678, 'okhandfruits': 8007, 'kyddque': 6197, 'lmfaooso': 6538, 'fra': 4078, 'otamendi': 8159, 'ny': 7927, 'oncomingtaxistatueoflibertynightwithstars': 8055, 'stealth': 10942, 'bastard': 942, 'therapy': 11569, 'exhausting': 3586, 'understandable': 12144, 'switzerland': 11267, 'wolrd': 12819, 'isi': 5590, 'fyn': 4219, 'genuinely': 4319, 'nightmares': 7755, 'audio': 690, 'noooooooooooooooooo': 7834, 'deck': 2727, 'chairs': 1844, 'yk': 13074, 'resy': 9502, 'strms': 11034, 'memory': 7093, 'bruh': 1526, 'locked': 6550, 'fbc': 3794, 'mork': 7363, 'hotspotwithdanris': 5145, 'choclbear': 1975, 'sone': 10700, 'produce': 9003, 'potagermy': 8798, 'blight': 1274, 'mych': 7518, 'shiiit': 10174, 'prompts': 9034, 'claimed': 2051, 'aready': 573, 'soulmate': 10752, 'smilingfacefacewithtearsofjoygrinningfacewithsmilingeyes': 10573, 'hws': 5250, 'jouch': 5835, 'por': 8766, 'que': 9152, 'liceooooo': 6419, 'ayala': 768, 'tunnel': 12008, 'promised': 9023, 'centres': 1819, 'thatscold': 11543, 'lourdes': 6643, 'bangfrown': 891, 'pv': 9130, 'anywhere': 502, 'showbox': 10242, 'companion': 2213, 'skinny': 10371, 'zzzz': 13208, 'dubai': 3175, 'laribuggy': 6273, 'nutella': 7917, 'loses': 6616, 'couldve': 2393, 'sirius': 10330, 'goat': 4437, 'naut': 7606, 'frudging': 4155, 'numbers': 7912, 'bbz': 979, 'angeke': 424, 'sbali': 9868, 'khang': 6027, 'euuuwwww': 3527, 'worker': 12855, 'reach': 9273, 'styles': 11072, 'poutingnelle': 8844, 'jaysus': 5717, 'insecurities': 5479, 'buti': 1623, 'poop': 8749, 'rome': 9653, 'throat': 11649, 'eshaaax': 3493, 'llama': 6528, 'getwellsoonamber': 4338, 'ajol': 259, 'heath': 4908, 'ledger': 6348, 'permission': 8469, 'mugabi': 7457, 'leadssupersport': 6331, 'milkshake': 7194, 'witcher': 12791, 'papertown': 8296, 'bale': 859, 'bahay': 840, 'bahayan': 841, 'magisa': 6803, 'lang': 6256, 'sadlyf': 9755, 'bunso': 1594, 'sleeeeep': 10392, 'astonvilla': 658, 'berigaud': 1107, 'bakar': 851, 'louisianafrown': 6639, 'streams': 11020, 'allergic': 308, 'depressing': 2811, 'blaines': 1251, 'acoustic': 87, 'hernia': 4963, 'doctored': 3025, 'toxin': 11866, 'ariel': 584, 'slapped': 10388, 'slammed': 10386, 'bees': 1038, 'finddjderek': 3891, 'uuughhhh': 12286, 'grabe': 4516, 'smi': 10443, 'poutingjust': 8835, 'nemesis': 7663, 'xdconfusion': 12945, 'appealing': 529, 'neeeeeeeeeeeeeeeeein': 7646, 'saaaad': 9737, 'hanging': 4776, 'creases': 2465, 'homed': 5071, 'tanned': 11358, 'dallas': 2604, 'nichols': 7736, 'walks': 12517, 'infront': 5452, 'beato': 1007, 'tim': 11699, 'minha': 7217, 'deleicious': 2763, 'skinner': 10370, 'bliss': 1276, 'pcb': 8393, 'peregrines': 8455, 'tram': 11895, 'hav': 4845, 'apt': 556, 'worki': 12859, 'bldg': 1261, 'mmmmm': 7281, 'nicki': 7738, 'fucjikg': 4169, 'buynotanapologyonitunes': 1637, 'avalible': 717, 'frustrated': 4159, 'nw': 7924, 'sch': 9883, 'jeslyn': 5752, 'rooting': 9661, 'kuch': 6181, 'hua': 5184, 'bitching': 1226, 'newbies': 7686, 'miracle': 7236, 'wings': 12771, 'fail': 3700, 'growingheartgrowingheartgrowingheart': 4602, 'nux': 7920, 'hinanap': 5012, 'uy': 12291, 'sched': 9885, 'anyare': 486, 'entertain': 3442, 'typa': 12069, 'buddies': 1552, 'transparency': 11907, 'photoshop': 8543, 'decided': 2723, 'planner': 8647, 'helpppp': 4949, 'wearig': 12591, 'dri': 3142, 'prey': 8954, 'drained': 3120, 'ausfailia': 702, 'snow': 10628, 'footy': 4032, 'countsnd': 2405, 'row': 9684, 'mampms': 6856, 'kitkat': 6112, 'steeez': 10945, 'wearyfacewearyfacewearyfacecryingface': 12596, 'suger': 11116, 'olivia': 8027, 'bamps': 873, 'audition': 691, 'injurys': 5466, 'appendix': 536, 'appendicitis': 535, 'fack': 3686, 'nhl': 7718, 'hays': 4860, 'khamis': 6025, 'kadhafi': 5908, 'reaaly': 9270, 'naomi': 7576, 'arias': 583, 'contemporary': 2306, 'messed': 7133, 'slacke': 10384, 'jahat': 5672, 'discount': 2958, 'thorpe': 11628, 'payneourdaddy': 8382, 'guessed': 4637, 'nicely': 7732, 'esnho': 3496, 'node': 7797, 'directx': 2937, 'shading': 10098, 'uploaded': 12227, 'blackberry': 1243, 'povertyyouareevil': 8879, 'emm': 3371, 'lied': 6422, 'poutingyou': 8877, 'elgin': 3331, 'vava': 12323, 'makati': 6826, 'yellowheart': 13037, 'baon': 901, 'soak': 10634, 'bread': 1451, 'mush': 7483, 'theyd': 11586, 'ouat': 8172, 'blinkin': 1275, 'headack': 4872, 'tension': 11474, 'eritation': 3481, 'perspective': 8480, 'poutinglouis': 8838, 'endlessly': 3409, 'batting': 954, 'kiddo': 6045, 'rumbelle': 9712, 'irresponsibly': 5578, 'pakighinabi': 8257, 'pinkfinite': 8598, 'beb': 1022, 'migraine': 7179, 'coyote': 2431, 'unamusedfaceunamusedface': 12130, 'headache': 4870, 'baechu': 830, 'calibraskaep': 1672, 'elgato': 3330, 'ant': 473, 'unexpect': 12152, 'poutingbestfriend': 8815, 'tri': 11930, 'fainted': 3705, 'bp': 1416, 'subway': 11096, 'fragile': 4080, 'gap': 4259, 'plots': 8692, 'net': 7671, 'bungie': 1590, 'woohyun': 12843, 'guilty': 4643, 'davao': 2665, 'luckyyyy': 6707, 'eunhae': 3522, 'misplaced': 7242, 'dae': 2594, 'bap': 902, 'modoki': 7307, 'irwinstagram': 5580, 'plsfrown': 8694, 'huehue': 5190, 'rice': 9544, 'krispy': 6171, 'marshmallows': 6928, 'mmjunction': 7280, 'soulsurvivor': 10754, 'stafford': 10882, 'mixture': 7272, 'theyve': 11589, 'shems': 10160, 'lage': 6228, 'ramd': 9214, 'sorryfrown': 10737, 'munchkin': 7473, 'todayfrown': 11756, 'poutingbought': 8816, 'parting': 8323, 'giftso': 4363, 'juja': 5870, 'murugan': 7480, 'handles': 4767, 'bgtau': 1147, 'harap': 4801, 'bagi': 837, 'aminn': 383, 'fraand': 4079, 'grimacingface': 4578, 'bigbang': 1169, 'gg': 4342, 'sian': 10266, 'nicoleapage': 7743, 'hellish': 4936, 'thirstyyyy': 11605, 'chesties': 1942, 'nandos': 7574, 'bow': 1403, 'madexx': 6775, 'hen': 4953, 'rdd': 9268, 'dissipating': 2986, 'capeee': 1726, 'japan': 5695, 'outlive': 8186, 'xrays': 12960, 'dental': 2798, 'spinenothing': 10821, 'relief': 9408, 'popol': 8759, 'stomach': 10985, 'frog': 4146, 'genad': 4303, 'negotiable': 7651, 'huhuhuhuhu': 5207, 'bbmadeinmanila': 970, 'findavip': 3890, 'boyirl': 1413, 'yasss': 13000, 'june': 5889, 'itnot': 5628, 'laine': 6239, 'difficiency': 2905, 'customer': 2559, 'speed': 10806, 'rapists': 9234, 'crime': 2488, 'bachpan': 799, 'yaadein': 12982, 'finnair': 3907, 'heathrow': 4909, 'norwegian': 7848, 'batteries': 952, 'upvotes': 12240, 'keeno': 5989, 'whatthefuck': 12688, 'grotty': 4592, 'seeker': 9967, 'morality': 7360, 'fern': 3836, 'mimi': 7204, 'bali': 860, 'poutingshes': 8859, 'tooo': 11817, 'todayyesterday': 11761, 'pleaseeeee': 8678, 'lowbat': 6676, 'poutingotwolgrandtrailer': 8850, 'funkfrown': 4199, 'wewanticecream': 12666, 'sweat': 11236, 'eugh': 3521, 'sara': 9824, 'occasionally': 7949, 'izzys': 5652, 'dorm': 3082, 'choppy': 1990, 'contacted': 2299, 'infinites': 5434, 'hion': 5018, 'cayton': 1797, 'darcey': 2637, 'connor': 2280, 'spoke': 10835, 'ripped': 9573, 'bottle': 1395, 'roomits': 9657, 'roommateexperience': 9659, 'ic': 5271, 'te': 11410, 'autofollowback': 711, 'asianbut': 627, 'penguin': 8435, 'poutingsad': 8857, 'ljp': 6527, 'nowdays': 7877, 'pleass': 8683, 'rawrx': 9257, 'numb': 7909, 'dentist': 2799, 'ag': 181, 'misss': 7253, 'rid': 9550, 'tab': 11292, 'ucas': 12085, 'bigtime': 1174, 'rumor': 9714, 'poutingzayniscomingbackonjuly': 8878, 'warmest': 12541, 'greeting': 4564, 'chin': 1965, 'tickle': 11684, 'zikra': 13190, 'lusi': 6730, 'hasyaconfusion': 4831, 'nuggets': 7906, 'soms': 10695, 'browning': 1519, 'millies': 7198, 'ano': 459, 'stans': 10901, 'camping': 1699, 'mags': 6807, 'hateeeee': 4835, 'lease': 6339, 'written': 12913, 'sizing': 10353, 'gs': 4613, 'unsuccessful': 12205, 'earlobes': 3234, 'sue': 11107, 'dreary': 3134, 'noooooo': 7830, 'denise': 2796, 'murielle': 7478, 'ahours': 221, 'prbrand': 8901, 'lyra': 6751, 'kathd': 5965, 'responded': 9487, 'chopped': 1989, 'wbu': 12582, 'activity': 94, 'kme': 6130, 'cram': 2447, 'curious': 2543, 'announcements': 453, 'trespassers': 11928, 'clandestins': 2053, 'loudlycryingfacesparklingheart': 6636, 'muller': 7464, 'obvious': 7944, 'mufc': 7455, 'stu': 11050, 'buddyyyy': 1554, 'feelgoodfriday': 3812, 'camp': 1696, 'forest': 4039, 'babysit': 793, 'stayed': 10937, 'opixer': 8097, 'pilllow': 8586, 'fool': 4026, 'bragging': 1424, 'skrillah': 10376, 'drowned': 3163, 'gue': 4635, 'muchha': 7446, 'grans': 4537, 'sjkaos': 10356, 'angrynow': 436, 'disappointedmmsrry': 2950, 'stuffy': 11063, 'boconfusion': 1323, 'honma': 5090, 'yeh': 13027, 'walay': 12509, 'bohat': 1329, 'wailay': 12492, 'preseason': 8929, 'shor': 10216, 'machaya': 6765, 'samjha': 9797, 'shopaholic': 10213, 'smilingfacewithopenmouthsmilingeyesthumbsup': 10581, 'pensivefacefacewithcoldsweat': 8442, 'sirkay': 10331, 'wali': 12511, 'pyaaz': 9135, 'daal': 2587, 'onions': 8061, 'vinegar': 12404, 'cooking': 2344, 'soho': 10666, 'wobbly': 12812, 'ciao': 2025, 'masaan': 6939, 'muv': 7507, 'beast': 1001, 'hayst': 4861, 'hnnnnnnn': 5044, 'saaad': 9738, 'rsa': 9692, 'ates': 662, 'whileconfusion': 12704, 'optimisation': 8111, 'soniii': 10706, 'kahaaaa': 5912, 'freezing': 4099, 'fml': 3987, 'jackets': 5659, 'sleepy': 10403, 'bullying': 1580, 'racial': 9184, 'halls': 4750, 'looooool': 6598, 'slutcake': 10429, 'playfrown': 8664, 'sobs': 10640, 'busier': 1617, 'onwards': 8071, 'coincidence': 2144, 'imac': 5341, 'grams': 4526, 'lived': 6511, 'nearer': 7631, 'blainedarren': 1250, 'fuuuuuuuuuck': 4212, 'gishwhes': 4392, 'excluded': 3576, 'angelica': 426, 'pulled': 9090, 'poutinggot': 8824, 'movement': 7399, 'frou': 4151, 'vaccines': 12296, 'armor': 589, 'legendary': 6360, 'cash': 1773, 'effort': 3289, 'trips': 11947, 'nat': 7590, 'brake': 1434, 'grumpy': 4610, 'hahahax': 4711, 'wrecking': 12905, 'gahhh': 4225, 'terible': 11482, 'kiligs': 6063, 'sugg': 11117, 'weeksurprise': 12627, 'blacktomorrow': 1248, 'weaker': 12585, 'shravan': 10253, 'maas': 6758, 'stooooop': 10991, 'giguilty': 4369, 'akooooooooooooooo': 268, 'imveryverysorry': 5377, 'poutingso': 8861, 'greyed': 4572, 'basename': 932, 'theme': 11556, 'sessions': 10051, 'cigar': 2028, 'speakers': 10792, 'anyways': 500, 'offered': 7970, 'promethazine': 9021, 'zopiclone': 13201, 'addition': 117, 'quetiapine': 9161, 'modified': 7305, 'releasetypically': 9404, 'prescription': 8928, 'greska': 4569, 'macedonian': 6764, 'slovak': 10420, 'hiking': 4999, 'tested': 11490, 'browsers': 1524, 'zokay': 13195, 'accentconfusion': 56, 'bbut': 975, 'gintama': 4379, 'shinsengumi': 10177, 'chapters': 1877, 'crapple': 2452, 'agrees': 206, 'phandroid': 8508, 'tline': 11740, 'orchestra': 8124, 'rehearsal': 9380, 'bittersweetness': 1236, 'eunji': 3523, 'bakit': 853, 'ehdar': 3296, 'pegea': 8427, 'panga': 8285, 'dosto': 3091, 'poutingopen': 8849, 'realliampayne': 9297, 'dmed': 3014, 'canceled': 1710, 'alreaddyyy': 334, 'luceleva': 6699, 'naeun': 7538, 'kidneys': 6047, 'ink': 5468, 'themselvesihatesomepeople': 11559, 'table': 11293, 'hardwired': 4808, 'canadian': 1704, 'acne': 84, 'poutinggulo': 8826, 'poutingkandekjs': 8836, 'rize': 9593, 'meydanconfusion': 7149, 'experienced': 3605, 'fcking': 3798, 'crei': 2477, 'connection': 2277, 'dormmates': 3083, 'bo': 1314, 'activation': 92, 'cod': 2137, 'redeem': 9347, 'invalid': 5545, 'wag': 12482, 'hopia': 5108, 'editor': 3270, 'revealed': 9522, 'boooo': 1366, 'extensions': 3626, 'rightnow': 9562, 'btu': 1539, 'blaine': 1249, 'licence': 6418, 'apb': 511, 'mbf': 7003, 'hahahaokay': 4710, 'capcom': 1725, 'basara': 928, 'url': 12250, 'booking': 1356, 'grumbling': 4609, 'migrant': 7180, 'awsme': 754, 'picking': 8560, 'tmwuwuwell': 11747, 'jinki': 5778, 'taems': 11301, 'gifs': 4359, 'cambridge': 1689, 'viathe': 12363, 'cyprus': 2582, 'zayncomebacktod': 13172, 'spazzing': 10789, 'soobin': 10711, 'dances': 2622, 'poutingsummer': 8862, 'unmarried': 12191, 'floating': 3968, 'pressure': 8938, 'lifetime': 6431, 'hiondsheings': 5019, 'sexdate': 10074, 'demis': 2786, 'junjou': 5893, 'romantica': 9652, 'isla': 5593, 'cruel': 2506, 'privilege': 8981, 'mixtape': 7271, 'convince': 2335, 'friexs': 4136, 'shaylan': 10145, 'ylona': 13076, 'reality': 9292, 'waitting': 12499, 'andgore': 411, 'noon': 7825, 'likeouh': 6449, 'riseso': 9581, 'tax': 11384, 'ohhhh': 7990, 'nm': 7789, 'encanta': 3399, 'vale': 12301, 'osea': 8154, 'bea': 991, 'muchi': 7448, 'followplease': 4015, 'ennyselise': 3431, 'awwwwwwwwwww': 765, 'starz': 10923, 'baek': 832, 'dunwan': 3203, 'haiz': 4734, 'dc': 2686, 'hill': 5004, 'confusedfacefrown': 2257, 'insonia': 5489, 'rick': 9548, 'ross': 9670, 'balling': 866, 'poutingo': 8847, 'heartbreaking': 4894, 'millie': 7197, 'pagehappy': 8242, 'golden': 4456, 'homes': 5074, 'rosebury': 9665, 'familyhome': 3729, 'monkey': 7334, 'durance': 3208, 'erica': 3478, 'shikinrizal': 10175, 'istg': 5610, 'jackson': 5662, 'nsbzhdnxndamal': 7892, 'amhours': 377, 'amsigh': 394, 'mahilig': 6810, 'mambully': 6852, 'mtaani': 7427, 'tunaita': 12004, 'viazi': 12364, 'choma': 1986, 'celebrated': 1807, 'jerk': 5750, 'magicfrown': 6799, 'menille': 7100, 'kams': 5927, 'meeee': 7048, 'diz': 3004, 'bioooo': 1198, 'ay': 767, 'taray': 11362, 'yumuyoutuber': 13149, 'hurting': 5239, 'parijat': 8308, 'willmissyouparijat': 12754, 'sets': 10056, 'jollies': 5817, 'needing': 7644, 'mcnuggets': 7016, 'sophie': 10727, 'caramello': 1731, 'koalas': 6153, 'suckmejimin': 11102, 'sucky': 11104, 'pou': 8807, 'goddamn': 4442, 'barking': 919, 'nje': 7783, 'dbn': 2685, 'ribbon': 9542, 'hardest': 4805, 'wout': 12896, 'abbeytaylor': 24, 'pledge': 8686, 'viber': 12366, 'mwah': 7513, 'estate': 3505, 'lansi': 6264, 'hp': 5168, 'waah': 12474, 'vandag': 12313, 'monthre': 7343, 'kgola': 6022, 'neng': 7664, 'eintlik': 3307, 'cryingface': 2517, 'porn': 8769, 'repost': 9456, 'whaaaaaat': 12673, 'whyyyyyyy': 12735, 'magpie': 6806, 'marshmallow': 6927, 'chswiyfxcskcalum': 2014, 'nvm': 7922, 'fooffrown': 4025, 'casillas': 1775, 'manchester': 6865, 'madritongue': 6785, 'xi': 12949, 'rmtour': 9602, 'songcan': 10702, 'irl': 5574, 'bloopers': 1296, 'huhuhuhu': 5206, 'natake': 7591, 'sorta': 10740, 'unfriend': 12166, 'poutingltgreysonchance': 8839, 'disappointed': 2948, 'sandwich': 9813, 'thenfrown': 11562, 'belle': 1075, 'sebastian': 9948, 'rewatched': 9531, 'sers': 10040, 'outdated': 8180, 'abandoned': 22, 'peopleshooting': 8449, 'theater': 11545, 'smhdamn': 10442, 'ceceli': 1805, 'ekk': 3312, 'giriboy': 4381, 'danni': 2634, 'harriet': 4814, 'gegu': 4296, 'gray': 4548, 'mcclure': 7006, 'politics': 8737, 'blaming': 1256, 'representing': 9459, 'corbyn': 2359, 'labours': 6211, 'fortunes': 4060, 'ry': 9730, 'lfccw': 6404, 'ontheroadagain': 8067, 'halaaaang': 4739, 'recieved': 9321, 'flip': 3961, 'flops': 3972, 'caesarspalace': 1655, 'socialrewards': 10647, 'esse': 3502, 'cali': 1669, 'wo': 12808, 'fuckboys': 4171, 'chennai': 1937, 'chrompet': 2011, 'immune': 5357, 'lush': 6729, 'bathtub': 949, 'phpmysql': 8545, 'libmysqlclientdev': 6414, 'sync': 11281, 'dev': 2850, 'pleasanton': 8674, 'wala': 12505, 'earlyconfusion': 3236, 'heed': 4920, 'gwss': 4675, 'poutingredheartredheart': 8856, 'thankyouuredheart': 11532, 'charade': 1882, 'piano': 8555, 'pippy': 8607, 'scored': 9907, 'complaint': 2225, 'yelling': 13034, 'whatsoever': 12687, 'wentz': 12654, 'shogi': 10207, 'blameshoghicp': 1255, 'kinds': 6087, 'classmate': 2066, 'friendseverything': 4132, 'ordering': 8128, 'fixedgearfrenzy': 3932, 'dispatch': 2981, 'hats': 4843, 'shamuons': 10115, 'toe': 11766, 'poutingmanaged': 8840, 'horrendously': 5115, 'jumping': 5886, 'someones': 10687, 'hasb': 4822, 'atty': 687, 'mujy': 7461, 'sirf': 10329, 'helpfulsensible': 4945, 'cyclerevolution': 2577, 'caaaannntttt': 1648, 'overdrawn': 8201, 'tbf': 11397, 'complained': 2223, 'zayniscomingbackonjulyim': 13177, 'cryingi': 2519, 'perfume': 8465, 'samples': 9801, 'chanelburberryprada': 1861, 'noesss': 7800, 'topgear': 11827, 'bridesmaid': 1471, 'gathering': 4273, 'sudden': 11105, 'randomrestart': 9223, 'randomreboot': 9222, 'lumia': 6718, 'windowsphone': 12765, 'microsofts': 7168, 'addressing': 122, 'maana': 6757, 'solos': 10676, 'rappings': 9236, 'striker': 11029, 'lvg': 6738, 'fuuuuuck': 4211, 'helppp': 4948, 'refurbished': 9366, 'cintiq': 2037, 'originates': 8145, 'finnicks': 3908, 'askfinnick': 632, 'container': 2303, 'hairy': 4730, 'buried': 1602, 'omaygad': 8040, 'vic': 12369, 'surgery': 11198, 'tttt': 11986, 'hyper': 5257, 'imiss': 5347, 'knowfor': 6143, 'prepaid': 8922, 'grandma': 4532, 'grandpas': 4535, 'cow': 2429, 'sheeps': 10151, 'vegetables': 12331, 'puking': 9088, 'delirious': 2772, 'motilium': 7384, 'shite': 10189, 'schoolworks': 9895, 'phoebes': 8526, 'pothole': 8804, 'sorryi': 10738, 'robyn': 9625, 'necklaces': 7639, 'rachel': 9183, 'upfrown': 12219, 'shan': 10116, 'ramzan': 9217, 'crosssed': 2501, 'clapham': 2055, 'investigate': 5548, 'sth': 10967, 'purchased': 9109, 'essentially': 3503, 'photoshoooot': 8541, 'mahone': 6815, 'shut': 10259, 'andaming': 409, 'memorization': 7092, 'cotton': 2385, 'sickfrown': 10272, 'concept': 2236, 'swallowing': 11228, 'snot': 10627, 'chokes': 1984, 'taknottem': 11324, 'btob': 1537, 'percentage': 8454, 'swift': 11256, 'ait': 252, 'poutingwsalelove': 8876, 'sexyjane': 10083, 'goodmusic': 4474, 'lart': 6276, 'sew': 10069, 'nogrimacingface': 7803, 'chairfrown': 1843, 'clevermonkey': 2082, 'skyfall': 10381, 'premiere': 8915, 'yummyjust': 13148, 'pee': 8419, 'suns': 11158, 'xgn': 12948, 'manteca': 6883, 'chair': 1842, 'shiatsu': 10168, 'setting': 10058, 'heat': 4906, 'risk': 9583, 'edward': 3278, 'hopper': 5111, 'eyyah': 3649, 'dughtr': 3187, 'utd': 12279, 'cart': 1767, 'logged': 6555, 'aaaaaaaaaaaaa': 2, 'waifu': 12490, 'breakup': 1458, 'bias': 1161, 'syndrome': 11284, 'shy': 10262, 'biases': 1162, 'pixelated': 8626, 'karltovera': 5952, 'weh': 12629, 'apayor': 510, 'maymay': 6994, 'advance': 143, 'allowance': 317, 'magpaalam': 6805, 'tf': 11501, 'changs': 1866, 'backstory': 814, 'cabangal': 1649, 'neato': 7636, 'wru': 12918, 'scissors': 9899, 'komal': 6160, 'creations': 2469, 'amtired': 396, 'imysm': 5379, 'tut': 12018, 'trop': 11953, 'tard': 11363, 'deadline': 2697, 'frogs': 4147, 'ryry': 9733, 'shepherds': 10162, 'premiun': 8919, 'notcool': 7855, 'lahat': 6235, 'araw': 562, 'naging': 7543, 'gyu': 4680, 'lmfaoooo': 6536, 'deleted': 2765, 'mashup': 6946, 'eu': 3520, 'lcs': 6324, 'yass': 12999, 'relatives': 9395, 'yr': 13131, 'sydney': 11276, 'perf': 8456, 'programmes': 9013, 'agoconfusion': 201, 'hashtags': 4828, 'omfg': 8043, 'combat': 2182, 'dosent': 3088, 'sods': 10656, 'yahoo': 12988, 'yodel': 13080, 'jokingly': 5816, 'seriousness': 10039, 'gahd': 4224, 'zayns': 13178, 'dragging': 3115, 'discover': 2959, 'obyun': 7947, 'wayhh': 12576, 'prevalent': 8949, 'controversy': 2327, 'teacupwithouthandlehotbeverage': 11415, 'tube': 11989, 'strike': 11028, 'youthats': 13118, 'inches': 5393, 'meck': 7037, 'sleeeep': 10393, 'mcfc': 7009, 'ucan': 12084, 'poc': 8714, 'blocking': 1281, 'specific': 10800, 'sinhala': 10321, 'billionaires': 1185, 'maldives': 6837, 'dheena': 2872, 'fasgadah': 3765, 'alvadhaau': 349, 'burned': 1605, 'rolls': 9648, 'countdown': 2397, 'functions': 4194, 'desktop': 2832, 'evelineconrade': 3531, 'kikmsn': 6060, 'selfshot': 9995, 'backkk': 812, 'heartsuitheartsuitheartsuit': 4904, 'relaxing': 9400, 'dull': 3191, 'overcast': 8200, 'phipps': 8523, 'missin': 7249, 'hangin': 4775, 'wiff': 12743, 'interactive': 5524, 'dept': 2812, 'cherry': 1940, 'bakewell': 852, 'collecting': 2162, 'teal': 11416, 'sect': 9955, 'tennunb': 11473, 'skip': 10373, 'doomsday': 3078, 'neglected': 7649, 'postie': 8792, 'bellamy': 1074, 'raven': 9255, 'clarke': 2059, 'helmy': 4940, 'uh': 12098, 'cnt': 2121, 'whereisthesun': 12699, 'summerismissing': 11138, 'longggg': 6589, 'ridiculous': 9558, 'stocko': 10981, 'lucozade': 6708, 'shootings': 10211, 'explosions': 3621, 'zarryy': 13170, 'beh': 1052, 'halfremembered': 4743, 'melodys': 7082, 'recall': 9311, 'mouths': 7396, 'mile': 7185, 'dropped': 3157, 'travelling': 11915, 'expo': 3622, 'jisoo': 5779, 'anon': 460, 'mager': 6793, 'wifrown': 12745, 'wht': 12727, 'worlds': 12875, 'londonim': 6577, 'yourselfit': 13113, 'buffering': 1563, 'insane': 5477, 'charli': 1893, 'ganas': 4247, 'studio': 11058, 'arch': 565, 'methey': 7143, 'mspxo': 7424, 'lyin': 6744, 'kian': 6037, 'youuuuuufrown': 13128, 'supercars': 11169, 'gurgaon': 4659, 'locations': 6548, 'whyyyy': 12732, 'satire': 9837, 'regretting': 9375, 'peanut': 8409, 'viners': 12405, 'palembang': 8267, 'sorrrrryyyyy': 10734, 'fany': 3754, 'mcinnes': 7014, 'boner': 1346, 'mercy': 7118, 'jake': 5676, 'gyllenhaal': 4676, 'impact': 5359, 'ledgers': 6349, 'coldcough': 2148, 'depleted': 2808, 'mbasa': 7000, 'client': 2087, 'hopeconfusion': 5100, 'aah': 10, 'blocks': 1283, 'pacopy': 8236, 'biomes': 1197, 'mosque': 7374, 'smelly': 10439, 'emptier': 3387, 'ciaras': 2026, 'everythings': 3553, 'clipsconfusion': 2099, 'tall': 11340, 'intention': 5520, 'kissed': 6103, 'ambs': 370, 'harrys': 4817, 'smileyfrown': 10499, 'mayday': 6992, 'parade': 8299, 'lyf': 6743, 'pensivefacepensivefacepensiveface': 8443, 'animal': 438, 'risky': 9585, 'brooks': 1509, 'cologne': 2169, 'avonsparadise': 725, 'twodevils': 12052, 'babes': 786, 'duo': 3204, 'ballads': 864, 'bish': 1217, 'intern': 5530, 'yumyum': 13150, 'cathys': 1789, 'missyou': 7254, 'bishes': 1218, 'ruby': 9702, 'pora': 8767, 'karlia': 5950, 'khatam': 6030, 'bandi': 884, 'loudlycryingfacecrown': 6629, 'pyaari': 9134, 'gawd': 4277, 'massis': 6955, 'poutingthatselfiethough': 8866, 'aishhh': 249, 'viewer': 12393, 'sighssss': 10284, 'burnt': 1609, 'toffee': 11768, 'honesty': 5086, 'slight': 10411, 'cheatday': 1910, 'protein': 9050, 'sissi': 10335, 'tote': 11847, 'slowly': 10426, 'church': 2019, 'pll': 8690, 'sel': 9978, 'serbia': 10031, 'serbian': 10032, 'selenators': 9987, 'motavators': 7380, 'kissmarkredheartkissmarkredheart': 6106, 'zayyyyyn': 13179, 'died': 2892, 'happend': 4781, 'rebelinme': 9307, 'imperative': 5362, 'cleaned': 2071, 'panas': 8277, 'quickest': 9166, 'sake': 9770, 'hamstring': 4759, 'injury': 5465, 'rodwell': 9635, 'likeme': 6448, 'tracing': 11871, 'powder': 8881, 'snaeun': 10604, 'wider': 12740, 'pains': 8248, 'omnibus': 8050, 'bruno': 1531, 'ed': 3260, 'croke': 2496, 'toll': 11778, 'shape': 10121, 'khooie': 6033, 'muchconfusion': 7445, 'unluckiest': 12189, 'bettor': 1135, 'nstp': 7896, 'sem': 10002, 'tan': 11352, 'chipotle': 1970, 'chickfila': 1950, 'stole': 10983, 'ramadhan': 9213, 'stexpert': 10966, 'ripstegi': 9576, 'nickyyyyyyyyyyyyyyyy': 7741, 'emotion': 3378, 'centralise': 1817, 'discontinued': 2957, 'disappointment': 2952, 'sniff': 10620, 'struggles': 11047, 'bother': 1392, 'nurannisaaa': 7914, 'atirahmatasri': 666, 'nfr': 7707, 'ainzuhraa': 234, 'therese': 11577, 'creds': 2473, 'sheenapadua': 10149, 'ttit': 11985, 'eliminated': 3334, 'poutingteamzipalsmtm': 8864, 'assingnment': 651, 'editied': 3267, 'nakaka': 7556, 'beastmode': 1002, 'gaaawd': 4221, 'robs': 9624, 'colombia': 2171, 'yots': 13096, 'labyo': 6213, 'prettyfaceblowingakissfaceblowingakiss': 8946, 'pano': 8288, 'nalamannn': 7560, 'bebeee': 1024, 'toogot': 11813, 'hardheaded': 4806, 'zachs': 13162, 'xpress': 12959, 'hopkins': 5110, 'melatonin': 7076, 'tasks': 11369, 'hahaah': 4697, 'frequently': 4104, 'jail': 5674, 'weirddd': 12635, 'bestfriends': 1122, 'donghyuk': 3061, 'beks': 1063, 'reynoldsgrl': 9536, 'shade': 10096, 'ole': 8024, 'beardy': 1000, 'saaaaaaad': 9736, 'kaussies': 5973, 'pixels': 8628, 'bummer': 1584, 'fightingmcirene': 3863, 'michaels': 7165, 'exercising': 3584, 'ignored': 5312, 'meheartsuitheartsuitheartsuitheartsuit': 7066, 'miserable': 7241, 'poutingsweatdropletsumbrellawithraindrops': 8863, 'sunwithfacewomandancingmediumskintonetropicaldrink': 11163, 'meangtfrown': 7025, 'snakeskeptical': 10607, 'shouldve': 10233, 'stalking': 10889, 'saffron': 9764, 'queens': 9155, 'nfinite': 7706, 'adminmyung': 130, 'slp': 10427, 'saddest': 9749, 'downyou': 3109, 'laomma': 6266, 'kebaya': 5985, 'bandung': 885, 'indonesialine': 5428, 'dfwhatsapp': 2865, 'laommacouture': 6267, 'beatrizhoran': 1010, 'haizzz': 4735, 'urghhh': 12248, 'workingonatightschedule': 12863, 'ganbarimasu': 4248, 'livid': 6520, 'whammy': 12678, 'meheartsuitheartsuitheartsuit': 7065, 'quuuuuuee': 9177, 'friooooo': 4139, 'meheartsuitheartsuit': 7064, 'meheartsuit': 7063, 'stereo': 10959, 'kicks': 6041, 'chwang': 2022, 'lorm': 6609, 'unhappy': 12167, 'nathansinclair': 7598, 'lolzz': 6572, 'dats': 2661, 'corey': 2360, 'bng': 1312, 'mahirap': 6811, 'accept': 57, 'foods': 4024, 'fried': 4126, 'noodles': 7824, 'lovepurpleheartpurpleheartpurpleheart': 6660, 'veeeeerry': 12328, 'orig': 8141, 'starholicxx': 10906, 'amconfusion': 371, 'notr': 7870, 'hwy': 5251, 'fraud': 4086, 'diplomacy': 2929, 'survival': 11210, 'fittest': 3925, 'zero': 13188, 'tolerant': 11776, 'performances': 8462, 'pier': 8573, 'approach': 547, 'rattle': 9252, 'robe': 9614, 'hack': 4690, 'emphasis': 3383, 'vocals': 12440, 'chose': 1997, 'broco': 1497, 'leemiller': 6352, 'abbycan': 26, 'persuade': 8481, 'lyrics': 6752, 'emilys': 3369, 'elected': 3322, 'shiiiit': 10172, 'kamiss': 5926, 'mwa': 7512, 'scream': 9918, 'beforeduring': 1042, 'cafe': 1656, 'melbourne': 7077, 'anyonneeee': 493, 'loner': 6581, 'fricken': 4111, 'rito': 9587, 'friendzone': 4134, 'panels': 8283, 'startim': 10915, 'everythingfrown': 3552, 'hsm': 5178, 'canarios': 1708, 'bothering': 1394, 'ukiss': 12104, 'kurt': 6189, 'fatmam': 3775, 'lmfao': 6535, 'honestlymore': 5085, 'poutingplease': 8852, 'flapjacks': 3937, 'countthecost': 2406, 'ihop': 5315, 'cyclists': 2579, 'infra': 5448, 'rolled': 9645, 'lq': 6682, 'sotired': 10748, 'mybrainneedstoshutoff': 7517, 'maccies': 6763, 'wtfi': 12925, 'againand': 183, 'silicon': 10297, 'kbye': 5981, 'ini': 5457, 'tuilagi': 11993, 'embarrassed': 3358, 'citizens': 2043, 'compared': 2216, 'ranking': 9230, 'mcountdown': 7017, 'hesitantfrown': 4968, 'thapelo': 11536, 'chocolates': 1977, 'peanuts': 8410, 'civ': 2046, 'crashed': 2454, 'wooden': 12839, 'mic': 7161, 'organization': 8138, 'translate': 11903, 'mechatotems': 7036, 'nak': 7555, 'tgk': 11507, 'townssssss': 11864, 'jokid': 5814, 'rent': 9432, 'fuckfrown': 4174, 'spacey': 10770, 'tinker': 11713, 'inconsiderate': 5403, 'softball': 10661, 'tomcat': 11786, 'chel': 1929, 'jemma': 5738, 'matchy': 6966, 'elsa': 3345, 'postpone': 8795, 'karin': 5949, 'vist': 12429, 'unhealthy': 12168, 'propa': 9036, 'knockin': 6141, 'owwww': 8224, 'sandwiches': 9814, 'preholiday': 8913, 'meanies': 7026, 'deathbybaconsmell': 2707, 'inital': 5459, 'location': 6547, 'destination': 2839, 'fxconfusion': 4215, 'victoriaamberlunaand': 12374, 'krystal': 6175, 'ajaaaa': 257, 'sarajevo': 9827, 'wearyfacewearyfacewearyfacewearyfaceloudlycryingface': 12597, 'haix': 4733, 'sp': 10767, 'wii': 12748, 'bayonetta': 962, 'transfers': 11901, 'doable': 3019, 'drove': 3161, 'trains': 11893, 'awfulconfusion': 746, 'agencies': 191, 'storymiss': 11008, 'everone': 3542, 'jps': 5848, 'mamabear': 6848, 'poutingimintoher': 8831, 'ughi': 12095, 'nowi': 7880, 'understands': 12146, 'underrated': 12142, 'slovakias': 10421, 'saklap': 9774, 'rizal': 9592, 'lib': 6413, 'lam': 6243, 'enthusiast': 3444, 'advisory': 151, 'period': 8467, 'dit': 2996, 'dus': 3211, 'harsh': 4819, 'ohgod': 7987, 'abligaverins': 34, 'sexygirlbypreciouslemmy': 10082, 'ripsandrabland': 9575, 'cri': 2483, 'catching': 1785, 'damnits': 2615, 'edel': 3261, 'salam': 9778, 'mubark': 7440, 'eidfeeling': 3302, 'badsad': 827, 'dong': 3060, 'tammirossm': 11348, 'speck': 10801, 'abbymill': 27, 'cigarella': 2029, 'ion': 5557, 'lock': 6549, 'hse': 5177, 'noob': 7823, 'pensiveface': 8441, 'fck': 3796, 'nae': 7536, 'naefrown': 7537, 'whit': 12711, 'vans': 12317, 'bristol': 1490, 'subserver': 11090, 'platon': 8656, 'tub': 11988, 'penyfan': 8445, 'breconbeacons': 1463, 'tittheir': 11734, 'hottie': 5147, 'fuzzy': 4213, 'antonio': 481, 'kang': 5928, 'junhee': 5891, 'pz': 9137, 'waitfrown': 12497, 'somersetlooking': 10688, 'sunburnt': 11145, 'safer': 9763, 'releasing': 9405, 'sim': 10301, 'kg': 6021, 'inputs': 5475, 'gamestomp': 4243, 'desc': 2819, 'sirplease': 10332, 'angelos': 429, 'yna': 13077, 'fiver': 3928, 'sakho': 9771, 'threat': 11639, 'goalscorer': 4436, 'nooooooo': 7831, 'sham': 10111, 'baao': 780, 'alrightfrown': 338, 'nisrina': 7776, 'bcs': 983, 'ladygaga': 6223, 'marrish': 6925, 'otps': 8166, 'yesssad': 13052, 'edomnt': 3274, 'qih': 9140, 'shxbs': 10261, 'stolen': 10984, 'chilton': 1964, 'wasted': 12554, 'creepy': 2476, 'boohoo': 1353, 'roar': 9608, 'lions': 6483, 'victory': 12376, 'tweepsmatchout': 12027, 'angelinahoran': 427, 'nein': 7657, 'communism': 2209, 'willlow': 12753, 'sowwy': 10766, 'grinds': 4581, 'gears': 4288, 'winning': 12777, 'mode': 7300, 'shotsfrown': 10227, 'madi': 6778, 'mochila': 7297, 'gtgtfrown': 4623, 'shanzay': 10120, 'salabraty': 9776, 'journo': 5839, 'lure': 6726, 'gang': 4252, 'twisting': 12046, 'mashaket': 6944, 'pets': 8499, 'bapak': 903, 'prima': 8962, 'hospitalconfusion': 5127, 'mune': 7475, 'plisss': 8689, 'poutingmy': 8842, 'sunway': 11161, 'petaling': 8489, 'jaya': 5713, 'selangorconfusion': 9980, 'glow': 4423, 'huhuu': 5208, 'margo': 6905, 'konga': 6163, 'topeh': 11826, 'wathrice': 12567, 'ode': 7959, 'etopeh': 3518, 'disvirginedlike': 2995, 'negotiate': 7652, 'yulin': 13143, 'meat': 7033, 'syawal': 11275, 'lapar': 6269, 'foundation': 4069, 'facil': 3685, 'dh': 2867, 'chalet': 1846, 'suay': 11076, 'anot': 463, 'buggers': 1567, 'chandauli': 1860, 'majhwar': 6823, 'titos': 11733, 'titas': 11729, 'critical': 2492, 'condition': 2245, 'steals': 10941, 'narcos': 7580, 'regens': 9370, 'unfaved': 12156, 'tofrom': 11769, 'benadryl': 1087, 'yg': 13063, 'sxrew': 11273, 'dissappeared': 2985, 'swapping': 11230, 'bleeding': 1265, 'placefrown': 8636, 'ishal': 5587, 'thaanks': 11514, 'jhezz': 5767, 'sneaks': 10618, 'defence': 2741, 'defensive': 2745, 'nrltigersroosters': 7891, 'indiana': 5412, 'hibbs': 4977, 'nh': 7715, 'biblethump': 1165, 'rlyyyy': 9599, 'septum': 10029, 'pierced': 8575, 'tonightfrown': 11805, 'venomous': 12339, 'carriage': 1761, 'furtrimmed': 4207, 'stetsons': 10962, 'error': 3485, 'xue': 12964, 'midori': 7176, 'disabled': 2944, 'sakit': 9773, 'disgusts': 2971, 'shityoure': 10195, 'mateo': 6968, 'luckygrowingheart': 6706, 'transferred': 11900, 'bartender': 926, 'poutingfeel': 8821, 'shockingwhy': 10203, 'livesometimes': 6517, 'despair': 2833, 'poutinghope': 8827, 'iwantin': 5644, 'fault': 3776, 'helpcouk': 4942, 'benedictervention': 1093, 'contents': 2309, 'popcorn': 8757, 'joycecalm': 5842, 'oooops': 8082, 'paalam': 8228, 'sair': 9769, 'sazballs': 9866, 'cuting': 2567, 'incident': 5394, 'aaaahh': 7, 'gooooo': 4493, 'stomachs': 10987, 'growling': 4604, 'shave': 10140, 'beard': 999, 'nooooope': 7835, 'faceblowingakisspartypopper': 3661, 'marta': 6930, 'hundred': 5223, 'meg': 7055, 'veritys': 12351, 'rupert': 9723, 'mancio': 6866, 'studying': 11061, 'pleaaase': 8671, 'backhandindexpointinguplightskintonebackhandindexpointinguplightskintone': 811, 'woaah': 12809, 'solvo': 10680, 'cryingdont': 2516, 'twin': 12041, 'crow': 2502, 'loveam': 6647, 'lego': 6365, 'barefooted': 912, 'twelvyy': 12037, 'boaz': 1320, 'myhill': 7519, 'takeover': 11319, 'wba': 12581, 'taeyeons': 11303, 'derp': 2817, 'pd': 8399, 'zoom': 13199, 'sunnys': 11156, 'besst': 1116, 'plague': 8638, 'pits': 8621, 'serwiee': 10049, 'sight': 10285, 'frail': 4082, 'twurkin': 12058, 'razzist': 9264, 'reports': 9455, 'tumblr': 11999, 'shek': 10157, 'plssssssssssssssss': 8699, 'taissa': 11313, 'farmiga': 3762, 'roberts': 9617, 'ahs': 223, 'software': 10662, 'restore': 9497, 'momo': 7325, 'pharma': 8511, 'sugaralmost': 11114, 'immovable': 5356, 'gots': 4506, 'messy': 7138, 'anshe': 467, 'hippie': 5020, 'sadfrown': 9750, 'billions': 1186, 'rands': 9224, 'phoi': 8527, 'bein': 1059, 'sadlolgo': 9753, 'tla': 11739, 'tweng': 12038, 'genes': 4308, 'upcome': 12213, 'county': 2407, 'dmhelperr': 3015, 'cooler': 2347, 'minhyuks': 7219, 'sleepyfacesleepyfacesleepyface': 10404, 'nas': 7587, 'hz': 5264, 'wolle': 12818, 'emta': 3391, 'hatigii': 4840, 'baa': 778, 'anesthesia': 421, 'penrithemu': 8438, 'plains': 8639, 'missconfusion': 7245, 'knowvery': 6148, 'tiffanyhwang': 11691, 'slotted': 10419, 'untouched': 12209, 'brienne': 1475, 'lsh': 6687, 'gunna': 4656, 'former': 4054, 'darn': 2648, 'pakistans': 8262, 'juudiciary': 5904, 'hortons': 5125, 'dunkin': 3200, 'socialising': 10645, 'cara': 1729, 'delevingnes': 2766, 'laced': 6215, 'fank': 3745, 'takfaham': 11322, 'melissa': 7080, 'ufffff': 12090, 'sr': 10865, 'dard': 2638, 'katekyn': 5962, 'ehh': 3298, 'amiir': 381, 'mylife': 7520, 'hacharatt': 4689, 'niwll': 7779, 'poorfrown': 8752, 'define': 2747, 'witnessed': 12795, 'goa': 4433, 'orrhettofrappe': 8148, 'linis': 6476, 'kasi': 5956, 'sweating': 11238, 'brothers': 1515, 'pissed': 8614, 'rhd': 9537, 'physical': 8549, 'wae': 12480, 'subsidized': 11091, 'lunches': 6722, 'youngjae': 13107, 'breakups': 1459, 'harumph': 4820, 'soggy': 10663, 'weeding': 12616, 'experiencing': 3607, 'errors': 3486, 'minutesfrown': 7235, 'teas': 11434, 'sakura': 9775, 'flavour': 3946, 'chokkie': 1985, 'wearyfaceloudlycryingfaceteacupwithouthandlecherryblossom': 12595, 'siman': 10302, 'unavailable': 12133, 'richard': 9546, 'satya': 9842, 'aditya': 128, 'steamingbowlsteamingbowlsteamingbowlsteamingbowlsteamingbowl': 10944, 'vibrating': 12368, 'cus': 2553, 'parker': 8312, 'dhaka': 2869, 'poutingguess': 8825, 'jam': 5681, 'cheering': 1921, 'cornettos': 2366, 'nosebleed': 7850, 'nintendo': 7771, 'wew': 12665, 'ramos': 9216, 'ground': 4593, 'shawn': 10142, 'mendes': 7097, 'xconfusion': 12942, 'dinghy': 2924, 'skye': 10380, 'namedescription': 7565, 'gagal': 4223, 'txt': 12060, 'sims': 10309, 'noooot': 7838, 'notch': 7854, 'thts': 11661, 'starving': 10921, 'pyjamas': 9136, 'envy': 3459, 'suks': 11130, 'swifties': 11257, 'sorna': 10733, 'db': 2684, 'lurgy': 6727, 'kits': 6114, 'scaring': 9877, 'fenestoscope': 3831, 'etienne': 3516, 'bandana': 881, 'dressfrown': 3139, 'valeria': 12304, 'iyer': 5647, 'suriya': 11199, 'dangling': 2630, 'mjhe': 7275, 'aaj': 12, 'kisi': 6101, 'eyesight': 3645, 'everyonegood': 3547, 'aftenoon': 170, 'booooore': 1367, 'uuu': 12285, 'boyfriend': 1412, 'concerts': 2242, 'garage': 4260, 'denim': 2793, 'ce': 1803, 'gws': 4674, 'greys': 4574, 'anatomy': 406, 'morisettes': 7362, 'nontrial': 7821, 'sayhername': 9863, 'lootcrate': 6602, 'inca': 5389, 'trail': 11886, 'sandboarding': 9807, 'cancels': 1713, 'derby': 2815, 'mugs': 7458, 'unable': 12125, 'lavigne': 6307, 'signature': 10289, 'dishes': 2973, 'unfamiliar': 12155, 'mediadesk': 7040, 'coldest': 2150, 'thirdwheeling': 11602, 'lovebirds': 6648, 'imo': 5358, 'babytalk': 795, 'isfamiliar': 5585, 'ahhhhh': 216, 'stages': 10884, 'realising': 9289, 'copies': 2354, 'sensiesha': 10016, 'eldest': 3321, 'worriedface': 12879, 'keedz': 5987, 'pleaseif': 8681, 'thoits': 11620, 'taybigail': 11391, 'jordans': 5828, 'tournaments': 11857, 'kinks': 6094, 'chargers': 1887, 'streak': 11016, 'scorch': 9904, 'srsky': 10869, 'tdc': 11406, 'insensitivenessridiculousfrown': 5481, 'cooperating': 2351, 'conversion': 2333, 'thurston': 11672, 'collins': 2168, 'quietly': 9169, 'kennel': 6001, 'rnfrown': 9604, 'pluckerssss': 8700, 'gion': 4380, 'kidschoiceawards': 6049, 'recorded': 9334, 'ming': 7215, 'pbr': 8391, 'periscope': 8468, 'uts': 12282, 'shawty': 10143, 'naw': 7609, 'sterlings': 10960, 'muses': 7482, 'hrryok': 5173, 'wnt': 12807, 'bueno': 1559, 'receptionist': 9319, 'minsyou': 7229, 'ella': 3338, 'ketchup': 6010, 'tasteless': 11371, 'deantd': 2702, 'justgotkanekified': 5898, 'notgonnabeactiveforweeksdontmissittoomuch': 7859, 'vlog': 12435, 'chills': 1963, 'turtles': 12017, 'cnn': 2119, 'strapline': 11013, 'theatres': 11546, 'guncontrol': 4653, 'lafayette': 6226, 'stung': 11065, 'poutinga': 8812, 'powerpoint': 8884, 'expectations': 3597, 'diner': 2922, 'nono': 7818, 'hinde': 5013, 'jc': 5722, 'circuit': 2039, 'secondary': 9951, 'joins': 5808, 'sodders': 10655, 'mobitel': 7294, 'playstation': 8670, 'charged': 1885, 'exp': 3594, 'misspelt': 7252, 'hyungwon': 5263, 'alarms': 277, 'cancelling': 1712, 'needicecreamnow': 7643, 'shakes': 10107, 'repeatedly': 9443, 'nuuh': 7919, 'jace': 5657, 'mostest': 7377, 'ar': 560, 'supercard': 11168, 'ridacongratulation': 9552, 'vips': 12414, 'babefrown': 785, 'sadbad': 9748, 'urgh': 12247, 'grigsons': 4575, 'carrot': 1764, 'ughh': 12094, 'enable': 3396, 'otter': 8167, 'protection': 9049, 'argh': 576, 'pon': 8740, 'otl': 8164, 'sleepovers': 10400, 'jesse': 5754, 'fabina': 3655, 'meantfacewithtearsofjoyfacewithtearsofjoy': 7031, 'gardening': 4263, 'barristas': 924, 'pup': 9104, 'drivers': 3152, 'brolly': 1505, 'buckner': 1549, 'dey': 2863, 'serve': 10041, 'bitin': 1230, 'pretzels': 8948, 'bblf': 967, 'vanilla': 12315, 'latte': 6296, 'poa': 8713, 'rog': 9637, 'skulker': 10377, 'threatongue': 11643, 'pgq': 8505, 'hungrrrryyy': 5228, 'icloud': 5279, 'ipod': 5564, 'hallyu': 4751, 'mac': 6759, 'buuuut': 1632, 'ber': 1104, 'okie': 8008, 'harlo': 4809, 'torrentialrain': 11838, 'aconfusion': 85, 'lloyds': 6529, 'knowwww': 6149, 'runny': 9720, 'sweater': 11237, 'ford': 4035, 'rightand': 9561, 'intolerant': 5539, 'xenophobes': 12946, 'wtfff': 12924, 'newer': 7690, 'pish': 8611, 'comparison': 2218, 'remastered': 9415, 'fe': 3799, 'icons': 5281, 'barker': 917, 'appear': 530, 'cornetto': 2364, 'strawberry': 11015, 'cornettoconfusion': 2365, 'nowwhen': 7886, 'kapatidkongpogi': 5935, 'mel': 7074, 'longest': 6588, 'carmen': 1754, 'login': 6558, 'wingstop': 12772, 'loool': 6596, 'budge': 1555, 'fuq': 4202, 'ilhoon': 5326, 'getthescoop': 4334, 'hearess': 4889, 'txtshot': 12061, 'unfollowing': 12161, 'standby': 10895, 'inatall': 5386, 'zenmate': 13186, 'carefulnamechecking': 1745, 'whistle': 12710, 'junmyeon': 5894, 'dddy': 2691, 'ketepi': 6011, 'arini': 585, 'je': 5726, 'igbo': 5307, 'blamehoney': 1254, 'whhr': 12703, 'trilla': 11940, 'samhernandez': 9796, 'snuggle': 10633, 'usage': 12259, 'warning': 12544, 'animator': 441, 'poutingcrying': 8819, 'vertigo': 12356, 'panic': 8286, 'attacks': 679, 'dual': 3173, 'carriageway': 1762, 'poutingaragalang': 8813, 'tams': 11351, 'boses': 1386, 'theo': 11564, 'anymoreeeeee': 489, 'cactus': 1652, 'sorrry': 10735, 'bowel': 1404, 'disease': 2966, 'tumour': 12002, 'waited': 12495, 'puffy': 9085, 'eyelids': 3642, 'musicas': 7488, 'campsites': 1701, 'sooooo': 10718, 'miah': 7158, 'hahays': 4716, 'churro': 2020, 'montana': 7338, 'chantay': 1871, 'okayx': 8002, 'reign': 9383, 'example': 3566, 'inflation': 5437, 'sic': 10270, 'reset': 9475, 'entlerbountly': 3448, 'dirtykik': 2941, 'sexcam': 10071, 'swollen': 11268, 'nams': 7570, 'kafi': 5909, 'mene': 7098, 'koi': 6158, 'rewert': 9533, 'bunta': 1595, 'warnaaaaaaafrown': 12543, 'torture': 11839, 'jumped': 5885, 'iran': 5567, 'irandeal': 5568, 'usiran': 12273, 'nuclear': 7899, 'mits': 7265, 'experts': 3610, 'severely': 10068, 'li': 6406, 'rumpy': 9715, 'gallon': 4232, 'responsibilities': 9491, 'dandia': 2626, 'caged': 1659, 'parrot': 8315, 'ly': 6740, 'election': 3323, 'commission': 2201, 'cag': 1657, 'striped': 11032, 'ilymelanie': 5337, 'todd': 11763, 'milli': 7196, 'unlike': 12184, 'talent': 11329, 'deepxcape': 2739, 'doin': 3040, 'thesis': 11582, 'redheartredheartredheart': 9354, 'laughed': 6298, 'hugged': 5194, 'poutingu': 8867, 'refresh': 9362, 'nowas': 7876, 'lurryface': 6728, 'gtg': 4621, 'compete': 2220, 'vv': 12468, 'nys': 7932, 'optouted': 8115, 'vam': 12309, 'speced': 10794, 'ells': 3342, 'sexyamelie': 10081, 'float': 3967, 'fineandyu': 3895, 'nonot': 7819, 'daftbut': 2597, 'imsorry': 5376, 'koe': 6155, 'emyu': 3394, 'confetti': 2249, 'sams': 9802, 'byfrown': 1644, 'poutingsini': 8860, 'dipoppofaceblowingakissfaceblowingakissfaceblowingakissfaceblowingakiss': 2930, 'hop': 5098, 'bestweekend': 1126, 'okayish': 8001, 'html': 5183, 'imjinah': 5350, 'geneva': 4310, 'getupandtry': 4337, 'patml': 8360, 'abouty': 36, 'reaally': 9269, 'meter': 7141, 'displays': 2983, 'unanswered': 12131, 'bri': 1466, 'magcon': 6792, 'merch': 7115, 'rightthe': 9564, 'sinuend': 10325, 'laper': 6270, 'rage': 9190, 'divined': 3001, 'brendon': 1464, 'uries': 12249, 'sumer': 11136, 'repackage': 9435, 'yongbe': 13087, 'suede': 11108, 'spending': 10811, 'belly': 1077, 'jannatul': 5691, 'ferdous': 3835, 'ami': 378, 'ekta': 3314, 'kharap': 6029, 'manush': 6889, 'mart': 6929, 'gua': 4631, 'cans': 1720, 'khloes': 6032, 'nhe': 7716, 'yar': 12996, 'minkyuk': 7225, 'hols': 5066, 'isolated': 5600, 'sensor': 10017, 'broker': 1504, 'wna': 12805, 'flaviana': 3944, 'chickmt': 1951, 'letsfootball': 6393, 'atk': 667, 'greymind': 4573, 'gayle': 4281, 'mooddump': 7351, 'livestream': 6518, 'sneaking': 10617, 'felton': 3827, 'verity': 12350, 'standens': 10896, 'sponge': 10837, 'smilingfacewithopenmouthclosedeyessmilingfacewithopenmouthclosedeyes': 10579, 'takoyaki': 11325, 'aisyah': 250, 'ffvi': 3851, 'youtubegpctsojkw': 13123, 'donutsss': 3073, 'pizzas': 8631, 'grated': 4543, 'sparse': 10788, 'lagifrown': 6230, 'hoodafc': 5095, 'yescant': 13048, 'riderfrown': 9555, 'hueeeee': 5189, 'passwordrecall': 8343, 'thingy': 11596, 'george': 4321, 'chewing': 1945, 'stella': 10951, 'yelled': 13033, 'theaccidentalcouple': 11544, 'smooth': 10597, 'handover': 4769, 'spick': 10814, 'offense': 7966, 'bebii': 1025, 'happenend': 4783, 'dr': 3112, 'poutingwhat': 8874, 'balms': 868, 'hmph': 5041, 'bubba': 1542, 'cuddles': 2528, 'cuddling': 2529, 'floor': 3971, 'oi': 7994, 'bengalis': 1096, 'masterchef': 6959, 'whatchya': 12680, 'petrol': 8498, 'diesel': 2898, 'cock': 2128, 'drown': 3162, 'nyquil': 7931, 'poooootek': 8748, 'leaking': 6333, 'thermos': 11579, 'casting': 1779, 'smilingfacewithsmilingeyessmilingfacewithsmilingeyessmilingfacewithsmilingeyesredheart': 10584, 'skate': 10359, 'twoheartstwoheartstwoheartstwohearts': 12054, 'tae': 11300, 'kita': 6109, 'crys': 2521, 'ia': 5265, 'nation': 7599, 'corrupted': 2373, 'anythingsurely': 496, 'october': 7956, 'ene': 3412, 'zehr': 13183, 'khany': 6028, 'grocery': 4589, 'hubba': 5187, 'bubble': 1543, 'gum': 4650, 'closet': 2110, 'jhalak': 5764, 'bakwas': 854, 'seehiah': 9964, 'omggg': 8046, 'goys': 4512, 'nachos': 7533, 'braid': 1425, 'braids': 1426, 'boongfrown': 1365, 'recommendations': 9331, 'cwnt': 2573, 'trivia': 11949, 'crack': 2440, 'bdays': 988, 'ownconfusion': 8219, 'rohingya': 9641, 'muslims': 7493, 'indicted': 5415, 'trafficking': 11883, 'charges': 1888, 'thailand': 11516, 'savetherohingya': 9855, 'sabaa': 9741, 'rumble': 9713, 'kumble': 6185, 'scold': 9900, 'cleaning': 2072, 'phrased': 8547, 'include': 5396, 'themi': 11558, 'melting': 7085, 'tfw': 11502, 'jestconfusion': 5756, 'relaxes': 9398, 'offended': 7964, 'sleepingwithsirens': 10398, 'bringmethehorizon': 1487, 'dzul': 3227, 'carva': 1770, 'louisiana': 6638, 'regularly': 9377, 'sympathis': 11278, 'expected': 3598, 'revamps': 9521, 'mosquito': 7375, 'headphones': 4877, 'poutingnow': 8846, 'wacha': 12476, 'niende': 7745, 'kk': 6122, 'calibraksaep': 1670, 'darlin': 2646, 'grounded': 4594, 'doednt': 3029, 'meaningful': 7027, 'horrific': 5119, 'royally': 9687, 'sweedy': 11243, 'maui': 6983, 'pooo': 8745, 'nobodies': 7794, 'sacconejolys': 9744, 'bethesda': 1131, 'likee': 6442, 'kateee': 5961, 'iloveyouu': 5335, 'riixc': 9567, 'linux': 6481, 'meital': 7072, 'nawwwe': 7613, 'chikka': 1954, 'ug': 12091, 'rata': 9247, 'soonest': 10714, 'mwamwa': 7514, 'juarez': 5857, 'faggot': 3695, 'opener': 8089, 'bitsenpai': 1232, 'nicest': 7733, 'mehendi': 7067, 'dash': 2652, 'bookmark': 1357, 'whay': 12690, 'shaa': 10092, 'pramis': 8896, 'kissingfacewithclosedeyes': 6105, 'ngee': 7710, 'crikey': 2487, 'snit': 10622, 'nathanielhinanakit': 7596, 'naya': 7615, 'spinny': 10823, 'loading': 6541, 'wheel': 12693, 'poutingi': 8830, 'notifs': 7868, 'poutingin': 8832, 'dammitstop': 2611, 'guyslol': 4667, 'albeit': 280, 'disappointing': 2951, 'athlete': 665, 'stripes': 11033, 'gfriend': 4340, 'fugly': 4181, 'smilingfacewithsmilingeyesrevolvinghearts': 10583, 'jongdaei': 5823, 'tlists': 11741, 'budget': 1556, 'pabebegirls': 8230, 'pabebe': 8229, 'altered': 340, 'sandra': 9809, 'storify': 11003, 'londonfrown': 6576, 'mtvhottest': 7433, 'gaga': 4222, 'cage': 1658, 'cryingfacedizzyfaceloudlycryingface': 2518, 'hulkamania': 5210, 'unloved': 12188, 'ihhh': 5314, 'stackare': 10879, 'brownxox': 1520, 'remedy': 9417, 'ov': 8196, 'raiz': 9204, 'nvr': 7923, 'gv': 4670, 'upwt': 12242, 'nowwt': 7887, 'achievedin': 79, 'thr': 11638, 'soln': 10672, 'whaaaaaaaaaat': 12671, 'pipes': 8603, 'airwolf': 248, 'lawn': 6311, 'pierce': 8574, 'cupids': 2542, 'torn': 11836, 'retainers': 9505, 'haiss': 4732, 'todayy': 11760, 'thoo': 11622, 'trainsec': 11894, 'sprees': 10854, 'everday': 3539, 'poutingit': 8834, 'william': 12751, 'umboh': 12118, 'addicts': 115, 'jadines': 5668, 'thiz': 11613, 'iz': 5648, 'emeged': 3364, 'kennat': 6000, 'sav': 9848, 'reunite': 9517, 'abi': 30, 'arctic': 569, 'chicsirific': 1953, 'struggle': 11046, 'structured': 11044, 'mootongue': 7359, 'cumbia': 2534, 'poutingwearyface': 8872, 'badlife': 824, 'nameisdan': 7567, 'kaslkdja': 5957, 'wks': 12802, 'feverfew': 3846, 'weddingflowers': 12611, 'diyflowers': 3003, 'insyaf': 5512, 'fitnes': 3922, 'wolverine': 12820, 'innocent': 5471, 'foldedhandslightskintonebirthdaycake': 3997, 'buyorder': 1638, 'mememe': 7090, 'krystoria': 6176, 'snob': 10623, 'poutingsakit': 8858, 'zumba': 13203, 'greekcrisis': 4560, 'greeks': 4561, 'leavework': 6343, 'remained': 9413, 'artistic': 609, 'dutch': 3213, 'legible': 6363, 'israeli': 5605, 'passport': 8341, 'froze': 4153, 'bananas': 876, 'theories': 11567, 'stomachache': 10986, 'agains': 187, 'otani': 8160, 'niaaaaaaaaaaaaaaaaaaaaaaaaaaapls': 7722, 'revolvinghearts': 9526, 'schemes': 9889, 'fckin': 3797, 'avoiding': 724, 'vin': 12401, 'plss': 8696, 'rply': 9689, 'rat': 9246, 'backup': 816, 'lunes': 6723, 'martes': 6931, 'robinhood': 9619, 'robinhoodies': 9620, 'sportutilityvehiclegreenheart': 10843, 'docopenhagen': 3023, 'setter': 10057, 'swiping': 11263, 'bbygurl': 978, 'shouyno': 10240, 'caribbean': 1748, 'takraw': 11326, 'niggacarlos': 7748, 'fersuree': 3839, 'angie': 433, 'plsss': 8697, 'sheriff': 10164, 'aaaaaaaaaaa': 1, 'aaaaages': 5, 'sulk': 11131, 'nonce': 7813, 'pad': 8237, 'bison': 1219, 'qdon': 9139, 'stabbed': 10876, 'cheated': 1911, 'poutingunknown': 8869, 'stomping': 10988, 'aaaaaaaaaaaah': 3, 'kanye': 5933, 'jdjdjdjd': 5724, 'jimins': 5775, 'fancafe': 3737, 'wtvd': 12927, 'waffle': 12481, 'himseek': 5010, 'glo': 4418, 'poutingpls': 8853, 'cory': 2375, 'monteith': 7340, 'louth': 6644, 'hashbrownsfacewithtearsofjoysmilingfacewithsmilingeyesheartwitharrowheartwitharrow': 4824, 'pgs': 8506, 'msc': 7417, 'hierro': 4984, 'shirleycam': 10185, 'pal': 8263, 'gilet': 4370, 'cheeks': 1918, 'squishy': 10864, 'lahhh': 6236, 'eon': 3460, 'sunrise': 11157, 'beety': 1040, 'getaway': 4332, 'committing': 2204, 'criminal': 2489, 'amiibo': 380, 'habe': 4686, 'siannn': 10267, 'chuckin': 2016, 'ampsha': 392, 'nia': 7721, 'seniors': 10013, 'replacement': 9445, 'strap': 11012, 'dz': 3226, 'entlead': 3447, 'ifsc': 5304, 'mayor': 6996, 'biodiversity': 1195, 'taxonomic': 11389, 'infrastructure': 5450, 'collaboration': 2154, 'species': 10799, 'suppl': 11179, 'pdf': 8400, 'collar': 2158, 'belting': 1082, 'smith': 10591, 'rides': 9557, 'eyeliner': 3643, 'therefore': 11572, 'netherlands': 7676, 'disgusted': 2969, 'el': 3315, 'jeb': 5730, 'blacklivesmatter': 1246, 'slogan': 10415, 'msnbc': 7423, 'cnnjebbush': 2120, 'famished': 3730, 'marino': 6911, 'qualified': 9145, 'suzy': 11225, 'qualify': 9146, 'skirt': 10374, 'tama': 11341, 'warrior': 12546, 'wounded': 12895, 'reported': 9454, 'iraq': 5569, 'camara': 1688, 'coveralls': 2425, 'inspy': 5495, 'sneezy': 10619, 'rogerwatch': 9639, 'stalkerfrown': 10888, 'velvet': 12337, 'plsssssss': 8698, 'traditions': 11879, 'beheaviour': 1055, 'loveredheart': 6662, 'aaron': 17, 'banged': 889, 'jelouse': 5737, 'mtg': 7429, 'thoughtseized': 11635, 'rounds': 9678, 'playables': 8660, 'oldies': 8020, 'goodies': 4471, 'mcg': 7012, 'inspirit': 5493, 'ised': 5584, 'assume': 655, 'waisted': 12493, 'jeansfrown': 5729, 'guinness': 4645, 'poutingthank': 8865, 'pepper': 8451, 'thessidew': 11583, 'genesis': 4309, 'november': 7874, 'mashed': 6945, 'whattsap': 12689, 'inuyasha': 5544, 'outfwith': 8183, 'myungsoo': 7527, 'yeol': 13039, 'victoria': 12373, 'satisfied': 9838, 'challo': 1849, 'pliss': 8688, 'juliana': 5874, 'enroll': 3434, 'vibares': 12365, 'darlene': 2645, 'emoji': 3375, 'brisbane': 1489, 'merlin': 7123, 'nawwwee': 7614, 'hyperbullies': 5258, 'tong': 11800, 'nga': 7709, 'seatmates': 9946, 'rajud': 9207, 'ores': 8132, 'kaylas': 5976, 'ericavan': 3479, 'jong': 5822, 'dongwoo': 3063, 'photocards': 8534, 'wh': 12670, 'dw': 3218, 'tumor': 12001, 'si': 10263, 'vivian': 12433, 'mmsmalubhangsakit': 7283, 'beive': 1062, 'jillcruz': 5771, 'qt': 9141, 'tossing': 11844, 'pushes': 9120, 'unsettled': 12203, 'showers': 10246, 'gh': 4345, 'rllyfrown': 9596, 'hamsters': 4758, 'sheeran': 10153, 'preform': 8909, 'monash': 7328, 'whatp': 12684, 'hitmarker': 5030, 'glitch': 4417, 'safaa': 9760, 'selenas': 9986, 'fansfrown': 3747, 'galat': 4230, 'tum': 11998, 'ab': 21, 'respected': 9482, 'lrka': 6683, 'bna': 1310, 'bhook': 1157, 'push': 9118, 'tonightive': 11806, 'excuses': 3579, 'gerry': 4328, 'afterschool': 175, 'bilal': 1179, 'ashraf': 621, 'icu': 5282, 'ruksar': 9710, 'thankssss': 11528, 'annnd': 451, 'winchester': 12761, 'porter': 8775, 'grepe': 4567, 'grepein': 4568, 'panem': 8284, 'jazdorothy': 5718, 'sulli': 11132, 'toowell': 11822, 'darren': 2649, 'cpm': 2436, 'condemned': 2244, 'political': 8735, 'heavycheckmark': 4915, 'occurred': 7950, 'mentality': 7104, 'unagi': 12126, 'elw': 3351, 'mesh': 7125, 'beytaad': 1140, 'fluent': 3979, 'varsity': 12321, 'sengenza': 10010, 'typos': 12075, 'movnat': 7408, 'yields': 13066, 'nbheroes': 7618, 'workfrown': 12857, 'agover': 203, 'brasileirao': 1442, 'abusive': 49, 'hashmi': 4825, 'unfollower': 12159, 'unparents': 12197, 'bianca': 1160, 'bun': 1587, 'dislike': 2976, 'burdensome': 1597, 'amelia': 372, 'melon': 7083, 'soccer': 10642, 'nevermind': 7683, 'jeon': 5747, 'thigh': 11591, 'traction': 11875, 'zoomed': 13200, 'damnit': 2614, 'prypv': 9061, 'relive': 9411, 'nycpv': 7929, 'communicate': 2208, 'klm': 6128, 'mcds': 7008, 'hung': 5225, 'seventh': 10065, 'otwolgrandtrailer': 8170, 'splendour': 10828, 'swedish': 11242, 'metal': 7140, 'legends': 6361, 'hirfrce': 5023, 'givecodpieceachance': 4397, 'stiles': 10974, 'prague': 8895, 'sadis': 9751, 'hayeee': 4856, 'patwari': 8368, 'iks': 5325, 'vision': 12423, 'awhhh': 749, 'nalang': 7561, 'opens': 8092, 'albanian': 278, 'cursed': 2548, 'tavas': 11383, 'chara': 1879, 'teteh': 11494, 'verry': 12352, 'colleagues': 2161, 'sb': 9867, 'nawee': 7611, 'macho': 6768, 'purity': 9111, 'kwento': 6195, 'whaaaaaaat': 12672, 'noooooooo': 7832, 'nakakapikon': 7557, 'nagbabasa': 7540, 'damned': 2613, 'answered': 469, 'leading': 6329, 'cancer': 1714, 'jonathas': 5820, 'betis': 1132, 'greatings': 4556, 'shits': 10192, 'psv': 9064, 'sogok': 10664, 'premium': 8917, 'instrument': 5508, 'dastardly': 2654, 'swine': 11262, 'envelope': 3456, 'pipol': 8604, 'wiper': 12781, 'kernels': 6008, 'facking': 3687, 'intelconfusion': 5514, 'bent': 1099, 'pins': 8600, 'pcgaming': 8395, 'pcupgrade': 8397, 'alreadyfrown': 336, 'brainwashed': 1432, 'smoshthe': 10598, 'plawnew': 8658, 'litter': 6506, 'mensch': 7102, 'sepanx': 10023, 'battle': 955, 'pcy': 8398, 'caerphilly': 1653, 'omwsmilingfacewithhearteyes': 8051, 'hahdhdhshhs': 4717, 'growinguppoor': 4603, 'loudlycryingfacei': 6630, 'bangtans': 892, 'jakub': 5677, 'mahmood': 6813, 'taimoor': 11312, 'meray': 7113, 'dost': 3090, 'tyas': 12063, 'offender': 7965, 'pissbaby': 8613, 'planks': 8645, 'inconsistent': 5404, 'booksanother': 1360, 'bin': 1189, 'osxchromevoiceover': 8158, 'devo': 2859, 'hulkhogan': 5211, 'unpleasantness': 12198, 'daaaaaamn': 2585, 'dadas': 2591, 'unniefrown': 12196, 'spike': 10816, 'panics': 8287, 'cov': 2422, 'launching': 6304, 'lizardz': 6525, 'emergency': 3366, 'dormtel': 3084, 'scho': 9891, 'siya': 10350, 'oneee': 8058, 'watson': 12568, 'feta': 3844, 'blaaaaze': 1241, 'nausea': 7605, 'aware': 736, 'topup': 11833, 'sharknado': 10131, 'ernie': 3483, 'ezoo': 3650, 'lilybutle': 6459, 'seduce': 9960, 'preforming': 8910, 'powai': 8880, 'neighbor': 7654, 'lielama': 6423, 'belieb': 1068, 'stocking': 10980, 'unsafe': 12202, 'evicted': 3557, 'halo': 4752, 'fred': 4092, 'gaon': 4258, 'infnt': 5442, 'eligible': 3333, 'acube': 104, 'bullshitfrown': 1579, 'drops': 3159, 'hanaaaa': 4761, 'jn': 5789, 'basta': 941, 'headachewoke': 4871, 'sexting': 10078, 'relation': 9392, 'dhisbackhandindexpointingright': 2874, 'fakmarey': 3712, 'doo': 3074, 'flags': 3935, 'ardi': 571, 'fulltime': 4189, 'frustrating': 4160, 'barcelonaconfusion': 909, 'beet': 1039, 'juice': 5867, 'dci': 2688, 'granddad': 4530, 'minion': 7221, 'bucket': 1546, 'war': 12534, 'kapan': 5934, 'poutingoh': 8848, 'udah': 12088, 'dihapus': 2915, 'hilang': 5000, 'dari': 2641, 'muka': 7462, 'bumii': 1582, 'fifty': 3860, 'gona': 4461, 'chellos': 1930, 'gates': 4272, 'guards': 4632, 'crepes': 2478, 'forsaken': 4057, 'kanin': 5930, 'hypixel': 5260, 'grrrr': 4606, 'thestruggleisreal': 11584, 'geek': 4291, 'gamers': 4241, 'afterbirth': 172, 'apinks': 516, 'overperhatianfrown': 8207, 'pox': 8887, 'predicted': 8905, 'ahmive': 220, 'londonnor': 6578, 'chancecash': 1858, 'wit': 12790, 'karlie': 5951, 'kloss': 6129, 'goofy': 4485, 'pcd': 8394, 'antagonised': 476, 'writer': 12909, 'nudges': 7902, 'skeleton': 10362, 'delved': 2783, 'horrendous': 5114, 'digits': 2914, 'marbles': 6901, 'grandad': 4529, 'grays': 4549, 'followk': 4013, 'pace': 8231, 'molly': 7314, 'higher': 4986, 'ceremony': 1828, 'partyin': 8330, 'layouts': 6318, 'christine': 2005, 'moody': 7354, 'throwbacks': 11654, 'barbs': 907, 'cooper': 2350, 'convinced': 2336, 'creasy': 2466, 'deputy': 2814, 'saludos': 9787, 'dissapointed': 2984, 'bbloggers': 968, 'tiredface': 11722, 'baes': 834, 'lasting': 6283, 'pimples': 8589, 'hais': 4731, 'pamela': 8270, 'goodanna': 4467, 'woes': 12814, 'solom': 10675, 'mini': 7220, 'mast': 6957, 'intermittent': 5529, 'janniecam': 5692, 'braxton': 1448, 'urban': 12244, 'unprecedented': 12199, 'brainwashing': 1433, 'tebow': 11439, 'smithconfusion': 10592, 'okaaay': 7999, 'sayanggggggggg': 9860, 'housework': 5157, 'bust': 1619, 'disneyland': 2980, 'thomas': 11621, 'tommyy': 11793, 'kisaragi': 6100, 'kevin': 6014, 'strictly': 11027, 'whyyyyy': 12733, 'nsc': 7893, 'mat': 6963, 'awhh': 748, 'rammed': 9215, 'vouchers': 12457, 'smadvow': 10432, 'acdcmissed': 72, 'jobe': 5792, 'akere': 263, 'gmail': 4429, 'hugots': 5197, 'sprevelink': 10855, 'lana': 6249, 'loveyoutilltheendcarter': 6673, 'sfvcapcom': 10090, 'winners': 12776, 'yesterdayjust': 13056, 'poutinghopefully': 8828, 'tito': 11732, 'rosie': 9669, 'hayoung': 4859, 'nlb': 7787, 'comforting': 2192, 'wattie': 12569, 'katies': 5968, 'forth': 4058, 'meningitis': 7101, 'viral': 12415, 'tonsillitis': 11810, 'ange': 423, 'facewithstuckouttongueclosedeyes': 3672, 'babyy': 796, 'kidston': 6050, 'cryingcatfacebeatingheartbeatinghearttwoheartskissingfacewithclosedeyeskissingfacewithclosedeyeskissingfacewithclosedeyeskissingfacewithclosedeyeskissingfacewithclosedeyesredheartredheartredheartredheartredheart': 2515, 'weigh': 12630, 'risks': 9584, 'premiums': 8918, 'keen': 5988, 'petrofac': 8497, 'wig': 12746, 'thisgt': 11608, 'pathetic': 8353, 'burdensays': 1596, 'respects': 9485, 'tolerates': 11777, 'itchy': 5614, 'cheaper': 1907, 'malaysia': 6836, 'nooooo': 7829, 'snapchattimg': 10611, 'funsmirkingfacesmirkingface': 4201, 'sin': 10310, 'dedicating': 2732, 'worriedly': 12880, 'stared': 10904, 'toneady': 11799, 'snapdirty': 10612, 'corpses': 2370, 'taeny': 11302, 'fyeah': 4217, 'andromeda': 417, 'yunnie': 13155, 'developers': 2854, 'writings': 12912, 'whdjwksja': 12691, 'ziam': 13189, 'fucks': 4178, 'upi': 12225, 'hawke': 4854, 'spoil': 10833, 'curtain': 2549, 'watchable': 12558, 'migrines': 7181, 'ltltltltltltltltltlt': 6693, 'gdcegamescom': 4286, 'loudlycryingfaceloudlycryingfaceloudlycryingfaceloudlycryingfaceloudlycryingfaceloudlycryingfaceloudlycryingfaceloudlycryingfaceloudlycryingfaceloudlycryingface': 6634, 'dot': 3093, 'tracking': 11873, 'num': 7908, 'lmfaooooo': 6537, 'oooooouch': 8081, 'pinky': 8599, 'footprints': 4031, 'podiatrists': 8718, 'opened': 8088, 'gusto': 4662, 'rodics': 9634, 'adoohh': 133, 'bbutt': 976, 'poutingget': 8823, 'tigermilk': 11693, 'huard': 5185, 'east': 3248, 'dulwich': 3192, 'intense': 5519, 'kagami': 5910, 'kuroko': 6188, 'sana': 9805, 'makita': 6834, 'spooky': 10840, 'smol': 10596, 'bean': 996, 'fagans': 3694, 'meadowhall': 7020, 'actuallyconfusion': 101, 'lola': 6562, 'nadalaw': 7535, 'labyu': 6214, 'okies': 8009, 'jot': 5833, 'homeslice': 5075, 'emoticon': 3377, 'eyebrows': 3639, 'prettylooks': 8947, 'whitney': 12716, 'shamil': 10114, 'tonnes': 11809, 'status': 10934, 'suddenly': 11106, 'ally': 324, 'mani': 6873, 'wraps': 12904, 'neck': 7638, 'pulls': 9091, 'heartbroken': 4895, 'chover': 1999, 'cebu': 1804, 'lechon': 6345, 'kitten': 6116, 'jannygreen': 5693, 'lyah': 6741, 'conno': 2279, 'brooo': 1510, 'lovebox': 6649, 'prods': 9002, 'ims': 5375, 'osad': 8151, 'itb': 5613, 'omigod': 8049, 'locate': 6546, 'ehem': 3297, 'smirkingfacesmirkingface': 10589, 'yeke': 13030, 'jumpa': 5884, 'maybefacesavouringdeliciousfood': 6991, 'ape': 512, 'bestfriendfrown': 1121, 'namin': 7569, 'epetitions': 3464, 'sleepover': 10399, 'pretending': 8941, 'hosting': 5130, 'irking': 5573, 'ruess': 9705, 'aigoo': 228, 'recover': 9338, 'doujin': 3100, 'ging': 4374, 'dicks': 2887, 'ginggon': 4377, 'iti': 5622, 'youguysall': 13100, 'lovefrown': 6652, 'ytd': 13137, 'poutingpdapaghimok': 8851, 'flexible': 3956, 'sheet': 10154, 'nanaman': 7572, 'curves': 2551, 'pinay': 8591, 'pie': 8570, 'jadi': 5667, 'langsung': 6259, 'flasback': 3939, 'los': 6612, 'nicknames': 7740, 'involve': 5556, 'scrapes': 9916, 'piles': 8583, 'dhesi': 2873, 'sare': 9832, 'bandar': 883, 'varge': 12319, 'hammering': 4755, 'nowlooks': 7881, 'dayfrown': 2673, 'edited': 3266, 'lolo': 6569, 'onlynay': 8064, 'xbsbabnb': 12941, 'stillllllllllll': 10976, 'seriousfrown': 10036, 'wakeupgop': 12503, 'adams': 108, 'mv': 7508, 'bull': 1575, 'trafficccccccc': 11881, 'translator': 11905, 'ummmm': 12122, 'filmed': 3878, 'pornography': 8771, 'slutshamed': 10430, 'faked': 3710, 'suicideect': 11121, 'poland': 8728, 'faraway': 3757, 'cookies': 2343, 'forgetting': 4045, 'kazanaki': 5978, 'cgi': 1835, 'punconfusion': 9098, 'upsets': 12236, 'xs': 12962, 'xxxxx': 12971, 'cheat': 1909, 'osaka': 8152, 'allypasturan': 325, 'aytona': 773, 'hala': 4738, 'mathird': 6974, 'jkjk': 5784, 'backtrack': 815, 'utilizing': 12281, 'pats': 8365, 'broh': 1499, 'callllllls': 1681, 'icarus': 5272, 'awning': 753, 'bach': 798, 'landlords': 6251, 'mps': 7411, 'bullshit': 1578, 'sainz': 9768, 'daming': 2609, 'tie': 11689, 'ishii': 5589, 'clara': 2057, 'yiling': 13067, 'ts': 11975, 'stil': 10973, 'sidharth': 10278, 'ndabenhle': 7624, 'doggy': 3035, 'antagal': 475, 'jhart': 5765, 'pages': 8243, 'thus': 11673, 'jenners': 5741, 'troubleshooting': 11958, 'dem': 2784, 'bang': 887, 'automatically': 712, 'redirected': 9356, 'chip': 1969, 'nnnoooo': 7792, 'cz': 2583, 'gorilla': 4499, 'matteo': 6977, 'hbm': 4865, 'admire': 131, 'jason': 5703, 'shackells': 10095, 'podcast': 8717, 'twers': 12039, 'performing': 8464, 'hahaahahahaha': 4698, 'bums': 1586, 'mutilated': 7503, 'robot': 9623, 'destroyedgtfrown': 2840, 'freakin': 4090, 'haestarr': 4693, 'hoju': 5051, 'jimz': 5776, 'grinningfaceflying': 4583, 'recordings': 9336, 'snippets': 10621, 'mefd': 7053, 'diana': 2880, 'led': 6347, 'sleeep': 10394, 'meowkd': 7112, 'trends': 11926, 'bryann': 1534, 'anymoreskeptical': 490, 'swimmers': 11260, 'leh': 6368, 'tht': 11659, 'ggfrown': 4343, 'didnot': 2888, 'ths': 11657, 'episodefrown': 3469, 'facewithtearsofjoyfacewithtearsofjoyfacewithtearsofjoythank': 3679, 'sangrinningfacezanele': 9816, 'usanele': 12262, 'replace': 9444, 'shuffling': 10258, 'micha': 7163, 'creep': 2475, 'throughout': 11650, 'ave': 718, 'triple': 11945, 'lectures': 6346, 'jaw': 5709, 'quarter': 9151, 'kimono': 6077, 'sony': 10709, 'followmeaaron': 4014, 'tzelumxoxo': 12078, 'mew': 7145, 'poutinghow': 8829, 'indicated': 5414, 'oulive': 8176, 'viernesderolenahot': 12389, 'longmorn': 6590, 'tobermory': 11750, 'recuerda': 9343, 'tanto': 11359, 'nurse': 7915, 'awaited': 730, 'urslef': 12255, 'lime': 6463, 'truckloads': 11963, 'favours': 3789, 'spectator': 10803, 'sailing': 9767, 'wend': 12651, 'bbc': 966, 'foiling': 3996, 'catamarans': 1782, 'shak': 10105, 'pely': 8430, 'sextaatequemfimseguesdvcomvalentino': 10077, 'befor': 1041, 'cinnamon': 2036, 'comfort': 2190, 'mtap': 7428, 'penge': 8433, 'frozen': 4154, 'bagus': 838, 'emang': 3357, 'preferably': 8907, 'puppiesconfusion': 9105, 'engg': 3418, 'cmc': 2116, 'maging': 6802, 'statement': 10927, 'moodswings': 7353, 'terminal': 11484, 'diseases': 2968, 'peep': 8424, 'removing': 9429, 'roguk': 9640, 'bandanafrown': 882, 'mef': 7052, 'rebound': 9309, 'pooooooooor': 8747, 'perpetually': 8471, 'resting': 9495, 'wished': 12787, 'suzaku': 11223, 'unfollows': 12162, 'poutingwearyfacesparklingheart': 8873, 'zayncomebackwemissyou': 13173, 'pmsl': 8710, 'mianhe': 7160, 'milkeu': 7192, 'lrt': 6684, 'bambam': 872, 'sodas': 10654, 'payback': 8376, 'kats': 5971, 'jobee': 5793, 'muchie': 7449, 'revolvingheartsballoonbirthdaycakepartypopperfaceblowingakiss': 9527, 'blr': 1299, 'bathroom': 948, 'lagg': 6229, 'banget': 890, 'dreamed': 3129, 'thered': 11571, 'invisible': 5552, 'beasts': 1003, 'scuttling': 9934, 'worm': 12877, 'tracks': 11874, 'bauuuuuuuukkk': 957, 'damnnn': 2616, 'fineeeeeeeee': 3896, 'omaigoshhhhhhh': 8037, 'quits': 9173, 'argument': 579, 'rlly': 9595, 'bummed': 1583, 'yellowheartyellowheartyellowheartyellowheart': 13038, 'bore': 1375, 'smilingcatfacewithopenmouthweddinggemstonepartypopper': 10571, 'feelin': 3813, 'biscuit': 1215, 'slathered': 10390, 'taxed': 11385, 'investigations': 5549, 'pups': 9107, 'jsut': 5853, 'beloved': 1081, 'grandmother': 4533, 'edits': 3271, 'babeeeee': 784, 'demn': 2787, 'hotaisndonwyvauwjoqhsjsnaihsuswtf': 5133, 'nirame': 7773, 'geng': 4311, 'fikri': 3870, 'tirtagangga': 11724, 'hotelconfusion': 5138, 'char': 1878, 'riprishikeshwariyour': 9574, 'peacefrown': 8404, 'artists': 610, 'creamist': 2464, 'pleaseeeeefrown': 8679, 'challenge': 1848, 'substitution': 11093, 'cplt': 2435, 'cp': 2434, 'hannah': 4778, 'blueheart': 1301, 'opus': 8116, 'bbq': 974, 'lolliver': 6567, 'split': 10829, 'collat': 2159, 'spilt': 10819, 'poutingquitkarwaoyaaro': 8855, 'sherep': 10163, 'nemen': 7662, 'behey': 1056, 'motherfucker': 7382, 'mads': 6786, 'reece': 9358, 'vm': 12437, 'deth': 2848, 'ovi': 8215, 'lest': 6389, 'departure': 2805, 'madhvitommo': 6777, 'ammilliam': 386, 'wipe': 12780, 'yuck': 13139, 'ystrday': 13134, 'seolhyun': 10021, 'spicy': 10815, 'owls': 8218, 'latersign': 6292, 'suunn': 11220, 'wallpaper': 12522, 'cba': 1799, 'hilife': 5003, 'rec': 9310, 'gotdamn': 4504, 'baaack': 779, 'srw': 10871, 'mobage': 7290, 'passes': 8338, 'digitals': 2913, 'sexcams': 10072, 'interruptd': 5536, 'strokes': 11036, 'gnr': 4431, 'partied': 8321, 'backstage': 813, 'slash': 10389, 'tagged': 11307, 'bunny': 1591, 'sooner': 10713, 'kwon': 6196, 'financial': 3887, 'analyst': 403, 'expedia': 3600, 'bellevue': 1076, 'waconfusion': 12478, 'finance': 3886, 'expediajobs': 3601, 'prison': 8977, 'dose': 3087, 'huhuh': 5204, 'heartburn': 4896, 'listens': 6497, 'againawalmu': 184, 'njareeeeem': 7782, 'nooooooooo': 7833, 'psycho': 9065, 'wahhh': 12488, 'abudhabi': 48, 'blues': 1303, 'hibye': 4978, 'shareyoursummer': 10127, 'mustbe': 7498, 'dairy': 2601, 'produxt': 9008, 'lactose': 6217, 'midlands': 7174, 'roblong': 9621, 'knackered': 6132, 'footage': 4029, 'icing': 5278, 'dragged': 3114, 'lifeless': 6428, 'knowwwww': 6150, 'pricing': 8960, 'lakeland': 6242, 'hoursembarrassed': 5153, 'pengen': 8434, 'boxing': 1409, 'indonesiaconfusion': 5427, 'girlll': 4387, 'tsunami': 11981, 'indies': 5419, 'wnick': 12806, 'picks': 8561, 'tirade': 11720, 'stoop': 10992, 'lower': 6677, 'thunder': 11667, 'fight': 3861, 'apologize': 521, 'adults': 141, 'facade': 3657, 'democracy': 2789, 'hye': 5253, 'brat': 1443, 'bade': 822, 'fursat': 4206, 'usey': 12271, 'banaya': 878, 'uppar': 12231, 'waale': 12475, 'ney': 7704, 'afsos': 169, 'humse': 5219, 'dur': 3207, 'gif': 4358, 'whod': 12718, 'naruhina': 7584, 'nameeee': 7566, 'haiqal': 4725, 'picc': 8557, 'instore': 5506, 'prevoting': 8952, 'durian': 3209, 'strudel': 11045, 'tsk': 11979, 'marines': 6910, 'kailan': 5917, 'separated': 10024, 'payday': 8377, 'payhour': 8378, 'immediately': 5354, 'mekele': 7073, 'wrongfrown': 12916, 'guppy': 4658, 'poorkid': 8753, 'misunderstood': 7263, 'cuddly': 2530, 'scratches': 9917, 'tries': 11939, 'thumb': 11663, 'ankles': 445, 'torturing': 11840, 'caste': 1778, 'kirkiri': 6098, 'hearted': 4899, 'facewithtearsofjoyrevolvinghearts': 3682, 'wonho': 12836, 'mission': 7251, 'pap': 8291, 'danzell': 2636, 'crafting': 2445, 'devil': 2858, 'sheff': 10155, 'utdyork': 12280, 'visa': 12419, 'bench': 1088, 'catlin': 1790, 'facewithtearsofjoywearyfacewearyface': 3684, 'harmful': 4811, 'motivation': 7386, 'yolo': 13086, 'bloating': 1277, 'ollys': 8032, 'alternis': 345, 'overall': 8199, 'continental': 2313, 'grinningfacepistol': 4585, 'thirsty': 11604, 'bhatti': 1152, 'konami': 6161, 'pes': 8485, 'rantie': 9232, 'perverse': 8483, 'bracelets': 1421, 'twins': 12043, 'bylfnnz': 1645, 'banned': 896, 'pakistan': 8258, 'failsatlife': 3704, 'poutingwaaah': 8870, 'jaebum': 5669, 'ahmad': 218, 'maslan': 6949, 'cooks': 2345, 'hull': 5212, 'supporter': 11183, 'misserable': 7247} In [97]: # Model Generation Using Multinomial Naive Bayes clf = MultinomialNB () . fit ( X_train , y_train ) predicted = clf . predict ( X_test ) print ( \"MultinomialNB Accuracy:\" , metrics . accuracy_score ( y_test , predicted )) MultinomialNB Accuracy: 0.973 Human language is astoundingly perplexing and diverse. NLP is an approach that helps us improve our communication and influence skills at a time these are becoming even more important. Even though computing systems enable fast and highly accurate communication channels, machines have never been good at understanding how and why we communicate in the first place. NLP is significant in light of the fact that it helps settle ambiguity in language and adds valuable numeric structure to the information for some downstream applications, for example, speech recognition or text analytics. In [0]:",
        "tags": "lectures",
        "url": "lectures/lecture23/notebook/"
    }, {
        "title": "Lecture 23: Natural Language Processing",
        "text": "Lec23_ex In [1]: import numpy as np import pandas as pd import re from sklearn.model_selection import train_test_split np . random . seed ( 0 ) Movie Review Classifier ðŸ¿ðŸ“½ï¸ In this exercise we'll be training a model to classify movie reviews as 'good' or 'bad.'\\ The data consists of 50,000 real move reviews from IMBD.\\ Obligatory Disclaimer: This is real-world data and so it's possible that it contains language or topics that some may find offensive. ðŸ™ˆ We'll load the data which is hosted on the course Github repo as a zipped csv (it's too big to upload to Ed).\\ Notice that pd.read_csv() can take a URL as the path argument and that we can read in a compressed file without first expanding it if we specify the compression format! In [2]: data_url = 'https://github.com/Harvard-IACS/2021-CS109A/raw/master/content/lectures/lecture23/data/movie_reviews.zip' df = pd . read_csv ( data_url , compression = 'zip' ) In [3]: df . head () In [4]: df . shape In [5]: df . label . unique () We see that the dataset consists of text reviews and binary labels. Intuitively, the positive class is \"good\" while the negative is \"bad.\" Here are two examples from the dataset: In [6]: labels = { 0 : 'bad' , 1 : 'good' } seen = { 'bad' : False , 'good' : False } for i in range ( df . shape [ 0 ]): label = df . loc [ i , 'label' ] if not seen [ labels [ label ]]: # display/print combination used to appease Ed's strange output behavior display ( df . loc [ i , 'text' ]) print () display ( f \"label: { labels [ label ] } \" ) print () seen [ labels [ label ]] = True if all ( val == True for val in seen . values ()): break Some Preprocessing In the 2nd example, we can see some html tags inside the review text. Complete the remove_br() function by providing its call to re.sub() with a regex that removes those pesky \"\\ \" tags from an input string, x .\\ Speciffically, we should replace 2 consecutive occurances of \"\\ \" with a single space (can you see why?). Hint: It is good practice to use 'raw' string when writing regular expressions to ensure that special characters are treated correctly. Raw strings are appended with an 'r' like this: r'this is a raw string' In [7]: ### edTest(test_remove_br) ### # fill in the regular expression remove_br = lambda x : re . sub ( ___ , ' ' , x ) Use the dataframe's apply() method to apply remove_br to each review in both train and test. In [8]: df [ 'text' ] = df . text . apply ( ___ ) And we can see that the tags have been removed! In [9]: df . loc [ 4 , 'text' ] Don't worry about any newline characters or backslashes you may see before apostrophes in the examples above. This is just a quirk of how Jupyter displays strings by default.\\ We don't see that these characters if we explicitly print the string. In [10]: example_str = df . loc [ 4 , 'text' ] print ( example_str ) We'll continue our preprocessing by next removing punctuation .\\ But first, let's keep a copy of the data with punctuation. This will be useful at the end of the notebook when we want to display the original text of specific observations. In [11]: # store copy of data with punctuation df_raw = df . copy () The next regex we need is a bit more involved.\\ This should match any non-whitespace, any non-alphanumeric characters, and underscores (strangly, underscores are not covered by the first 2 conditions). Hints: \\w matches alphanumeric characters \\s matches whitespace [] can be used to denote a set of characters. ex: r'[ab]' will match on 'a' or 'b' &#94; at the beginning of a character set denotes negation . ex: r'[&#94;0-9]' will matching any non-integer | is the logical or operator. ex: r'cat|dog' will match the strings 'cat' or 'dog' There are many helpful sites for testing regexes. Here's a nice one . In [12]: ### edTest(test_punc_regex) ### # create a regex that will match the characters described above punc_regex = ___ Here we'll use an alternative to the apply approach we saw above.\\ Pandas has its own set of built-in string methods which includes a version of replace . But unlike Python's str.replace() this can actually use regexes! In [13]: df [ 'text' ] = df . text . str . replace ( punc_regex , '' , regex = True ) # remove punctuation If all went well we can see that punctuation has been removed from our dataset. In [14]: example_str = df . loc [ 4 , 'text' ] print ( example_str ) Train/Test Split Rather than splitting the data directly with train_test_split we'll instead use it to generate indices for the train and test data.\\ This may seem strange, but there is a good reason for it. These indices will later allow us to recover the original, unprocessed text from df_raw for any given training and test observations. Notice too that we are stratifying on the label. This will help ensure that good and bad reviews appear in the same proportions in both train and test. In [15]: # generate indices to designate train and test observations train_idx , test_idx = train_test_split ( range ( df . shape [ 0 ]), test_size = 0.2 , random_state = 0 , stratify = df [ 'label' ]) In [16]: # Separate the predictor from the response x = df . text . values y = df . label . values In [17]: # Create train and test sets using the generated indices x_train = x [ train_idx ] y_train = y [ train_idx ] x_test = x [ test_idx ] y_test = y [ test_idx ] Building the Classifier Pipeline \\ Step 1: Vectorizor It's true that there are still several preprocessing steps to be done such as converting to lowercase and tokenizing the reviews, but these can be done for using sklearn's TfidfVectorizer . In [18]: from sklearn.feature_extraction.text import TfidfVectorizer Instantiate a TfidfVectorizer with parameters such that it will: set all reviews to lowercase remove english stopwords exclude words that occur in less than 1 review in 10,000 exclude words that occur in more than 90% of reviews Hint: Reading the documentation, you'll see the arguments you need are lowercase , stop_words , min_df , and max_df In [19]: ### edTest(test_tfidf) ### vec = TfidfVectorizer ( ___ ) Step 2: Classifier We'll use logistic regression with l2 regularization as our classifier model. The LogisticRegressionCV object allows us to easily tune for the best regularization parameter. In [20]: from sklearn.linear_model import LogisticRegressionCV With 40,000 training observations and each word in the vectorizer's vocabulary counting acting as a predictor training could be slow.\\ This issue is exacerbated when using cross validation as we need fit the model multiple times!\\ We'll set our classifier CV parameters so as to help keep the training time down to around 30 seconds or so.\\ l2 penalty (e.g., Ridge) 10 iterations per fit (remember, logistic regression has no closed form solution for the betas!) 5-fold CV random state of 0 (the fitting can be stochastic) In [21]: ### edTest(test_clf) ### # Instantiate our Classifier clf = LogisticRegressionCV ( ___ ) Step 3: Pipeline Any text data going into our classifier will have to first be converted to numerical data by our vectorizer.\\ One way to do this would be to: fit the vectorizor on the training data transform a dataset with the fitted vectorizer pass the transformed data to the classifier (1) only needs to be done once, but (2) & (3) would need to be done manually for train, test, and any other data we want to give them model.\\ This would be tedious! Luckily, sklearn's Pipline object allow use to connect one more 'transformers' (such as a scaler or vectorizer) with a model. In [22]: from sklearn.pipeline import make_pipeline Use make_pipeline() to connect the vectorizor, vec , and our classifier, clf , into a single pipeline. Hint: You can set verbose=True to see the individual steps during the fit process later. In [23]: ### edTest(test_pipeline) ### # Construct the pipeline pipe = make_pipeline ( ___ ) Step 4: Fitting When it comes to fitting, we can treat the pipeline object as if it were the classifier object itself, and simply call fit on the pipeline. In [24]: # For the sake of time, we are fitting quickly and we may not converge # We'll supress those pesky warnings from warnings import simplefilter from sklearn.exceptions import ConvergenceWarning # We also ignore FutureWarnings due to version issues on Ed simplefilter ( \"ignore\" , category = ( ConvergenceWarning , FutureWarning )) In [25]: ### edTest(test_fit) ### # Fit the model via the pipeline pipe . ___ ( ___ , ___ ) We can inspect the steps of the pipeline. In [26]: pipe . get_params ()[ 'steps' ] By default they are named using the all lowercase class name of each object.\\ We can use these names to access the fitted objects inside. Here we see the size of our vectorizer's vocabulary. In [27]: features = pipe . get_params ()[ 'tfidfvectorizer' ] . get_feature_names () print ( '# of features:' , len ( features )) There are too many to print, but we can peek at a random sample. In [28]: sample_size = 40 feature_sample_idx = np . random . choice ( len ( features ), size = sample_size , replace = False ) print ( np . array ( features )[ feature_sample_idx ]) Similarly, we can access the fitted logistic model and see what regularization parameter was used. In [29]: best_C = pipe . get_params ()[ 'logisticregressioncv' ] . C_ [ 0 ] print ( f 'Best C from cross-validation: { best_C : .4f } ' ) Step 5: Prediction Just like we did when fitting, we can treat the pipeline object as the classifier when making predictions.\\ Predict on the test data to get: class labels probabilities of being the positive class (i.e., 'good' reviews) test accuracy In [30]: ### edTest(test_pred) ### # Predict class labels on test data y_pred = pipe . ___ ( ___ ) # Predict probabilities of the positive on the test data y_pred_proba = pipe . ___ ( ___ )[ ___ , ___ ] # Calculate test accuracy (there are several ways to do this) test_acc = ___ print ( f \"test accuracy: { test_acc : 0.3f } \" ) Can you get better than 0.896 by tweaking the preprocessing, or vetorizer and classifier parameters? Perhaps inspecting how our model makes its predictions may help us decide how we might improve the model in the future. Step 6: Interpretation Below we'll use the eli5 library we saw in Model Interpretation Lab (#11) to have some fun interpreting what is driving our model's predictions on specific test observations. In [31]: # For interpretation import eli5 # for parsing/formating eli5's HTML output from bs4 import BeautifulSoup # for displaying formatted HTML output from IPython.display import HTML Here are the words driving positive class predictions. In [32]: eli5 . show_weights ( clf , vec = vec , top = 25 ) Hmm, those digits like 710, 810, and 410 driving predictions seems strange. What might they represent?\\ We'll use the 'raw' data with punctuation when inspecting the data (See! It is coming in handy!) In [33]: x_train_raw = df_raw . text [ train_idx ] . values x_test_raw = df_raw . text [ test_idx ] . values In [34]: df_raw [ df . text . str . contains ( ' 710 ' )] . iloc [ 0 ] . text These are actually numerical ratings embedded in the reviews! Looking at the text without the punctuation made it hard for us to see this at first. Here's a helper function used to remove some extraneous things from eli5 's output. We just want to see the highlighted text.\\ You don't need to read through the function but it is here as a nice resource/example. ðŸ¤“ In [35]: def eli5_html ( clf , vec , observation ): \"\"\" helper function for nicely formatting and displaying eli5 output \"\"\" # Get info on is driving a given observation's predictions eli5_results = eli5 . show_prediction ( estimator = clf , doc = observation , vec = vec , targets = [ True ], target_names = [ 'bad' , 'good' ]) # Convert eli5's HTML data to BS object for parsing/formatting soup = BeautifulSoup ( eli5_results . data , 'html.parser' ) # Remove a table we don't want soup . table . decompose () # Remove the first <p> tag with unwanted text soup . p . decompose () # Display the newly formatted HTML! display ( HTML ( str ( soup ))) Now all you need to do is find the specific observations requested.\\ You'll need your y_pred_proba values for this section to find which elements from x_test_raw to select. Hint: np.argsort() , np.flip() , and np.abs() may be useful here. What are the 5 worst movie reviews in the test set according to your model? ðŸ… In [36]: # Find indices of 5 worst reviews worst5 = x_test_raw [ ___ ] In [37]: for i , review in enumerate ( worst5 ): style = 'background-color:black;color:white;font-weight:bold;padding:4px' display ( HTML ( f \"<p style= { style } >Bad Movie # { i + 1 } ðŸ…</p>\" )) eli5_html ( clf , vec , review ) What are the 5 best movie review in the test set according to your model? ðŸ† In [38]: # Find indices of 5 best reviews best5 = x_test_raw [ ___ ] In [39]: for i , review in enumerate ( best5 ): display ( HTML ( f \"<p style= { style } >Good Movie # { i + 1 } ðŸ†</p>\" )) eli5_html ( clf , vec , review ) What are the 5 most 'meh' movie review in the test set according to your model? ðŸ˜\\ That is, which reviews are the most neutral according to your model?\\ Upon reading some of these reviews you may find their sentiment to actually not be very ambiguous. What might be confusing our model? In [40]: # Find indices of the 5 most neutral reviews meh5 = x_test_raw [ ___ ] In [41]: for i , review in enumerate ( meh5 ): display ( HTML ( f \"<p style= { style } >'Meh' Movie # { i + 1 } ðŸ˜</p>\" )) eli5_html ( clf , vec , review ) Despite some difficulties with a few of the 'meh' movies, our model is actually pretty good! In fact, it works so well you can actually use it to find mistakes in the manually labeled data!\\ This can be done by inspecting which training observation predictions differ the most from the provided labels.\\ (But if you do decide to explore this, just remember the disclaimer at the top of the notebook!) Write your own review Finally, you can try writing a review of your own and see what your model does with it! In [42]: my_review = \"\"\" your review here \"\"\" # Remove punctuation using your regex from earlier my_review = re . sub ( punc_regex , '' , my_review ) # Remove leading & trailing whitespace # and put into a numpy array (which the model expects) my_review = np . array ([ my_review . strip ()]) my_review In [43]: my_review_proba = pipe . predict_proba ( my_review )[:, 1 ][ 0 ] my_review_label = pipe . predict ( my_review )[ 0 ] print ( 'predicted class:' , my_review_label ) print ( 'predicted probability:' , my_review_proba ) In [44]: display ( HTML ( f \"<p style= { style } >My Review ðŸ¿</p>\" )) eli5_html ( clf , vec , my_review [ 0 ]) In [45]: In [46]:",
        "tags": "lectures",
        "url": "lectures/lecture23/notebook-2/"
    }, {
        "title": "Lecture 22: Working Example",
        "text": "Slides Lecture 22 : Classification Metrics (PDF) Lecture 22 : Multinomial Classification (PDF) Exercises Lecture 22: Exercise - Case Study: Covid-19 [Notebook]",
        "tags": "lectures",
        "url": "lectures/lecture22/"
    }, {
        "title": "Lecture 22: Working Example",
        "text": "Case-Study-covid19_scaffold Title : Case Study: Covid-19 Hints: sklearn.model_selection.GridsearchCV Exhaustive search over specified parameter values for an estimator. sklearn.metrics.confusion_matrix Compute confusion matrix to evaluate the accuracy of a classification. seaborn.heatmap Plot rectangular data as a color-encoded matrix. sklearn.metrics.roc_auc_score Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores. sklearn.metrics.roc_curve Compute Receiver operating characteristic (ROC). COVID-19 Machine Learning DatasetÂ® Adopted from the dataset provided by Dr. Karandeep Singh @kdpsinghlab The goal of this case study (intended for education) is to predict the urgency with which a COVID-19 patient will need to be admitted to the hospital from the time of onset of symptoms. The original dataset is located on this github repo . Please note that this dataset has been simplified for this case study The raw data comes from the following source . Intended For Educational Use Only Should this data be used for research? No. Students working with this dataset should understand that both the source data and the ML data have several limitations: The source data is crowdsourced and may contain inaccuracies. There may be duplicate patients in this dataset There is a substantial amount of missingness in the symptoms data. And most importantly: The entire premise is flawed. The fact that a patient was admitted the same day as experiencing symptoms may have more to do with the availability of hospital beds as opposed to the patient's acuity of illness. Also, the fact that less sick patients or asymptomatic patients may not have been captured in the source dataset mean that the probabilities estimated by any model fit on this data are unlikely to reflect reality. Primary predictors: age (if an age range was provided in the source data, only the first number is used) sex cough, fever, chills, sore_throat, headache, fatigue (all derived from the symptoms column using regular expressions) The goal of the exercise is to make a classification model to predict the urgency_of_admission based on the following criteria 0-1 days from onset of symptoms to admission => High 2+ days from onset of symptoms to admission or no admission => Low In [0]: # Import necessary libraries # Feel free to import other modules and libraries as you deem fit import sklearn import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from prettytable import PrettyTable from sklearn.metrics import confusion_matrix from sklearn.model_selection import GridSearchCV from sklearn.neighbors import KNeighborsClassifier from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from IPython.core.interactiveshell import InteractiveShell from sklearn.metrics import classification_report , roc_auc_score , roc_curve , accuracy_score % matplotlib inline InteractiveShell . ast_node_interactivity = \"all\" Calling the dataset We are using a modified dataset found here covid_19 dataset source The following changes were made: Categorical values changed to 1 and 0 SMOTE used to upsample in order to balance the dataset. Refer to the class imbalance exercise in the Random Forest session. In [0]: # Read the train and test data # Take a look at the data to understand the features and reponse # Your code here In [0]: # define X_train, y_train, X_test, and y_test # Urgency is the response variable, all other variables are the predictors # Your code here GridsearchCV for Logistic Regression Perform a hyper-parameter search to get the best C value for Logistic Regression using GridsearchCV . For simplicity, use accuracy as the metric to choose best hyper-parameter In [0]: # Perform GridSearchCV to get the best C value for a Logistic Regression model # Feel free to use the cv and set of C values of your choice # Remember to keep track of your best C value # Your code here Fitting the data and making predictions In [0]: # Using the C value from above, initialize a Logistic Regression model # Fit the model on the train data # Predict on the test data # Your code here In [0]: # Compute the accuracy of the model logistic_acc = ___ GridsearchCV for KNN classification Perform a hyper-parameter search to get the best k value for KNN Classification using GridsearchCV . For simplicity, use accuracy as the metric to choose best hyper-parameter In [0]: # Perform GridSearchCV to get the best k value for a kNN Classification model # Feel free to use the cv and set of k values of your choice # Remember to keep track of your best k value # Your code here Fitting the data and making predictions In [0]: # Using the k value from above, initialize a kNN Classification model # Fit the model on the train data # Predict on the test data # Your code here In [0]: # Compute the accuracy of the model knn_acc = ___ In [0]: # Store the Confusion Matrix of the trained Logistic Regression Model on the test data in a variable # Your code here In [0]: # Store the Confusion Matrix of the trained kNN Classification Model on the test data in a variable # Your code here What is a Confusion Matrix? A classifier will get some samples right, and some wrong. Generally we see which ones it gets right and which ones it gets wrong on the test set True Positive Samples that are +ive and the classifier predicts as +ive are called True Positives (TP) False Positive Samples that are -ive and the classifier predicts (wrongly) as +ive are called False Positives (FP) True Negative Samples that are -ive and the classifier predicts as -ive are called True Negatives (TN) False Negative Samples that are +ive and the classifier predicts as -ive are called False Negatives (FN) The boy who cried wolf: Data Science edition Predicted wolf, but no wolf Predicted no wolf, but actually wolf Plot the Confusion Matrix Plot the Confusion Matrix for each of the above models as a heatmap. Your plot should look similar to the following: In [0]: # Plot of the Confusion Matrix for the Logisitic Regression and kNN Classification model # Your code here Sensitivity The Sensitivity , also known as Recall or True Positive Rate(TPR) $$TPR = Recall = \\frac{TP}{OP} = \\frac{TP}{TP+FN},$$ also called the Hit Rate : the fraction of observed positives (1s) the classifier gets right, or how many true positives were recalled. Maximizing the recall towards 1 means keeping down the false negative rate In [0]: # Compute the Sensitivity for the Logistic Regression model logistic_recall = ___ # Compute the Sensitivity for the kNN Classification model knn_recall = ___ Specificity The Specificity or True Negative Rate is defined as $$TNR = \\frac{TN}{ON} = \\frac{TN}{FP+TN}$$ In [0]: # Compute the Specificity for the Logistic Regression model logistic_fpr = ___ # Compute the Specificity for the kNN Classification model knn_fpr = ___ Precision (Positive Predicted Value) Precision ,tells you how many of the predicted positive(1) hits were truly positive $$Precision = \\frac{TP}{PP} = \\frac{TP}{TP+FP}.$$ In [0]: # Compute the Precision for the Logistic Regression model logistic_precision = ___ # Compute the Precision for the kNN Classification model knn_precision = ___ F1 score F1 score gives us the Harmonic Mean of Precision and Recall. It tries to minimize both false positives and false negatives simultaneously $$F1 = \\frac{2*Recall*Precision}{Recall + Precision}$$ In [0]: # Compute the F1-Score for the Logistic Regression model logistic_fscore = ___ # Compute the F1-Score for the kNN Classification model knn_fscore = ___ In [0]: # Helper code to bring everything together pt = PrettyTable () pt . field_names = [ \"Metric\" , \"Logistic Regression\" , \"kNN Classification\" ] pt . add_row ([ \"Accuracy\" , round ( logistic_acc , 3 ), round ( knn_acc , 3 )]) pt . add_row ([ \"Sensitivity(Recall)\" , round ( logistic_recall , 3 ), round ( knn_recall , 3 )]) pt . add_row ([ \"Specificity\" , round ( logistic_fpr , 3 ), round ( knn_fpr , 3 )]) pt . add_row ([ \"Precision\" , round ( logistic_precision , 3 ), round ( knn_precision , 3 )]) pt . add_row ([ \"F1 Score\" , round ( logistic_fscore , 3 ), round ( knn_fscore , 3 )]) print ( pt ) BACK TO THE LECTURE Bayes Theorem & Diagnostic testing Refer to Dr. Rahul Dave's Covid19 Serological testing blog for an excellent introduction to all the above concepts. In [0]: # Compute the area under the ROC curve for the Logistic Regression model logreg_auc = ___ # Compute the area under the ROC curve for the kNN Classification model knnreg_auc = ___ ROC Curve To make a ROC curve you plot the True Positive Rate, against the False Positive Rate, The curve is actually a 3 dimensional plot, which each point representing a different value of threshold. In [0]: # Plot the ROC curve for the Logistic Regression model and kNN Classification model # You can refer to the end of homework 5 for example code # Your code here Which classifier to choose? Choice of classifier Scenario 1 - BRAZIL The new variant of the Covid-19 virus is contagious and is infecting many Brazilians *Brazilian officials however dictate that hospitals do not classify a large number of people at 'high' risk to avoid bad press and subsequent political global backlash** In numbers we need the best classifier with the following restriction $$TPR + FPR \\le 0.5$$ Choice of classifier Scenario 2 - GERMANY It's the month of February, and Germany, is now aware that the pandemic of Covid-19 has severely hit Europe. Italy is already decimated and there is suspected spread to other European nations as well German officials have a clear target. The want the fatality ratio to be as less as possible. Thus, it is imperative to find cases in need of urgent attention and give them the best chance of survival. In numbers we need the best classifier with the following restriction $$ 0.8 \\ge TPR \\le 0.85 $$ Choice of classifier Scenario 3 - INDIA It's the month of May, and India, now severly impacted by Covid-19, has now run a major shortage of hospital beds for suspected cases Owing to exponentially rising cases Indian officials cannot afford many False Positives to be given hospital beds. Hence, it is dictated that hospitals do not classify a large number of people at 'high' risk to avoid bed shortage for At risk patients India has only 1 million beds left, and there are already 2 million people suspected of having the disease. The officials need to work out a strategy to find the people at most need of urgent care. In numbers we need the best classifier with the following restriction $$TP + FP = 1000000$$$$TP = TPR*OP$$ $$FP = TPR*ON$$ $$TPR*OP + FPR*ON = 1000000$$$$Assuming\\ OP=ON = 500000$$$$TPR + FPR \\le 1 $$ ROC curve with boundary conditions Plot the ROC curve of the Logistic Regression model and kNN Classification model, along with the boundary conditions for each of the scenarios. Each of the scenarios can be represented as a region governed by straight line(s) based on the given conditions. The resulting plot will look similar to the following: In [0]: # Area under curve - Logistic regression & kNN # along with the boundary conditions # Your code here BACK TO THE LECTURE",
        "tags": "lectures",
        "url": "lectures/lecture22/notebook/"
    }, {
        "title": "Lecture 21: AdaBoost",
        "text": "Slides Lecture 21 : AdaBoost (PDF) Exercises Lecture 21: Exercise - Boosting Classification [Notebook]",
        "tags": "lectures",
        "url": "lectures/lecture21/"
    }, {
        "title": "Lecture 21: AdaBoost",
        "text": "adaboost_classifier Title : Exercise: Boosting Classification Description : The aim of this exercise to understand classification using boosting by plotting the decision boundary after each stump. Your plot may resemble the image below: Instructions: Read the dataset boostingclassifier.csv as pandas dataframe and take a quick look. All columns except landtype are predictors. landtype is the response variable. Define the AdaBoost classifier from scratch within the function AdaBoost_scratch : Recall the AdaBoost algorithm from the slides: - Remember, we can derive the learning rate, $$\\lambda&#94;{(i)}Î»(i)$$ , for our iith estimator, $T&#94;{(i)}T(i)$, analytically. - Note: In the exercise we call $$\\lambda&#94;{(i)}Î»(i)$$ the 'estimator weight.' This is because SKLearn's Adaboost implementation has a learning_rate parameter which refers to a global hyperparameter. Call the AdaBoost_scratch function with the predictor and response variables for 9 stumps. Use the helper code provided to visualize the classification decision boundary for the 9 stumps. Hints: DecisionTreeClassifier() A decision tree classifier. sklearn.fit() Builds a model from the training set. np.average() Computes the weighted average along the specified axis. np.mean() Computes the arithmetic mean along the specified axis. np.log() Natural logarithm, element-wise. np.exp() Calculates the exponential of all elements in the input array. sklearn.AdaBoostClassifier() An AdaBoost classifier. Note: This exercise is auto-graded and you can make multiple attempts. In [89]: # Import necessary libraries import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from helper import plot_decision_boundary from matplotlib.colors import ListedColormap from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import AdaBoostClassifier % matplotlib inline sns . set_style ( 'white' ) In [90]: # Read the dataset as a pandas dataframe df = pd . read_csv ( \"boostingclassifier.csv\" ) # Read the columns latitude and longitude as the predictor variables X = df [[ 'latitude' , 'longitude' ]] . values # Landtype is the response variable y = df [ 'landtype' ] . values In [91]: ### edTest(test_response) ### # update the class labels to appropriate values for AdaBoost y = ___ In [92]: # AdaBoost algorithm implementation from scratch def AdaBoost_scratch ( X , y , M = 10 ): ''' X: data matrix of predictors y: response variable M: number of estimators (e.g., 'stumps') ''' # Initialization of utility variables N = len ( y ) estimator_list = [] y_predict_list = [] estimator_error_list = [] estimator_weight_list = [] sample_weight_list = [] # Initialize the sample weights sample_weight = np . ones ( N ) / N # Store a copy of the sample weights to a list # Q: why do we want to use .copy() here? The implementation will make it clear. sample_weight_list . append ( sample_weight . copy ()) # Fit each boosted stump # Q: Why might we prefer the variable name '_' here over something like 'm'? for _ in range ( M ): # Instantiate a Decision Tree classifier for our stump # Note: our stumps should have only a single split estimator = ___ # Fit the stump on the entire data with using the sample_weight variable # Hint: check the estimator's documentation for how to use sample weights estimator . fit ( ___ ) # Predict on the entire data y_predict = estimator . predict ( X ) # Create a binary vector representing the misclassifications incorrect = ___ # Compute the error as the weighted average of the # 'incorrect' vector above using the sample weights # Hint: np.average() makes this very simple estimator_error = ___ # Compute the estimator weight using the estimator error # Note: The estimator weight here is refered to as the 'learning rate' in the slides estimator_weight = ___ # Update the sample weights (un-normalized!) # Note: Make use of the '*=' assignment statement sample_weight *= ___ # Renormalize the sample weights # Note: Make use of the '/=' assignment statement sample_weight /= ___ # Save the iteration values estimator_list . append ( estimator ) y_predict_list . append ( y_predict . copy ()) estimator_error_list . append ( estimator_error . copy ()) estimator_weight_list . append ( estimator_weight . copy ()) sample_weight_list . append ( sample_weight . copy ()) # Convert to numpy array for convenience estimator_list = np . asarray ( estimator_list ) y_predict_list = np . asarray ( y_predict_list ) estimator_error_list = np . asarray ( estimator_error_list ) estimator_weight_list = np . asarray ( estimator_weight_list ) sample_weight_list = np . asarray ( sample_weight_list ) # Compute the predictions # Q: Why do we want to use np.sign() here? preds = ( np . array ([ np . sign (( y_predict_list [:, point ] * \\ estimator_weight_list ) . sum ()) for point in range ( N )])) # Return the model, estimated weights and sample weights return estimator_list , estimator_weight_list , sample_weight_list , preds In [99]: ### edTest(test_adaboost) ### # Call the AdaBoost function to perform boosting classification estimator_list , estimator_weight_list , sample_weight_list , preds = \\ AdaBoost_scratch ( X , y , M = 9 ) # Calculate the model's accuracy from the predictions returned above accuracy = ___ print ( f 'accuracy: { accuracy : .3f } ' ) In [94]: # Helper code to plot the AdaBoost Decision Boundary stumps fig = plt . figure ( figsize = ( 16 , 16 )) for m in range ( 0 , 9 ): fig . add_subplot ( 3 , 3 , m + 1 ) s_weights = ( sample_weight_list [ m ,:] / sample_weight_list [ m ,:] . sum () ) * 300 plot_decision_boundary ( estimator_list [ m ], X , y , N = 50 , scatter_weights = s_weights , counter = m ) plt . tight_layout () In [95]: # Use sklearn's AdaBoostClassifier to take a look at the final decision boundary # Initialise the model with Decision Tree classifier as the base model same as above # Use SAMME as the algorithm and 9 estimators boost = AdaBoostClassifier ( base_estimator = DecisionTreeClassifier ( max_depth = 1 ), algorithm = 'SAMME' , n_estimators = 9 ) # Fit on the entire data boost . fit ( X , y ) # Call the plot_decision_boundary function to plot the decision boundary of the model plot_decision_boundary ( boost , X , y , N = 50 ) plt . title ( 'AdaBoost Decision Boundary' , fontsize = 16 ) plt . show () â¸ How does the num_estimators affect the model? In [96]: ### edTest(test_chow1) ### # Type your answer within in the quotes given answer1 = '___' In [97]:",
        "tags": "lectures",
        "url": "lectures/lecture21/notebook/"
    }, {
        "title": "Lecture 20: Boosting, Gradient Boosting",
        "text": "Slides Lecture 20 : Boosting Algorithms (PDF) Lecture 20 : Gradient Boosting (PDF) Exercises Lecture 20: Exercise - Regression with Boosting [Notebook]",
        "tags": "lectures",
        "url": "lectures/lecture20/"
    }, {
        "title": "Lecture 20: Boosting, Gradient Boosting",
        "text": "Boosting_Regressor Title : Exercise: Regression with Boosting Description : The goal of this exercise is to understand Gradient Boosting Regression. Instructions: Part A: Read the dataset airquality.csv as a pandas dataframe. Take a quick look at the dataset. Assign the predictor and response variables appropriately as mentioned in the scaffold. Fit a single decision tree stump and predict on the entire data. Calculate the residuals and fit another tree on the residuals. Take a combination of the trees and fit on the model. For each of these model use the helper code provided to plot the model prediction and data. Part B: Compare to bagging Split the data into train and test splits. Specify the number of bootstraps for bagging to be 30 and a maximum depth of 3. Define a Gradient Boosting Regression model that uses with 1000 estimators and depth of 1. Define a Bagging Regression model that uses the Decision Tree as its base estimator. Fit both the models on the train data. Use the helper code to predict using the mean model and individual estimators. The plot will look similar to the one given above. Compute the MSE of the 2 models on the test data. Hints: sklearn.DecisionTreeRegressor() A decision tree regressor. regressor.fit() Build a decision tree regressor from the training set (X, y). sklearn.DecisionTreeClassifier() Generates a Logistic Regression classifier. classifier.fit() Build a decision tree classifier from the training set (X, y). sklearn.train_test_split() Split arrays or matrices into om train and test subsets. BaggingRegressor() Returns a Bagging regressor instance. sklearn.mean_squared_error() Mean squared error regression loss. GradientBoostingRegressor() Gradient Boosting for regression. Note: This exercise is auto-graded and you can try multiple attempts. In [0]: # Import necessary libraries import itertools import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.ensemble import BaggingRegressor from sklearn.tree import DecisionTreeRegressor from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split from sklearn.ensemble import GradientBoostingRegressor % matplotlib inline In [0]: # Read the dataset airquality.csv df = pd . read_csv ( \"airquality.csv\" ) In [0]: # Take a quick look at the data # Remove rows with missing values df = df [ df . Ozone . notna ()] df . head () In [0]: # Assign \"x\" column as the predictor variable and \"y\" as the # We only use Ozone as a predictor for this exercise and Temp as the response x , y = df [ 'Ozone' ] . values , df [ 'Temp' ] . values # Sorting the data based on X values x , y = list ( zip ( * sorted ( zip ( x , y )))) x , y = np . array ( x ) . reshape ( - 1 , 1 ), np . array ( y ) Part A: Gradient Boosting by hand In [0]: # Initialise a single decision tree stump basemodel = ___ # Fit the stump on the entire data ___ # Predict on the entire data y_pred = ___ In [0]: # Helper code to plot the data plt . figure ( figsize = ( 10 , 6 )) xrange = np . linspace ( x . min (), x . max (), 100 ) plt . plot ( x , y , 'o' , color = '#EFAEA4' , markersize = 6 , label = \"True Data\" ) plt . xlim () plt . plot ( x , y_pred , alpha = 0.7 , linewidth = 3 , color = '#50AEA4' , label = 'First Tree' ) plt . xlabel ( \"Ozone\" , fontsize = 16 ) plt . ylabel ( \"Temperature\" , fontsize = 16 ) plt . xticks ( fontsize = 12 ) plt . yticks ( fontsize = 12 ) plt . legend ( loc = 'best' , fontsize = 12 ) plt . show () In [0]: ### edTest(test_first_residuals) ### # Calculate the error residuals residuals = ___ In [0]: # Helper code to plot the data with the residuals plt . figure ( figsize = ( 10 , 6 )) plt . plot ( x , y , 'o' , color = '#EFAEA4' , markersize = 6 , label = \"True Data\" ) plt . plot ( x , residuals , '.-' , color = '#faa0a6' , markersize = 6 , label = \"Residuals\" ) plt . plot ([ x . min (), x . max ()],[ 0 , 0 ], '--' ) plt . xlim () plt . plot ( x , y_pred , alpha = 0.7 , linewidth = 3 , color = '#50AEA4' , label = 'First Tree' ) plt . xlabel ( \"Ozone\" , fontsize = 16 ) plt . ylabel ( \"Temperature\" , fontsize = 16 ) plt . xticks ( fontsize = 12 ) plt . yticks ( fontsize = 12 ) plt . legend ( loc = 'center right' , fontsize = 12 ) plt . show () In [0]: ### edTest(test_fitted_residuals) ### # Initialise a tree stump dtr = ___ # Fit the tree stump on the residuals ___ # Predict on the entire data y_pred_residuals = ___ In [0]: # Helper code to add the fit of the residuals to the original plot plt . figure ( figsize = ( 10 , 6 )) plt . plot ( x , y , 'o' , color = '#EFAEA4' , markersize = 6 , label = \"True Data\" ) plt . plot ( x , residuals , '.-' , color = '#faa0a6' , markersize = 6 , label = \"Residuals\" ) plt . plot ([ x . min (), x . max ()],[ 0 , 0 ], '--' ) plt . xlim () plt . plot ( x , y_pred , alpha = 0.7 , linewidth = 3 , color = '#50AEA4' , label = 'First Tree' ) plt . plot ( x , y_pred_residuals , alpha = 0.7 , linewidth = 3 , color = 'red' , label = 'Residual Tree' ) plt . xlabel ( \"Ozone\" , fontsize = 16 ) plt . ylabel ( \"Temperature\" , fontsize = 16 ) plt . xticks ( fontsize = 12 ) plt . yticks ( fontsize = 12 ) plt . legend ( loc = 'center right' , fontsize = 12 ) plt . show () In [0]: ### edTest(test_new_pred) ### # Set a lambda value and compute the predictions based on # the residuals lambda_ = ___ y_pred_new = ___ In [0]: # Helper code to plot the boosted tree plt . figure ( figsize = ( 10 , 8 )) plt . plot ( x , y , 'o' , color = '#EFAEA4' , markersize = 6 , label = \"True Data\" ) plt . plot ( x , residuals , '.-' , color = '#faa0a6' , markersize = 6 , label = \"Residuals\" ) plt . plot ([ x . min (), x . max ()],[ 0 , 0 ], '--' ) plt . xlim () plt . plot ( x , y_pred , alpha = 0.7 , linewidth = 3 , color = '#50AEA4' , label = 'First Tree' ) plt . plot ( x , y_pred_residuals , alpha = 0.7 , linewidth = 3 , color = 'red' , label = 'Residual Tree' ) plt . plot ( x , y_pred_new , alpha = 0.7 , linewidth = 3 , color = 'k' , label = 'Boosted Tree' ) plt . xlabel ( \"Ozone\" , fontsize = 16 ) plt . ylabel ( \"Temperature\" , fontsize = 16 ) plt . xticks ( fontsize = 12 ) plt . yticks ( fontsize = 12 ) plt . legend ( loc = 'center right' , fontsize = 12 ) plt . show () Part 2: Comparison with Bagging To compare the two methods, we will be using sklearn's methods and not our own implementation from above. In [0]: # Split the data into train and test sets with train size as 0.8 # and random_state as 102 # The default value for shuffle is True for train_test_split, so the ordering we # did above is not a problem. x_train , x_test , y_train , y_test = train_test_split ( x , y , train_size = 0.8 , random_state = 102 ) In [0]: ### edTest(test_boosting) ### # Set a learning rate l_rate = ___ # Initialise a Boosting model using sklearn's boosting model # Use 1000 estimators, depth of 1 and learning rate as defined above boosted_model = ___ # Fit on the train data ___ # Predict on the test data y_pred = ___ In [0]: # Specify the number of bootstraps num_bootstraps = 30 # Specify the maximum depth of the decision tree max_depth = 100 # Define the Bagging Regressor Model # Use Decision Tree as your base estimator with depth as mentioned in max_depth # Initialise number of estimators using the num_bootstraps value # Set max_samples as 1 and random_state as 3 model = ___ # Fit the model on the train data ___ In [0]: # Helper code to plot the bagging and boosting model predictions plt . figure ( figsize = ( 10 , 8 )) xrange = np . linspace ( x . min (), x . max (), 100 ) . reshape ( - 1 , 1 ) y_pred_boost = boosted_model . predict ( xrange ) y_pred_bag = model . predict ( xrange ) plt . plot ( x , y , 'o' , color = '#EFAEA4' , markersize = 6 , label = \"True Data\" ) plt . xlim () plt . plot ( xrange , y_pred_boost , alpha = 0.7 , linewidth = 3 , color = '#77c2fc' , label = 'Bagging' ) plt . plot ( xrange , y_pred_bag , alpha = 0.7 , linewidth = 3 , color = '#50AEA4' , label = 'Boosting' ) plt . xlabel ( \"Ozone\" , fontsize = 16 ) plt . ylabel ( \"Temperature\" , fontsize = 16 ) plt . xticks ( fontsize = 12 ) plt . yticks ( fontsize = 12 ) plt . legend ( loc = 'best' , fontsize = 12 ) plt . show () In [0]: ### edTest(test_mse) ### # Compute the MSE of the Boosting model prediction on the test data boost_mse = ___ print ( \"The MSE of the Boosting model is\" , boost_mse ) In [0]: # Compute the MSE of the Bagging model prediction on the test data bag_mse = ___ print ( \"The MSE of the Bagging model is\" , bag_mse )",
        "tags": "lectures",
        "url": "lectures/lecture20/notebook/"
    }, {
        "title": "Lecture 19: Random Forrest II",
        "text": "Slides Lecture 19 : Random Forest - Feature Importance (PDF) Lecture 19 : Boosting (PDF) Exercises Lecture 19: [Notebook]",
        "tags": "lectures",
        "url": "lectures/lecture19/"
    }, {
        "title": "Lecture 19: Random Forrest II",
        "text": "rf_imbalance_challenge Title : Exercise: Random Forest with Class Imbalance Description : The goal of this exercise is to investigate the performance of Random Forest on a dataset with class imbalance, and then use corrections strategies to improve performance. Your final comparison may look like the table below (but not with these exact values): Instructions: Read the dataset diabetes.csv as a pandas dataframe. Take a quick look at the dataset. Split the data into train and test sets. Perform classification with a Vanilla Random Forest which does not take into account class imbalance. Perform classification with a Balanced Random Forest which does take into account class imbalance. Upsample the data and perform classification with a Balanced Random Forest. Downsample the data and perform classification with a Balanced Random Forest. Compare the F1-Score and AUC Score of all 4 models. Hints: np.ravel() Return a contiguous flattened array. f1_score() Compute the F1 score, also known as balanced F-score or F-measure. roc_auc_score() Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores. sklearn.train_test_split() Split arrays or matrices into random train and test subsets. RandomForestClassifier() Defines the RandomForestClassifier and includes more details on the definition and range of values for its tunable parameters. RandomForestClassifier.fit() Build a forest of trees from the training set (X, y). RandomForestClassifier.predict() Predict class for X. BalancedRandomForestClassifier() A balanced random forest classifier. BalancedRandomForestClassifier.fit() Build a forest of trees from the training set (X, y). BalancedRandomForestClassifier.predict() Predict class for X. SMOTE() Class to perform over-sampling using SMOTE. SMOTE.fit_resample() Resample the dataset. RandomUnderSampler() Class to perform random under-sampling. RandomUnderSampler.fit_resample() Resample the dataset. In [2]: # Import necessary libraries import numpy as np import pandas as pd import matplotlib.pyplot as plt from prettytable import PrettyTable from imblearn.over_sampling import SMOTE from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import f1_score , roc_auc_score from sklearn.model_selection import train_test_split from sklearn.inspection import permutation_importance from imblearn.under_sampling import RandomUnderSampler from imblearn.ensemble import BalancedRandomForestClassifier % matplotlib inline In [0]: # Code to read the dataset and take a quick look df = pd . read_csv ( \"diabetes.csv\" ) df . head () In [4]: # Investigate the response variable for data imbalance count0 , count1 = df [ 'Outcome' ] . value_counts () print ( f 'The percentage of diabetics in the dataset is only { 100 * count1 / ( count0 + count1 ) : .2f } %' ) The percentage of diabetics in the dataset is only 34.90% In [5]: # Assign the predictor and response variables # \"Outcome\" is the response and all the other columns are the predictors # Use the values of these features and response X = ___ y = ___ In [6]: # Fix a random_state random_state = 22 # Split the data into train and validation sets # Set random state as defined above and use a train size of 0.8 X_train , X_val , y_train , y_val = ___ In [7]: # Set the max_depth variable to 20 for all trees max_depth = ___ Strategy 1 - Vanilla Random Forest No correction for imbalance In [0]: # Define a Random Forest classifier with randon_state as above # Set the maximum depth to be max_depth and use 10 estimators random_forest = ___ # Fit the model on the training set ___ In [9]: ### edTest(test_vanilla) ### # Use the trained model to predict on the validation set predictions = ___ # Compute two metrics that better represent misclassification of minority classes # i.e `F1 score` and `AUC` # Compute the F1-score and assign it to variable score1 f_score = ___ score1 = round ( f_score , 2 ) # Compute the AUC and assign it to variable auc1 auc_score = ___ auc1 = round ( auc_score , 2 ) Strategy 2 - Random Forest with class weighting Balancing the class imbalance in each bootstrap In [0]: # Define a Random Forest classifier with randon_state as above # Set the maximum depth to be max_depth and use 10 estimators # Use class_weight as balanced_subsample to weigh the class accordingly random_forest = ___ # Fit the model on the training set ___ In [11]: ### edTest(test_balanced) ### # Use the trained model to predict on the validation set predictions = ___ # Compute two metrics that better represent misclassification of minority classes # i.e `F1 score` and `AUC` # Compute the F1-score and assign it to variable score2 f_score = ___ score2 = round ( f_score , 2 ) # Compute the AUC and assign it to variable auc2 auc_score = ___ auc2 = round ( auc_score , 2 ) Strategy 3 - Balanced Random Forest with SMOTE Using the imblearn BalancedRandomForestClassifier() Read more about this implementation here In [13]: # Perform upsampling using SMOTE # Define a SMOTE with random_state=2 sm = ___ # Use the SMOTE object to upsample the train data # You may have to use ravel() X_train_res , y_train_res = ___ In [0]: # Define a Random Forest classifier with randon_state as above # Set the maximum depth to be max_depth and use 10 estimators # Use class_weight as balanced_subsample to weigh the class accordingly random_forest = ___ # Fit the Random Forest on upsampled data ___ In [15]: ### edTest(test_upsample) ### # Use the trained model to predict on the validation set predictions = ___ # Compute the F1-score and assign it to variable score3 f_score = ___ score3 = round ( f_score , 2 ) # Compute the AUC and assign it to variable auc3 auc_score = ___ auc3 = round ( auc_score , 2 ) Strategy 4 - Downsample the data Using the imblearn RandomUnderSampler(). In [17]: # Define an RandomUnderSampler instance with random state as 2 rs = ___ # Downsample the train data # You may have to use ravel() X_train_res , y_train_res = ___ In [0]: # Define a Random Forest classifier with randon_state as above # Set the maximum depth to be max_depth and use 10 estimators # Use class_weight as balanced_subsample to weigh the class accordingly random_forest = ___ # Fit the Random Forest on downsampled data ___ In [20]: ### edTest(test_downsample) ### # Use the trained model to predict on the validation set predictions = ___ # Compute two metrics that better represent misclassification of minority classes # i.e `F1 score` and `AUC` # Compute the F1-score and assign it to variable score4 f_score = ___ score4 = round ( f_score , 2 ) # Compute the AUC and assign it to variable auc4 auc_score = ___ auc4 = round ( auc_score , 2 ) In [0]: # Compile the results from the implementations above pt = PrettyTable () pt . field_names = [ \"Strategy\" , \"F1 Score\" , \"AUC score\" ] pt . add_row ([ \"Random Forest - No imbalance correction\" , score1 , auc1 ]) pt . add_row ([ \"Random Forest - balanced_subsamples\" , score2 , auc2 ]) pt . add_row ([ \"Random Forest - Upsampling\" , score3 , auc3 ]) pt . add_row ([ \"Random Forest - Downsampling\" , score4 , auc4 ]) print ( pt ) â¸ Which of the metrics given below is not recommended when there is a imbalance in the dataset? A. Precision B. Recall C. F1-Score D. Accuracy E. AUC-Score In [0]: ### edTest(test_chow1) ### # Submit an answer choice as a string below (eg. if you choose option A, put 'A') answer1 = '___'",
        "tags": "lectures",
        "url": "lectures/lecture19/notebook/"
    }, {
        "title": "Lecture 18: Random Forest",
        "text": "Slides Lecture 18 : Bagging - B (PDF) Lecture 18 : Random Forest (PDF) Exercises Lecture 18: Exercise - Bagging vs Random Forest (Tree correlation) [Notebook] Lecture 18: Exercise - Hyperparameter tuning [Notebook] Lecture 18: Exercise - Feature Importance [Notebook]",
        "tags": "lectures",
        "url": "lectures/lecture18/"
    }, {
        "title": "Lecture 18: Random Forest",
        "text": "tree_correlation Title : Exercise: Bagging vs Random Forest (Tree correlation) Description : How does Random Forest improve on Bagging? The goal of this exercise is to investigate the correlation between randomly selected trees from Bagging and Random Forest. Instructions: Read the dataset diabetes.csv as a pandas dataframe, and take a quick look at the data. Split the data into train and validation sets. Define a BaggingClassifier model that uses DecisionTreClassifier as its base estimator. Specify the number of bootstraps as 1000 and a maximum depth of 3. Fit the BaggingClassifier model on the train data. Use the helper code to predict using the mean model and individual estimators. The plot will look similar to the one given below. Predict on the test data using the first estimator and the mean model. Compute and display the validation accuracy Repeat the modeling and classification process above, this time using a RandomForestClassifier . Your final output will look something like this: Hints: sklearn.train_test_split() Split arrays or matrices into random train and test subsets. sklearn.ensemble.BaggingClassifier() Returns a Bagging classifier instance. sklearn.tree.DecisionTreeClassifier() A Tree classifier can be used as the base model for the Bagging classifier. sklearn.ensemble.andomForestClassifier() Defines a Random forest classifier. sklearn.metrics.accuracy_score(y_true, y_pred) Accuracy classification score. In [1]: #!pip install -qq dtreeviz import os , sys sys . path . append ( f \" { os . getcwd () } /../\" ) In [2]: # Import the main packages import numpy as np import pandas as pd import matplotlib.pyplot as plt from dtreeviz.trees import dtreeviz from sklearn.metrics import accuracy_score from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier , BaggingClassifier % matplotlib inline colors = [ None , # 0 classes None , # 1 class [ '#FFF4E5' , '#D2E3EF' ], # 2 classes ] from IPython.display import Markdown , display def printmd ( string ): display ( Markdown ( string )) In [3]: # Read the dataset and take a quick look df = pd . read_csv ( \"diabetes.csv\" ) df . head () In [4]: ### edTest(test_assign) ### # Assign the predictor and response variables. # \"Outcome\" is the response and all the other columns are the predictors X = __ y = __ In [5]: # Fix a random_state random_state = 144 # Split the data into train and validation sets with 80% train size # and the above set random state X_train , X_val , y_train , y_val = train_test_split ( ___ ) Bagging Implementation In [6]: # Define a Bagging classifier with randon_state as above # and with a DecisionClassifier as a basemodel # Fix the max_depth variable to 20 for all trees max_depth = ___ # Set the 100 estimators n_estimators = ___ # Initialize the Decision Tree classsifier with the set max depth and # random state basemodel = ___ # Initialize a Bagging classsifier with the Decision Tree as the base and # estimator and the number of estimator defined above bagging = ___ # Fit the Bagging model on the training set ___ In [7]: ### edTest(test_bagging) ### # Use the trained model to predict on the validation data predictions = ___ # Compute the accuracy on the validation set acc_bag = round ( accuracy_score ( predictions , y_val ), 2 ) # Print the validation data accuracy print ( f 'For Bagging, the accuracy on the validation set is { acc_bag } ' ) Random Forest implementation In [8]: # Define a Random Forest classifier with random_state as defined above # and set the maximum depth to be max_depth and use 100 estimators random_forest = ___ # Fit the model on the training set ___ In [9]: ### edTest(test_RF) ### # Use the trained Random Forest model to predict on the validation data predictions = ___ # Compute the accuracy on the validation set acc_rf = round ( accuracy_score ( predictions , y_val ), 2 ) # Print the validation data accuracy print ( f 'For Random Forest, the accuracy on the validation set is { acc_rf } ' ) Visualizing the trees - Bagging In [10]: # Helper code to visualize the Bagging tree # Reduce the max_depth for better visualization max_depth = 3 basemodel = DecisionTreeClassifier ( max_depth = max_depth , random_state = random_state ) bagging = BaggingClassifier ( base_estimator = basemodel , n_estimators = 1000 ) # Fit the model on the training set bagging . fit ( X_train , y_train ) # Selecting two trees at random bagvati1 = bagging . estimators_ [ 0 ] bagvati2 = bagging . estimators_ [ 100 ] In [11]: vizA = dtreeviz ( bagvati1 , df . iloc [:,: 8 ], df . Outcome , feature_names = df . columns [: 8 ], target_name = 'Diabetes' , class_names = [ 'No' , 'Yes' ] , orientation = 'TD' , colors = { 'classes' : colors }, label_fontsize = 14 , ticks_fontsize = 10 , ) printmd ( '<center> <h2> <i> Bagging Tree 1 </h2> </center>' ) vizA In [12]: vizB = dtreeviz ( bagvati2 , df . iloc [:,: 8 ], df . Outcome , feature_names = df . columns [: 8 ], target_name = 'Diabetes' , class_names = [ 'No' , 'Yes' ] , orientation = 'TD' , colors = { 'classes' : colors }, label_fontsize = 14 , ticks_fontsize = 10 , scale = 1.1 ) printmd ( '<center> <h2> <i> Bagging Tree 2 </h2> </center>' ) vizB Visualizing the trees - Random Forest In [13]: # Helper code to visualize the Random Forest tree # Reduce the max_depth for visualization max_depth = 3 random_forest = RandomForestClassifier ( max_depth = max_depth , random_state = random_state , n_estimators = 1000 , max_features = \"sqrt\" ) # Fit the model on the training set random_forest . fit ( X_train , y_train ) # Selecting two trees at random forestvati1 = random_forest . estimators_ [ 0 ] forestvati2 = random_forest . estimators_ [ 100 ] In [14]: vizC = dtreeviz ( forestvati1 , df . iloc [:,: 8 ], df . Outcome , feature_names = df . columns [: 8 ], target_name = 'Diabetes' , class_names = [ 'No' , 'Yes' ] , orientation = 'TD' , colors = { 'classes' : colors }, label_fontsize = 14 , ticks_fontsize = 10 , scale = 1.1 ) printmd ( '<center> <h2> <i> Random Forest Tree 1 </h2> </center>' ) vizC In [15]: vizD = dtreeviz ( forestvati2 , df . iloc [:,: 8 ], df . Outcome , feature_names = df . columns [: 8 ], target_name = 'Diabetes' , class_names = [ 'No' , 'Yes' ] , orientation = 'TD' , colors = { 'classes' : colors }, label_fontsize = 14 , ticks_fontsize = 10 , scale = 1.1 ) printmd ( '<center> <h2> <i> Random Forest Tree 2 </h2> </center>' ) vizD",
        "tags": "lectures",
        "url": "lectures/lecture18/notebook/"
    }, {
        "title": "Lecture 18: Random Forest",
        "text": "hyper_tuning Title : Exercise: Hyperparameter tuning Description : Tuning the hyperparameters Random Forests perform very well out-of-the-box, with the pre-set hyperparameters in sklearn. Some of the tunable parameters are: The number of trees in the forest: n_estimators, int, default=100 The complexity of each tree: stop when a leaf has <= min_samples_leaf samples The sampling scheme: number of features to consider at any given split: max_features {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\". Instructions: Read the datafile diabetes.csv as a Pandas data frame. Assign the predictor and response variable as mentioned in the scaffold. Split the data into train and validation sets. Define a vanilla Random Forest and fit the model on the entire data. For various hyper parameters of the model, define different Random Forest models and train on the data. Compare the results with each model. Hints: RandomForestClassifier() Defines the RandomForestClassifier and includes more details on the definition and range of values for its tunable parameters. model.predict_proba(X) Predict class probabilities for X roc_auc(y_test, y_proba) Calculates the area under the receiver operating curve (AUC). GridSearchCV() Performes exhaustive search over specified parameter values for an estimator. In [1]: # Import necessary libraries import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.metrics import roc_auc_score from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.inspection import permutation_importance % matplotlib inline In [2]: # Read the dataset and take a quick look df = pd . read_csv ( \"diabetes.csv\" ) df . head () In [3]: # Assign the predictor and response variables. # Outcome is the response and all the other columns are the predictors X = df . drop ( \"Outcome\" , axis = 1 ) y = df [ 'Outcome' ] In [4]: # Set the seed for reproducibility of results seed = 0 # Split the data into train and test sets with the mentioned seed X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.33 , random_state = seed ) Vanila random forest Start by training a Random Forest Classifier using the default parameters and calculate the Receiver Operating Characteristic Area Under the Curve (ROC AUC). As we know, this metric is better than accuracy for a classification problem, since it covers the case of an imbalanced dataset. In [5]: ### edTest(test_vanilla) ### # Define a Random Forest classifier with randon_state = seed vanilla_rf = ___ # Fit the model on the entire data vanilla_rf . fit ( ___ , ___ ); # Calculate AUC/ROC on the test set y_proba = ___ [:, 1 ] auc = np . round ( roc_auc_score ( y_test , y_proba ), 2 ) print ( f 'Plain RF AUC on test set: { auc } ' ) In [6]: # Number of samples and features num_features = X_train . shape [ 1 ] num_samples = X_train . shape [ 0 ] num_samples , num_features 1. Number of trees, num_iterators , default = 100 The number of trees needs to be large enough for the $oob$ error to stabilize in its lowest possible value. Plot the $oob$ error of a random forest as a function of the number of trees. Trees in a RF are called estimators . A good start is 10 times the number of features, however, adjusting other hyperparameters will influence the optimum number of trees. In [7]: %%time from collections import OrderedDict clf = RandomForestClassifier ( warm_start = True , oob_score = True , min_samples_leaf = 40 , max_depth = 10 , random_state = seed ) error_rate = {} # Range of `n_estimators` values to explore. min_estimators = 150 max_estimators = 500 for i in range ( min_estimators , max_estimators + 1 ): clf . set_params ( n_estimators = i ) clf . fit ( X_train . values , y_train . values ) # Record the OOB error for each `n_estimators=i` setting. oob_error = 1 - clf . oob_score_ error_rate [ i ] = oob_error In [8]: %%time # Generate the \"OOB error rate\" vs. \"n_estimators\" plot. # OOB error rate = num_missclassified/total observations (%)\\ xs = [] ys = [] for label , clf_err in error_rate . items (): xs . append ( label ) ys . append ( clf_err ) plt . plot ( xs , ys ) plt . xlim ( min_estimators , max_estimators ) plt . xlabel ( \"n_estimators\" ) plt . ylabel ( \"OOB error rate\" ) plt . show (); 2. min_samples_leaf , default = 1 The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression. We will plot various values of the min_samples_leaf with num_iterators . In [9]: %%time from collections import OrderedDict ensemble_clfs = [ ( 1 , RandomForestClassifier ( warm_start = True , min_samples_leaf = 1 , oob_score = True , max_depth = 10 , random_state = seed )), ( 5 , RandomForestClassifier ( warm_start = True , min_samples_leaf = 5 , oob_score = True , max_depth = 10 , random_state = seed )) ] # Map a label (the value of `min_samples_leaf`) to a list of (model, oob error) tuples. error_rate = OrderedDict (( label , []) for label , _ in ensemble_clfs ) min_estimators = 80 max_estimators = 500 for label , clf in ensemble_clfs : for i in range ( min_estimators , max_estimators + 1 ): clf . set_params ( n_estimators = i ) clf . fit ( X_train . values , y_train . values ) # Record the OOB error for each model. Error is 1 - oob_score # oob_score: score of the training dataset obtained using an # out-of-bag estimate. # OOB error rate is % of num_missclassified/total observations oob_error = 1 - clf . oob_score_ error_rate [ label ] . append (( i , oob_error )) for label , clf_err in error_rate . items (): xs , ys = zip ( * clf_err ) plt . plot ( xs , ys , label = f 'min_samples_leaf= { label } ' ) plt . xlim ( min_estimators , max_estimators ) plt . xlabel ( \"n_estimators\" ) plt . ylabel ( \"OOB error rate\" ) plt . legend ( loc = \"upper right\" ) plt . show (); In [10]: err = 100 best_num_estimators = 0 for label , clf_err in error_rate . items (): num_estimators , error = min ( clf_err , key = lambda n : ( n [ 1 ], - n [ 0 ])) if error < err : err = error ; best_num_estimators = num_estimators ; best_leaf = label print ( f 'Optimum num of estimators: { best_num_estimators } \\n min_samples_leaf: { best_leaf } ' ) Re-train the Random Forest Classifier using the new values for the parameters and calculate the AUC/ROC. Include another parameter, the max_features , the number of features to consider when looking for the best split. In [11]: ### edTest(test_estimators) ### estimators_rf = RandomForestClassifier ( n_estimators = best_num_estimators , random_state = seed , oob_score = True , min_samples_leaf = best_leaf , max_features = 'sqrt' ) # Fit the model on the entire data estimators_rf . fit ( X_train , y_train ); # Calculate AUC/ROC on the test set y_proba = ___ [:, 1 ] estimators_auc = np . round ( roc_auc_score ( y_test , y_proba ), 2 ) print ( f 'Educated RF AUC on test set: { estimators_auc } ' ) Look at the model's parameters In [12]: estimators_rf . get_params () 3. Performing a cross-validation search After we have some idea of the range of optimum values for the number of trees and maybe a couple of other parameters, and have enough computing power, you may perform an exhaustive search over other parameter values. In [13]: %%time from sklearn.model_selection import GridSearchCV do_grid_search = True if do_grid_search : rf = RandomForestClassifier ( n_jobs =- 1 , n_estimators = best_num_estimators , oob_score = True , max_features = 'sqrt' , min_samples_leaf = best_leaf , random_state = seed ) . fit ( X_train , y_train ) param_grid = { 'min_samples_split' : [ 2 , 5 , None ]} scoring = { 'AUC' : 'roc_auc' } grid_search = GridSearchCV ( rf , param_grid , scoring = scoring , refit = 'AUC' , return_train_score = True , n_jobs =- 1 ) results = grid_search . fit ( X_train , y_train ) print ( results . best_estimator_ . get_params ()) best_rf = results . best_estimator_ # Calculate AUC/ROC y_proba = best_rf . predict_proba ( X_test )[:, 1 ] auc = np . round ( roc_auc_score ( y_test , y_proba ), 2 ) print ( f 'GridSearchCV RF AUC on test set: { auc } ' )",
        "tags": "lectures",
        "url": "lectures/lecture18/notebook-2/"
    }, {
        "title": "Lecture 18: Random Forest",
        "text": "feature_importance Title : Exercise: Feature Importance The goal of this exercise is to compare two feature importance methods; MDI, and Permutation Importance. For a discussion on the merits of each go to this link . Description : Instructions: Read the dataset heart.csv as a pandas dataframe, and take a quick look at the data. Assign the predictor and response variables as per the instructions given in the scaffold. Set a max_depth value. Define a DecisionTreeClassifier and fit on the entire data. Define a RandomForestClassifier and fit on the entire data. Calculate Permutation Importance for each of the two models. Remember that the MDI is automatically computed by sklearn when you call the classifiers. Use the routines provided to display the feature importance of bar plots. The plots will look similar to the one given above. Hints: forest.feature_importances_ Calculate the impurity-based feature importance. sklearn.inspection.permutation_importance() Calculate the permutation-based feature importance. sklearn.RandomForestClassifier() Returns a random forest classifier object. sklearn.DecisionTreeClassifier() Returns a decision tree classifier object. NOTE - MDI is automatically computed by sklearn by calling RandomForestClassifier and/or DecisionTreeClassifier. In [0]: # Import necessary libraries import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.inspection import permutation_importance from sklearn.tree import DecisionTreeClassifier from helper import plot_permute_importance , plot_feature_importance % matplotlib inline In [0]: # Read the dataset \"heart.csv\" df = pd . read_csv ( \"heart.csv\" ) # Take a quick look at the data df . head () In [0]: # Assign the predictor and response variables. # 'AHD' is the response and all the other columns are the predictors X = ___ y = ___ In [0]: # Set the model parameters # The random state is fized for testing purposes random_state = 44 # Choose a `max_depth` for your trees max_depth = ___ SINGLE TREE In [0]: ### edTest(test_decision_tree) ### # Define a Decision Tree classifier with random_state as the above defined variable # Set the maximum depth to be max_depth tree = ___ # Fit the model on the entire data tree . fit ( X , y ); # Using Permutation Importance to get the importance of features for the Decision Tree # with random_state as the above defined variable tree_result = ___ RANDOM FOREST In [0]: ### edTest(test_random_forest) ### # Define a Random Forest classifier with random_state as the above defined variable # Set the maximum depth to be max_depth and use 10 estimators forest = ___ # Fit the model on the entire data forest . fit ( X , y ); # Use Permutation Importance to get the importance of features for the Random Forest model # with random_state as the above defined variable forest_result = ___ PLOTTING THE FEATURE RANKING In [0]: # Helper code to visualize the feature importance using 'MDI' plot_feature_importance ( tree , forest , X , y ); # Helper code to visualize the feature importance using 'permutation feature importance' plot_permute_importance ( tree_result , forest_result , X , y ); â¸ A common criticism for the MDI method is that it assigns a lot of importance to noisy features (more here). Did you make such an observation in the plots above? In [0]: ### edTest(test_chow1) ### # Type your answer within in the quotes given answer1 = '___' â¸ After marking, change the max_depth for your classifiers to a very low value such as 3 3, and see if you see a change in the relative importance of predictors. In [0]: ### edTest(test_chow2) ### # Type your answer within in the quotes given answer2 = '___'",
        "tags": "lectures",
        "url": "lectures/lecture18/notebook-3/"
    }, {
        "title": "Lecture 17: Bagging",
        "text": "Slides Lecture 17 : Decision Trees - B (PDF) Lecture 17 : Decision Trees - C (PDF) Lecture 17 : Decision Trees - Bagging (PDF) Exercises Lecture 17: Exercise - Classification using Decision Tree [Notebook] Lecture 17: Exercise - Bagging Classification with Decision Boundary [Notebook] Lecture 17: Exercise - Regression with Bagging [Notebook]",
        "tags": "lectures",
        "url": "lectures/lecture17/"
    }, {
        "title": "Lecture 17: Bagging",
        "text": "dt_classifier_scaffold Title : Classification using Decision Tree Description : The goal of this exercise is to get comfortable using Decision Trees for classification in sklearn. Eventually, you will produce a plot similar to the one given below: Instructions: Read the train and test datafile as Pandas data frame. Use minority and bachelor as the predictor variables and won as the response. Fit a decision tree of depth 2 and another of depth 10 on the training data. Call the function plot_boundary to visualise the decision boundary of these 2 classifiers. Increase the number of predictor variables as mentioned in scaffold. Initialize a decision tree classifier of depth 2, 10 and 15. Fit the model on the train data. Compute the train and test accuracy scores for each classifier. Use the helper code to look at the feature importance of the predictors from the decision tree of depth 15. Hints: sklearn.DecisionTreeClassifier() Generates a Logistic Regression classifier sklearn.score() Accuracy classification score. classifier.fit() Build a decision tree classifier from the training set (X, y). Note: This exercise is auto-graded and you can try multiple attempts. In [0]: # Import necessary libraries import numpy as np import pandas as pd import sklearn as sk import seaborn as sns from sklearn import tree import matplotlib.pyplot as plt from helper import plot_boundary from prettytable import PrettyTable from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import cross_val_score pd . set_option ( 'display.width' , 100 ) pd . set_option ( 'display.max_columns' , 20 ) plt . rcParams [ \"figure.figsize\" ] = ( 12 , 8 ) In [0]: # Read the data file \"election_train.csv\" as a Pandas dataframe elect_train = pd . read_csv ( \"election_train.csv\" ) # Read the data file \"election_test.csv\" as a Pandas dataframe elect_test = pd . read_csv ( \"election_test.csv\" ) # Take a quick look at the train data elect_train . head () In [0]: # Set the columns minority and bachelor as train data predictors X_train = ___ # Set the columns minority and bachelor as test data predictors X_test = ___ # Set the column \"won\" as the train response variable y_train = ___ # Set the column \"won\" as the test response variable y_test = ___ In [0]: # Initialize a Decision Tree classifier with a depth of 2 dt1 = ___ # Fit the classifier on the train data ___ # Initialize a Decision Tree classifier with a depth of 10 dt2 = ___ # Fit the classifier on the train data ___ In [0]: # Call the function plot_boundary from the helper file to get # the decision boundaries of both the classifiers plot_boundary ( elect_train , dt1 , dt2 ) In [0]: # Set of predictor columns pred_cols = [ 'minority' , 'density' , 'hispanic' , 'obesity' , 'female' , 'income' , 'bachelor' , 'inactivity' ] # Use the columns above as the predictor data from the train data X_train = elect_train [ pred_cols ] # Use the columns above as the predictor data from the test data X_test = elect_test [ pred_cols ] # Initialize a Decision Tree classifier with a depth of 2 dt1 = ___ # Initialize a Decision Tree classifier with a depth of 10 dt2 = ___ # Initialize a Decision Tree classifier with a depth of 15 dt3 = ___ # Fit all the classifier on the train data ___ ___ ___ In [0]: ### edTest(test_accuracy) ### # Compute the train and test accuracy for the first decision tree classifier of depth 2 dt1_train_acc = ___ dt1_test_acc = ___ # Compute the train and test accuracy for the second decision tree classifier of depth 10 dt2_train_acc = ___ dt2_test_acc = ___ # Compute the train and test accuracy for the third decision tree classifier of depth 15 dt3_train_acc = ___ dt3_test_acc = ___ In [0]: # Helper code to plot the scores of each classifier as a table pt = PrettyTable () pt . field_names = [ 'Max Depth' , 'Number of Features' , 'Train Accuracy' , 'Test Accuracy' ] pt . add_row ([ 2 , 2 , round ( dt1_train_acc , 4 ), round ( dt1_test_acc , 4 )]) pt . add_row ([ 10 , 2 , round ( dt2_train_acc , 4 ), round ( dt2_test_acc , 4 )]) pt . add_row ([ 15 , len ( pred_cols ), round ( dt3_train_acc , 4 ), round ( dt3_test_acc , 4 )]) print ( pt ) In [0]:",
        "tags": "lectures",
        "url": "lectures/lecture17/notebook/"
    }, {
        "title": "Lecture 17: Bagging",
        "text": "bagging_classification_scaffold Title : Bagging Classification with Decision Boundary Description : The goal of this exercise is to use Bagging (Bootstrap Aggregated) to solve a classification problem and visualize the influence on Bagging on trees with varying depths. Your final plot will resemble the one below. Instructions: Read the dataset agriland.csv . Assign the predictor and response variables as X and y . Split the data into train and test sets with test_split=0.2 and random_state=44 . Fit a single DecisionTreeClassifier() and find the accuracy of your prediction. Complete the helper function prediction_by_bagging() to find the average predictions for a given number of bootstraps. Perform Bagging using the helper function, and compute the new accuracy. Plot the accuracy as a function of the number of bootstraps. Use the helper code to plot the decision boundaries for varying max_depth along with num_bootstraps . Investigate the effect of increasing bootstraps on the variance. Hints: sklearn.tree.DecisionTreeClassifier() A decision tree classifier. DecisionTreeClassifier.fit() Build a decision tree classifier from the training set (X, y). DecisionTreeClassifier.predict() Predict class or regression value for X. train_test_split() Split arrays or matrices into random train and test subsets. np.random.choice Generates a random sample from a given 1-D array. plt.subplots() Create a figure and a set of subplots. ax.plot() Plot y versus x as lines and/or markers Note: This exercise is auto-graded and you can try multiple attempts. In [2]: # Import necessary libraries % matplotlib inline import numpy as np import pandas as pd from sklearn import metrics import scipy.optimize as opt import matplotlib.pyplot as plt from sklearn.metrics import accuracy_score from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split # Used for plotting later from matplotlib.colors import ListedColormap cmap_bold = ListedColormap ([ '#F7345E' , '#80C3BD' ]) cmap_light = ListedColormap ([ '#FFF4E5' , '#D2E3EF' ]) In [0]: # Read the file 'agriland.csv' as a Pandas dataframe df = pd . read_csv ( 'agriland.csv' ) # Take a quick look at the data # Note that the latitude & longitude values are normalized df . head () In [0]: # Set the values of latitude & longitude predictor variables X = ___ . values # Use the column \"land_type\" as the response variable y = ___ . values In [0]: # Split data in train an test, with test size = 0.2 # and set random state as 44 X_train , X_test , y_train , y_test = ___ In [0]: # Define the max_depth of the decision tree max_depth = ___ # Define a decision tree classifier with a max depth as defined above # and set the random_state as 44 clf = ___ # Fit the model on the training data ___ In [0]: # Use the trained model to predict on the test set prediction = ___ # Calculate the accuracy of the test predictions of a single tree single_acc = ___ # Print the accuracy of the tree print ( f 'Single tree Accuracy is { single_acc * 100 } %' ) In [0]: # Complete the function below to get the prediction by bagging # Inputs: X_train, y_train to train your data # X_to_evaluate: Samples that you are goin to predict (evaluate) # num_bootstraps: how many trees you want to train # Output: An array of predicted classes for X_to_evaluate def prediction_by_bagging ( X_train , y_train , X_to_evaluate , num_bootstraps ): # List to store every array of predictions predictions = [] # Generate num_bootstraps number of trees for i in range ( num_bootstraps ): # Sample data to perform first bootstrap, here, we actually bootstrap indices, # because we want the same subset for X_train and y_train resample_indexes = np . random . choice ( np . arange ( y_train . shape [ 0 ]), size = y_train . shape [ 0 ]) # Get a bootstrapped version of the data using the above indices X_boot = X_train [ ___ ] y_boot = y_train [ ___ ] # Initialize a Decision Tree on bootstrapped data # Use the same max_depth and random_state as above clf = ___ # Fit the model on bootstrapped training set clf . fit ( ___ , ___ ) # Use the trained model to predict on X_to_evaluate samples pred = clf . predict ( ___ ) # Append the predictions to the predictions list predictions . append ( pred ) # The list \"predictions\" has [prediction_array_0, prediction_array_1, ..., prediction_array_n] # To get the majority vote for each sample, we can find the average # prediction and threshold them by 0.5 average_prediction = ___ # Return the average prediction return average_prediction In [0]: ### edTest(test_bag_acc) ### # Define the number of bootstraps num_bootstraps = 200 # Calling the prediction_by_bagging function with appropriate parameters y_pred = prediction_by_bagging ( X_train , y_train , X_test , num_bootstraps = num_bootstraps ) # Compare the average predictions to the true test set values # and compute the accuracy bagging_accuracy = ___ # Print the bagging accuracy print ( f 'Accuracy with Bootstrapped Aggregation is { bagging_accuracy * 100 } %' ) In [0]: # Helper code to plot accuracy vs number of bagged trees n = np . linspace ( 1 , 250 , 250 ) . astype ( int ) acc = [] for n_i in n : acc . append ( np . mean ( prediction_by_bagging ( X_train , y_train , X_test , n_i ) == y_test )) plt . figure ( figsize = ( 10 , 8 )) plt . plot ( n , acc , alpha = 0.7 , linewidth = 3 , color = '#50AEA4' , label = 'Model Prediction' ) plt . title ( 'Accuracy vs. Number of trees in Bagging ' , fontsize = 24 ) plt . xlabel ( 'Number of trees' , fontsize = 16 ) plt . ylabel ( 'Accuracy' , fontsize = 16 ) plt . xticks ( fontsize = 12 ) plt . yticks ( fontsize = 12 ) plt . legend ( loc = 'best' , fontsize = 12 ) plt . show (); Bagging Visualization Bagging does well to reduce overfitting, but only upto a certain extent. Vary the max_depth and numboot variables to see how Bagging helps reduce overfitting with the help of the visualization below In [0]: # Making plots for three different values of `max_depth` fig , axes = plt . subplots ( 1 , 3 , figsize = ( 20 , 6 )) # Make a list of three max_depths to investigate max_depth = [ 2 , 5 , 100 ] # Fix the number of bootstraps numboot = 100 for index , ax in enumerate ( axes ): for i in range ( numboot ): df_new = df . sample ( frac = 1 , replace = True ) y = df_new . land_type . values X = df_new [[ 'latitude' , 'longitude' ]] . values dtree = DecisionTreeClassifier ( max_depth = max_depth [ index ]) dtree . fit ( X , y ) ax . scatter ( X [:, 0 ], X [:, 1 ], c = y - 1 , s = 50 , alpha = 0.5 , edgecolor = \"k\" , cmap = cmap_bold ) plot_step_x1 = 0.1 plot_step_x2 = 0.1 x1min , x1max = X [:, 0 ] . min (), X [:, 0 ] . max () x2min , x2max = X [:, 1 ] . min (), X [:, 1 ] . max () x1 , x2 = np . meshgrid ( np . arange ( x1min , x1max , plot_step_x1 ), np . arange ( x2min , x2max , plot_step_x2 ) ) # Re-cast every coordinate in the meshgrid as a 2D point Xplot = np . c_ [ x1 . ravel (), x2 . ravel ()] # Predict the class y = dtree . predict ( Xplot ) y = y . reshape ( x1 . shape ) cs = ax . contourf ( x1 , x2 , y , alpha = 0.02 ) ax . set_xlabel ( 'Latitude' , fontsize = 14 ) ax . set_ylabel ( 'Longitude' , fontsize = 14 ) ax . set_title ( f 'Max depth = { max_depth [ index ] } ' , fontsize = 20 ) Mindchow ðŸ² Play around with the following parameters: max_depth numboot Based on your observations, answer the questions below: How does the plot change with varying max_depth How does the plot change with varying numboot How are the three plots essentially different? Does more bootstraps reduce overfitting for High depth Low depth In [0]:",
        "tags": "lectures",
        "url": "lectures/lecture17/notebook-2/"
    }, {
        "title": "Lecture 17: Bagging",
        "text": "overfitting_bagging Title : Regression with Bagging Description : The aim of this exercise is to understand regression using Bagging. Instructions: Read the dataset airquality.csv as a Pandas dataframe. Take a quick look at the dataset. Split the data into train and test sets. Specify the number of bootstraps as 30 and a maximum depth of 3. Define a Bagging Regression model that uses Decision Tree as its base estimator. Fit the model on the train data. Use the helper code to predict using the mean model and individual estimators. The plot will look similar to the one given above. Predict on the test data using the first estimator and the mean model. Compute and display the test MSEs. Hints: sklearn.train_test_split() Split arrays or matrices into random train and test subsets. BaggingRegressor() Returns a Bagging regressor instance. DecisionTreeRegressor() A decision tree regressor. DecisionTreeRegressor.fit() Build a decision tree regressor from the training set (X, y). DecisionTreeRegressor.predict() Build a decision tree regressor from the training set (X, y). DecisionTreeRegressor().estimators_ A list of estimators. Use this to access any of the estimators. sklearn.mean_squared_error() Mean squared error regression loss. In [1]: # Import necessary libraries import itertools import numpy as np import pandas as pd from numpy import std from numpy import mean import matplotlib.pyplot as plt from sklearn.datasets import make_regression from sklearn.ensemble import BaggingRegressor from sklearn.tree import DecisionTreeRegressor from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split % matplotlib inline In [3]: # Read the dataset df = pd . read_csv ( \"airquality.csv\" , index_col = 0 ) # Take a quick look at the data df . head ( 10 ) In [12]: # Use the column Ozone to drop any NaNs from the dataframe df = df [ df . Ozone . notna ()] In [17]: # Assign the values of Ozon column as the predictor variable x = df [[ 'Ozone' ]] . values # Use temperature as the response data y = df [ 'Temp' ] In [20]: # Split the data into train and test sets with train size as 0.8 # and set random_state as 102 x_train , x_test , y_train , y_test = train_test_split ( x , y , train_size = 0.8 , random_state = 102 ) Bagging Regressor In [0]: # Specify the number of bootstraps as 30 num_bootstraps = 30 # Specify the maximum depth of the decision tree as 3 max_depth = 3 # Define the Bagging Regressor Model # Use Decision Tree as your base estimator with depth as mentioned in max_depth # Initialise number of estimators using the num_bootstraps value model = ___ # Fit the model on the train data ___ In [24]: # Helper code to plot the predictions of individual estimators plt . figure ( figsize = ( 10 , 8 )) xrange = np . linspace ( x . min (), x . max (), 80 ) . reshape ( - 1 , 1 ) plt . plot ( x_train , y_train , 'o' , color = '#EFAEA4' , markersize = 6 , label = \"Train Data\" ) plt . plot ( x_test , y_test , 'o' , color = '#F6345E' , markersize = 6 , label = \"Test Data\" ) plt . xlim () for i in model . estimators_ : y_pred1 = i . predict ( xrange ) plt . plot ( xrange , y_pred1 , alpha = 0.5 , linewidth = 0.5 , color = '#ABCCE3' ) plt . plot ( xrange , y_pred1 , alpha = 0.6 , linewidth = 1 , color = '#ABCCE3' , label = \"Prediction of Individual Estimators\" ) y_pred = model . predict ( xrange ) plt . plot ( xrange , y_pred , alpha = 0.7 , linewidth = 3 , color = '#50AEA4' , label = 'Model Prediction' ) plt . xlabel ( \"Ozone\" , fontsize = 16 ) plt . ylabel ( \"Temperature\" , fontsize = 16 ) plt . xticks ( fontsize = 12 ) plt . yticks ( fontsize = 12 ) plt . legend ( loc = 'best' , fontsize = 12 ) plt . show (); In [25]: # Compute the test MSE of the prediction of every individual estimator y_pred1 = ___ # Print the test MSE print ( \"The test MSE of one estimator in the model is\" , round ( mean_squared_error ( y_test , y_pred1 ), 2 )) In [0]: ### edTest(test_mse) ### # Compute the test MSE of the model prediction y_pred = ___ # Print the test MSE print ( \"The test MSE of the model is\" , round ( mean_squared_error ( y_test , y_pred ), 2 ))",
        "tags": "lectures",
        "url": "lectures/lecture17/notebook-3/"
    }, {
        "title": "Lecture 16: Decision Trees",
        "text": "Slides Lecture 16 : Decision Trees - A (PDF) Lecture 16 : Decision Trees - B (PDF) Exercises Lecture 16: Exercise - Decision Boundaries [Notebook]",
        "tags": "lectures",
        "url": "lectures/lecture16/"
    }, {
        "title": "Lecture 16: Decision Trees",
        "text": "visualize_dt_scaffold Title : Exercise: Visualizing a Decision Tree Description : The aim of this exercise is to visualize the decision tree that is created when performing Decision Tree Classification or Regression. The tree will look similar to the one given below. Data Description: We are trying to predict the winner of the 2016 Presidential election (Trump vs. Clinton) in each county in the US. To do this, we will consider several predictors including minority: the percentage of residents that are minorities and bachelor: the percentage of resident adults with a bachelor's degree (or higher). Instructions: Read the datafile county_election_train.csv into a Pandas data frame. Create the response variable based on the columns trump and clinton . Initialize a Decision Tree classifier of depth 3 and fit on the training data. Visualise the Decision Tree. Hints: sklearn.DecisionTreeClassifier() Generates a Logistic Regression classifier. classifier.fit() Build a decision tree classifier from the training set (X, y). plt.scatter() A scatter plot of y vs. x with varying marker size and/or color. plt.xlabel() Set the label for the x-axis. plt.ylabel() Set the label for the y-axis. plt.legend() Place a legend on the Axes. tree.plot_tree() Plot a decision tree. Note: This exercise is auto-graded and you can try multiple attempts. In [0]: # Import necessary libraries import numpy as np import pandas as pd import sklearn as sk import seaborn as sns from sklearn import tree import matplotlib.pyplot as plt from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import cross_val_score pd . set_option ( 'display.width' , 100 ) pd . set_option ( 'display.max_columns' , 20 ) plt . rcParams [ \"figure.figsize\" ] = ( 12 , 8 ) In [0]: # Read the datafile \"county_election_train.csv\" as a Pandas dataframe elect_train = pd . read_csv ( \"data/county_election_train.csv\" ) # Read the datafile \"county_election_test.csv\" as a Pandas dataframe elect_test = pd . read_csv ( \"data/county_election_test.csv\" ) # Take a quick look at the dataframe elect_train . head () In [0]: ### edTest(test_response) ### # Creating the response variable # Set all the rows in the train data where \"trump\" value is more than \"clinton\" as 1 y_train = ___ # Set all the rows in the test data where \"trump\" value is more than \"clinton\" as 1 y_test = ___ In [0]: # Plot \"minority\" vs \"bachelor\" as a scatter plot # Set colours blue for Trump and green for Clinton # Your code here In [0]: # Initialize a Decision Tree classifier of depth 3 and choose # splitting criteria to be the gini dtree = ___ # Fit the classifier on the train data # but only use the minority column as the predictor variable ___ In [0]: # Code to set the size of the plot plt . figure ( figsize = ( 30 , 20 )) # Plot the Decision Tree trained above with parameters filled as True tree . plot_tree ( ___ ) plt . show ();",
        "tags": "lectures",
        "url": "lectures/lecture16/notebook-2/"
    }, {
        "title": "Lecture 15: Logistic Regression II",
        "text": "Slides Lecture 15 : Handling High Dimensionality and PCA (PDF) Exercises Lecture 15: Exercise - Decision Boundaries [Notebook] Lecture 15: Exercise - ROC & AUC [Notebook]",
        "tags": "lectures",
        "url": "lectures/lecture15/"
    }, {
        "title": "Lecture 15: Logistic Regression II",
        "text": "Exercise_2_skeleton Title : Exercise: Decision Boundaries Description : In this exercise we will be comparing the classification boundaries we receive from regularized and unregularized logistic regression models. Don't forget the LogisticRegression documentation. In [0]: import pandas as pd import numpy as np import sklearn as sk import matplotlib.pyplot as plt % matplotlib inline from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import PolynomialFeatures import statsmodels.api as sm In [0]: heart = pd . read_csv ( 'Heart.csv' ) # Force the response into a binary indicator: heart [ 'AHD' ] = 1 * ( heart [ 'AHD' ] == \"Yes\" ) heart . describe () In [0]: # split train and test data heart_train , heart_test = train_test_split ( heart , test_size = 0.3 , random_state = 109 ) Fit an unregularized logistic regression model ( logit1 ) to predict AHD from Age and MaxHR in the training set (with penalty='none' and max_iter = 5000 ). Print out the coefficient estimates, and interpret general trends. In [0]: ### edTest(test_logit1) ### degree = 1 predictors = [ 'Age' , 'MaxHR' ] X_train1 = PolynomialFeatures ( degree = degree , include_bias = False ) . fit_transform ( heart_train [ predictors ]) y_train = heart_train [ 'AHD' ] logit1 = ___ print ( \"Logistic Regression Estimated Betas:\" , logit1 . intercept_ , logit1 . coef_ ) Fit an unregularized 4th order polynomial (with interactions) logistic regression model ( logit4 ) to predict AHD from Age and MaxHR in the training set (with penalty='none' and max_iter = 5000 ). Print out the coefficient estimates. In [0]: degree = ___ predictors = [ 'Age' , 'MaxHR' ] X_train4 = PolynomialFeatures ( degree = degree , include_bias = False ) . fit_transform ( heart_train [ predictors ]) logit4 = ___ print ( \"Logistic Regression Estimated Betas:\" , logit4 . intercept_ , logit4 . coef_ ) Evaluate the models based on misclassification rate in both the test set. In [0]: ### edTest(test_misclass) ### ###### # your code here ###### predictors = [ 'Age' , 'MaxHR' ] X_test1 = PolynomialFeatures ( degree = 1 , include_bias = False ) . fit_transform ( heart_test [ predictors ]) X_test4 = PolynomialFeatures ( degree = 4 , include_bias = False ) . fit_transform ( heart_test [ predictors ]) y_test = heart_test [ 'AHD' ] misclass_logit1 = ___ misclass_logit4 = ___ print ( \"Overall misclassification rate in test for logit1:\" , misclass_logit1 ) print ( \"Overall misclassification rate in test for logit4:\" , misclass_logit4 ) The code below performs the classification predictions for the model at all values in the range of the two predictors for logit1 . Then the predictions and the train dataset are added to a scatterplot in the second code chunk: In [0]: n = 100 x1 = np . linspace ( np . min ( heart [[ 'Age' ]]), np . max ( heart [[ 'Age' ]]), n ) x2 = np . linspace ( np . min ( heart [[ 'MaxHR' ]]), np . max ( heart [[ 'MaxHR' ]]), n ) x1v , x2v = np . meshgrid ( x1 , x2 ) # To do the predictions and keep the yhats on 2-D (to match the dummy predictor shapes), use this X = np . c_ [ x1v . ravel (), x2v . ravel ()] X_dummy = PolynomialFeatures ( degree = 1 , include_bias = False ) . fit_transform ( X ) yhat1 = logit1 . predict ( X_dummy ) In [0]: plt . pcolormesh ( x1v , x2v , yhat1 . reshape ( x1v . shape ), alpha = 0.05 ) plt . scatter ( heart_train [ 'Age' ], heart_train [ 'MaxHR' ], c = heart_train [ 'AHD' ]) plt . ylabel ( \"MaxHR\" ) plt . xlabel ( \"Age\" ) plt . title ( \"Yellow = Predicted to have AHD, Purple = Predicted to not have AHD\" ) plt . colorbar () plt . show () In [0]: X_dummy = PolynomialFeatures ( degree = 4 , include_bias = False ) . fit_transform ( X ) yhat4 = logit4 . predict ( X_dummy ) plt . pcolormesh ( x1v , x2v , yhat4 . reshape ( x1v . shape ), alpha = 0.05 ) plt . scatter ( heart_train [ 'Age' ], heart_train [ 'MaxHR' ], c = heart_train [ 'AHD' ]) plt . ylabel ( \"MaxHR\" ) plt . xlabel ( \"Age\" ) plt . title ( \"Yellow = Predicted to have AHD, Purple = Predicted to not have AHD\" ) plt . colorbar () plt . show () Compare the two models above on how they create the classification boundary. Which is more likely to be overfit? How would regularization affect these boundaries? your answer here Fit a ridge-like Logistic Regression model with C=0.0001 and max_iter=5000 on the 4th order polynomial as before. Compare this regularized model with the unregularized one by using the classification boundary. In [0]: ### edTest(test_ridge) ### logit_ridge = LogisticRegression ( ___ ) . fit ( X_train4 , y_train ) In [0]: yhat_ridge = logit_ridge . predict ( X_dummy ) plt . pcolormesh ( x1v , x2v , yhat_ridge . reshape ( x1v . shape ), alpha = 0.05 ) plt . scatter ( heart_train [ 'Age' ], heart_train [ 'MaxHR' ], c = heart_train [ 'AHD' ]) plt . ylabel ( \"MaxHR\" ) plt . xlabel ( \"Age\" ) plt . title ( \"Yellow = Predicted to have AHD, Purple = Predicted to not have AHD\" ) plt . colorbar () plt . show () your answer here Perfect Separation We modify the data to demonstrate perfect separation. In [0]: predictors = [ 'Age' , 'MaxHR' ] X_train_new = heart_train [ predictors ] . copy () X_train_new [ 'Age' ] = X_train_new [ 'Age' ] + 100 * y_train . values plt . plot ( X_train_new [ 'Age' ], y_train , 'o' , markersize = 7 , color = \"#011DAD\" , label = \"Data\" ) plt . xlabel ( \"Age\" ) plt . ylabel ( \"AHD\" ) plt . yticks (( 0 , 1 ), labels = ( 'No' , 'Yes' )) plt . legend () plt . show () In [0]: # Try to train a logistic regression model X_train_new = sm . add_constant ( X_train_new ) try : logreg = sm . Logit ( y_train , X_train_new ) . fit () except Exception as e : print ( e )",
        "tags": "lectures",
        "url": "lectures/lecture15/notebook/"
    }, {
        "title": "Lecture 15: Logistic Regression II",
        "text": "Exercise_3_skeleton Title : Exercise: ROC & AUC Description : In this exercise, we will look at ways to evaluate our classifiers across all thresholds, rather than just one. Investigate your options in sklearn.metrics for filling in the blanks. In [0]: import pandas as pd import numpy as np import sklearn as sk import matplotlib.pyplot as plt % matplotlib inline from sklearn.linear_model import LogisticRegression from sklearn import metrics from sklearn.preprocessing import PolynomialFeatures from sklearn.model_selection import train_test_split In [0]: # read the data heart = pd . read_csv ( 'Heart.csv' ) # Force the response into a binary indicator: heart [ 'AHD' ] = 1 * ( heart [ 'AHD' ] == \"Yes\" ) heart_train , heart_test = train_test_split ( heart , test_size = 0.3 , random_state = 109 ) In [0]: # Train two logistic regression models, one with l2 penalty and the other one with no penalty degree = 3 predictors = [ 'Age' , 'Sex' , 'MaxHR' , 'RestBP' , 'Chol' ] X_train = PolynomialFeatures ( degree = degree , include_bias = False ) . fit_transform ( heart_train [ predictors ]) y_train = heart_train [ 'AHD' ] X_test = PolynomialFeatures ( degree = degree , include_bias = False ) . fit_transform ( heart_test [ predictors ]) y_test = heart_test [ 'AHD' ] logit = LogisticRegression ( penalty = 'none' , max_iter = 10000 ) . fit ( X_train , y_train ) logit_ridge = LogisticRegression ( C = 0.001 , penalty = 'l2' , solver = 'lbfgs' , max_iter = 10000 ) . fit ( X_train , y_train ) In [0]: # Predict the probabilities, and then predict the labels based on threshold = 0.5 yhat_logit = logit . predict_proba ( X_test )[:, 1 ] yhat_logit_ridge = logit_ridge . predict_proba ( X_test )[:, 1 ] threshold = 0.5 # Print the confusion matrices print ( 'The confusion matrix in test for logit when cut-off is' , threshold , ': \\n ' , sk . metrics . ___ ( y_test , yhat_logit > threshold )) print ( 'The confusion matrix in test for logit_ridge when cut-off is' , threshold , ': \\n ' , sk . metrics . ___ ( y_test , yhat_logit_ridge > threshold )) In [0]: ### edTest(test_roc) ### # Plot the ROC curve yhat_logit = logit . predict_proba ( X_test )[:, 1 ] yhat_logit_ridge = logit_ridge . predict_proba ( X_test )[:, 1 ] fpr , tpr , thresholds = metrics . ___ ( y_test , yhat_logit ) fpr_ridge , tpr_ridge , thresholds_ridge = metrics . ___ ( y_test , yhat_logit_ridge ) x = np . arange ( 0 , 100 ) / 100 plt . plot ( x , x , '--' , color = \"gray\" , alpha = 0.3 ) plt . plot ( fpr , tpr , label = \"logit\" ) plt . plot ( fpr_ridge , tpr_ridge , label = \"logit_ridge\" ) plt . ylabel ( \"True Positive Rate\" ) plt . xlabel ( \"False Positive Rate\" ) plt . title ( \"ROC Curve for Predicting AHD in a Logistic Regression Model\" ) plt . legend () plt . show () In [0]: ### edTest(test_auc) ### # print the AUC scores auc_no_reg = metrics . ___ ( fpr , tpr ) auc_ridge = metrics . ___ ( fpr_ridge , tpr_ridge ) print ( auc_no_reg ) print ( auc_ridge )",
        "tags": "lectures",
        "url": "lectures/lecture15/notebook-2/"
    }, {
        "title": "Lecture 14: Logistic Regression I",
        "text": "Slides Lecture 14 : Handling High Dimensionality and PCA (PDF) Exercises Lecture 14: Exercise - Logistic Regression [Notebook]",
        "tags": "lectures",
        "url": "lectures/lecture14/"
    }, {
        "title": "Lecture 14: Logistic Regression I",
        "text": "Exercise_1_skeleton Title : Exercise - Logistic Regression Description : Fit logistic regression models using: SKLearn LogisticRegression (sklearn.linear_model.LogisticRegression) Statsmodels Logit (statsmodels.api.Logit) In [0]: # import libraries import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression , LinearRegression import statsmodels.api as sm In [0]: heart = pd . read_csv ( 'Heart.csv' ) # Force the response into a binary indicator: heart [ 'AHD' ] = 1 * ( heart [ 'AHD' ] == \"Yes\" ) heart . describe () In [0]: # Make a plot of the response (AHD) vs the predictor (Age) plt . plot ( heart [[ 'Age' ]] . values , heart [ 'AHD' ] . values , 'o' , markersize = 7 , color = \"#011DAD\" , label = \"Data\" ) plt . xticks ( np . arange ( 18 , 80 , 4.0 )) plt . xlabel ( \"Age\" ) plt . ylabel ( \"AHD\" ) plt . yticks (( 0 , 1 ), labels = ( 'No' , 'Yes' )) plt . legend () plt . show () In [0]: # split into train and validation heart_train , heart_val = train_test_split ( heart , train_size = 0.75 , random_state = 5 ) # select variables for model estimation x_train = heart_train [[ 'Age' ]] y_train = heart_train [ 'AHD' ] x_val = heart_val [[ 'Age' ]] y_val = heart_val [ 'AHD' ] Simple linear regression model fitting Define and fit a linear regression model to predict Age from MaxHR . In [0]: # Create a linear regression model, with random state=5 regress1 = LinearRegression ( fit_intercept = True ) . fit ( x_train , y_train ) print ( \"Linear Regression Estimated Betas:\" , regress1 . intercept_ , regress1 . coef_ [ 0 ]) In [0]: # Plot the estimated probability for training data dummy_x = np . linspace ( np . min ( x_train ) - 30 , np . max ( x_train ) + 30 ) yhat_regress = regress1 . predict ( dummy_x . reshape ( - 1 , 1 )) plt . plot ( x_train , y_train , 'o' , alpha = 0.2 , label = 'Data' ) plt . plot ( dummy_x , yhat_regress , label = \"OLS\" ) plt . ylim ( - 0.2 , 1.2 ) plt . show () What could go wrong with this linear regression model? your answer here Simple logisitc regression model fitting Define and fit a logistic regression model with random state=5 to predict Age from MaxHR . In [0]: ### edTest(test_logit1) ### # Create a logistic regression model, with random state=5 and no penalty logit1 = ___ ( penalty = ___ , max_iter = 1000 , random_state = 5 ) #Fit the model using the training set logit1 . fit ( x_train , y_train ) # Get the coefficient estimates print ( \"Logistic Regression Estimated Betas (B0,B1):\" , logit1 . intercept_ , logit1 . coef_ ) Interpret the Coefficient Estimates Calculate the estimated probability that a person with age 60 will have AHD in the ICU. your answer here In [0]: # Confirm the probability calculation above using logit1.predict() # Be careful as to how you define the new observation. Hint: double brackets is one way to do it logit1 . predict_proba ([[ ___ ]]) Accuracy computation In [0]: ### edTest(test_accuracy) ### # Compute the training & validation accuracy train_accuracy = logit1 . ___ ( x_train , y_train ) val_accuracy = logit1 . ___ ( x_val , y_val ) # Print the two accuracies below print ( \"Train Accuracy\" , train_accuracy ) print ( \"Validation Accuracy\" , val_accuracy ) Plot the predictions In [0]: x = np . linspace ( np . min ( heart [[ 'Age' ]]) - 10 , np . max ( heart [[ 'Age' ]]) + 10 , 200 ) yhat_class_logit = logit1 . predict ( x ) yhat_prob_logit = logit1 . predict_proba ( x )[:, 1 ] # plot the observed data plt . plot ( x_train , y_train , 'o' , alpha = 0.1 , label = 'Train Data' ) plt . plot ( x_val , 0.94 * y_val + 0.03 , 'o' , alpha = 0.1 , label = 'Validation Data' ) # plot the predictions plt . plot ( x , yhat_class_logit , label = 'logit1 Classifications' ) plt . plot ( x , yhat_prob_logit , label = 'logit1 Probabilities' ) # put the lower-left part of the legend 5% to the right along the x-axis, and 45% up along the y-axis plt . legend ( loc = ( 0.05 , 0.45 )) # Don't forget your axis labels! plt . xlabel ( \"Age\" ) plt . ylabel ( \"Heart disease (AHD)\" ) plt . show () Statistical Inference Train a new logistic regression model using statsmodels package. Print model summary and interpret the results. In [0]: ### edTest(test_logit2) ### # adding a column of ones to X x_train_with_constant = sm . add_constant ( x_train ) x_val_with_constant = sm . add_constant ( x_val ) # train a new model using statsmodels package logreg = sm . ___ ( y_train , x_train_with_constant ) . fit () print ( logreg . summary ()) What is an estimated 95% confidence interval for the coefficient corresponding to 'Age' variable? your answer here",
        "tags": "lectures",
        "url": "lectures/lecture14/notebook/"
    }, {
        "title": "Lecture 13: EthiCS",
        "text": "Slides Lecture 13 : EthiCS - UNFAIRNESS IN ML ALGORITHMS (PDF)",
        "tags": "lectures",
        "url": "lectures/lecture13/"
    }, {
        "title": "Lecture 12: Visualization",
        "text": "Slides Lecture 12 : Visualization (PDF) Exercises Lecture 12: Exercise: Visualization Improvisation [Notebook]",
        "tags": "lectures",
        "url": "lectures/lecture12/"
    }, {
        "title": "Lecture 12: Visualization",
        "text": "L14_Ex1_solutions Title : Exercise: Visualization Improvisation Description : For this exercise we would like you to get creative and experiment! You have the freedom to plot anything that you'd like from this data . You're expected to produce two plots , both of which should adhere to the principles learned in lecture (e.g., make it clear to understand/digest, effective, simple, not misleading, etc). Please feel inspired to challenge yourself by making a type of plot you've never made before -- perhaps never even seen before! Give a brief explanation of the reason and usefulness of the plot. Your data is the Boston housing prices dataset . We will load it directly from sklearn Resource : for tons of great coding examples, visit the matplotlib website. CS109A Introduction to Data Science Lecture 14, Exercise: Visualization Harvard University Fall 2020 Instructors : Pavlos Protopapas, Kevin Rader, and Chris Tanner In [6]: import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.datasets import load_boston In [7]: # load the boston housing dataset boston = load_boston () boston_pd = pd . DataFrame ( boston . data ) boston_pd . columns = boston . feature_names boston_pd . describe () Out[7]: CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT count 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 mean 3.613524 11.363636 11.136779 0.069170 0.554695 6.284634 68.574901 3.795043 9.549407 408.237154 18.455534 356.674032 12.653063 std 8.601545 23.322453 6.860353 0.253994 0.115878 0.702617 28.148861 2.105710 8.707259 168.537116 2.164946 91.294864 7.141062 min 0.006320 0.000000 0.460000 0.000000 0.385000 3.561000 2.900000 1.129600 1.000000 187.000000 12.600000 0.320000 1.730000 25% 0.082045 0.000000 5.190000 0.000000 0.449000 5.885500 45.025000 2.100175 4.000000 279.000000 17.400000 375.377500 6.950000 50% 0.256510 0.000000 9.690000 0.000000 0.538000 6.208500 77.500000 3.207450 5.000000 330.000000 19.050000 391.440000 11.360000 75% 3.677083 12.500000 18.100000 0.000000 0.624000 6.623500 94.075000 5.188425 24.000000 666.000000 20.200000 396.225000 16.955000 max 88.976200 100.000000 27.740000 1.000000 0.871000 8.780000 100.000000 12.126500 24.000000 711.000000 22.000000 396.900000 37.970000 In [8]: # our canonical example plt . figure ( figsize = ( 5 , 4 )) plt . hist ( boston . target ) plt . title ( 'Boston Housing Prices' ) plt . xlabel ( 'Price ($1000)' ) plt . ylabel ( '# of Houses' ) plt . show () In [4]: # YOUR FIRST PLOT In [5]: # YOUR SECOND PLOT",
        "tags": "lectures",
        "url": "lectures/lecture12/notebook/"
    }, {
        "title": "Lab 06: Principal Components Analysis (PCA)",
        "text": "Jupyter Notebooks Lab 6: Principal Components Analysis (PCA)",
        "tags": "labs",
        "url": "labs/lab06/"
    }, {
        "title": "Lecture 10: Principal Component Analysis",
        "text": "Slides Lecture 10 : Handling High Dimensionality and PCA (PDF) Exercises Lecture 10: Lecture Notebook - Principal Component Analysis [Notebook] Lecture 10: Exercise: PCA 1 [Notebook] Lecture 10: Exercise: PCA 2 [Notebook]",
        "tags": "lectures",
        "url": "lectures/lecture10/"
    }, {
        "title": "Lecture 10: Principal Component Analysis",
        "text": "PCA Title : Lecture Notebook Description : Data Description: Instructions: Hints: Statsmodels statsmodels is a Python module that provides classes and functions for the estimation of many different statistical models, as well as for conducting statistical tests, and statistical data exploration. Basic code structure is shown below: import statsmodels.api as sm X is our dataset Add intercept (bias constant): X = sm.add_constant(X) Fit regression model: results = sm.OLS(y, X).fit() Inspect the results: print(results.summary()) CS109A Introduction to Data Science Principal Component Analysis Harvard University Fall 2021 Instructors : Pavlos Protopapas, Natesh Pillai In [6]: % matplotlib inline import matplotlib.pyplot as plt import numpy as np import pandas as pd import scipy import seaborn as sns import plotly import plotly.graph_objs as go import sklearn as sk from sklearn.decomposition import PCA import statsmodels.api as sm from statsmodels.stats.outliers_influence import variance_inflation_factor pd . set_option ( \"display.width\" , 500 ) pd . set_option ( \"display.max_columns\" , 100 ) sns . set_style ( \"darkgrid\" ) sns . set_palette ( \"colorblind\" ) PCA Part 0: Reading the data In this notebook, we will be using a Heart dataset. The variables we will be using today include: AHD : whether or not the patient presents atherosclerotic heart disease (a heart attack): Yes or No Sex : a binary indicator for whether the patient is male (Sex=1) or female (Sex=0) Age : age of patient, in years MaxHR : the maximum heart rate of patient based on exercise testing RestBP : the resting systolic blood pressure of the patient Chol : the HDL cholesterol level of the patient Oldpeak : ST depression induced by exercise relative to rest (on an ECG) Slope : the slope of the peak exercise ST segment (1 = upsloping; 2 = flat; 3 = downsloping) Ca : number of major vessels (0-3) colored by flourosopy For further information on the dataset, please see the UC Irvine Machine Learning Repository . In [7]: df_heart = pd . read_csv ( \"Heart.csv\" ) # Force the response into a binary indicator: df_heart [ \"AHD\" ] = ( df_heart [ \"AHD\" ] == \"Yes\" ) . astype ( \"int\" ) print ( df_heart . shape ) df_heart . head () df_heart . drop ( columns = [ 'Unnamed: 0' ], inplace = True ) In [8]: df_heart Here are some basic summaries and EDA from last time: In [9]: df_heart . describe () In [10]: pd . crosstab ( df_heart [ \"Sex\" ], df_heart [ \"AHD\" ]) In [11]: pd . crosstab ( df_heart [ \"Thal\" ], df_heart [ \"AHD\" ]) In [12]: pd . crosstab ( df_heart [ \"ChestPain\" ], df_heart [ \"AHD\" ]) In [13]: _ = sns . histplot ( data = df_heart , x = \"Age\" , hue = \"AHD\" ) In [14]: _ = sns . histplot ( data = df_heart , x = \"MaxHR\" , hue = \"AHD\" ) Part 1: Principal Components Analysis (PCA) Q1.1 Just a sidebar (and a curiosity), what happens when two of the identical predictor is used in linear regression? Is an error created? Should one be? Investigate by predicting AHD from two copies of Age , and compare to the simple linear regression model with Age alone. In [15]: X = sm . add_constant ( df_heart [[ \"Age\" ]]) y = df_heart [ \"AHD\" ] reg1 = sm . OLS ( y , X ) . fit () reg1 . summary () Solution: The single coefficient for Age is distributed equally across the two predictors. This is a very reasonable approach as predictions will still be stable. In [16]: # investigating what happens when two identical predictors are used ###### # your code here ###### X = sm . add_constant ( df_heart [[ \"Age\" , \"Age\" ]]) reg2 = sm . OLS ( y , X ) . fit () print ( reg2 . summary ()) We will apply PCA to the heart dataset when there are just 7 predictors considered (remember: PCA is used when dimensionality is high (lots of predictors), but this will help us get our heads around what is going on): In [17]: columns = [ \"Age\" , \"RestBP\" , \"Chol\" , \"MaxHR\" , \"Sex\" , \"Oldpeak\" , \"Slope\" ] X = df_heart [ columns ] y = df_heart [ \"AHD\" ] X . describe () In [18]: X . corr () First let's fit the full linear regression model to predict AHD from the 7 predictors above. Remember: PCA is an approach to handling the predictors, so it does not matter if we are using it for a regression or classification type problem. In [19]: reg_full = sm . OLS ( y , sm . add_constant ( X )) . fit () reg_full . summary () Q1.2 Is there any evidence of multicollinearity in the set of predictors? How do you know? How will PCA handle these correlations? Solution: In [20]: # VIF dataframe vif_data = pd . DataFrame () vif_data [ \"feature\" ] = X . columns # calculating VIF for each feature vif_data [ \"VIF\" ] = [ variance_inflation_factor ( X . values , i ) for i in range ( len ( X . columns ))] vif_data Because we have high VIFs, this indicates that we have multicollinearity. Part 2: PCA in Regression (PCR) Next we apply the PCA transformation in a few steps, and show some of the results below: In [21]: # create/fit the 'full' pca transformation pca = PCA () . fit ( X ) # apply the pca transformation to the full predictor set pcaX = pca . transform ( X ) # convert to a data frame pcr_columns = [ \"PCA1\" , \"PCA2\" , \"PCA3\" , \"PCA4\" , \"PCA5\" , \"PCA6\" , \"PCA7\" ] pcaX_df = pd . DataFrame ( pcaX , columns = pcr_columns ) # here are the weighting (eigen-vectors) of the variables (first 2 at least) print ( \"First PCA Component (w1):\" , pca . components_ [ 0 ,:]) print ( \"Second PCA Component (w2):\" , pca . components_ [ 1 ,:]) In [22]: pcaX_df In [23]: # here is the variance explained: print ( \"Variance explained by each component:\" , pca . explained_variance_ratio_ ) blue = sns . color_palette ( \"colorblind\" )[ 0 ] sns . barplot ( y = list ( range ( 1 , 8 )), x = pca . explained_variance_ratio_ , orient = \"h\" , color = blue ) plt . xscale ( \"log\" ) In [24]: _ = sns . barplot ( y = list ( range ( 1 , 8 )), x = pca . explained_variance_ratio_ , orient = \"h\" , color = blue ) In [25]: # create/fit the 'full' pca transformation Z = sk . preprocessing . StandardScaler () . fit ( X ) . transform ( X ) pca_standard = PCA () . fit ( Z ) pcaZ = pca_standard . transform ( Z ) # convert to a data frame pcaZ_df = pd . DataFrame ( pcaZ , columns = pcr_columns ) In [26]: print ( pca_standard . components_ . shape ) print ( pcaZ . shape ) In [27]: pd . DataFrame . from_dict ({ \"Variable\" : X . columns , \"PCA1\" : pca . components_ [ 0 ], \"PCA2\" : pca . components_ [ 1 ], \"PCA-Z1\" : pca_standard . components_ [ 0 ], \"PCA-Z2\" : pca_standard . components_ [ 1 ]}) Q2.3 Interpret the results above. What doss $w_1$ represent? Why do the values make sense? What does it's values squared sum up to? Why does this make sense? Solution: $w_1$ represents the transformation (change in basis) to convert the columns of $\\mathbf{X}$ to the first PCA vector, $z_1$. They elements after quaring sum up to 1, so the magnitude represents euclidean weighting in the transformation (the larger value means more weight in the transformation). In [28]: np . sum ( pca . components_ [ 0 ,:] ** 2 ) It is common for a model with high dimensional data (lots of predictors) to be plotted along the first 2 PCA components (with the classification boundaries added). Below is the scatter plot for these data (without a classificaiton boundary, since we do not have a model yet): In [29]: # Plot the response over the first 2 PCA component vectors sns . scatterplot ( data = pcaX_df , x = \"PCA1\" , y = \"PCA2\" , hue = df_heart [ \"AHD\" ], legend = \"full\" ) plt . xlabel ( \"First PCA Component Vector (Z1)\" ) plt . ylabel ( \"Second PCA Component Vector (Z2)\" ); Q2.4 What would a classification boundary look like if a linear regression model were fit using the first 2 principal components as the predictors? Does there appear to be good potential here? Solution: It would again be linear. Here, most likely the boundary would be a line with negative slope. Below is the result of the PCR-1 (linear) to predict AHD from the first principal component vector. In [30]: X = sm . add_constant ( pcaX_df [[ \"PCA1\" ]]) reg_pcr1 = sm . OLS ( y , X ) . fit () reg_pcr1 . summary () In [31]: print ( \"First PCA Component (w1):\" , pca . components_ [ 0 : 1 ,:]) Q2.5 What does this PCR-1 model tell us about how the predictors relate to the response (aka, estimate the coefficient(s) in the original predictor space)? Is it truly a simple linear regression model in the original predictor space? In [32]: beta = reg_pcr1 . params [ 1 ] ( beta * pca . components_ [ 0 : 1 ,:]) Solution: The estimated slope from PCR1 ($\\hat{\\beta} \\approx 0.0009$) is distributed across the 7 actual predictors, so that the formula would be: $$\\hat{y} = 0.0009(Z_1) + 0.4587 = 0.0009(w&#94;T_1\\mathbf{X}) + 0.4587 \\\\ = 0.0009(0.0384X_1+0.0505X_2+0.998X_3-0.00374X_4-0.0018X_5+0.00115X_6-0.0000036X_7) + 0.4587 \\\\ = 3.31 \\cdot 10&#94;{-5} X_1 + 4.35 \\cdot 10&#94;{-5} X_2 + 8.6 \\cdot 10&#94;{-4} X_3 - 3.23 \\cdot 10&#94;{-6} X_4 - 1.56 \\cdot 10&#94;{-6} X_5 + 9.955 \\cdot 10&#94;{-7} X_6 - 3.1 \\cdot 10&#94;{-9} X_7 + 0.4587$$ This is how to interpret the estimated coefficients from a regression with PCA components as the predictors: some transformation back to the original space is required. Here is the above claculation for all 7 PCR linear regressions, and then plotted on a pretty plot: In [33]: results_arr = [] for i in range ( 1 , 8 ): reg_pcr_tmp = sm . OLS ( y , sm . add_constant ( pcaX_df [ pcr_columns [: i ]])) . fit () pcr_tmp = np . transpose ( pca . components_ [: i ,:]) @ reg_pcr_tmp . params [ 1 : i + 1 ] results_arr . append ( pcr_tmp ) betas = reg_full . params [ 1 :] results_arr . append ( betas ) results = np . vstack ( results_arr ) print ( results ) In [34]: plt . plot ( pcr_columns + [ \"Linear\" ], results ) plt . ylabel ( \"Back-calculated Beta Coefficients\" ) plt . legend ( df_heart . columns ) Q2.6 Interpret the plot above. Specifically, compare how each PCA vector \"contributes\" to the original linear regression model using all 7 original predictors. How Does PCR-7 compare to the original linear regression model (in estimated coefficients)? Solution: This plot shows that as more PCA vectors are included in the PCA-Regression, the estimated $\\beta$s from the original regression model are recovered: if PCR($p$) is used (where $p$ is the number of predictors we started with), they are mathemtaically equivalent. All of this PCA work should have been done using the standardized versions of the predictors. Below is the code that does exactly that: In [35]: X = df_heart [ columns ] scaler = sk . preprocessing . StandardScaler () Z = scaler . fit_transform ( X ) pca = PCA () . fit ( Z ) pcaZ = pca . transform ( Z ) pcaZ_df = pd . DataFrame ( pcaZ , columns = pcr_columns ) print ( \"First PCA Component (w1):\" , pca . components_ [ 0 ,:]) print ( \"Second PCA Component (w2):\" , pca . components_ [ 1 ,:]) In [36]: regZ_full = sm . OLS ( y , sm . add_constant ( pd . DataFrame ( Z , columns = columns ))) . fit () regZ_full . summary () In [37]: # Fit the PCR results_arr = [] for i in range ( 1 , 8 ): reg_pcrZ_tmp = sm . OLS ( y , sm . add_constant ( pcaZ_df [ pcr_columns [: i ]])) . fit () pcrZ_tmp = np . transpose ( pca . components_ [: i ,:]) @ reg_pcrZ_tmp . params [ 1 : i + 1 ] results_arr . append ( pcrZ_tmp ) betasZ = regZ_full . params [ 1 :] results_arr . append ( betasZ ) resultsZ = np . vstack ( results_arr ) print ( resultsZ ) In [38]: plt . plot ( pcr_columns + [ \"Linear\" ], resultsZ ) plt . ylabel ( \"Back-calculated Beta Coefficients\" ); plt . legend ( X . columns ); Q2.7 Compare this plot to the previous one; why does this plot make sense?. What does this illustrate? Solution: This plot shows that the components are now more evenly composed of the predictors, rather than the first component being dominated by the predictor with the most variability. The 7 lines move more similarly here than in the previous plot where they essentially moved one predictor for one component. Part 3: Underlying Math What is PCA doing with these eigenvectors? Why does it all work? To answer these questions, it is easiest to restrict ourselves to two dimensions so that we can easily visualize. To show what is going on, we will focus on Age and MaxHR because there is a clear negative relationship between these two due to biology. In [39]: _ = sns . scatterplot ( data = df_heart , x = \"Age\" , y = \"MaxHR\" ) Note, be careful looking at things with unequal axes, relationships can be lost. Let us set the axes to be equal proportion and the extremely linear relationship reveals itself. In [40]: sns . scatterplot ( data = df_heart , x = \"Age\" , y = \"MaxHR\" ) _ = plt . axis ( \"equal\" ) Now, let's suppose we wanted to summarize this data, how would you do this? One way to do this is to give the direction that explains the greatest variance. Why variance? The equation for sample variance is $S = \\frac{(X-\\mu)&#94;2}{n-1}$ which centers the data for us and so the direction of the greatest variance really describes the direction in which the data tends to go. In practice, we can get rid of the $(n-1)$ in the denominator because the operations that follow are scale invariant. In [41]: X = df_heart [[ \"Age\" , \"MaxHR\" ]] . values mu = np . mean ( X , axis = 0 ) S = ( X - mu ) . T @ ( X - mu ) S As the variance is a matrix, finding the direction of the greatest variance is equivalent to $\\max_{\\lVert w \\rVert = 1} {\\lVert S w \\rVert&#94;2}$ which turns out to be the largest eigenvalue. So, the direction of largest variance is simply the largest eigenvalue of $S$. In [42]: eigen_values , eigen_vectors = scipy . linalg . eig ( S ) w_1 = eigen_vectors [:, np . argmax ( eigen_values )] w_1 To find the second greatest direction of variance, we should remove the effects of the first direction. This is done by projecting $w_1$ onto $X$ and subtracting it. In [43]: X_hat = X - X @ np . outer ( w_1 , w_1 ) mu_hat = np . mean ( X_hat , axis = 0 ) S_hat = ( X_hat - mu_hat ) . T @ ( X_hat - mu_hat ) S_hat By the same reasoning as before, the largest eigenvector of this new matrix will correspond to the second direction. In [44]: eigen_values , eigen_vectors = scipy . linalg . eig ( S_hat ) w_2 = eigen_vectors [:, np . argmax ( eigen_values )] w_2 In [45]: np . sqrt ( np . max ( eigen_values )) Comparing to the results from Sklearn, we see that these precisely correspond to the components retrieved by PCA. Thus, PCA is simply an iterative procedure of finding the direction of greatest variance using the sample variance matrix by finding the largest eigenvector, removing its effects via projection, then repeating the procedure. In [46]: pca = PCA () . fit ( X ) pca . components_ To gain a geometric intuition, let us plot the points in the original space (blue), then projected to remove the largest eigenvector (orange) alongside both the eigenvectors. What we notice is that the direction of the first eigenvector is indeed responsible for most of the variance. Just by looking, the data has a spread of length 100 whereas the orange points is something closer to a spread of 50. The second thing to notice is that both eigenvectors together completely summarize the data. That is, after removing the second eigenvector, the data would collapse to the origin. Thus, using all the eigenvectors (PCA components) ultimately retrieves the information in the original data. In [47]: sns . scatterplot ( x = X [:, 0 ], y = X [:, 1 ]) sns . scatterplot ( x = X_hat [:, 0 ], y = X_hat [:, 1 ]) x = np . stack ([ X [:, 0 ], X_hat [:, 0 ]]) . T y = np . stack ([ X [:, 1 ], X_hat [:, 1 ]]) . T for i in range ( len ( X ) // 20 ): sns . lineplot ( x = x [ i ], y = y [ i ], color = \"k\" ) x = [ 0 , - 100 * w_1 [ 0 ]] y = [ 0 , - 100 * w_1 [ 1 ]] sns . lineplot ( x = x , y = y ) x = [ 0 , 100 * w_2 [ 0 ]] y = [ 0 , 100 * w_2 [ 1 ]] sns . lineplot ( x = x , y = y ) plt . xlabel ( \"Age\" ) plt . ylabel ( \"MaxHR\" ) _ = plt . axis ( \"equal\" ) Let us repeat this procedure but for data in 3-dimensions so that you can try to extend the visualization to higher dimensions. Here, we switch to plotly because it handles 3D much better. You immediately notice the linear relationship between all 3 variables. In [48]: X = df_heart [[ \"Age\" , \"MaxHR\" , \"RestBP\" ]] . values # Configure Plotly to be rendered inline in the notebook. plotly . offline . init_notebook_mode () # Configure the trace. trace = go . Scatter3d ( x = X [:, 0 ], y = X [:, 1 ], z = X [:, 2 ], mode = \"markers\" , marker = { \"size\" : 10 , \"opacity\" : 0.8 , } ) # Configure the layout. layout = go . Layout ( margin = { \"l\" : 0 , \"r\" : 0 , \"b\" : 0 , \"t\" : 0 }, scene = go . layout . Scene ( xaxis = go . layout . scene . XAxis ( title = \"Age\" ), yaxis = go . layout . scene . YAxis ( title = \"MaxHR\" ), zaxis = go . layout . scene . ZAxis ( title = \"RestBP\" ) ) ) data = [ trace ] plot_figure = go . Figure ( data = data , layout = layout ) # Render the plot. plotly . offline . iplot ( plot_figure ) Now, let's do the PCA procedure. In [49]: X = df_heart [[ \"Age\" , \"MaxHR\" , \"RestBP\" ]] . values X_orig = X . copy () # Configure Plotly to be rendered inline in the notebook. plotly . offline . init_notebook_mode () # Configure the trace. trace = go . Scatter3d ( x = X [:, 0 ], y = X [:, 1 ], z = X [:, 2 ], mode = \"markers\" , name = \"Original Data\" , marker = { \"size\" : 10 , \"opacity\" : 0.8 , } ) # Configure the layout. layout = go . Layout ( margin = { \"l\" : 0 , \"r\" : 0 , \"b\" : 0 , \"t\" : 0 }, scene = go . layout . Scene ( xaxis = go . layout . scene . XAxis ( title = \"Age\" ), yaxis = go . layout . scene . YAxis ( title = \"MaxHR\" ), zaxis = go . layout . scene . ZAxis ( title = \"RestBP\" ) ) ) data = [ trace ] plot_figure = go . Figure ( data = data , layout = layout ) ## First projection mu = np . mean ( X , axis = 0 ) S = ( X - mu ) . T @ ( X - mu ) eigen_values , eigen_vectors = scipy . linalg . eig ( S ) w_1 = eigen_vectors [:, np . argmax ( eigen_values )] X_prev = X . copy () X = X_orig - X_orig @ np . outer ( w_1 , w_1 ) # Configure the trace. trace = go . Scatter3d ( x = X [:, 0 ], y = X [:, 1 ], z = X [:, 2 ], mode = \"markers\" , name = \"First Projection\" , marker = { \"size\" : 10 , \"opacity\" : 0.8 , } ) data . append ( trace ) x_lines = [] y_lines = [] z_lines = [] #create the coordinate list for the lines for i in range ( len ( X ) // 10 ): trace = go . Scatter3d ( x = [ X_prev [ i , 0 ], X [ i , 0 ]], y = [ X_prev [ i , 1 ], X [ i , 1 ]], z = [ X_prev [ i , 2 ], X [ i , 2 ]], mode = \"lines\" , showlegend = False , line = go . scatter3d . Line ( color = \"black\" ) ) data . append ( trace ) ## Second projection mu = np . mean ( X , axis = 0 ) S = ( X - mu ) . T @ ( X - mu ) eigen_values , eigen_vectors = scipy . linalg . eig ( S ) w_2 = eigen_vectors [:, np . argmax ( eigen_values )] X_prev = X . copy () X = X_orig - X_orig @ np . outer ( w_1 , w_1 ) - X_orig @ np . outer ( w_2 , w_2 ) # Configure the trace. trace = go . Scatter3d ( x = X [:, 0 ], y = X [:, 1 ], z = X [:, 2 ], mode = \"markers\" , name = \"Second Projection\" , marker = { \"size\" : 10 , \"opacity\" : 0.8 , } ) data . append ( trace ) #create the coordinate list for the lines for i in range ( len ( X ) // 10 ): trace = go . Scatter3d ( x = [ X_prev [ i , 0 ], X [ i , 0 ]], y = [ X_prev [ i , 1 ], X [ i , 1 ]], z = [ X_prev [ i , 2 ], X [ i , 2 ]], mode = \"lines\" , showlegend = False , line = go . scatter3d . Line ( color = \"black\" ) ) data . append ( trace ) ## Third projection mu = np . mean ( X , axis = 0 ) S = ( X - mu ) . T @ ( X - mu ) eigen_values , eigen_vectors = scipy . linalg . eig ( S ) w_3 = eigen_vectors [:, np . argmax ( eigen_values )] ## Eigenvectors trace = go . Scatter3d ( x = [ 0 , 200 * w_1 [ 0 ]], y = [ 0 , 200 * w_1 [ 1 ]], z = [ 0 , 200 * w_1 [ 2 ]], mode = \"lines\" , name = \"First Eigenvector\" , line = go . scatter3d . Line ( color = \"blue\" ) ) data . append ( trace ) trace = go . Scatter3d ( x = [ 0 , 200 * w_2 [ 0 ]], y = [ 0 , 200 * w_2 [ 1 ]], z = [ 0 , 200 * w_2 [ 2 ]], mode = \"lines\" , name = \"Second Eigenvector\" , line = go . scatter3d . Line ( color = \"red\" ) ) data . append ( trace ) trace = go . Scatter3d ( x = [ 0 , 200 * w_3 [ 0 ]], y = [ 0 , 200 * w_3 [ 1 ]], z = [ 0 , 200 * w_3 [ 2 ]], mode = \"lines\" , name = \"Third Eigenvector\" , line = go . scatter3d . Line ( color = \"green\" ) ) data . append ( trace ) plot_figure = go . Figure ( data = data , layout = layout ) # Render the plot. plotly . offline . iplot ( plot_figure ) Notice again that the lines are parallel to each other. Now, the original data, a cloud of points in 3D, first gets projected to a plane (the red points), then projected to a line (the green points). Imagine first squishing a ball of Play-Doh into a pancake and then taking the pancake and squishing the outsides to form a rope. This is exactly what PCA is doing except that it does the squishing in directions that have the maximal variance. Why is this all useful? Because, instead of projecting the points as we have done, we can perform dimensionality reduction by projecting the original data on a subset of the eigenvalues. For example, we can take our 3D cloud of points and reduce the dimensions by 66% by only keeping the first eigenvector which is going to responsible for most of the variance and so keeps most of the information in those features. In [50]: X = df_heart [[ \"Age\" , \"MaxHR\" , \"RestBP\" ]] . values X_orig = X . copy () # Configure Plotly to be rendered inline in the notebook. plotly . offline . init_notebook_mode () # Configure the trace. trace = go . Scatter3d ( x = X [:, 0 ], y = X [:, 1 ], z = X [:, 2 ], mode = \"markers\" , name = \"Original Data\" , marker = { \"size\" : 10 , \"opacity\" : 0.8 , } ) # Configure the layout. layout = go . Layout ( margin = { \"l\" : 0 , \"r\" : 0 , \"b\" : 0 , \"t\" : 0 }, scene = go . layout . Scene ( xaxis = go . layout . scene . XAxis ( title = \"Age\" ), yaxis = go . layout . scene . YAxis ( title = \"MaxHR\" ), zaxis = go . layout . scene . ZAxis ( title = \"RestBP\" ) ) ) data = [ trace ] plot_figure = go . Figure ( data = data , layout = layout ) ## Remove smallest eigenvalues X = X_orig - X_orig @ np . outer ( w_2 , w_2 ) - X_orig @ np . outer ( w_3 , w_3 ) # Configure the trace. trace = go . Scatter3d ( x = X [:, 0 ], y = X [:, 1 ], z = X [:, 2 ], mode = \"markers\" , name = \"Data Along First Eigenvector\" , marker = { \"size\" : 10 , \"opacity\" : 0.8 , } ) data . append ( trace ) plot_figure = go . Figure ( data = data , layout = layout ) # Render the plot. plotly . offline . iplot ( plot_figure ) As you can see, the red line is only 1 dimensional but the spread between the two furthest points is almost equal to the original data. We can use this to construct curves of how many dimensions are needed to retain a certain amount of variance. For instance, suppose we want to decrease the dimensions of our 7 dimensional dataset so that we may visualize the data more readily. Suppose furthermore we want the data to keep 90% of the original variance. In [51]: columns = [ \"Age\" , \"RestBP\" , \"Chol\" , \"MaxHR\" , \"Sex\" , \"Oldpeak\" , \"Slope\" ] X = df_heart [ columns ] . values mu = np . mean ( X , axis = 0 ) S = ( X - mu ) . T @ ( X - mu ) / ( len ( X ) - 1 ) total_variance = np . diag ( S ) . sum () print ( f \"Total variance is: { total_variance } \" ) In [52]: n = len ( columns ) var_arr = [] eigenvector_arr = [] for i in range ( n ): eigen_values , eigen_vectors = scipy . linalg . eig ( S ) w = np . real ( eigen_vectors [:, np . argmax ( eigen_values )]) eigenvector_arr . append ( w ) X = X - X @ np . outer ( w , w ) mu = np . mean ( X , axis = 0 ) S = ( X - mu ) . T @ ( X - mu ) / ( len ( X ) - 1 ) variance = np . diag ( S ) . sum () var_arr . append (( total_variance - variance ) / total_variance ) sns . lineplot ( x = list ( range ( 1 , n + 1 )), y = var_arr ) plt . xlabel ( \"Number of Components\" ) plt . ylabel ( \"Proportion of Total Variance\" ) print ( var_arr ) So we see that 2 dimensions keeps almost 90% of the original variance and when we jump to 3 dimension it keeps 98% of the original data. Of course, this is a very common task for PCA and so is provided by many packages. In [53]: X = df_heart [ columns ] . values pca = PCA () . fit ( X ) print ( pca . explained_variance_ratio_ ) So, we can compress our data in 2 dimensions and now visualize our 7 dimensional dataset. In [54]: n_components = 2 X_hat = pca . transform ( X )[:,: n_components ] sns . scatterplot ( x = X_hat [:, 0 ], y = X_hat [:, 1 ], hue = df_heart [ \"AHD\" ], legend = \"full\" ) plt . xlabel ( \"First PCA Component Vector (Z1)\" ) plt . ylabel ( \"Second PCA Component Vector (Z2)\" ); _ = plt . axis ( \"equal\" ) To summarize, PCA is not a tool to help you make better predictions. It cannot be because it is simple linear transformations of the data. However, it gives one a way to compress the data and to better visualize it without losing information. In the lens of compression, PCA can be thought of as feature engineering as your new compressed data retains much of the information that is now exogenous of the dataset.",
        "tags": "lectures",
        "url": "lectures/lecture10/notebook/"
    }, {
        "title": "Lecture 10: Principal Component Analysis",
        "text": "PCA1 Title : Exercise: PCA 1 Description : Goals: To produce the plot below after PCA transformation (Using Scikit Learn) Data Description: Instructions: Perform (i.e fit and transform) Principal Components Analysis (PCA) on the given dataset X Hints: StandardScaler PCA.fit() -Note that this is unsupervised learning method. Note: This exercise is auto-graded and you can try multiple attempts. PCA Exercise 1 In [1]: import numpy as np import matplotlib.pylab as plt % matplotlib inline Loading data In [2]: X = np . loadtxt ( 'PCA1.csv' ) Standardize your $X$ matrix. In [0]: from sklearn.preprocessing import StandardScaler X = ___ In [3]: plt . plot ( X [:, 0 ], X [:, 1 ], \"+\" , alpha = 0.8 ) plt . xticks ( fontsize = 15 ) plt . yticks ( fontsize = 15 ) plt . xlabel ( 'X1' , fontsize = 15 ) plt . ylabel ( 'X2' , fontsize = 15 ) plt . grid () plt . show () PCA Transform your $X$ matrix using PCA with 2 components. In [0]: ### edTest(test_PCA_fit) ### from sklearn.decomposition import PCA pca = PCA ( ___ ) . fit ( ___ ) X_pca = pca . transform ( ___ ) print ( \"PCA shape:\" , X_pca . shape ) Visualization In [7]: plt . figure ( figsize = ( 6 , 5 )) plt . plot ( X [:, 0 ], X [:, 1 ], \"+\" , alpha = 0.8 , label = 'Original' ) plt . plot ( X_pca [:, 0 ], X_pca [:, 1 ], \"+\" , alpha = 0.8 , label = 'Transformed' ) plt . xticks ( fontsize = 15 ) plt . yticks ( fontsize = 15 ) plt . xlabel ( 'X1' , fontsize = 15 ) plt . ylabel ( 'X2' , fontsize = 15 ) plt . legend ( fontsize = 12 ) plt . grid () plt . show () In [0]:",
        "tags": "lectures",
        "url": "lectures/lecture10/notebook-1/"
    }, {
        "title": "Lecture 10: Principal Component Analysis",
        "text": "PCA2 Title : Exercise: PCA 2 - Implementing PCA from scratch (Numpy) Description : To produce a plot that roughly looks like following: Data Description: Instructions: Part - I In this question, you have to implement the PCA technique using numpy . The idea is to maximize the variance along axes by rotating the points. Given the rotation matrix: $$R\\ =\\ \\left[\\begin{matrix}\\cos\\theta&-\\sin\\theta\\\\\\sin\\theta&\\cos\\theta\\end{matrix}\\right]$$ the rotation of a matrix X is given by $$X_{R\\ }=\\ X\\cdot R$$ where X_R is the rotated matrix and Â· symbol is the dot product operator. Once you have the transform_pca function, you have to evaluate and save the variance for each angle in thetas list. The best angle will be: For every $\\theta$ we find $var(X_p)$ Best angle is $\\theta$ corresponding to $max(var(X_p))$ Note that we are only using variance for first predictor. Notice that the angle is in radians then: $$\\theta_{\\deg ree}\\ =\\ \\theta_{\\left\\{radians\\right\\}}\\cdot\\frac{180}{pi}$$ Finally, you have to visualize the rotation given the best angle. Part - II On this part, the idea is to compare the results with scikit-learn First, fit the PCA model using PCA(n_components = 2) as you did before in Q1. Given the components matrix C, the rotation angle is defined as follow: $$\\theta\\ =\\ \\arctan\\left(\\frac{component\\left(0,0\\right)}{component\\left(0,1\\right)}\\right)$$ Hints: test_transform_pca - You may use np.dot to calculate dot product. transform_pca() returns $X_p$ test_variances - You may use np.var to calculate variances, using the correct axis parameter. test_angle - See np.argmax to find index of the maximum value in an array, use np.pi for $\\pi$ test_PCA_fit - PCA.fit() - Note that this is unsupervised learning method. (Similar to Q1) test_angle_sklearn - You may use np.arctan with component(0,0) and component(0,1) as mentioned above. All the blanks are vectorized code (one liners, no looping constructs required). Note: You do not need to standardize for this particular exercise. This exercise is auto-graded and you can try multiple attempts. PCA 2 In [1]: import numpy as np import matplotlib.pylab as plt % matplotlib inline Loading data In [5]: X = np . loadtxt ( 'PCA.csv' ) print ( X . shape ) Numpy PCA In [6]: ### edTest(test_transform_pca) ### def transform_pca ( X , theta ): \"\"\" Make linear transformation given particular angle Parameters: X (np.array) : Input matrix theta (float) : Radians angle Returns: Transformed input matrix Xp (np.array) \"\"\" R = np . array ( [[ np . cos ( theta ), - np . sin ( theta )],[ np . sin ( theta ), np . cos ( theta )]]) Xp = ___ return Xp Selecting the best angle For every $\\theta$ we find $var(X_p)$ Best angle is $\\theta$ corresponding to $max(var(X_p))$ where $X_p = X \\cdot R$ Note that we are only using variance for first predictor. In [7]: ### edTest(test_variances) ### thetas = np . arange ( 0 , np . pi / 2 , 0.01 ) # Angles for rotation var_a1 = [] # First component variances for theta in thetas : Xp = ___ var = ___ var_a1 . append ( var [ 0 ]) In [0]: ### edTest(test_angle) ### #We have an array of theta values (thetas). Here we want to pick the # value of theta that corresponds to maximum variance for first component. angle_numpy = ___ angle_np_degree = angle_numpy * 180 / np . pi # converting to degrees print ( 'Best angle: {:.2f} ' . format ( angle_np_degree )) In [9]: ### edTest(test_linear_transformation) ### Xp = transform_pca ( X , angle_numpy ) # Linear transformation of the input In [10]: plt . figure ( figsize = ( 6 , 5 )) plt . plot ( X [:, 0 ], X [:, 1 ], \"+\" , alpha = 0.8 , label = 'Original' ) plt . plot ( Xp [:, 0 ], Xp [:, 1 ], \"+\" , alpha = 0.8 , label = 'Transformed' ) plt . xticks ( fontsize = 15 ) plt . yticks ( fontsize = 15 ) plt . xlabel ( 'X1' , fontsize = 15 ) plt . ylabel ( 'X2' , fontsize = 15 ) plt . legend ( fontsize = 12 ) plt . grid () plt . show () Comparing with Scikit-learn PCA In [11]: ### edTest(test_PCA_fit) ### from sklearn.decomposition import PCA pca = PCA ( ___ ) . fit ( __ ) pca_x = pca . transform ( __ ) Getting angle from components In [0]: ### edTest(test_angle_sklearn) ### components = pca . components_ angle_sklearn = ___ angle_sklearn_degrees = angle_sklearn * 180 / np . pi print ( 'Best angle: {:.2f} ' . format ( angle_sklearn_degrees )) In [0]: plt . figure () plt . plot ( X [:, 0 ], X [:, 1 ], '+' , label = 'original' , alpha = 0.6 ) plt . plot ( Xp [:, 0 ], Xp [:, 1 ], '+' , label = 'numpy' , alpha = 0.6 ) plt . plot ( pca_x [:, 0 ], pca_x [:, 1 ], '+' , label = 'sklearn' , alpha = 0.6 ) plt . legend ( fontsize = 12 ) plt . xticks ( fontsize = 15 ) plt . yticks ( fontsize = 15 ) plt . xlabel ( 'X1' , fontsize = 15 ) plt . ylabel ( 'X2' , fontsize = 15 ) plt . title ( r '$\\theta$ numpy: {:.2f} - $\\theta$ sklearn: {:.2f} ' . format ( angle_np_degree , angle_sklearn_degrees ), fontsize = 18 ) plt . grid () plt . show () In [0]:",
        "tags": "lectures",
        "url": "lectures/lecture10/notebook-2/"
    }, {
        "title": "Lecture 9: Missing Data & Imputation",
        "text": "Slides Lecture 9 : Dealing with Missing Data (PDF) Exercises Lecture 9: Lecture Notebook - Handling Missing Data [Notebook] Lecture 9: Exercise: Dealing with Missingness [Notebook]",
        "tags": "lectures",
        "url": "lectures/lecture09/"
    }, {
        "title": "Lecture 9: Missing Data & Imputation",
        "text": "Handling_of_missing_data_v2 CS109A Introduction to Data Science Missing Data Harvard University Fall 2021 Instructors : Pavlos Protopapas, Natesh Pillai Handling Missing Data Visualising Missing Data Dealing with Missingness Types of Missingness Imputation Methods Visualising missing data We will use the library missingno. https://github.com/ResidentMario/missingno Also see this blog: https://datasciencechalktalk.wordpress.com/2019/09/02/handling-missing-values-with-missingo/ In [9]: # You'll need to install missingno the first time you run this notebook ! pip install missingno Defaulting to user installation because normal site-packages is not writeable Requirement already satisfied: missingno in ./.local/lib/python3.9/site-packages (0.5.0) Requirement already satisfied: seaborn in /usr/lib/python3.9/site-packages (from missingno) (0.11.2) Requirement already satisfied: scipy in /usr/lib/python3.9/site-packages (from missingno) (1.7.1) Requirement already satisfied: numpy in /usr/lib/python3.9/site-packages (from missingno) (1.21.2) Requirement already satisfied: matplotlib in /usr/lib/python3.9/site-packages (from missingno) (3.4.3) Requirement already satisfied: cycler>=0.10 in /usr/lib/python3.9/site-packages (from matplotlib->missingno) (0.10.0) Requirement already satisfied: kiwisolver>=1.0.1 in /usr/lib/python3.9/site-packages (from matplotlib->missingno) (1.3.2) Requirement already satisfied: pillow>=6.2.0 in /usr/lib/python3.9/site-packages (from matplotlib->missingno) (8.3.2) Requirement already satisfied: pyparsing>=2.2.1 in /usr/lib/python3.9/site-packages (from matplotlib->missingno) (2.4.7) Requirement already satisfied: python-dateutil>=2.7 in /usr/lib/python3.9/site-packages (from matplotlib->missingno) (2.8.2) Requirement already satisfied: six in /usr/lib/python3.9/site-packages (from cycler>=0.10->matplotlib->missingno) (1.16.0) Requirement already satisfied: pandas>=0.23 in /usr/lib/python3.9/site-packages (from seaborn->missingno) (1.3.3) Requirement already satisfied: pytz>=2017.3 in /usr/lib/python3.9/site-packages (from pandas>=0.23->seaborn->missingno) (2021.1) Make sure to restart the notebook kernel after installing missingno or the import will not work. In [10]: import missingno as msno import matplotlib.pyplot as plt import seaborn as sns import pandas as pd import numpy as np import sklearn kaggle: Missing Migrants Project . In [11]: df = pd . read_csv ( 'MissingMigrants-Global-2019-12-31_correct.csv.zip' , compression = 'zip' ) df . shape Out[11]: (5987, 20) In [12]: df . dtypes Out[12]: Web ID int64 Region of Incident object Reported Date object Reported Year int64 Reported Month object Number Dead float64 Minimum Estimated Number of Missing float64 Total Dead and Missing int64 Number of Survivors float64 Number of Females float64 Number of Males float64 Number of Children float64 Cause of Death object Location Description object Information Source object Location Coordinates object Migration Route object URL object UNSD Geographical Grouping object Source Quality int64 dtype: object In [13]: df . head () Out[13]: Web ID Region of Incident Reported Date Reported Year Reported Month Number Dead Minimum Estimated Number of Missing Total Dead and Missing Number of Survivors Number of Females Number of Males Number of Children Cause of Death Location Description Information Source Location Coordinates Migration Route URL UNSD Geographical Grouping Source Quality 0 52673 Mediterranean December 30, 2019 2019 Dec 1.0 NaN 1 11.0 NaN NaN NaN Hypothermia Unspecififed location off the coast of Algeria El Watan 35.568972356329, -1.289773129748 Western Mediterranean https://bit.ly/2FqQHo4 Uncategorized 1 1 52666 Mediterranean December 30, 2019 2019 Dec 1.0 NaN 1 NaN NaN 1.0 NaN Presumed drowning Recoverd on Calamorcarro Beach, Ceuta El Foro de Ceuta 35.912383552874, -5.357673338898 Western Mediterranean https://bit.ly/39yKRyF Uncategorized 1 2 52663 East Asia December 27, 2019 2019 Dec 5.0 NaN 5 NaN NaN 3.0 NaN Unknown Bodies found on boat near Sado Island, Niigata... Japan Times, Kyodo News, AFP 38.154018233313, 138.086032653130 NaN http://bit.ly/2sCnBz1, http://bit.ly/2sEra83, ... Eastern Asia 3 3 52662 Middle East December 26, 2019 2019 Dec 7.0 NaN 7 64.0 NaN NaN NaN Drowning Van lake near Adilcevaz, Bitlis, Turkey EFE, BBC, ARYnews 38.777228612085, 42.739257582031 NaN http://bit.ly/2ZG2Y19, http://bit.ly/2MLamDf, ... Western Asia 3 4 52661 Middle East December 24, 2019 2019 Dec 12.0 NaN 12 NaN NaN NaN NaN Air strike Al-Raqw market in Saada, Yemen UN Humanitarian Coordinator in Yemen, Qatar Tr... 17.245364805636, 43.239093360326 NaN http://bit.ly/2FjolvD, http://bit.ly/2sD42GR, ... Western Asia 4 In [14]: print ( f 'Columns with at least one nan value: { df . isna () . any ( 0 ) . sum () } ' ) print ( f 'Rows with at least one nan value: { df . isna () . any ( 1 ) . sum () } ' ) Columns with at least one nan value: 12 Rows with at least one nan value: 5954 missingno library Matrix The msno.matrix nullity matrix is a data-dense display which lets you quickly visually pick out patterns in data completion. In [15]: msno . matrix ( df . sample ( 100 )); Out[15]: <AxesSubplot:> The sparkline on the right summarizes the general shape of the data completeness and points out the rows with the maximum and minimum nullity in the dataset. Bar msno.bar is a simple visualization of nullity by column: In [16]: msno . bar ( df . sample ( 500 )); Out[16]: <AxesSubplot:> Heatmap The missingno correlation heatmap measures nullity correlation: how strongly the presence or absence of one variable affects the presence of another: In [17]: msno . heatmap ( df . sample ( 500 )); Out[17]: <AxesSubplot:> Minimum Estimated Number of Missing is negative correlated to Number of Dead (-.6) and that means that usually when one feature is not null the other is null and vice versa. Minimum Estimated Number of Missing is positive correlated to Number of Survivors (.4) and that means that usually when one feature is null the other is also null. Dendrogram The dendrogram allows you to more fully correlate variable completion, revealing trends deeper than the pairwise ones visible in the correlation heatmap. To interpret this graph, read it from a top-down perspective. Cluster leaves which linked together at a distance of zero fully predict one another's presenceâ€”one variable might always be empty when another is filled, or they might always both be filled or both empty, and so on. In this specific example the dendrogram glues together the variables which are required and therefore present in every record. In [18]: msno . dendrogram ( df . sample ( 500 )); Out[18]: <AxesSubplot:> So, just a few lines of missingno can work great to see what data is missing and some relations of missing data. But we can use other tools like pandas or seaborn to visualize it. NaN bars with seaborn In [19]: g = df [ df . columns [ df . isna () . any ( 0 )]] . isna () . sum () . sort_values ( ascending = False ) / df . shape [ 0 ] g = g . reset_index () . rename ( columns = { 'index' : 'column' , 0 : 'nans' }) sns . barplot ( x = 'nans' , y = 'column' , data = g , ax = plt . subplots ( 1 , 1 , figsize = ( 12 , 5 ))[ 1 ], palette = 'rocket' ) plt . xlabel ( 'ratio of nans' ); Out[19]: Text(0.5, 0, 'ratio of nans') NaN heatmap with seaborn In [20]: sns . heatmap ( df [ df . columns [ df . isna () . any ( 0 )]] . isna () . astype ( int ), ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 8 ))[ 1 ]); Out[20]: <AxesSubplot:> nullity matrix using matplotlib In [21]: plt . figure ( figsize = ( 16 , 10 )) # we will plot only columns where there is at least one nan value nan_columns = df . columns [ df . isna () . any ( 0 )] # save the number of nan values per column count_nans = df [ nan_columns ] . isna () . sum () values = df [ nan_columns ] . sample ( 500 ) . isna () . astype ( int ) . values column_width = 100 columns = None for i in range ( values . shape [ 1 ]): name = nan_columns [ i ] print ( f ' { name : 40 } : { count_nans [ name ] / df . shape [ 0 ] : .2% } ' ) column = values [:, i ] column = np . array ([ column for _ in range ( column_width )]) . T columns = column if columns is None else np . hstack ([ columns , column ]) plt . imshow ( columns , cmap = 'gray' ); Number Dead : 3.76% Minimum Estimated Number of Missing : 90.38% Number of Survivors : 84.73% Number of Females : 82.33% Number of Males : 45.11% Number of Children : 87.42% Location Description : 0.17% Information Source : 0.03% Location Coordinates : 0.02% Migration Route : 51.04% URL : 36.36% UNSD Geographical Grouping : 0.77% Out[21]: <matplotlib.image.AxesImage at 0x7fea348d19a0> What are the simplest ways to handle missing data? Naively handling missingness! Drop the observations that have any missing values. Use pd.DataFrame.dropna(axis=0) Impute the mean/median (if quantitative) or most common class (if categorical) for all missing values. Use pd.DataFrame.fillna(value=x.mean()) How do statsmodels and sklearn handle these NaNs? What are some consequences in handling missingness in these fashions? To face these questions we will work on a very simple dataset with missing data. Let's write some helper methods In [22]: # sample data with nans def get_data (): return pd . DataFrame ( data = { 'A' : [ 1 , 2 , 3 , np . nan , np . nan , 6 ], 'B' : [ 1 , - 1 , 1 , - 1 , 1 , np . nan ], 'C' : [ 1 , 2 , np . nan , 8 , 16 , 32 ], 'D' : [ 'male' , 'female' , 'male' , 'female' , None , 'male' ], 'E' : [ 'red' , 'blue' , 'red' , 'green' , None , 'yellow' ],} ) df = get_data () df Out[22]: A B C D E 0 1.0 1.0 1.0 male red 1 2.0 -1.0 2.0 female blue 2 3.0 1.0 NaN male red 3 NaN -1.0 8.0 female green 4 NaN 1.0 16.0 None None 5 6.0 NaN 32.0 male yellow In [23]: # used to plot original series with a new one where we already handled missing data def plot_compare ( df , column , title = '' ): org_df = get_data () nans = org_df . index [ org_df [ column ] . isna ()] fig , axes = plt . subplots ( 1 , 2 , figsize = ( 16 , 2 )) org_df [[ column ]] . plot ( marker = 'o' , ms = 10 , lw = 0 , ax = axes [ 0 ]) if len ( nans ) > 0 : for x in nans : axes [ 0 ] . axvline ( x = x , lw = 1 , ls = 'dashed' , label = f 'NaN: { x } ' ) axes [ 0 ] . set_title ( 'original series' ) df [[ column ]] . plot ( marker = 'o' , ms = 10 , lw = 0 , ax = axes [ 1 ]) if len ( nans ) > 0 : df . loc [ df . index . isin ( nans ), column ] . plot ( marker = 'o' , ms = 10 , lw = 0 , color = 'orange' , ax = axes [ 1 ]) for x in nans : axes [ 1 ] . axvline ( x = x , lw = 1 , ls = 'dashed' , label = f 'NaN: { x } ' ) if len ( title ) > 0 : title = f 'new series ( { title } )' else : title = 'new series' axes [ 1 ] . set_title ( title ) plt . suptitle ( f 'column { column } ' ) plt . show () Method: dropna() In [24]: df = get_data () df = df . dropna ( axis = 0 ) df Out[24]: A B C D E 0 1.0 1.0 1.0 male red 1 2.0 -1.0 2.0 female blue In [25]: plot_compare ( df , 'A' , 'method=dropna' ) plot_compare ( df , 'B' , 'method=dropna' ) plot_compare ( df , 'C' , 'method=dropna' ) It looks like dropping was not a good idea here. From 6 samples we've obtained just two. We've lost samples at index 3 and 5 for feature 'A' and samples at index 3 and 4 for feature 'B'. Method: fillna with mean In [26]: df = get_data () for c in df . select_dtypes ( 'number' ): df [ c ] = df [ c ] . fillna ( df [ c ] . mean ()) df Out[26]: A B C D E 0 1.0 1.0 1.0 male red 1 2.0 -1.0 2.0 female blue 2 3.0 1.0 11.8 male red 3 3.0 -1.0 8.0 female green 4 3.0 1.0 16.0 None None 5 6.0 0.2 32.0 male yellow In [27]: plot_compare ( df , 'A' , 'method=fillna(mean)' ) plot_compare ( df , 'B' , 'method=fillna(mean)' ) plot_compare ( df , 'C' , 'method=fillna(mean)' ) Method: fillna with median In [28]: df = get_data () for c in df . select_dtypes ( 'number' ): df [ c ] = df [ c ] . fillna ( df [ c ] . median ()) df Out[28]: A B C D E 0 1.0 1.0 1.0 male red 1 2.0 -1.0 2.0 female blue 2 3.0 1.0 8.0 male red 3 2.5 -1.0 8.0 female green 4 2.5 1.0 16.0 None None 5 6.0 1.0 32.0 male yellow In [29]: plot_compare ( df , 'A' , 'method=fillna(median)' ) plot_compare ( df , 'B' , 'method=fillna(median)' ) plot_compare ( df , 'C' , 'method=fillna(median)' ) Forward and backward filling methods In [30]: df = get_data () nans = df . isna () # helper to highligh original NaN cells def highlight_nans ( s ): return np . where ( nans [ s . name ], 'color:white;background-color:black;text-align:center' , '' ) df . style . format ( formatter = None , na_rep = '' ) . apply ( highlight_nans , axis = 0 ) Out[30]: A B C D E 0 1.000000 1.000000 1.000000 male red 1 2.000000 -1.000000 2.000000 female blue 2 3.000000 1.000000 male red 3 -1.000000 8.000000 female green 4 1.000000 16.000000 5 6.000000 32.000000 male yellow Method: bfill / pad In [31]: df_ffill = df . fillna ( method = 'ffill' ) df_ffill . style . apply ( highlight_nans , axis = 0 ) Out[31]: A B C D E 0 1.000000 1.000000 1.000000 male red 1 2.000000 -1.000000 2.000000 female blue 2 3.000000 1.000000 2.000000 male red 3 3.000000 -1.000000 8.000000 female green 4 3.000000 1.000000 16.000000 female green 5 6.000000 1.000000 32.000000 male yellow In [32]: plot_compare ( df_ffill , 'A' , 'method=fillna(ffill)' ) plot_compare ( df_ffill , 'B' , 'method=fillna(ffill)' ) plot_compare ( df_ffill , 'C' , 'method=fillna(ffill)' ) Method: bfill / backfill In [33]: df_bfill = df . fillna ( method = 'bfill' ) df_bfill . style . apply ( highlight_nans , axis = 0 ) Out[33]: A B C D E 0 1.000000 1.000000 1.000000 male red 1 2.000000 -1.000000 2.000000 female blue 2 3.000000 1.000000 8.000000 male red 3 6.000000 -1.000000 8.000000 female green 4 6.000000 1.000000 16.000000 male yellow 5 6.000000 nan 32.000000 male yellow Sometimes these imputation techniques are not enough to fill missing data by definition. How do you fill missing data with future data when there is no future data? Take a look at column B of the above table. In [34]: plot_compare ( df_bfill , 'A' , 'method=fillna(bfill)' ) plot_compare ( df_bfill , 'B' , 'method=fillna(bfill)' ) plot_compare ( df_bfill , 'C' , 'method=fillna(bfill)' ) Imputation using the most frequent value For numerical values sometime using the mean or the median value might make sense but what about categorial variables? Pandas is great when counting for us. In [35]: df [[ 'D' , 'E' ]] . describe () Out[35]: D E count 5 5 unique 2 4 top male red freq 3 2 The mode of a set of values is the value that appears most often. It can be multiple values. mode() : Get the mode(s) of each element along the selected axis. DataFrame . mode ( axis = 0 , numeric_only = False , dropna = True ) In [36]: most_frequent = df [[ 'D' , 'E' ]] . mode () . head ( 1 ) most_frequent Out[36]: D E 0 male red Transform dataframe's first row into a dictionary to be used with fillna() In [37]: most_frequent = { c : most_frequent [ c ] . item () for c in most_frequent . columns } most_frequent Out[37]: {'D': 'male', 'E': 'red'} In [38]: df . fillna ( most_frequent ) . style . apply ( highlight_nans , axis = 0 ) Out[38]: A B C D E 0 1.000000 1.000000 1.000000 male red 1 2.000000 -1.000000 2.000000 female blue 2 3.000000 1.000000 nan male red 3 nan -1.000000 8.000000 female green 4 nan 1.000000 16.000000 male red 5 6.000000 nan 32.000000 male yellow Imputation through interpolation Pandas offers the interpolate ) method that can be used for imputation. It's powerfull comes from making use of the scipy.interpolate method. pd.DataFrame.interpolate() : Fill NaN values using an interpolation method Forward padding In [39]: df . interpolate ( method = 'pad' , limit = 1 ) . style . apply ( highlight_nans , axis = 0 ) Out[39]: A B C D E 0 1.000000 1.000000 1.000000 male red 1 2.000000 -1.000000 2.000000 female blue 2 3.000000 1.000000 2.000000 male red 3 3.000000 -1.000000 8.000000 female green 4 nan 1.000000 16.000000 female green 5 6.000000 1.000000 32.000000 male yellow The limit parameter value affects the number of consecutive NaN values we can fill with this method. There is another parameter named limit_direction that can be use to make it work forward, backward or both direction depending on the selected method. In [40]: df . interpolate ( method = 'bfill' , limit = 2 ) . style . apply ( highlight_nans , axis = 0 ) Out[40]: A B C D E 0 1.000000 1.000000 1.000000 male red 1 2.000000 -1.000000 2.000000 female blue 2 3.000000 1.000000 8.000000 male red 3 6.000000 -1.000000 8.000000 female green 4 6.000000 1.000000 16.000000 male yellow 5 6.000000 nan 32.000000 male yellow Linear interpolation TIP 1: This is the only method that doesn't make use of the index to do the interpolation. TIP 2: It is really worth it to read the interpolate documentation. In [41]: df . interpolate ( method = 'linear' ) . style . apply ( highlight_nans , axis = 0 ) Out[41]: A B C D E 0 1.000000 1.000000 1.000000 male red 1 2.000000 -1.000000 2.000000 female blue 2 3.000000 1.000000 5.000000 male red 3 4.000000 -1.000000 8.000000 female green 4 5.000000 1.000000 16.000000 None None 5 6.000000 1.000000 32.000000 male yellow Quadratic interpolation In [42]: df . interpolate ( method = 'quadratic' ) . style . apply ( highlight_nans , axis = 0 ) Out[42]: A B C D E 0 1.000000 1.000000 1.000000 male red 1 2.000000 -1.000000 2.000000 female blue 2 3.000000 1.000000 3.945946 male red 3 4.000000 -1.000000 8.000000 female green 4 5.000000 1.000000 16.000000 None None 5 6.000000 nan 32.000000 male yellow Take a look at the NaN at column B. The reason is we are doing ... interpolation. Remember that when using interpolation for imputation there still could exist NaN values. Quadratic interpolation with a different index In [43]: df . set_index ( np . square ( df . index )) . interpolate ( method = 'quadratic' ) . style . apply ( highlight_nans , axis = 0 ) Out[43]: A B C D E 0 1.000000 1.000000 1.000000 male red 1 2.000000 -1.000000 2.000000 female blue 4 3.000000 1.000000 4.444847 male red 9 3.841270 -1.000000 8.000000 female green 16 4.885714 1.000000 16.000000 None None 25 6.000000 nan 32.000000 male yellow Polynomial interpolation of order 3 In [44]: df . interpolate ( method = 'polynomial' , order = 3 ) . style . apply ( highlight_nans , axis = 0 ) Out[44]: A B C D E 0 1.000000 1.000000 1.000000 male red 1 2.000000 -1.000000 2.000000 female blue 2 3.000000 1.000000 3.928571 male red 3 4.000000 -1.000000 8.000000 female green 4 5.000000 1.000000 16.000000 None None 5 6.000000 nan 32.000000 male yellow Polynomial interpolation of order 3 with a different index In [45]: df . set_index ( np . square ( df . index )) . interpolate ( method = 'polynomial' , order = 3 ) . style . apply ( highlight_nans , axis = 0 ) Out[45]: A B C D E 0 1.000000 1.000000 1.000000 male red 1 2.000000 -1.000000 2.000000 female blue 4 3.000000 1.000000 4.408574 male red 9 0.285714 -1.000000 8.000000 female green 16 -4.714286 1.000000 16.000000 None None 25 6.000000 nan 32.000000 male yellow Take a look at the column A. The index really affects the results. Interpolation using nearest We can use the method nearest that will make use of the index to do its work. TIP : if you think that there is a feature (column) that is related to the feature with missing values, you can make use of the feature as index to look for the nearest neighbor's value. We are going to split an example procedure into different steps, but you can add these into a recursive method once you understand what's going on. In [46]: # reset_index will free the index as a column that will help later temp = df . reset_index () # we need B without nans to work as index for interpolation temp = temp . loc [ df . B . notna (), [ 'index' , 'B' , 'D' ]] temp = temp . set_index ( 'B' ) temp Out[46]: index D B 1.0 0 male -1.0 1 female 1.0 2 male -1.0 3 female 1.0 4 None In [47]: # use this dictionary to create a one hot encoding version of column D male_ohe = { 'male' : 1 , 'female' : 0 , None : None } temp [ 'is_male' ] = temp [ 'D' ] . apply ( lambda x : male_ohe [ x ]) . astype ( float ) temp Out[47]: index D is_male B 1.0 0 male 1.0 -1.0 1 female 0.0 1.0 2 male 1.0 -1.0 3 female 0.0 1.0 4 None NaN In [48]: temp = temp . interpolate ( method = 'nearest' ) temp Out[48]: index D is_male B 1.0 0 male 1.0 -1.0 1 female 0.0 1.0 2 male 1.0 -1.0 3 female 0.0 1.0 4 None 1.0 In [49]: # we reverse the one hot encoded at is_male column to fill column D male_ohe_r = { v : k for k , v in male_ohe . items ()} temp [ 'D' ] = temp [ 'is_male' ] . apply ( lambda x : male_ohe_r [ x ]) temp Out[49]: index D is_male B 1.0 0 male 1.0 -1.0 1 female 0.0 1.0 2 male 1.0 -1.0 3 female 0.0 1.0 4 male 1.0 In [50]: # recover original index to be used to return values to df temp = temp . set_index ( 'index' ) #df = df.reset_index().set_index('age') In [51]: temp Out[51]: D is_male index 0 male 1.0 1 female 0.0 2 male 1.0 3 female 0.0 4 male 1.0 In [52]: df . loc [ df . index . isin ( temp . index ), 'D' ] = temp . D df Out[52]: A B C D E 0 1.0 1.0 1.0 male red 1 2.0 -1.0 2.0 female blue 2 3.0 1.0 NaN male red 3 NaN -1.0 8.0 female green 4 NaN 1.0 16.0 male None 5 6.0 NaN 32.0 male yellow We fixed column 'D', now let's fix column 'B' We will use column D to fix column B, since, as we saw, they are related. In [53]: # reset_index will free the index as a column that will help later temp = df . reset_index () # we need D without nans to work as index for interpolation temp = temp . loc [ df . D . notna (), [ 'index' , 'B' , 'D' ]] temp = temp . set_index ( 'D' ) temp Out[53]: index B D male 0 1.0 female 1 -1.0 male 2 1.0 female 3 -1.0 male 4 1.0 male 5 NaN In [54]: temp . reset_index ( inplace = True ) temp Out[54]: D index B 0 male 0 1.0 1 female 1 -1.0 2 male 2 1.0 3 female 3 -1.0 4 male 4 1.0 5 male 5 NaN In [55]: # use the relationship between D and B to recreate B. male_ohe = { 'male' : 1 , 'female' : - 1 , None : None } temp [ 'B' ] = temp [ 'D' ] . apply ( lambda x : male_ohe [ x ]) . astype ( float ) temp Out[55]: D index B 0 male 0 1.0 1 female 1 -1.0 2 male 2 1.0 3 female 3 -1.0 4 male 4 1.0 5 male 5 1.0 In [56]: df . loc [ df . index . isin ( temp . index ), 'B' ] = temp . B df Out[56]: A B C D E 0 1.0 1.0 1.0 male red 1 2.0 -1.0 2.0 female blue 2 3.0 1.0 NaN male red 3 NaN -1.0 8.0 female green 4 NaN 1.0 16.0 male None 5 6.0 1.0 32.0 male yellow We can select the best method for every feature In [57]: temp . head () Out[57]: D index B 0 male 0 1.0 1 female 1 -1.0 2 male 2 1.0 3 female 3 -1.0 4 male 4 1.0 In [58]: df = df . assign ( A = df [ 'A' ] . interpolate ( method = 'linear' ), B = df [ 'B' ], C = df [ 'C' ] . interpolate ( method = 'quadratic' ), D = df [ 'D' ] ) df . style . apply ( highlight_nans , axis = 0 ) Out[58]: A B C D E 0 1.000000 1.000000 1.000000 male red 1 2.000000 -1.000000 2.000000 female blue 2 3.000000 1.000000 3.945946 male red 3 4.000000 -1.000000 8.000000 female green 4 5.000000 1.000000 16.000000 male None 5 6.000000 1.000000 32.000000 male yellow It's pending to fill missing data at column E Computers and algorithms can do a lot for us, but remember that we can do a lot for them. We observe that column E denotes colors. We know that categorical variables can be converted into numerical variables using One Hot Encoding. But in particular this kind of categorical variables can be converted into a better representation of numerical variables. There are many ways to represent colors as numbers but for instance we will try RGB format. In [59]: from matplotlib.patches import Rectangle In [60]: plt . figure ( figsize = ( 16 , 1 )) ax = plt . subplot ( 111 ) colors = { 'red' : ( 1 , 0 , 0 ), 'green' : ( 0 , 1 , 0 ), 'blue' : ( 0 , 0 , 1 ), 'magenta' : ( 1 , 0 , 1 ), 'yellow' : ( 1 , 1 , 0 ), 'cyan' : ( 0 , 1 , 1 ), 'black' : ( 0 , 0 , 0 ), 'white' : ( 1 , 1 , 1 ), } col , row = 0. , 0. for name , c in colors . items (): ax . add_patch ( Rectangle (( col , row ), 1 , 1 , color = c )) col += 1 / len ( colors ) plt . show () In [61]: def rgb_part ( x , rgb_index ): global colors if x == None : return None c = colors [ x ] return c [ rgb_index ] df [ 'E_r' ] = df [ 'E' ] . apply ( lambda x : rgb_part ( x , rgb_index = 0 )) df [ 'E_g' ] = df [ 'E' ] . apply ( lambda x : rgb_part ( x , rgb_index = 1 )) df [ 'E_b' ] = df [ 'E' ] . apply ( lambda x : rgb_part ( x , rgb_index = 2 )) df Out[61]: A B C D E E_r E_g E_b 0 1.0 1.0 1.000000 male red 1.0 0.0 0.0 1 2.0 -1.0 2.000000 female blue 0.0 0.0 1.0 2 3.0 1.0 3.945946 male red 1.0 0.0 0.0 3 4.0 -1.0 8.000000 female green 0.0 1.0 0.0 4 5.0 1.0 16.000000 male None NaN NaN NaN 5 6.0 1.0 32.000000 male yellow 1.0 1.0 0.0 Using 0 for RED part balances the samples but at the same time when we see that there are not samples where D is male and RED part is not 1. BLUE part is 1 just for one sample (and a female one). It makes sense to select 0 for blue part. In [62]: df [ df . D == 'male' ] Out[62]: A B C D E E_r E_g E_b 0 1.0 1.0 1.000000 male red 1.0 0.0 0.0 2 3.0 1.0 3.945946 male red 1.0 0.0 0.0 4 5.0 1.0 16.000000 male None NaN NaN NaN 5 6.0 1.0 32.000000 male yellow 1.0 1.0 0.0 We are insisting from the begining that column D and B where related and now we insist with color related to gender. Is this some kind of bias or is the data telling us about this relation ship? Imputation through modeling We can make use of K-Nearest Neighbor from Scikit-Learn library to simplify things In [63]: # one hot encode column D (into is_female and is_male and we drop first: is_female) cat_dummies = pd . get_dummies ( df [[ 'D' ]], drop_first = True ) # we drop categorical variables and we add one hot encoded one X = pd . concat ([ df . drop ( columns = [ 'D' , 'E' ]), cat_dummies ], axis = 1 ) X Out[63]: A B C E_r E_g E_b D_male 0 1.0 1.0 1.000000 1.0 0.0 0.0 1 1 2.0 -1.0 2.000000 0.0 0.0 1.0 0 2 3.0 1.0 3.945946 1.0 0.0 0.0 1 3 4.0 -1.0 8.000000 0.0 1.0 0.0 0 4 5.0 1.0 16.000000 NaN NaN NaN 1 5 6.0 1.0 32.000000 1.0 1.0 0.0 1 In [64]: temp . loc [ temp . index == 5 , 'B' ] = np . random . choice ( temp [ temp . B . notna ()] . B . unique ()) temp Out[64]: D index B 0 male 0 1.0 1 female 1 -1.0 2 male 2 1.0 3 female 3 -1.0 4 male 4 1.0 5 male 5 -1.0 Scaled data is a must for most algorithms. Does KNN require scaling? In [65]: from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler () X = pd . DataFrame ( scaler . fit_transform ( X ), columns = X . columns ) X Out[65]: A B C E_r E_g E_b D_male 0 0.0 1.0 0.000000 1.0 0.0 0.0 1.0 1 0.2 0.0 0.032258 0.0 0.0 1.0 0.0 2 0.4 1.0 0.095031 1.0 0.0 0.0 1.0 3 0.6 0.0 0.225806 0.0 1.0 0.0 0.0 4 0.8 1.0 0.483871 NaN NaN NaN 1.0 5 1.0 1.0 1.000000 1.0 1.0 0.0 1.0 We create our model for imputation In [66]: from sklearn.impute import KNNImputer # based of nearest 3 neighbors imputer = KNNImputer ( n_neighbors = 3 ) We do the imputation In [67]: X = pd . DataFrame ( imputer . fit_transform ( X ), columns = X . columns ) We verify there are no nan values in any column nor a single row In [68]: X . isna () . any ( 0 ) . sum (), X . isna () . any ( 1 ) . sum () Out[68]: (0, 0) In [69]: X = pd . DataFrame ( scaler . inverse_transform ( X ), columns = X . columns ) In [70]: X Out[70]: A B C E_r E_g E_b D_male 0 1.0 1.0 1.000000 1.0 0.000000 0.0 1.0 1 2.0 -1.0 2.000000 0.0 0.000000 1.0 0.0 2 3.0 1.0 3.945946 1.0 0.000000 0.0 1.0 3 4.0 -1.0 8.000000 0.0 1.000000 0.0 0.0 4 5.0 1.0 16.000000 1.0 0.333333 0.0 1.0 5 6.0 1.0 32.000000 1.0 1.000000 0.0 1.0 We can see that the model has selected the color: (1, 1/3, 0) = #FF5500. And based on that color, if we are not so strict with number we can round it to #FF0000, resulting in red color. In [71]: plt . figure ( figsize = ( 1 , 1 )) plt . subplot ( 111 ) . add_patch ( Rectangle (( 0 , 0 ), 1 , 1 , color = ( 1 , 1 / 3 , 0 ))); Out[71]: <matplotlib.patches.Rectangle at 0x7fea3691fdf0> In [72]: # we save it for later comparison version_1 = X . copy () X [ 'E' ] = df [ 'E' ] X . loc [ X . index == 4 , 'E' ] = 'red' X . drop ( columns = [ 'E_r' , 'E_g' , 'E_b' ], inplace = True ) X [ 'D' ] = X [ 'D_male' ] . apply ( lambda x : 'male' if x == 1 else 'female' ) X = X . drop ( columns = [ 'D_male' ]) version_1 = X . copy () X Out[72]: A B C E D 0 1.0 1.0 1.000000 red male 1 2.0 -1.0 2.000000 blue female 2 3.0 1.0 3.945946 red male 3 4.0 -1.0 8.000000 green female 4 5.0 1.0 16.000000 red male 5 6.0 1.0 32.000000 yellow male Does the number of neighbors affect the color selected for this particular missing color? In [73]: def best_color_for ( neighbors ): scaler = MinMaxScaler () X = pd . concat ([ df . drop ( columns = [ 'D' , 'E' ]), cat_dummies ], axis = 1 ) X = pd . DataFrame ( scaler . fit_transform ( X ), columns = X . columns ) imputer = KNNImputer ( n_neighbors = neighbors ) X = pd . DataFrame ( imputer . fit_transform ( X ), columns = X . columns ) c = tuple ( X . loc [ X . index == 4 , [ 'E_r' , 'E_g' , 'E_b' ]] . head ( 1 ) . values . flatten ()) print ( f 'Neighbors: { neighbors } Imputed color: { c } ' ) plt . figure ( figsize = ( 1 , 1 )) plt . subplot ( 111 ) . add_patch ( Rectangle (( 0 , 0 ), 1 , 1 , color = c )) plt . show () In [74]: for neighbors in range ( 1 , 6 ): best_color_for ( neighbors ) Neighbors: 1 Imputed color: (1.0, 1.0, 0.0) Neighbors: 2 Imputed color: (1.0, 0.5, 0.0) Neighbors: 3 Imputed color: (1.0, 0.3333333333333333, 0.0) Neighbors: 4 Imputed color: (0.75, 0.5, 0.0) Neighbors: 5 Imputed color: (0.6, 0.4, 0.2) You can see that depending on the number of neighbors selected the imputed color changes, but everything is around red tone. What about using KNN to fill everything from the start? We have used this algorithm to fill just the last missing color data. And it worked great. And it will work great in many situations. You will find many examples in internet using it as the default tool to go. But it is good to question it. In [75]: df = get_data () # one hot encode D D_dummies = pd . get_dummies ( df [[ 'D' ]], drop_first = True ) # one hot encode color in a better way df [ 'E_r' ] = df [ 'E' ] . apply ( lambda x : rgb_part ( x , rgb_index = 0 )) df [ 'E_g' ] = df [ 'E' ] . apply ( lambda x : rgb_part ( x , rgb_index = 1 )) df [ 'E_b' ] = df [ 'E' ] . apply ( lambda x : rgb_part ( x , rgb_index = 2 )) # we drop categorical variables and we add one hot encoded one X = pd . concat ([ df . drop ( columns = [ 'D' , 'E' ]), D_dummies ], axis = 1 ) scaler = MinMaxScaler () X = pd . DataFrame ( scaler . fit_transform ( X ), columns = X . columns ) imputer = KNNImputer ( n_neighbors = 3 ) X = pd . DataFrame ( imputer . fit_transform ( X ), columns = X . columns ) X = pd . DataFrame ( scaler . inverse_transform ( X ), columns = X . columns ) X [ 'D' ] = X [ 'D_male' ] . apply ( lambda x : 'male' if x == 1 else 'female' ) X = X . drop ( columns = [ 'D_male' ]) X Out[75]: A B C E_r E_g E_b D 0 1.0 1.000000 1.000000 1.000000 0.000000 0.000000 male 1 2.0 -1.000000 2.000000 0.000000 0.000000 1.000000 female 2 3.0 1.000000 16.333333 1.000000 0.000000 0.000000 male 3 3.0 -1.000000 8.000000 0.000000 1.000000 0.000000 female 4 2.0 1.000000 16.000000 0.333333 0.333333 0.333333 female 5 6.0 0.333333 32.000000 1.000000 1.000000 0.000000 male The color filled at index 4th is #555555 (darkgray) In [76]: X [ 'E' ] = df [ 'E' ] X . loc [ X . index == 4 , 'E' ] = 'gray' X . drop ( columns = [ 'E_r' , 'E_g' , 'E_b' ], inplace = True ) version_2 = X . copy () version_2 . style . apply ( highlight_nans , axis = 0 ) Out[76]: A B C D E 0 1.000000 1.000000 1.000000 male red 1 2.000000 -1.000000 2.000000 female blue 2 3.000000 1.000000 16.333333 male red 3 3.000000 -1.000000 8.000000 female green 4 2.000000 1.000000 16.000000 female gray 5 6.000000 0.333333 32.000000 male yellow So, we can see in the above table the results of using brute force for imputation even with a great algorithm like KNN. Column A breaks the pattern Column B breaks the variable domain (-1, 1) Column C breaks the pattern Column D loses relation with column B Column E loses the red pattern for most colors and even with relation to columns D and B. In [77]: plot_compare ( version_2 , 'A' , 'method=global KNN' ) plot_compare ( version_2 , 'B' , 'method=global KNN' ) plot_compare ( version_2 , 'C' , 'method=global KNN' ) And we can compare these results with the table below that comes from the step by step imputation using different methods. In [78]: version_1 . style . apply ( highlight_nans , axis = 0 ) Out[78]: A B C E D 0 1.000000 1.000000 1.000000 red male 1 2.000000 -1.000000 2.000000 blue female 2 3.000000 1.000000 3.945946 red male 3 4.000000 -1.000000 8.000000 green female 4 5.000000 1.000000 16.000000 red male 5 6.000000 1.000000 32.000000 yellow male In [79]: plot_compare ( version_1 , 'A' , 'method=linear interpolation' ) plot_compare ( version_1 , 'B' , 'method=random choice from domain' ) plot_compare ( version_1 , 'C' , 'method=quadratic interpolation' )",
        "tags": "lectures",
        "url": "lectures/lecture09/notebook/"
    }, {
        "title": "Lecture 9: Missing Data & Imputation",
        "text": "lecture9-ex1 Title : Exercise: Dealing with Missingness Description : The goal of the exercise is to get comfortable with different types of missingness and ways to try and handle them with a few basic imputations methods using numpy, pandas, and sklearn. The examples will show how the combination of different types of missingness and imputation methods can affect inference. Data Description: Instructions: We are using synthetic data to illustrate the issues with missing data. We will Create a synthetic dataset from two predictors Create missingness in 3 different ways Handle it 4 different ways (dropping rows, mean imputation, OLS imputation, and k-NN imputation) Hints: pandas.dropna Drop rows with missingness pandas.fillna Fill in missingness either with a single values or a with a Series sklearn.impute.SimpleImputer Imputation transformer for completing missing values. sklearn.LinearRegression Generates a Linear Regression Model sklearn.impute.KNNImputer Fill in missingness with a KNN model Note: This exercise is auto-graded and you can try multiple attempts. In [2]: % matplotlib inline import matplotlib.pyplot as plt import pandas as pd import numpy as np from sklearn.linear_model import LinearRegression from sklearn.impute import SimpleImputer , KNNImputer Dealing with Missingness Missing Data We'll create data of the form: $$ y = 3x_1 - 2x_2 + \\varepsilon,\\hspace{0.1in} \\varepsilon \\sim N(0,1)$$ We will then be inserting missingness into x1 in various ways, and analyzing the results of different methods for handling those missing values. In [3]: # Number of data points to generate n = 500 # Set random seed for numpy to ensure reproducible results np . random . seed ( 109 ) # Generate our predictors... x1 = np . random . normal ( 0 , 1 , size = n ) x2 = 0.5 * x1 + np . random . normal ( 0 , np . sqrt ( 0.75 ), size = n ) X = pd . DataFrame ( data = np . transpose ([ x1 , x2 ]), columns = [ \"x1\" , \"x2\" ]) # Generate our response... y = 3 * x1 - 2 * x2 + np . random . normal ( 0 , 1 , size = n ) y = pd . Series ( y ) # And put them all in a nice DataFrame df = pd . DataFrame ( data = np . transpose ([ x1 , x2 , y ]), columns = [ \"x1\" , \"x2\" , \"y\" ]) In [4]: fig , axs = plt . subplots ( 1 , 3 , figsize = ( 16 , 5 )) plot_pairs = [( 'x1' , 'y' ), ( 'x2' , 'y' ), ( 'x1' , 'x2' )] for ax , ( x_var , y_var ) in zip ( axs , plot_pairs ): df . plot . scatter ( x_var , y_var , ax = ax , title = f ' { y_var } vs. { x_var } ' ) Poke holes in $X_1$ in 3 different ways: Missing Completely at Random (MCAR): missingness is not predictable. Missing at Random (MAR): missingness depends on other observed data, and thus can be recovered in some way Missingness not at Random (MNAR): missingness depends on unobserved data and thus cannot be recovered Here we generate indices of $X_1$ to be dropped due to 3 types of missingness using $n$ single bernoulli trials.\\ The only difference between the 3 sets of indices is the probabilities of success for each trial (i.e., the probability that a given observation will be missing). In [5]: missing_A = np . random . binomial ( 1 , 0.05 + 0.85 * ( y > ( y . mean () + y . std ())), n ) . astype ( bool ) missing_B = np . random . binomial ( 1 , 0.2 , n ) . astype ( bool ) missing_C = np . random . binomial ( 1 , 0.05 + 0.85 * ( x2 > ( x2 . mean () + x2 . std ())), n ) . astype ( bool ) In [6]: # Helper function to replace x_1 with nan at specified indices def create_missing ( missing_indices , df = df ): df_new = df . copy () df_new . loc [ missing_indices , 'x1' ] = np . nan return df_new Fill in the blank to match the index sets above (missing_A, B, or C) with the type of missingness they represent. In [7]: ### edTest(test_missing_type) ### # Missing completely at random (MCAR) df_mcar = create_missing ( missing_indices = ___ ) # Missing at random (MAR) df_mar = create_missing ( missing_indices = ___ ) # Missing not at random (MNAR) df_mnar = create_missing ( missing_indices = ___ ) First, let's fit a model with no missing data. In [8]: # no missingness: on the full dataset ols = LinearRegression () . fit ( df [[ 'x1' , 'x2' ]], df [ 'y' ]) print ( 'No missing data:' , ols . intercept_ , ols . coef_ ) â¸ Q1.1 Why aren't the estimates exactly $\\hat{\\beta_0} = 0$, $\\hat{\\beta}_1 = 3$ and $\\hat{\\beta}_2 = -2$ ? Isn't that our true data generating function? your answer here Now, let's naively fit a linear regression on the dataset with MCAR missingness and see what happens... In [9]: # Fit inside a try/except block just in case... try : ouch = LinearRegression () . fit ( df_mcar [[ 'x1' , 'x2' ]], df_mcar [ 'y' ]) except Exception as e : print ( e ) â¸ Q1.2 How did sklearn handle the missingness? (feel free to add some code above to experiment if you are still unsure) A : It ignored the columns with missing values\\ B : It ignored the rows with missing values\\ C : It didn't handle the missingness and the fit failed In [10]: ### edTest(test_Q1_2) ### # Submit an answer choice as a string below # (Eg. if you choose option A, put 'A') answer1_2 = '___' â¸ Q1.3 What would be a first naive approach to handling missingness? your answer here What happens if we ignore problematic rows? In [11]: # MCAR: drop the rows that have any missingness ols_mcar = LinearRegression () . fit ( df_mcar . dropna ()[[ 'x1' , 'x2' ]], df_mcar . dropna ()[ 'y' ]) print ( 'MCAR (drop):' , ols_mcar . intercept_ , ols_mcar . coef_ ) Use the same strategy for the other types of missingness. In [12]: ### edTest(test_mar) ### # MAR: drop the rows that have any missingness ols_mar = LinearRegression () . fit ( df_mar . dropna ()[[ ___ ]], df_mar . dropna ()[ __ ]) print ( 'MAR (drop):' , ols_mar . intercept_ , ols_mar . coef_ ) In [13]: # MNAR: drop the rows that have any missingness ols_mnar = LinearRegression () . fit ( ___ , ___ ) print ( 'MNAR (drop):' , ols_mnar . intercept_ , ols_mnar . coef_ ) â¸ï¸ Q2 Compare the various estimates above and how well they were able to recover the value of $\\beta_1$. For which form of missingness did dropping result in the worst estimate? A : MCAR\\ B : MAR\\ C : MNAR In [14]: ### edTest(test_Q2) ### # Submit an answer choice as a string below # (Eg. if you choose option A, put 'A') answer2 = '___' Let's Start Imputing In [15]: # Make backup copies for later since we'll have lots of imputation approaches. X_mcar_raw = df_mcar . drop ( 'y' , axis = 1 ) . copy () X_mar_raw = df_mar . drop ( 'y' , axis = 1 ) . copy () X_mnar_raw = df_mnar . drop ( 'y' , axis = 1 ) . copy () Mean Imputation: Perform mean imputation using the fillna , dropna , and mean functions. In [16]: # Here's an example of one way to do the mean imputation with the above methods X_mcar = X_mcar_raw . copy () X_mcar [ 'x1' ] = X_mcar [ 'x1' ] . fillna ( X_mcar [ 'x1' ] . dropna () . mean ()) ols_mcar_mean = LinearRegression () . fit ( X_mcar , y ) print ( 'MCAR (mean):' , ols_mcar_mean . intercept_ , ols_mcar_mean . coef_ ) In [17]: ### edTest(test_mar_mean) ### X_mar = X_mar_raw . copy () # You can add as many lines as you see fit, so long as the final model is correct ols_mar_mean = ___ print ( 'MAR (mean):' , ols_mar_mean . intercept_ , ols_mar_mean . coef_ ) We can also use SKLearn's SimpleImputer object. By default it will replace NaN values with the column's mean. In [18]: ### edTest(test_mnar_mean) ### X_mnar = X_mnar_raw . copy () # instantiate imputer object imputer = ___ # fit & transform X_mnar with the imputer X_mnar = ___ # fit OLS model on imputed data ols_mnar_mean = ___ print ( 'MNAR (mean):' , ols_mnar_mean . intercept_ , ols_mnar_mean . coef_ ) â¸ï¸ Q3 In our examples, how do these estimates compare when performing mean imputation vs. just dropping rows? A : They are better\\ B : They are worse\\ C : They are the same In [19]: ### edTest(test_Q3) ### # Submit an answer choice as a string below # (Eg. if you choose option A, put 'A') answer3 = '___' Linear Regression Imputation If you're not careful, it can be difficult to keep things straight. There are two models here: an imputation model concerning just the predictors (to predict $X_1$ from $X_2$) and the substantive model we really care about used to predict $Y$ from the 'improved' $X_1$ (now with imputed values) and $X_2$. In [20]: X_mcar = X_mcar_raw . copy () # Fit the imputation model ols_imputer_mcar = LinearRegression () . fit ( X_mcar . dropna ()[[ 'x2' ]], X_mcar . dropna ()[ 'x1' ]) # Perform some imputations yhat_impute = pd . Series ( ols_imputer_mcar . predict ( X_mcar [[ 'x2' ]])) X_mcar [ 'x1' ] = X_mcar [ 'x1' ] . fillna ( yhat_impute ) # Fit the model we care about ols_mcar_ols = LinearRegression () . fit ( X_mcar , y ) print ( 'MCAR (OLS):' , ols_mcar_ols . intercept_ , ols_mcar_ols . coef_ ) In [21]: ### edTest(test_mar_ols) ### X_mar = X_mar_raw . copy () # Fit imputation model ols_imputer_mar = LinearRegression () . fit ( ___ , ___ ) # Get values to be imputed yhat_impute = pd . Series ( ols_imputer_mar . predict ( ___ )) # Fill missing values with imputer's predictions X_mar [ 'x1' ] = X_mar [ 'x1' ] . fillna ( ___ ) # Fit our final, 'substantive' model ols_mar_ols = LinearRegression () . fit ( ___ , ___ ) print ( 'MAR (OLS):' , ols_mar_ols . intercept_ , ols_mar_ols . coef_ ) In [22]: ### edTest(test_mnar_ols) ### X_mnar = X_mnar_raw . copy () # your code here # You can add as many lines as you see fit, so long as the final model is correct ols_mnar_ols = ___ print ( 'MNAR (OLS):' , ols_mnar_ols . intercept_ , ols_mnar_ols . coef_ ) â¸ï¸ Q4 : Compare the estimates when performing OLS model-based imputation vs. mean imputation? Which type of missingness saw the biggest improvement? A : MCAR\\ B : MAR\\ C : MNAR In [23]: ### edTest(test_Q4) ### # Submit an answer choice as a string below # (Eg. if you choose option A, put 'A') answer4 = '___' $k$-NN Imputation ($k$=3) As an alternative to linear regression, we can also use $k$-NN as our imputation model.\\ SKLearn's KNNImputer object makes this very easy. In [31]: X_mcar = X_mcar_raw . copy () X_mcar = KNNImputer ( n_neighbors = 3 ) . fit_transform ( X_mcar ) ols_mcar_knn = LinearRegression () . fit ( X_mcar , y ) print ( 'MCAR (KNN):' , ols_mcar_knn . intercept_ , ols_mcar_knn . coef_ ) In [32]: ### edTest(test_mar_knn) ### X_mar = X_mar_raw . copy () # Add imputed values to X_mar X_mar = KNNImputer ( ___ ) . fit_transform ( ___ ) # Fit substantive model on imputed data ols_mar_knn = LinearRegression () . fit ( __ , __ ) print ( 'MAR (KNN):' , ols_mar_knn . intercept_ , ols_mar_knn . coef_ ) In [26]: ### edTest(test_mnar_knn) ### X_mnar = X_mnar_raw . copy () # your code here # You can add as many lines as you see fit, so long as the final model is correct ols_mnar_knn = ___ print ( 'MNAR (KNN):' , ols_mnar_knn . intercept_ , ols_mnar_knn . coef_ ) â¸ï¸ Q5 : True or False - While some methods may work better than others depending on the context, any imputation method is better than none (that is, as opposed to simply dropping). In [0]: ### edTest(test_Q5) ### # Submit an answer choice as boolean value answer5 = ___ â¸ï¸ Q6 : Suppose your friends makes the following suggestion: \"The MNAR missing data can be predicted in part from the response $y$. Why not impute these missing $x_1$ values with an imputation model using $y$ as a predictor? It's true we can't impute like this with new data for which we don't have the $y$ values. But it will improve our training data, our model's fit, and so too its performance on new data!\" What is a big problem with this idea? your answer here",
        "tags": "lectures",
        "url": "lectures/lecture09/notebook-1/"
    }, {
        "title": "Lecture 8: Inference in Regression and Hypothesis Testing",
        "text": "Slides Lecture 8 : Inference in Linear Regression (PDF) Lecture 8 : Bootstrapping and Confidence Intervals (PDF) Lecture 8 : Prediction Intervals (PDF) Lecture 8 : Evaluating Significance of Predictors Hypothesis Testing (PDF) Exercises Lecture 8: Exercise: Beta Values for Data from Random Universe [Notebook] Lecture 8: Exercise: Beta Values for Data using Bootstrapping [Notebook] Lecture 8: Exercise: Confidence Intervals for Beta value [Notebook] Lecture 8: Exercise: Computing the CI [Notebook] Lecture 8: Exercise: Hypothesis Testing [Notebook]",
        "tags": "lectures",
        "url": "lectures/lecture08/"
    }, {
        "title": "Lecture 8: Inference in Regression and Hypothesis Testing",
        "text": "s2-ex1 Title : Exercise: Beta Values for Data from Random Universe Description : Given a RandomUniverse(dataframe)->dataframe function that gives a new dataset from a \"parallel\" universe, calculate the $\\beta_0$'s and $\\beta_1$'s and plot a histogram like the one below. Data Description: Instructions: Get a new dataframe using the RandomUniverse function already provided in the exercise Calculate $\\beta_0$, $\\beta_1$ for that particular dataframe Add the calculated $\\beta_0$ and $\\beta_1$ values to a python list Plot a histogram using the lists calculated above Hints: $${\\widehat {\\beta_1 }}={\\frac {\\sum _{i=1}&#94;{n}(x_{i}-{\\bar {x}})(y_{i}-{\\bar {y}})}{\\sum _{i=1}&#94;{n}(x_{i}-{\\bar {x}})&#94;{2}}}$$$${\\widehat {\\beta_0 }}={\\bar {y}}-{\\widehat {\\beta_1 }}\\,{\\bar {x}}$$ plt.subplots() Create a figure and a set of subplots ax.hist() Plot a histogram from a list or series. In [1]: # Import necessary libraries import numpy as np import pandas as pd import matplotlib.pyplot as plt from randomuniverse import RandomUniverse % matplotlib inline In [2]: # Read the advertising dataset as a pandas dataframe df = pd . read_csv ( 'Advertising_adj.csv' ) # Take a quick look at the dataframe df . head () Out[2]: tv sales 0 230.1 465.26 1 44.5 218.95 2 17.2 195.79 3 151.5 389.47 4 180.8 271.58 In [10]: # Create two empty lists that will store the beta values beta0_list , beta1_list = [],[] # Choose the number of \"parallel\" Universes to generate # that many new versions of the dataset parallelUniverses = 10000 # Loop over the maximum number of parallel Universes for i in range ( parallelUniverses ): # Call the RandomUniverse helper function with the dataframe # read from the data file df_new = RandomUniverse ( df ) # Find the mean of the predictor values i.e. tv xmean = df_new . tv . mean () # Find the mean of the response values i.e. sales ymean = df_new . sales . mean () # Compute the analytical values of beta0 and beta1 using the # equation given in the hints beta1 = np . sum (( df_new . tv - xmean ) * ( df_new . sales - ymean )) / np . sum (( df_new . tv - xmean ) ** 2 ) beta0 = ymean - beta1 * xmean # Append the calculated values of beta1 and beta0 to the appropriate lists beta0_list . append ( beta0 ) beta1_list . append ( beta1 ) In [11]: ### edTest(test_beta) ### # Compute the mean of the beta values beta0_mean = np . mean ( beta0_list ) beta1_mean = np . mean ( beta1_list ) In [12]: # Plot histograms of beta_0 and beta_1 using lists created above fig , ax = plt . subplots ( 1 , 2 , figsize = ( 18 , 8 )) ax [ 0 ] . hist ( beta0_list ) ax [ 1 ] . hist ( beta1_list ) ax [ 0 ] . set_xlabel ( 'Beta 0' ) ax [ 1 ] . set_xlabel ( 'Beta 1' ) ax [ 0 ] . set_ylabel ( 'Frequency' ); Out[12]: Text(0, 0.5, 'Frequency') â¸ Increase the number of parallelUniverses . Which of the following do you observe? A. The spread increases B. The frequency of points decreases C. The spread decreases D. There is no change In [13]: ### edTest(test_chow1) ### # Submit an answer choice as a string below # (Eg. if you choose option C, put 'C') answer1 = 'D' In [0]:",
        "tags": "lectures",
        "url": "lectures/lecture08/notebook-1/"
    }, {
        "title": "Lecture 8: Inference in Regression and Hypothesis Testing",
        "text": "s2-ex2 Title : Exercise: Beta Values for Data using Bootstrapping Description : Solve the previous exercise by building your own bootstrap function. Data Description: Instructions: Define a function bootstrap that takes a dataframe as the input. Use NumPy's random.randint() function to generate random integers in the range of the length of the dataset. These integers will be used as the indices to access the rows of the dataset. Similar to the previous exercise, compute the $\\beta_0$ and $\\beta_1$ values for each instance of the dataframe. Plot the $\\beta_0$, $\\beta_1$ histograms. Hints: To compute the beta values use the following equations: $\\beta_{0}=\\bar{y}-\\left(b_{1} * \\bar{x}\\right)$ $\\beta_{1}=\\frac{\\sum(x-\\bar{x}) *(y-\\bar{y})}{\\sum(x-\\bar{x})&#94;{2}}$ where $\\bar{x}$ is the mean of $x$ and $\\bar{y}$ is the mean of $y$ np.random.randint() Returns list of integers as per mentioned size np.dot() Computes the dot product of two arrays df.iloc[] Purely integer-location based indexing for selection by position ax.hist() Plots a histogram ax.set_xlabel() Sets label for x-axthe is ax.set_ylabel() Sets label for the y-axis Note: This exercise is auto-graded and you can try multiple attempts. In [1]: # Import necessary libraries import pandas as pd import numpy as np import matplotlib.pyplot as plt from randomuniverse import RandomUniverse % matplotlib inline In [2]: # Read the file \"Advertising_csv\" df = pd . read_csv ( 'Advertising_adj.csv' ) # Take a quick look at the data df . head () Out[2]: tv sales 0 230.1 465.26 1 44.5 218.95 2 17.2 195.79 3 151.5 389.47 4 180.8 271.58 In [4]: # Define a bootstrap function, which takes as input a dataframe # It must output a bootstrapped version of the input dataframe def bootstrap ( df ): selectionIndex = np . random . randint ( 0 , df . shape [ 0 ], size = df . __len__ ()) new_df = df . iloc [ selectionIndex ] return new_df Alternate approach to $\\beta$ computation In [18]: # Initialize two empty lists to store the beta values beta0_list , beta1_list = [],[] # Choose the number of \"parallel\" Universes to generate the new dataset number_of_bootstraps = 1000 # Loop through the number of bootstraps for i in range ( number_of_bootstraps ): # Call the bootstrap function to get a bootstrapped version of the data df_new = bootstrap ( df ) # Find the mean of the predictor values i.e. tv xmean = df_new . tv . mean () # Find the mean of the response values i.e. sales ymean = df_new . sales . mean () #'X' is the predictor variable given by df_new.tv values X = df_new . tv #'y' is the reponse variable given by df_new.sales values y = df_new . sales # Compute the analytical values of beta0 and beta1 using the # equation given in the hints beta1 = ((( df_new . tv - xmean ) * ( df_new . sales - ymean )) . sum ()) / ((( df_new . tv - xmean ) ** 2 ) . sum ()) beta0 = ymean - beta1 * xmean # Append the calculated values of beta1 and beta0 to the appropriate lists beta0_list . append ( beta1 ) beta1_list . append ( beta0 ) In [19]: ### edTest(test_beta) ### # Compute the mean of the beta values beta0_mean = np . mean ( beta0_list ) beta1_mean = np . mean ( beta1_list ) In [20]: # Plot histograms of beta_0 and beta_1 using lists created above fig , ax = plt . subplots ( 1 , 2 , figsize = ( 18 , 8 )) ax [ 0 ] . hist ( beta0_list ) ax [ 1 ] . hist ( beta1_list ) ax [ 0 ] . set_xlabel ( 'beta 0' ) ax [ 1 ] . set_xlabel ( 'beta 1' ) ax [ 0 ] . set_ylabel ( 'Frequency' ) plt . show (); Compare the plots with the results from the RandomUniverse() function In [21]: # Helper code to visualise the similarity between the bootstrap # function here & the RandomUniverse() function from last exercise beta0_randUni , beta1_randUni = [],[] parallelUniverses = 1000 for i in range ( parallelUniverses ): df_new = RandomUniverse ( df ) xmean = df_new . tv . mean () ymean = df_new . sales . mean () # Using linear algebra result as discussed in lecture beta1 = ((( df_new . tv - xmean ) * ( df_new . sales - ymean )) . sum ()) / ((( df_new . tv - xmean ) ** 2 ) . sum ()) beta0 = ymean - beta1 * xmean beta0_randUni . append ( beta0 ) beta1_randUni . append ( beta1 ) In [22]: # Helper code to plot the bootstrapped beta values & the ones from random universe def plotmulti ( list1 , list2 ): fig , axes = plt . subplots ( 1 , 2 , figsize = ( 10 , 4 ), sharey = 'row' ) axes [ 0 ] . hist ( list1 ); axes [ 0 ] . set_xlabel ( 'Beta Distribution' ) axes [ 0 ] . set_ylabel ( 'Frequency' ) axes [ 0 ] . set_title ( 'Bootstrap' ) axes [ 1 ] . hist ( list2 ); axes [ 1 ] . set_xlabel ( 'Beta Distribution' ) axes [ 1 ] . set_title ( 'Random Universe' ) plt . show (); In [23]: # Call the 'plotmulti' function above to compare the two histograms for beta0 plotmulti ( beta0_list , beta0_randUni ) In [24]: # Call the 'plotmulti' function above to compare the two histograms for beta1 plotmulti ( beta1_list , beta1_randUni ) In [0]:",
        "tags": "lectures",
        "url": "lectures/lecture08/notebook-2/"
    }, {
        "title": "Lecture 8: Inference in Regression and Hypothesis Testing",
        "text": "s2-ex3 Title : Exercise: Confidence Intervals for Beta value Description : The goal of this exercise is to create a plot like the one given below for $\\beta_0$ and $\\beta_1$. Data Description: Instructions: Follow the steps from the previous exercise to get the lists of beta values. Sort the list of beta values in ascending order (from low to high). To compute the 95% confidence interval, find the 2.5 percentile and the 97.5 percentile using np.percentile() . Use the helper code plot_simulation() to visualise the $\\beta$ values along with its confidence interval Hints: $${\\widehat {\\beta_1 }}={\\frac {\\sum _{i=1}&#94;{n}(x_{i}-{\\bar {x}})(y_{i}-{\\bar {y}})}{\\sum _{i=1}&#94;{n}(x_{i}-{\\bar {x}})&#94;{2}}}$$$${\\widehat {\\beta_0 }}={\\bar {y}}-{\\widehat {\\beta_1 }}\\,{\\bar {x}}$$ np.random.randint() Returns list of integers as per mentioned size df.iloc[] Purely integer-location based indexing for selection by position plt.hist() Plots a histogram plt.axvline() Adds a vertical line across the axes plt.axhline() Add a horizontal line across the axes plt.xlabel() Sets the label for the x-axis plt.ylabel() Sets the label for the y-axis plt.legend() Place a legend on the axes ndarray.sort() Returns the sorted ndarray. np.percentile(list, q) Returns the q-th percentile value based on the provided ascending list of values. Note: This exercise is auto-graded and you can try multiple attempts . In [1]: # Import necessary libraries import numpy as np import pandas as pd import matplotlib.pyplot as plt % matplotlib inline Reading the standard Advertising dataset In [2]: # Read the 'Advertising_adj.csv' file df = pd . read_csv ( 'Advertising_adj.csv' ) # Take a quick look at the data df . head ( 3 ) In [7]: # Use the bootstrap function defined in the previous exercise def bootstrap ( df ): selectionIndex = np . random . randint ( len ( df ), size = len ( df )) new_df = df . iloc [ selectionIndex ] return new_df In [8]: # Initialize empty lists to store beta values from 100 bootstraps # of the original data beta0_list , beta1_list = [],[] # Set the number of bootstraps numberOfBootstraps = 100 # Loop over the number of bootstraps for i in range ( numberOfBootstraps ): # Call the function bootstrap with the original dataframe df_new = bootstrap ( df ) # Compute the mean of the predictor i.e. the TV column xmean = df_new . tv . mean () # Compute the mean of the response i.e. the Sales column ymean = df_new . sales . mean () # Compute beta1 analytical using the equation in the hints beta1 = ((( df_new . tv - xmean ) * ( df_new . sales - ymean )) . sum ()) / ((( df_new . tv - xmean ) ** 2 ) . sum ()) # Compute beta1 analytical using the equation in the hints beta0 = ymean - beta1 * xmean # Append the beta values to their appropriate lists beta0_list . append ( beta0 ) beta1_list . append ( beta1 ) In [9]: ### edTest(test_sort) ### # Sort the two lists of beta values from the lowest value to highest beta0_list . ___ ; beta1_list . ___ ; In [10]: ### edTest(test_beta) ### # Find the 95% percent confidence for beta0 interval using the # percentile function beta0_CI = ( np . ___ , np . ___ ) # Find the 95% percent confidence for beta1 interval using the # percentile function beta1_CI = ( np . ___ , np . ___ ) In [0]: # Print the confidence interval of beta0 upto 3 decimal points print ( f 'The beta0 confidence interval is { ___ } ' ) In [0]: # Print the confidence interval of beta1 upto 3 decimal points print ( f 'The beta1 confidence interval is { ___ } ' ) In [15]: # Helper function to plot the histogram of beta values along with # the 95% confidence interval def plot_simulation ( simulation , confidence ): plt . hist ( simulation , bins = 30 , label = 'beta distribution' , align = 'left' , density = True ) plt . axvline ( confidence [ 1 ], 0 , 1 , color = 'r' , label = 'Right Interval' ) plt . axvline ( confidence [ 0 ], 0 , 1 , color = 'red' , label = 'Left Interval' ) plt . xlabel ( 'Beta value' ) plt . ylabel ( 'Frequency' ) plt . title ( 'Confidence Interval' ) plt . legend ( frameon = False , loc = 'upper right' ) In [0]: # Call the function plot_simulation to get the histogram for beta 0 # with the confidence interval plot_simulation ( ___ , ___ ) In [0]: # Call the function plot_simulation to get the histogram for beta 1 # with the confidence interval plot_simulation ( ___ , ___ )",
        "tags": "lectures",
        "url": "lectures/lecture08/notebook-3/"
    }, {
        "title": "Lecture 8: Inference in Regression and Hypothesis Testing",
        "text": "s3-exd1-challenge Title : Exercise: Hypothesis Testing Description : The goal of this exercise is to identify the relevant features of the dataset using Hypothesis testing and to plot a bar plot like the one given below: Data Description: Instructions: Read the file Advertising.csv as a dataframe. Fit a simple multi-linear regression with \"medv\" as the response variable and the remaining columns as the predictor variables. Compute the coefficients of the model and plot a bar chart to depict these values. To find the distributions of the coefficients perform bootstrap. For each bootstrap: Fit a simple multi-linear regression with the same conditions as before. Compute the coefficient values and store as a list. Compute the |t|âˆ£tâˆ£ values for each of the coefficient value in the list. Plot a bar chart of the varying |t|âˆ£tâˆ£ values. Compute the p-value from the |t|âˆ£tâˆ£ values. Plot a bar chart of 1-p1âˆ’p values of the coefficients. Also mark the 0.95 line on the chart as shown above. Hints: pd.read_csv(filename) Returns a pandas dataframe containing the data and labels from the file data sklearn.preprocessing.normalize() Scales input vectors individually to unit norm (vector length). np.interp() Returns one-dimensional linear interpolation sklearn.train_test_split() Splits the data into random train and test subsets sklearn.LinearRegression() LinearRegression fits a linear model sklearn.fit() Fits the linear model to the training data sklearn.predict() Predict using the linear model. Note: This exercise is auto-graded and you can try multiple attempts . In [2]: # Import necessary libraries % matplotlib inline import numpy as np import pandas as pd from scipy import stats import matplotlib.pyplot as plt from sklearn import preprocessing from sklearn.linear_model import Lasso from sklearn.linear_model import Ridge from sklearn.metrics import mean_squared_error from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.preprocessing import PolynomialFeatures In [3]: # Read the file \"Advertising.csv\" as a dataframe df = pd . read_csv ( \"Advertising.csv\" , index_col = 0 ) # Take a quick look at the dataframe df . head () In [333]: # Get all the columns except 'sales' as the predictors X = df . drop ([ 'sales' ], axis = 1 ) # Select 'sales' as the response variable y = df [ 'sales' ] In [0]: # Initialize a linear regression model with normalize=True lreg = LinearRegression ( normalize = True ) # Fit the model on the entire data lreg . fit ( X , y ) In [335]: # Get the coefficient of each predictor as a dictionary coef_dict = dict ( zip ( df . columns [: - 1 ], np . transpose ( lreg . coef_ ))) predictors , coefficients = list ( zip ( * sorted ( coef_dict . items (), key = lambda x : x [ 1 ]))) In [0]: # Helper code to visualize the coefficients of all predictors fig , ax = plt . subplots () ax . barh ( predictors , coefficients , align = 'center' , color = \"#336600\" , alpha = 0.7 ) ax . grid ( linewidth = 0.2 ) ax . set_xlabel ( \"Coefficient\" ) ax . set_ylabel ( \"Predictors\" ) plt . show () In [337]: # Helper function to compute the t-statistic def get_t ( arr ): means = np . abs ( arr . mean ( axis = 0 )) stds = arr . std ( axis = 0 ) return np . divide ( means , stds ) In [338]: # Initialize an empty list to store the coefficient values coef_dist = [] # Set the number of bootstraps numboot = 1000 # Loop over the all the bootstraps for i in range ( ___ ): # Get a bootstrapped version of the dataframe df_new = df . sample ( frac = 1 , replace = True ) # Get all the columns except 'sales' as the predictors X = df_new . drop ( ___ , axis = 1 ) # Select 'sales' as the response variable y = df_new [ ___ ] # Initialize a linear regression model with normalize=True lreg = LinearRegression ( normalize = ___ ) # Fit the model on the entire data lreg . fit ( ___ , ___ ) # Append the coefficients of all predictors to the list coef_dist . append ( lreg . coef_ ) # Convert the list to a numpy array coef_dist = np . array ( coef_dist ) In [339]: # Use the helper function get_t to find the T-test values tt = get_t ( ___ ) n = df . shape [ 0 ] In [340]: # Get the t-value associated with each predictor tt_dict = dict ( zip ( df . columns [: - 1 ], tt )) predictors , tvalues = list ( zip ( * sorted ( tt_dict . items (), key = lambda x : x [ 1 ]))) In [0]: # Helper code below to visualise the t-values fig , ax = plt . subplots () ax . barh ( predictors , tvalues , align = 'center' , color = \"#336600\" , alpha = 0.7 ) ax . grid ( linewidth = 0.2 ) ax . set_xlabel ( \"T-test values\" ) ax . set_ylabel ( \"Predictors\" ) plt . show (); In [342]: ### edTest(test_pval) ### # From t-test values compute the p values using scipy.stats # T-distribution function pval = stats . t . sf ( tt , n - 1 ) * 2 # Here we use sf i.e 'Survival function' which is 1 - CDF of the t distribution. # We also multiply by two because its a two tailed test. # Please refer to lecture notes for more information # Since p values are in reversed order, we find the 'confidence' # which is 1-p conf = ___ In [343]: # Get the 'confidence' values associated with each predictor conf_dict = dict ( zip ( df . columns [: - 1 ], conf )) predictors , confs = list ( zip ( * sorted ( conf_dict . items (), key = lambda x : x [ 1 ]))) In [0]: # Helper code below to visualise the confidence values fig , ax = plt . subplots () ax . barh ( predictors , confs , align = 'center' , color = \"#336600\" , alpha = 0.7 ) ax . grid ( linewidth = 0.2 ) ax . axvline ( x = 0.95 , linewidth = 3 , linestyle = '--' , color = 'black' , alpha = 0.8 , label = '0.95' ) ax . set_xlabel ( \"$1-p$ value\" ) ax . set_ylabel ( \"Predictors\" ) ax . legend () plt . show ();",
        "tags": "lectures",
        "url": "lectures/lecture08/notebook-4/"
    }, {
        "title": "Lecture 8: Inference in Regression and Hypothesis Testing",
        "text": "s3-exd2-challenge Title : Exercise: Computing the CI Description : You are the manager of the Advertising division of your company, and your boss asks you the question, \"How much more sales will we have if we invest $1000 dollars in TV advertising?\" The goal of this exercise is to estimate the Sales with a 95% confidence interval using the Advertising.csv dataset. Data Description: Instructions: Read the file Advertising.csv as a dataframe. Fix a budget amount of 1000 dollars for TV advertising as variable called Budget. Select the number of bootstraps. For each bootstrap: Select a new dataframe with the predictor as TV and the response as Sales. Fit a simple linear regression on the data. Predict on the budget and compute the error estimate using the helper function error_func() . Store the sales as a sum of the prediction and the error estimate and append to sales_list . Sort the sales_list which is a distribution of predicted sales over numboot bootstraps. Compute the 95% confidence interval of sales_list . Use the helper function plot_simulation to visualize the distribution and print the estimated sales. Hints: np.random.randint() Returns list of integers as per mentioned size df.sample() Get a new sample from a dataframe plt.hist() Plots a histogram plt.axvline() Adds a vertical line across the axes plt.axhline() Add a horizontal line across the axes plt.legend() Place a legend on the axes ndarray.sort() Returns the sorted ndarray. np.percentile(list, q) Returns the q-th percentile value based on the provided ascending list of values. Note: This exercise is auto-graded and you can try multiple attempts . In [2]: # Import necessary libraries % matplotlib inline import numpy as np import pandas as pd from scipy import stats import matplotlib.pyplot as plt from sklearn import preprocessing from sklearn.metrics import mean_squared_error from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.preprocessing import PolynomialFeatures In [95]: # Read the `Advertising.csv` dataframe df = pd . read_csv ( 'Advertising.csv' ) # Take a quick look at the data df . head () In [134]: # Helper function to compute the variance of the error term def error_func ( y , y_p ): n = len ( y ) return np . sqrt ( np . sum (( y - y_p ) ** 2 / ( n - 2 ))) In [147]: # Set the number of bootstraps numboot = 1000 # Set the budget as per the instructions given # Use 2D list to facilitate model prediction (sklearn.LinearRegression requires input as a 2d array) budget = [[ ___ ]] # Initialize an empty list to store sales predictions for each bootstrap sales_list = [] In [148]: # Loop through each bootstrap for i in range ( ___ ): # Create bootstrapped version of the data using the sample function # Set frac=1 and replace=True to get a bootstrap df_new = df . sample ( ___ , replace = ___ ) # Get the predictor data ('TV') from the new bootstrapped data x = df_new [[ ___ ]] # Get the response data ('Sales') from the new bootstrapped data y = df_new . ___ # Initialize a Linear Regression model linreg = LinearRegression () # Fit the model on the new data linreg . fit ( ___ , ___ ) # Predict on the budget from the original data prediction = linreg . predict ( budget ) # Predict on the bootstrapped data y_pred = linreg . predict ( x ) # Compute the error using the helper function error_func error = np . random . normal ( 0 , error_func ( y , y_pred )) # The final sales prediction is the sum of the model prediction # and the error term sales = ___ # Convert the sales to float type and append to the list sales_list . append ( np . float64 ( ___ )) In [137]: ### edTest(test_sales) ### # Sort the list containing sales predictions in ascending order sales_list . sort () # Find the 95% confidence interval using np.percentile function # at 2.5% and 97.5% sales_CI = ( np . percentile ( ___ , ___ ), np . percentile ( ___ , ___ )) In [138]: # Helper function to plot the histogram of beta values along # with the 95% confidence interval def plot_simulation ( simulation , confidence ): plt . hist ( simulation , bins = 30 , label = 'beta distribution' , align = 'left' , density = True , edgecolor = 'k' ) plt . axvline ( confidence [ 1 ], 0 , 1 , color = 'r' , label = 'Right Interval' ) plt . axvline ( confidence [ 0 ], 0 , 1 , color = 'red' , label = 'Left Interval' ) plt . xlabel ( 'Beta value' ) plt . ylabel ( 'Frequency' ) plt . legend ( frameon = False , loc = 'upper right' ) plt . show (); In [0]: # Call the plot_simulation function above with the computed sales # distribution and the confidence intervals computed earlier plot_simulation ( sales_list , sales_CI ) In [0]: # Print the computed values print ( f \"With a TV advertising budget of $ { budget [ 0 ][ 0 ] } ,\" ) print ( f \"we can expect an increase of sales anywhere between { sales_CI [ 0 ] : 0.2f } to { sales_CI [ 1 ] : .2f } \\ with a 95% confidence interval\" ) â¸ The sales predictions here is based on the Simple-Linear regression model between TV and Sales . Re-run the above exercise by fitting the model considering all variables in Advertising.csv . Keep the budget the same, i.e $1000 for 'TV' advertising. You may have to change the budget variable to something like [[1000,0,0]] for proper computation. Does your predicted sales interval change? Why, or why not? In [149]: ### edTest(test_chow1) ### # Type your answer within in the quotes given answer1 = '___'",
        "tags": "lectures",
        "url": "lectures/lecture08/notebook-5/"
    }, {
        "title": "Lecture 7: Probability",
        "text": "Slides Lecture 7 : From Probability to Maximum Likelihood Estimation (MLE) (PDF) Lecture 7 : Debug_Except (PDF) Exercises Lecture 7: Exercise: CS109A Olympics [Notebook] Lecture 7: Exercise: CS109A Olympics - Solution [Notebook]",
        "tags": "lectures",
        "url": "lectures/lecture07/"
    }, {
        "title": "Lecture 7: Probability",
        "text": "100mDash Title : Exercise: CS109A Olympics Description : Data Description: Instructions: In this exercise, you will simulate the 100m sprint race discussed during the lecture. We have already defined for you a Sprinter() class which has two characteristics for each sprinter: Base time Performance variance Run the code cell that makes four instances of the Sprinter() class. You will work with those for the entire exercise. Call the time attribute of the helper class to get the time taken by a competitor in the actual race. First run the race simulation five times; you will do this by creating a dictionary with participant name as keys, and time taken in a simulated race as the values. You will sort this dictionary by values and determine the winner of the simulated race. Repeat the simulation of the race for 10,000 times and count who won the race for how many times. Based on this observation, you will then investigate why a particular participant won as many times? Repeat the simulation for 10,000 times, but this time get the distribution of times for each participant over these runs. Calculate the mean race time, standard deviation of the race time and the confidence interval for each participant. Use the helper code to observe a plot similar to the one given below: Hints: Counter() Helps accumulating counts of objects in a certain data structure. np.mean() Used to calculate the mean of an array. sorted() Used to sort data. np.std() Used to calculate the std deviation of an array. np.percentile Used to calculate percentile of data inbetween a given range. Frequently used for calculating confidence intervals. CS109A Olympics : 100m dash We are going to have 4 of our team members compete against each other in the 100m dash. In [1]: # Importing libraries import numpy as np from time import sleep import os from IPython.display import clear_output from collections import Counter from helper import Sprinter from helper import run_sim import matplotlib.pyplot as plt from prettytable import PrettyTable plt . xkcd ( scale = 0 , randomness = 4 ) Out[1]: <matplotlib.pyplot._xkcd at 0x7f8dbe7bdfd0> Taking a look at the competitors Each participant has a characteristic assigned to them. The characteristic has 2 parts : Base speed : This is the time they gave in a non-competitive environment. Performance variance : Based on the mood, weather and other conditions this measure determines how much a participant's time will vary. In [2]: # Name of sprinters sprinters = [ 'Pavlos' , 'Tale' , 'Varshini' , 'Hayden' ] # Defining charactersistics, ('Base pace','performance variance') characteristics = [( 13 , 0.25 ),( 12.5 , 0.5 ),( 12.25 , 1 ),( 14.5 , 1 )] sprinters_dict = {} for idx , sprinter in enumerate ( sprinters ): # Take note of the * before characteristics sprinters_dict [ sprinter ] = Sprinter ( * characteristics [ idx ]) Running a race sprinters_dict has keys as the name of each participant, and the value as a class. The time attribute of the class is the time taken by that person to run a race. Call sprinters_dict['Pavlos'].time for 10 different times. In [3]: # Call time attribute ___ â¸ Pause & Think Run the cell above, once again. What do you observe? A. Output is different because the python compile memory location has changed B. Output is the same C. Output changes because it is a new sample from random process In [0]: ### edTest(test_chow0) ### # Submit an answer choice as a string below (eg. if you choose option A put 'A') answer = '___' Get the times for each participant by calling the time attribute and create a dictionary called race , which has the key as the name of the participant and value as the time taken by participant to run the race. Sort race.items() according to time and get the item in dictionary with the least time taken to finish and assign it to winner . In [4]: ### edTest(test_race) ### # Get the times for each participant and make a dictionary race = ___ # Sort the items of the dictionary to get the winner # Hint: Remember to sort by the values and not the keys winner = ___ Race simulation As you would have noticed, every time you make a new dictionary race , the results would differ. Redefine the race dictionary, and run the cell below for a simulation of the race! In [5]: # Get the times for each participant and make a dictionary race = { sprinter : dash . time for sprinter , dash in sprinters_dict . items ()} # Sort the items of the dictionary to get the winner winner = sorted ( race . items (), key = lambda x : x [ 1 ])[ 0 ] # Uncomment and execute the following code # run_sim(race,winner) Multiple simulations Earlier was just one race, we want to find out who performs better over multiple races. So let's run the race 5 times Run a loop for 5 times In each loop generate the race dictionary as done earlier, and get the winner after sorting race.items() Append name of the winners to the winner_list Keep track of everyone's timings In [6]: # Run the simulation and append winners to the winner_list # Create an empty list winner_list = [] # Run a simulation for 5 loops for simulation in range ( 5 ): # Create a race dictionary race = { k : v . time for k , v in sprinters_dict . items ()} # Sort the items winner = sorted ( race . items (), key = lambda x : x [ 1 ])[ 0 ] # Append the name of the winner to winners_list winner_list . append ( winner ) # Take a look at the winners list winner_list Even more simulations We will run 10,000 simulations and use the Counter to see who wins how many times. Check the hints for how to use Counter() . In [7]: # Run the simulation and append winners to the winner_list # Create an empty list winner_list = [] # Run a simulation for 10000 loops for simulation in range ( 10000 ): # Create race dictionary race = { k : v . time for k , v in sprinters_dict . items ()} # Sort the items winner = sorted ( race . items (), key = lambda x : x [ 1 ])[ 0 ] # Append the name of the winner to winners_list winner_list . append ( winner [ 0 ]) # Display first 5 entries from winner_list winner_list___ In [8]: ### edTest(test_wins) ### # Get the counts for each person winning the race # Hint: Use counter, look at the hints wins = Counter ( winner_list ) # Print wins to see the output of the simulation print ( ___ ) In [9]: # Helper code to plot the wins of each sprinter plt . bar ( list ( wins . keys ()), list ( wins . values ()), alpha = 0.5 ) plt . xlabel ( 'Sprinters' ) plt . ylabel ( 'Race wins' , rotation = 0 , labelpad = 30 ) plt . show (); Why is Varshini winning so much ? Let us analyze why exactly is Varshini winning so frequently in our simulations. But first, we will need to record the sprint timings for each sprinter in every simulation. We will again run 10,000 simulations but this time record the individual sprint timings for each simulation instead. Make a new dictionary race_results with keys as the name of sprinters and the value as an empty list. We will append race results to this list after each simulation. Run a simulation loop for 10000 times In each simulation loop over sprinters_dict.items() and for each participant: Calculate time by calling .time append time to the list for particular key of race_results In [10]: # Run the earlier simulation loop for 10000 times # Loop over the sprinters_dict items and for each participant # Call time and append to the corresponding list in race_results race_results = { k :[] for k in sprinters_dict . keys ()} for simulation in range ( 10000 ): for sprinter , dash in sprinters_dict . items (): # For a given participant call the .time attribute sprint_timing = dash . time race_results [ sprinter ] . append ( sprint_timing ) Sample mean $\\bar{x}$ sample standard deviation $s$ Now we have a list of times given by each participant. We have the complete distribution, so let's calculate the mean, standard deviation and confidence interval. As discussed in the lecture, if we have a given sample, we can quickly compute the mean and standard deviation using np.mean() and np.std() . Let's begin with the race results for Pavlos . In [11]: # Using the race_results dictionary, find the mean # and std for 'Pavlos' pavlos_mean = ___ pavlos_std = ___ print ( f 'The average pace of Pavlos is { pavlos_mean : .2f } and the sample std is { pavlos_std : 2f } ' ) Sample mean $\\bar{x}$ sample standard deviation $s$ for all sprinters For each sprinter in the race_results dicitionary, find the mean and standard deviation of the 10,000 simulations using the np.mean() and np.std() functions. Store your findings in a new dictionary called race_stats . In [12]: # Calculate mean and std of each participant # Initialize an empty dictionary race_stats = {} # Loop over race_results.keys() for sprinter in race_results . keys (): sprinter_mean = np . mean ( race_results [ sprinter ]) sprinter_std = np . std ( race_results [ sprinter ]) # Store it as a list [mean,std] corresponding to each # participant key in race_stats race_stats [ sprinter ] = [ sprinter_mean , sprinter_std ] In [13]: # Use the helper code below to print your findings pt = PrettyTable () pt . field_names = [ \"Sprinter\" , \"Sample mean\" , \"Sample std\" ] for sprinter , stats in race_stats . items (): pt . add_row ([ sprinter , round ( stats [ 0 ], 3 ), round ( stats [ 1 ], 3 )]) print ( pt ) Confidence Interval Confidence interval is the range of values for which we can claim a certain confidence level(95% mostly). The confidence interval represents values for the population parameter for which the difference between the parameter and the observed estimate is not significant at the 5% level. Calculate the 95% CI by getting np.percentile at 2.5 and 97.5. Calculate and append these to the list of stats in the race_stats dictionary, for each participant In [14]: #By using the race_results dictionary defined above, # Find the 2.5 and 97.5 percentile of Tale's race runs. # Hint : Use race_results['Tale's'] CI = np . percentile ( ___ ,[ ___ , ___ ]) print ( f 'The 95% confidence interval for Tale is { round ( CI [ 0 ], 2 ), round ( CI [ 1 ], 2 ) } ' ) Confidence intervals for all sprinters. Let's repeat the above for each sprinter. You will add this information to your race_stats dictionary. We expect you to extend stats list with the $2.5$ and the $97.5$ percentile values for each sprinter. For e.g., if for Pavlos , we have mean=13.00 , std=0.1 , and CI as (12.8,13.2) , your race_stats['Pavlos'] must look like: [13.00,0.1,12.8,13.2] . In [15]: # Repeat the same as above, but for every sprinter # run through the race_results dictionary for each sprinter # find the confidence interval, and add it to the race_stats dictionary # defined above for sprinter , runs in race_results . items (): ci = np . percentile ( runs ,[ 2.5 , 97.5 ]) # Hint: You can use the .extend() method to add it to the # existing list of stats race_stats [ sprinter ] . extend ( ci ) In [16]: # Use the helper code below to print your findings pt = PrettyTable () pt . field_names = [ \"Sprinter\" , \"Sample mean\" , \"Sample std\" , \"95% CI\" ] for sprinter , stats in race_stats . items (): mean = round ( stats [ 0 ], 3 ) std = round ( stats [ 1 ], 3 ) confidence_interval = ( round ( stats [ 2 ], 3 ), round ( stats [ 3 ], 3 )) pt . add_row ([ sprinter , mean , std , confidence_interval ]) print ( pt ) Histogram plot for each sprinter Run the following cell to get a cool plot for distribution of times. In [17]: # Helper code to plot the distribution of times fig = plt . gcf () fig . set_size_inches ( 10 , 6 ) bins = np . linspace ( 10 , 17 , 50 ) for sprinter , runs in race_results . items (): height , bins , patches = plt . hist ( runs , bins , alpha = 0.5 , \\ label = sprinter , density = True , edgecolor = 'k' ) plt . fill_betweenx ([ 0 , height . max ()], race_stats [ sprinter ][ 2 ], race_stats [ sprinter ][ 3 ], alpha = 0.2 ) plt . legend ( loc = 'upper left' , fontsize = 16 ) plt . xlabel ( 'Seconds' ) plt . ylabel ( 'Frequency' , rotation = 0 , labelpad = 25 ) ax = plt . gca () ax . spines [ 'right' ] . set_visible ( False ) ax . spines [ 'top' ] . set_visible ( False ) ax . set_title ( 'Time distribution for sprinters' ) plt . show () â¸ Pause & Think Take a look at the histograms for each participant and comment on why do you think is Varshini winning more races? In [0]: ### edTest(test_chow1) ### # Write your answer as a string below answer = '___' â¸ What one parameter should Tale change in order to win more races? Note : Pick one that is most influential A. Improve consistency B. Reduce base time C. Increase base time D. Relax and hydrate before the race In [0]: ### edTest(test_chow2) ### # Submit an answer choice as a string below (eg. if you choose option A put 'A') answer = '___' In [0]: # Before you click mark, please comment out the run_sim function above ðŸ‘©ðŸ»â€ðŸŽ“ Bonus (Not graded) Find out who among has would have the most podium finishes (top 3). In [18]: # Your code here",
        "tags": "lectures",
        "url": "lectures/lecture07/notebook-1/"
    }, {
        "title": "Lecture 7: Probability",
        "text": "simulation_solution Title : Exercise: CS109A Olympics Description : Data Description: Instructions: In this exercise, you will simulate the 100m sprint race discussed during the lecture. We have already defined for you a Sprinter() class which has two characteristics for each sprinter: Base time Performance variance Run the code cell that makes four instances of the Sprinter() class. You will work with those for the entire exercise. Call the time attribute of the helper class to get the time taken by a competitor in the actual race. First run the race simulation five times; you will do this by creating a dictionary with participant name as keys, and time taken in a simulated race as the values. You will sort this dictionary by values and determine the winner of the simulated race. Repeat the simulation of the race for 10,000 times and count who won the race for how many times. Based on this observation, you will then investigate why a particular participant won as many times? Repeat the simulation for 10,000 times, but this time get the distribution of times for each participant over these runs. Calculate the mean race time, standard deviation of the race time and the confidence interval for each participant. Use the helper code to observe a plot similar to the one given below: Hints: Counter() Helps accumulating counts of objects in a certain data structure. np.mean() Used to calculate the mean of an array. sorted() Used to sort data. np.std() Used to calculate the std deviation of an array. np.percentile Used to calculate percentile of data inbetween a given range. Frequently used for calculating confidence intervals. PyDS Olymipics : 100m dash We are going to have 4 of our team members compete against each other in the 100m dash. In [1]: # Importing libraries import numpy as np from time import sleep import os from IPython.display import clear_output from collections import Counter from helper import Sprinter import matplotlib.pyplot as plt from prettytable import PrettyTable plt . xkcd ( scale = 0 , randomness = 4 ) Taking a look at the competitors Each participant has a characteristic assigned to him. The characteristic has 2 parts : Base speed : This is the time they gave in a non-competitive environment. Performance variance : Based on the mood, weather and other conditions this measure determines how much a participant's time will vary. In [2]: # Name of sprinters sprinters = [ 'Pavlos' , 'Hargun' , 'Joy' , 'Hayden' ] # Defining charactersistics, ('Base pace','performance variance') characteristics = [( 13 , 0.25 ),( 12.5 , 0.5 ),( 12.25 , 1 ),( 14.5 , 1 )] sprinters_dict = {} for idx , sprinter in enumerate ( sprinters ): sprinters_dict [ sprinter ] = Sprinter ( * characteristics [ idx ]) Running a race sprinters_dict has keys as the name of each participant, and the value as a class. The time attribute of the class is the time taken by that person to run a race. Call sprinters_dict['Pavlos'].time for 10 different times. In [3]: # Call time attribute ___ Get the times for each participant by calling the time attribute. Create a dictionary called race , which has the key as the name of the participant and value as the time taken by participant to run the race. Sort race.items() according to time and get the item in dictionary with the least time taken to finish and assign it to winner . Note: The time taken by a participant to finish the race is the value of the dictionary so remember to sort by values In [4]: # Get the times for each participant and make a dictionary race = ___ # Then sort the items of the dictionary to get the winner # Hint: Remember to sort by the values and not the keys winner = ___ Race simulation As you would have noticed, every time you make a new dictionary race , the results would differ. Redefine the race dictionary, and run the cell below for a simulation of the race! In [5]: # Again get the times for each participant and make a dictionary race = ___ # Then sort the items of the dictionary to get the winner winner = ___ # Execute the following code for i in range ( 1 , 11 ): clear_output ( wait = True ) print ( \"|START|\" + \" \\n |START|\" . join ([ '----' * min ( 10 , int (( 15 * i ) / race [ runner ])) + ' ' * ( 10 - min ( 10 , int (( 15 * i ) / race [ runner ]))) + '|' + runner for runner in race . keys ()])) sleep ( 0.5 ) print ( f ' \\n The winner is { winner [ 0 ] } with a time of { winner [ 1 ] : .2f } s!' ) Multiple simulations Earlier was just one race, we want to find out who performs better over multiple races. So let's run the race 5 times Run a loop for 5 times In each loop generate the race dictionary as done earlier, and get the winner after sorting race.items() Append winners to the winner_list Keep track of everyone's timings In [6]: # Run the simulation and append winners to the winner_list winner_list = [] for simulation in range ( 5 ): race = ___ winner = ___ ___ winner_list Even more simulations We will run 10,000 simulations and use the Counter to see who wins how many times. Check the hints for how to use Counter() . In [7]: # Run the simulation and append winners to the winner_list ___ In [8]: # Get the counts for each person winning the race wins = Counter ( ___ ) print ( wins ) In [9]: # Execute the code plt . bar ( list ( wins . keys ()), list ( wins . values ()), alpha = 0.5 ) plt . xlabel ( 'Sprinters' ) plt . ylabel ( 'Race wins' , rotation = 0 , labelpad = 30 ) Why is Joy winning so much ? Let us analyze why exactly is Joy winning so frequently in our simulations. But first, we will need to record the sprint timings for each sprinter in every simulation. We will again run 10,000 simulations but this time record the individual sprint timings for each simulation instead. Make a new dictionary race_results with keys as the name of sprinters and the value as an empty list. We will append race results to this list after each simulation. Inside the simulation loop, loop through the items of the race_results dictionary, and for each participant : Calculate time by calling .time append time to the list for participant in race_results In [10]: # Run the earlier simulation and store all 10000 times given by a participant # race_results has a list of times as values for a given key( i.e participant) # So for a key it has a corresponding list of times for that participant. race_results = { ___ : ___ for ___ in sprinters_dict . ___ } for simulation in range ( 10000 ): for sprinter , dash in sprinters_dict . items (): sprint_timing = ___ race_results [ ___ ] . append ( ___ ) Sample mean $\\bar{x}$ sample standard deviation $s$ Now we have a list of times given by each participant. We have the complete distribution, so let's calculate the mean, std and confidence interval. As discussed in the lecture, if we have a given sample, we can quickly compute the mean and standard deviation using np.mean() and np.std() . Let's begin with the race results for Pavlos . In [11]: # Using the race_results dictionary, find the mean # and std for 'Pavlos' pavlos_mean = np . mean ( ___ ) pavlos_std = np . std ( ___ ) print ( f 'The average pace of Pavlos is { pavlos_mean : .2f } and the sample std is { pavlos_std : 2f } ' ) Sample mean $\\bar{x}$ sample standard deviation $s$ for all sprinters For each sprinter in the race_results dicitionary, find the mean and standard deviation of the 10,000 simulations using the np.mean() and np.std() functions. Store your findings in a new dictionary called race_stats as a list. So the race_stats dictionary has a list of corresponding stats for each participant(key) In [12]: # loop through the keys of race_results # calculate mean and std of each participant using np.mean() and np.std() # Assign these stats to the key, as a list race_stats = {} for sprinter in race_results . keys (): sprinter_mean = ___ sprinter_std = ___ race_stats [ sprinter ] = [ ___ , ___ ] In [13]: # Use the helper code below to print your findings pt = PrettyTable () pt . field_names = [ \"Sprinter\" , \"Sample mean\" , \"Sample std\" ] for sprinter , stats in race_stats . items (): pt . add_row ([ sprinter , round ( stats [ 0 ], 3 ), round ( stats [ 1 ], 3 )]) print ( pt ) Confidence Interval Confidence interval is the range of values for which we can claim a certain confidence level(95% mostly). The confidence interval represents values for the population parameter for which the difference between the parameter and the observed estimate is not significant at the 5% level. Use np.percentile() to calculate the 95% CI. Calculate np.percentile at 2.5 and 97.5 to get the interval. Calculate and append these to the list of stats in the race_stats dictionary, for each participant In [14]: #By using the race_results dictionary defined above, # Find the 2.5 and 97.5 percentile of Hargun's race runs. CI = np . percentile ( ___ ,[ ___ , ___ ]) print ( f 'The 95% confidence interval for Hargun is { round ( CI [ 0 ], 2 ), round ( CI [ 1 ], 2 ) } ' ) Confidence intervals for all sprinters. Let's repeat the above for each sprinter. You will add this information to your race_stats dictionary. We expect you to append the $2.5$ and the $97.5$ percentile values to the existing stats list for each sprinter. For e.g., if for Pavlos , we have mean=13.00 , std=0.1 , and CI as (12.8,13.2) , your race_stats['Pavlos'] must look like: [13.00,0.1,12.8,13.2] . In [15]: # Now lets repeat the same, but for every sprinter # run through the race_results dictionary for each sprinter # find the confidence interval, and add it to the race_stats dictionary # defined above # Hint: You can use the .extend() method to add it to the existing list of stats for sprinter , runs in race_results . items (): ci = np . percentile ( ___ ) race_stats [ ___ ] . ___ In [16]: # Use the helper code below to print your findings pt = PrettyTable () pt . field_names = [ \"Sprinter\" , \"Sample mean\" , \"Sample std\" , \"95% CI\" ] for sprinter , stats in race_stats . items (): mean = round ( stats [ 0 ], 3 ) std = round ( stats [ 1 ], 3 ) confidence_interval = ( round ( stats [ 2 ], 3 ), round ( stats [ 3 ], 3 )) pt . add_row ([ sprinter , mean , std , confidence_interval ]) print ( pt ) Histogram plot for each sprinter Run the following cell to get a cool plot for distribution of times. In [17]: fig = plt . gcf () fig . set_size_inches ( 10 , 6 ) bins = np . linspace ( 10 , 17 , 50 ) for sprinter , runs in race_results . items (): height , bins , patches = plt . hist ( runs , bins , alpha = 0.5 , \\ label = sprinter , density = True , edgecolor = 'k' ) plt . fill_betweenx ([ 0 , height . max ()], race_stats [ sprinter ][ 2 ], race_stats [ sprinter ][ 3 ], alpha = 0.2 ) plt . legend ( loc = 'upper left' , fontsize = 16 ) plt . xlabel ( 'Seconds' ) plt . ylabel ( 'Frequency' , rotation = 0 , labelpad = 25 ) ax = plt . gca () ax . spines [ 'right' ] . set_visible ( False ) ax . spines [ 'top' ] . set_visible ( False ) plt . show () â¸ Take a look at the histograms for each participant and comment on why do you think Joy is winning the most races? A. Very consistent distribution B. Low base time and not a very high spread C. High base time but variation causes lower times to show more frequently D. Joy is not winning the most races In [0]: ### edTest(test_chow1) ### # Submit an answer choice as a string below (eg. if you choose option A put 'A') answer = '___' â¸ What one parameter should Hargun change in order to win more races? A. Reduce base time B. Reduce consistency C. Relax before the race D. Increase consistency In [0]: ### edTest(test_chow2) ### # Submit an answer choice as a string below (eg. if you choose option A put 'A') answer = '___' ðŸ‘©ðŸ»â€ðŸŽ“ Bonus (Not graded) Find out who among has would have the most podium finishes (top 3). In [18]: # Your code here",
        "tags": "lectures",
        "url": "lectures/lecture07/notebook-2/"
    }, {
        "title": "Lab 4",
        "text": "cs109a_lab_04_solutions_part2 CS109A Introduction to Data Science Lab 4: Bonus material: Polynomial Regression Harvard University Fall 2021 Instructors : Pavlos Protopapas and Natesh Pillai Lab Team : Marios Mattheakis, Hayden Joy, Chris Gumb, and Eleni Kaxiras Authors : Eleni Kaxiras, Rahul Dave, David Sondak, Will Claybaugh, and Pavlos Protopapas In [1]: ## RUN THIS CELL TO GET THE RIGHT FORMATTING import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\" ) . text HTML ( styles ) Out[1]: Learning Objectives Practice Polynomial Regression. In [2]: import numpy as np import pandas as pd import matplotlib.pyplot as plt # from sklearn import preprocessing from sklearn.preprocessing import PolynomialFeatures , MinMaxScaler from sklearn.preprocessing import OneHotEncoder , OrdinalEncoder , StandardScaler from sklearn.metrics import r2_score , mean_squared_error from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from pandas.api.types import CategoricalDtype from sklearn.compose import make_column_transformer , TransformedTargetRegressor from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline , make_pipeline from sklearn.linear_model import Ridge from sklearn.impute import SimpleImputer from pandas.plotting import scatter_matrix import seaborn as sns % matplotlib inline Polynomial Regression, and Revisiting the Cab Data In [3]: # read in the data, break into train and test cab_df = pd . read_csv ( \"../data/dataset_1.txt\" ) train_data , test_data = train_test_split ( cab_df , test_size =. 2 , random_state = 42 ) cab_df . head () Out[3]: TimeMin PickupCount 0 860.0 33.0 1 17.0 75.0 2 486.0 13.0 3 300.0 5.0 4 385.0 10.0 In [4]: cab_df . shape Out[4]: (1250, 2) In [5]: # do some data cleaning X_train = train_data [ 'TimeMin' ] . values . reshape ( - 1 , 1 ) / 60 y_train = train_data [ 'PickupCount' ] . values X_test = test_data [ 'TimeMin' ] . values . reshape ( - 1 , 1 ) / 60 y_test = test_data [ 'PickupCount' ] . values def plot_cabs ( cur_model , poly_transformer = None ): # build the x values for the prediction line x_vals = np . arange ( 0 , 24 , . 1 ) . reshape ( - 1 , 1 ) # if needed, build the design matrix if poly_transformer : design_mat = poly_transformer . fit_transform ( x_vals ) else : design_mat = x_vals # make the prediction at each x value prediction = cur_model . predict ( design_mat ) # plot the prediction line, and the test data plt . plot ( x_vals , prediction , color = 'k' , label = \"Prediction\" ) plt . scatter ( X_test , y_test , label = \"Test Data\" ) # label your plots plt . ylabel ( \"Number of Taxi Pickups\" ) plt . xlabel ( \"Time of Day (Hours Past Midnight)\" ) plt . legend () plt . show () In [6]: from sklearn.linear_model import LinearRegression fitted_cab_model0 = LinearRegression () . fit ( X_train , y_train ) plot_cabs ( fitted_cab_model0 ) In [7]: fitted_cab_model0 . score ( X_test , y_test ) Out[7]: 0.240661535615741 We can see that there's still a lot of variation in cab pickups that's not being caught by a linear fit. And the linear fit is predicting massively more pickups at 11:59pm than at 12:00am. However, we can add columns to our design matrix for $TimeMin&#94;2$ and $TimeMin&#94;3$ and so on, allowing a wigglier polynomial that will better fit the data. We'll be using sklearn's PolynomialFeatures to take some of the tedium out of building the new design matrix. In fact, if all we want is a formula like $y \\approx \\beta_0 + \\beta_1 x + \\beta_2 x&#94;2 + ...$ it will directly return the new design matrix. In [8]: degree = 3 transformer_3 = PolynomialFeatures ( degree , include_bias = False ) new_features = transformer_3 . fit_transform ( X_train ) new_features Out[8]: array([[6.73333333e+00, 4.53377778e+01, 3.05274370e+02], [2.18333333e+00, 4.76694444e+00, 1.04078287e+01], [1.41666667e+00, 2.00694444e+00, 2.84317130e+00], ..., [1.96666667e+01, 3.86777778e+02, 7.60662963e+03], [1.17333333e+01, 1.37671111e+02, 1.61534104e+03], [1.42000000e+01, 2.01640000e+02, 2.86328800e+03]]) A few notes on PolynomialFeatures : The interface is a bit strange. PolynomialFeatures is a 'transformer' in sklearn. We'll be using several transformers that learn a transformation on the training data and then apply that transformation on future data. On these (more typical) transformers it makes sense to have a .fit() and a separate .transform() . With PolynomialFeatures, the .fit() is pretty trivial, and we often fit and transform in one command, as seen above. You rarely want to include_bias (a column of all 1s), since sklearn will add it automatically. If you want polynomial features for a several different variables, you should call .fit_transform() separately on each column and append all the results to the design matrix (unless you also want interaction terms between the newly-created features). See np.concatenate for joining arrays. In [9]: fitted_cab_model3 = LinearRegression () . fit ( new_features , y_train ) plot_cabs ( fitted_cab_model3 , transformer_3 ) Questions: Calculate the polynomial model's $R&#94;2$ performance on the test set. Does the polynomial model improve on the purely linear model? Make a residual plot for the polynomial model. What does this plot tell us about the model? your answer here See code below Yes, the test set $R&#94;2$ is higher, and the visual fit to both data sets is much better. It even looks like the predicted number of pickups at 11:59 pm and 12:00 am are nearly equal. See the code below. The residuals are much more evenly spread than with the linear model [not shown], but they still don't look like an even spread of gaussian noise. This makes it unlikely that the statsmodel assumptions are valid, and we might want to be careful about trusting confidence intervals, etc, and we may want to search for other models entirely. In [10]: # your code here # test r-squared print ( \"Test R-squared:\" , fitted_cab_model3 . score ( transformer_3 . fit_transform ( X_test ), y_test )) Test R-squared: 0.33412512570778774 In [11]: # your code here design_mat = transformer_3 . fit_transform ( X_train ) prediction = fitted_cab_model3 . predict ( design_mat ) residual = y_train - prediction plt . scatter ( X_train , residual , label = \"Residual\" ) plt . axhline ( 0 , color = 'k' ) plt . title ( \"Residuals for the Cubic Model\" ) plt . ylabel ( \"Residual Number of Taxi Pickups\" ) plt . xlabel ( \"Time of Day (Hours Past Midnight)\" ) plt . legend () Out[11]: <matplotlib.legend.Legend at 0x160eed760> In [12]: # your code here design_mat = X_train prediction = fitted_cab_model0 . predict ( design_mat ) residual = y_train - prediction plt . scatter ( X_train , residual , label = \"Residual\" ) plt . axhline ( 0 , color = 'k' ) plt . title ( \"Residuals for the Linear Model\" ) plt . ylabel ( \"Residual Number of Taxi Pickups\" ) plt . xlabel ( \"Time of Day (Hours Past Midnight)\" ) plt . legend () Out[12]: <matplotlib.legend.Legend at 0x160fc6d60>",
        "tags": "labs",
        "url": "labs/lab4/notebook-1/"
    }, {
        "title": "Lab 4",
        "text": "cs109a_lab_04_students CS109A Introduction to Data Science Lab 4: Multiple Regression and Feature engineering Harvard University Fall 2021 Instructors : Pavlos Protopapas and Natesh Pillai Lab Team : Marios Mattheakis, Hayden Joy, Chris Gumb, and Eleni Kaxiras Authors : Eleni Kaxiras, Rahul Dave, David Sondak, Will Claybaugh, and Pavlos Protopapas In [0]: ## RUN THIS CELL TO GET THE RIGHT FORMATTING import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\" ) . text HTML ( styles ) Learning Objectives After this lab, you should be able to Implement multiple regression models with sklearn . Work with categorical variables including transforming them. Incorporate pipelines into your workflow In [0]: import numpy as np import pandas as pd import matplotlib.pyplot as plt # from sklearn import preprocessing from sklearn.preprocessing import PolynomialFeatures , MinMaxScaler from sklearn.preprocessing import OneHotEncoder , OrdinalEncoder , StandardScaler from sklearn.metrics import r2_score , mean_squared_error from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from pandas.api.types import CategoricalDtype from sklearn.compose import make_column_transformer , TransformedTargetRegressor from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline , make_pipeline from sklearn.linear_model import Ridge from sklearn.impute import SimpleImputer from pandas.plotting import scatter_matrix import seaborn as sns % matplotlib inline 1 - Exploring the Football data Introduction The data imported below were scraped by Shubham Maurya and record various facts about players in the English Premier League. Our goal is to fit models that predict the players' market value (what the player could earn when hired by a new team). There are all sorts of questions we could answer, for example is there a relationship between a player's popularity and his market value? Or we could make interesting observations about players in the top 6 teams. The data were scraped by Shubham Maurya from a variety of sources, including transfermrkt.com and Fantasy Premier League (FPL) . They record various facts about players in the English Premier League. Data description name : Name of the player club : Club of the player age : Age of the player position : The usual position on the pitch position_cat : 1 for attackers, 2 for midfielders, 3 for defenders, 4 for goalkeepers page_views : Average daily Wikipedia page views from September 1, 2016 to May 1, 2017 fpl_points : FPL points accumulated over the previous season region : 1 for England, 2 for EU, 3 for Americas, 4 for Rest of World nationality : Player's nationality new_signing : Whether a new signing for 2017/18 (till 20th July) new_foreign : Whether a new signing from a different league, for 2017/18 (till 20th July) club_id : a numerical version of the Club feature Our return variable market_value : As on transfermrkt.com on July 20th, 2017 Import the data In [0]: league_df = pd . read_csv ( \"league_data.csv\" ) league_df . head () In [0]: league_df . shape In [0]: league_df . isnull () . sum () We have not talked about handling missing values so we will just drop this here. In [0]: league_df = league_df . dropna () In [0]: league_df . isnull () . sum () In [0]: response = 'market_value' y = league_df [ response ] In [0]: league_df . describe ( include = \"all\" ) ðŸ‹ðŸ»â€â™‚ï¸ TEAM ACTIVITY 1: Let's start with some feature engineering. The people that hired us to predict on this data want to know if being in a big club affects the market value of a player. So we need to create a new binary categorical variable named big_clubs with values $0$ or $1$ designating if a club belongs to the Top 6 clubs: big_clubs = ['Arsenal', 'Chelsea', 'Liverpool', 'Manchester+City', 'Manchester+United', 'Tottenham'] They also want to look at players in age groups and not just by age. Put the age feature in bins according to the values below, and name the variable age_cat : pandas has the .cut() method that breaks a variable into bins with labels age_bins = [___] age_labels = [___] league_df['age_cat'] = pd.cut(x=league_df['age'],\\ bins=age_bins, labels=age_labels) In [0]: # 1. your code here In [0]: # check list ( league_df [[ 'club' , 'big_club' ]] . groupby ([ 'big_club' ]) . apply ( np . unique )) Applying functions to pandas DataFrames and Series A simpler but less generic way to do the previous exercise would be league_df['big_club2'] = league_df.apply(lambda row: 1 if row['club'] in big_clubs else 0, axis=1) If the function to create the new column is simple, there is a more direct way to create the new column (feature), e.g.: df['new_column'] = df['column']**2 In [0]: # 2. your code here In [0]: # check list ( league_df [[ 'age_cat' , 'age' , ]] . sort_values ( by = 'age_cat' ) . groupby ([ 'age_cat' ]) . apply ( np . unique )) Looking at data types more closely In [0]: league_df . dtypes In [0]: # let's see what features we want to use in the model categorical_cols = [ 'position_cat' , 'new_signing' , 'big_club' , 'age_cat' , 'region' ] # non-ordinal numerical_cols = [ 'age' , 'page_views' , 'fpl_points' ] ordinal_cols = [] # we do not have any In [0]: league_df . head () In [0]: # cast categorical variables as pandas type `category` cat_type = CategoricalDtype ( ordered = False ) for var in categorical_cols : league_df [ var ] = league_df [ var ] . astype ( cat_type ) In [0]: league_df [ categorical_cols + numerical_cols ] . dtypes In [0]: # Shape of things league_df . age . values . reshape ( - 1 , 1 ) . shape Stratified train/test split We want to split before we do any EDA since, ideally, we do not want our test set to influence our design decisions. Also, to make sure that the training and test data have appropriate representation of each region; it would be bad for the training data to entirely miss a region. This is especially important because some regions are rather rare. train and test subsets = sklearn.model_selection.train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)[source] https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html ðŸ‹ðŸ»â€â™‚ï¸ TEAM ACTIVITY 2: Practice stratified train-test split. Stratify by `region`. Use the train_test_split function and its stratify argument to split the data, keeping equal representation of each region. Note: This will not work if the dataset contains missing data. In [0]: # your code here In [0]: # check train_data . shape , test_data . shape , y_train . shape , y_test . shape Now that we won't be peeking at the test set, let's explore and look for patterns! We'll practice a number of useful pandas and numpy functions along the way. We notice that our dataset contains columns with different data types. We need to apply a specific preprocessing for each one of them. Categorical variables that are ordinal need to be coded as integers, while the rest of them need to be one-hot-encoded. We can do this sequentially or better use sklearn's pipeline structure. Our pipeline could conveniently include any standardization/normalisation of numerical values. For now we will let them as they are. In [0]: train_data . head () In [0]: sns . pairplot ( train_data [[ 'age' , 'page_views' , 'market_value' ]], \\ kind = 'reg' , diag_kind = 'hist' ); In [0]: train_data . columns In [0]: train_data [[ 'club' , 'club_id' ]] . \\ groupby ([ 'club_id' ]) . agg ({ 'club' : np . unique , }) In [0]: train_data . groupby ( 'position' ) . agg ({ 'market_value' : np . mean , 'page_views' : np . median , 'fpl_points' : np . max }) 2 - Transform categorical variables In [0]: categorical_cols , numerical_cols In [0]: X_train = train_data [ categorical_cols + numerical_cols ] . copy () X_test = test_data [ categorical_cols + numerical_cols ] . copy () X_train . shape , X_test . shape , y_train . shape , y_test . shape Using sklearn OneHotEncoder() By default, keeps all one-hot created columns. Fine-grained drop mechanism, can drop only binary variables, or the first in the list of categories, or even a specific one ($cats[i]$). drop{â€˜first', â€˜if_binary'} or a array-like of shape (n_features,), default=None It also has a mechanism for handling the presence of unknown categories in the test set. handle_unknown{â€˜error', â€˜ignore'}, default='error' In [0]: oh = OneHotEncoder ( drop = 'if_binary' , sparse = False , handle_unknown = 'error' ) oh_train = oh . fit_transform ( train_data [ categorical_cols ]) oh_train [: 10 ] In [0]: list ( zip ( categorical_cols , oh . categories_ )) In [0]: oh_train . shape , train_data [ categorical_cols ] . shape In [0]: oh_test = oh . transform ( test_data [ categorical_cols ]) oh_test . shape , test_data [ categorical_cols ] . shape In [0]: # remember these are \"views\" of the dataframe # the dataframe remains unchanged train_data [ categorical_cols ] . head ( 5 ) In [0]: train_data [ numerical_cols ] . values . shape , oh_train . shape Using pandas get_dummies() By default keeps all $k$ dummies out of $k$ categorical levels. Can be made to remove the first level, so that we have $k-1 dummies$. In [0]: dummies_train = pd . get_dummies ( train_data [ categorical_cols ]) #drop_first=True dummies_train . head () In [0]: # transform the test set dummies_test = pd . get_dummies ( test_data [ categorical_cols ]) Note : if the test dataset has a category that does not exist in the training set, this will throw an error. In [0]: pd . set_option ( 'display.max_columns' , None ) # create the design matrix for the train set design_train_df = pd . concat ([ train_data [ numerical_cols ], dummies_train ], axis = 1 ) design_train_df . head () In [0]: # for the test set design_test_df = pd . concat ([ test_data [ numerical_cols ], dummies_test ], axis = 1 ) In [0]: design_train_df . dtypes In [0]: # the dataframe remains unchanged train_data [ categorical_cols ] . head ( 5 ) In [0]: list ( zip ( categorical_cols , oh . categories_ )) Now, let's run the model using our design matrices In [0]: #create linear model regression = LinearRegression () #fit linear model regression . fit ( design_train_df , y_train ) y_pred = regression . predict ( design_test_df ) r2_train = regression . score ( design_train_df , y_train ) r2_test = regression . score ( design_test_df , y_test ) print ( f 'R&#94;2 train = { r2_train : .5 } ' ) print ( f 'R&#94;2 test = { r2_test : .5 } ' ) 3 - Using Transformation Pipelines There could be many transformations that need to be executed sequentialy in order to construct the design matrix. As we saw, it is possible to handcraft the design matrix ourselves by transforming individual columns, it is more efficient and error-free to create an sklearn pipeline to do this for you. Sklearn can work directly with $numpy$ arrays or $DataFrames$. When using the latter, sklearn.compose.ColumnTransformer is useful, as it applies transformers to columns of an array or pandas DataFrame. This estimator allows different columns or column subsets of the input to be transformed separately and the features generated by each transformer will be concatenated to form the design matrix. Making a pipeline from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler my_pipeline = Pipeline([ ('imputer', Imputer(strategy='median')), # we will be using later ('std_scaler', StandardScaler()), # optional ('selector', ColumnTransformer()) # for one-hot encoding ('regressor', lr) # actual regressor model ]) In [0]: # transform categoricals categorical_encoder = OneHotEncoder ( drop = 'if_binary' , handle_unknown = 'error' ) #handle_unknown='ignore' # transform numericals numerical_pipe = Pipeline ([ ( 'imputer' , SimpleImputer ( strategy = 'mean' )), # for later #('stdscaler', StandardScaler()) # for later ]) # bring all transformations together preprocessor = ColumnTransformer ([ ( 'cat' , categorical_encoder , categorical_cols ), ( 'num' , numerical_pipe , numerical_cols ) ]) # add a regressor lr = LinearRegression () model = Pipeline ([ ( 'preprocessor' , preprocessor ), ( 'regressor' , lr ) ]) model . fit ( X_train , y_train ) In [0]: ohe = ( model . named_steps [ 'preprocessor' ] . named_transformers_ [ 'cat' ]) feature_names = ohe . get_feature_names ( input_features = categorical_cols ) feature_names = np . r_ [ feature_names , numerical_cols ] feature_names = list ( feature_names ) feature_names In [0]: print ( f 'LR train R&#94;2: { model . score ( X_train , y_train ) : .3f } ' ) print ( f 'LR test R&#94;2: { model . score ( X_test , y_test ) : .3f } ' ) In [0]: # grab the linear regressor linear_regressor = model . named_steps [ 'regressor' ] linear_regressor . coef_ . shape In [0]: pd . DataFrame ( zip ( feature_names + numerical_cols , linear_regressor . coef_ ), columns = [ 'feature' , 'coeff' ]) A different way to construct the pipeline In [0]: preprocessor = make_column_transformer ( ( OneHotEncoder ( drop = 'if_binary' , handle_unknown = 'error' ), categorical_cols ), #(StandardScaler(), numerical_columns), ( SimpleImputer ( strategy = 'mean' ), numerical_cols ), remainder = 'passthrough' ) In [0]: model = make_pipeline ( preprocessor , LinearRegression () ) model . fit ( X_train , y_train ) In [0]: feature_names = ( model . named_steps [ 'columntransformer' ] . named_transformers_ [ 'onehotencoder' ] . get_feature_names ( input_features = categorical_cols )) feature_names = np . concatenate ( [ feature_names , numerical_cols ]) coefs = pd . DataFrame ( model . named_steps [ 'linearregression' ] . coef_ , columns = [ 'Coefficients' ], index = feature_names ) coefs In [0]: print ( f 'LR train R&#94;2: { model . score ( X_train , y_train ) : .3f } ' ) print ( f 'LR test R&#94;2: { model . score ( X_test , y_test ) : .3f } ' ) 4 - Feature Engineering ðŸ‹ðŸ»â€â™‚ï¸ TEAM ACTIVITY 4: Let's focus on introducing new features to see if our model performs better. After talking to our client for four hours and doing some some thought, we concluded that the mean predicted market value should be: $$\\hat{y} = \\beta_0 + \\beta_1\\cdot \\text{fpl_points} + \\beta_2\\cdot\\text{age} + \\beta_3\\cdot\\text{age}&#94;2 + \\beta_4\\cdot \\text{new_signing} +\\beta_5\\cdot \\text{big_club} + \\beta_6\\cdot \\text{position_cat} \\\\ + \\beta_7\\cdot \\text{age_cat} + \\beta_8\\cdot \\text{page_views}\\times \\text{fpl_points}$$ We're including a 2nd degree polynomial in age because we expect pay to increase as a player gains experience, but then decrease as they continue aging. We also include an interaction term between page_views and fpl_points . Build a design matrix function and fit this model to the training data. How good is the overall model? Interpret the regression model. What is the meaning of the coefficient for: age and age$&#94;2$ big_club What should a player do in order to improve their market value? How many page views should a player go get to increase their market value by 10? In [0]: # load a fresh train and test set. train_data = pd . read_csv ( \"train_data.csv\" ) test_data = pd . read_csv ( \"test_data.csv\" ) train_data . head ( 2 ) In [0]: # your code here In [0]: # check print ( f 'LR train R&#94;2: { model . score ( X_train , y_train ) : .3f } ' ) print ( f 'LR test R&#94;2: { model . score ( X_test , y_test ) : .3f } ' ) Conceptual questions The model is reasonably good. We're capturing about 76% of the variation in market values, and the test set confirms that we're not overfitting too badly. Look at the coefficients, depends upon your split.. Linear regression on non-experimental data can't determine causation, so we can't prove that a given relationship runs in the direction we might think. For instance, doing whatever it takes to get more page views probably doesn't meaningfully increase market value; it's likely the causation runs in the other direction and great players get more views. Even so, we can use page views to help us tell who is a great player and thus likely to be paid well. In [0]: agecoef = float ( coefs . loc [ 'age' ] . values ) age2coef = float ( coefs . loc [ 'age_sq' ] . values ) agecoef , age2coef In [0]: x_vals = np . linspace ( - 100 , 100 , 1000 ) y_vals = agecoef * x_vals + age2coef * x_vals ** 2 plt . plot ( x_vals , y_vals ) plt . title ( \"Effect of Age on Player Market value\" ) plt . xlabel ( \"Age\" ) plt . ylabel ( \"Contribution to Predicted Market Value\" ) plt . show () Conceptual questions If our model does not have a constant, we must include all four dummy variable columns. If we drop one, we're not modeling any effect of being in that category, and effectively assuming the dropped category's effect is 0. Being in position 2 (instead of position 1) has an impact between -1.54 and +2.38 on a player's market value. Since we're using an intercept, the dropped category becomes the baseline and the effect of any dummy variable is the effect of being in that category instead of the baseline category. END OF LAB 04",
        "tags": "labs",
        "url": "labs/lab4/notebook-2/"
    }, {
        "title": "Lecture 6: Regularization Ridge and Lasso Regression",
        "text": "Slides Lecture 6 : Multi-Linear Regression (PDF) Lecture 6 : Ridge and Lasso - Hyperparameters (PDF) Lecture 6 : Comparison of Ridge and Lasso (PDF) Exercises Lecture 6: Exercise: Bias Variance Tradeoff [Notebook] Lecture 6: Exercise: Simple Lasso and Ridge Regularization [Notebook] Lecture 6: Exercise: Variation of Coefficients for Lasso and Ridge Regression [Notebook] Lecture 6: Exercise: Hyper-parameter Tuning for Ridge Regression [Notebook] Lecture 6: Exercise: Regularization with Cross-validation [Notebook]",
        "tags": "lectures",
        "url": "lectures/lecture06/"
    }, {
        "title": "Lecture 6: Regularization Ridge and Lasso Regression",
        "text": "s4_ex1_challenge Title : Exercise: Simple Lasso and Ridge Regularization Description : The aim of this exercise is to understand Lasso and Ridge regularization. For this we will plot the predictor vs coefficient as a horizontal bar chart. The graph will look similar to the one given below. Data Description: Instructions: Read the dataset and assign the predictor and response variables. Split the dataset into train and validation sets. Fit a multi-linear regression model. Compute the validation MSE of the model. Compute the coefficient of the predictors and store to the plot later. Implement Lasso regularization by specifying an alpha value. Repeat steps 4 and 5. Implement Ridge regularization by specifying the same alpha value. Repeat steps 4 and 5. Plot the coefficient of all the 3 models in one graph as shown above. Hints: sklearn.normalize() Scales input vectors individually to the unit norm (vector length) sklearn.train_test_split() Splits the data into random train and test subsets sklearn.LinearRegression() LinearRegression fits a linear model sklearn.fit() Fits the linear model to the training data sklearn.predict() Predict using the linear modReturns the coefficient of the predictors in the model. mean_squared_error() Mean squared error regression loss sklearn.Lasso() Linear Model trained with L1 prior as a regularizer sklearn.Ridge() Linear least squares with L2 regularization Note: This exercise is auto-graded and you can try multiple attempts. In [99]: # Import necessary libraries % matplotlib inline import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn import preprocessing from sklearn.linear_model import Lasso from sklearn.linear_model import Ridge from sklearn.metrics import mean_squared_error from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.preprocessing import PolynomialFeatures Reading the dataset In [100]: # Read the file \"Boston_housing.csv\" as a Pandas dataframe df = pd . read_csv ( \"Boston_housing.csv\" ) Predictors & Response variables Select the following columns as predictors crim indus nox rm age dis rad tax ptratio black lstat Select the 'medv' column as response variable In [101]: # Select a subdataframe of predictors mentioned above X = df [ ___ ] # Normalize the values of the dataframe X_norm = preprocessing . normalize ( ___ ) # Select medv as the response variable y = df [ ___ ] Split the dataset into train and validation sets In [102]: ### edTest(test_random) ### # Split the data into train and validation sets with 70% train data and # random_state as 31 X_train , X_val , y_train , y_val = train_test_split ( ___ ) Multi-linear Regression Analysis In [103]: # Initialize a Linear Regression model lreg = LinearRegression () # Fit the linear model on the train data lreg . fit ( ___ ) # Predict on the validation data y_val_pred = lreg . predict ( ___ ) In [0]: # Use the mean_squared_error function to compute the validation mse mse = mean_squared_error ( ___ , ___ ) # Print the MSE value print ( \"Multi-linear regression validation MSE is\" , mse ) Obtaining the coefficients of the predictors In [105]: # Helper code to create a dictionary of the coefficients # along with the predictors as keys lreg_coef = dict ( zip ( X . columns , np . transpose ( lreg . coef_ ))) # Linear regression coefficients for plotting lreg_x = list ( lreg_coef . keys ()) lreg_y = list ( lreg_coef . values ()) Implementing Lasso regularization In [106]: # Create a Lasso Regression model with alpha as 0.008 lasso_reg = Lasso ( ___ ) # Fit the model on the train data lasso_reg . fit ( ___ ) # Predict on the validation data using the trained model y_val_pred = lasso_reg . predict ( ___ ) Computing the MSE with Lasso regularization In [0]: # Calculate the validation MSE mse_lasso = mean_squared_error ( ___ , ___ ) # Print the validation MSE print ( \"Lasso validation MSE is\" , mse_lasso ) In [108]: # Hhelper code to make a dictionary of the predictors # along with the coefficients associated with them lasso_coef = dict ( zip ( X . columns , np . transpose ( lasso_reg . coef_ ))) # Get the Lasso regularisation coefficients for plotting lasso_x = list ( lasso_coef . keys ()) lasso_y = list ( lasso_coef . values ()) Implementing Ridge regularization In [109]: # Create a Ridge Regression model with alpha as 0.008 ridgeReg = Ridge ( ___ ) # Fit the model on the train data ridgeReg . fit ( ___ ) # Predict the trained model on the validation data y_val_pred = ridgeReg . predict ( ___ ) Computing the MSE with Ridge regularization In [0]: ### edTest(test_mse) ### # Calculate the validation MSE mse_ridge = mean_squared_error ( ___ ) # Print he valdiation MSE print ( \"Ridge validation MSE is\" , mse_ridge ) Obtaining the coefficients of the predictors In [111]: # Helper code to make a dictionary of the predictors # along with the coefficients associated with them ridge_coef = dict ( zip ( X . columns , np . transpose ( ridgeReg . coef_ ))) # Ridge regularisation coefficients for plotting ridge_x = list ( ridge_coef . keys ()) ridge_y = list ( ridge_coef . values ()) Plotting the graph In [0]: # Helper code below to visualise your results plt . rcdefaults () plt . barh ( lreg_x , lreg_y , 1.0 , align = 'edge' , color = \"#D3B4B4\" , label = \"Linear Regression\" ) plt . barh ( lasso_x , lasso_y , 0.75 , align = 'edge' , color = \"#81BDB2\" , label = \"Lasso regularisation\" ) plt . barh ( ridge_x , ridge_y , 0.25 , align = 'edge' , color = \"#7E7EC0\" , label = \"Ridge regularisation\" ) plt . grid ( linewidth = 0.2 ) plt . xlabel ( \"Coefficient\" ) plt . ylabel ( \"Predictors\" ) plt . legend ( loc = 'best' ) plt . xlim ( - 6500 , 3500 ) plt . show () â¸ How does the performance of Lasso and Ridge regression compare with that of Linear regression? In [0]: ### edTest(test_chow1) ### # Submit an answer choice as a string below answer1 = '___' â¸ Change the alpha values for both, Lasso and Ridge, to 1000. What happens to the coefficients? In [0]: ### edTest(test_chow2) ### # Submit an answer choice as a string below answer2 = '___'",
        "tags": "lectures",
        "url": "lectures/lecture06/notebook-1/"
    }, {
        "title": "Lecture 6: Regularization Ridge and Lasso Regression",
        "text": "s4-exa2-challenge Title : Exercise: Bias Variance Tradeoff Description : The aim of this exercise is to understand bias variance tradeoff . For this, you will fit a polynomial regression model with different degrees on the same data and plot them as given below. Data Description: Instructions: Read the file noisypopulation.csv as a Pandas dataframe. Assign the response and predictor variables appropriately as mentioned in the scaffold. Perform sampling on the dataset to get a subset. For each sampled version fo the dataset: For degree of the chosen degree value: Compute the polynomial features for the training Fit the model on the given data Select a set of random points in the data to predict the model Store the predicted values as a list Plot the predicted values along with the random data points and true function as given above. Hints: FUNCTION SIGNATURE: gen(degree, number of samples, number of points, x, y) sklearn.PolynomialFeatures() Generates polynomial and interaction features sklearn.LinearRegression() LinearRegression fits a linear model sklearn.fit() Fits the linear model to the training data sklearn.predict() Predict using the linear model. Note: This exercise is auto-graded and you can try multiple attempts. In [31]: #Import necessary libraries % matplotlib inline import scipy as sp import numpy as np import pandas as pd import matplotlib as mpl import matplotlib.cm as cm import matplotlib.pyplot as plt from sklearn.linear_model import LinearRegression from sklearn.preprocessing import PolynomialFeatures In [32]: # Helper function to define plot characteristics def make_plot (): fig , axes = plt . subplots ( figsize = ( 20 , 8 ), nrows = 1 , ncols = 2 ); axes [ 0 ] . set_ylabel ( \"$p_R$\" , fontsize = 18 ) axes [ 0 ] . set_xlabel ( \"$x$\" , fontsize = 18 ) axes [ 1 ] . set_xlabel ( \"$x$\" , fontsize = 18 ) axes [ 1 ] . set_yticklabels ([]) axes [ 0 ] . set_ylim ([ 0 , 1 ]) axes [ 1 ] . set_ylim ([ 0 , 1 ]) axes [ 0 ] . set_xlim ([ 0 , 1 ]) axes [ 1 ] . set_xlim ([ 0 , 1 ]) plt . tight_layout (); return axes In [33]: # Reading the file into a dataframe df = pd . read_csv ( \"noisypopulation.csv\" ) In [34]: ###edTest(get_data)### # Set column x is the predictor and column y is the response variable. # Column f is the true function of the given data # Select the values of the columns x = df . ___ f = df . ___ y = df . ___ In [36]: # Function to compute the Polynomial Features for the data x # for the given degree d def polyshape ( d , x ): return PolynomialFeatures ( ___ ) . fit_transform ( ___ . reshape ( - 1 , 1 )) In [37]: # Function to fit a Linear Regression model def make_predict_with_model ( x , y , x_pred ): # Create a Linear Regression model with fit_intercept as False lreg = ___ # Fit the model to the data x and y got parameters to the function lreg . fit ( ___ , ___ ) # Predict on the x_pred data got as a parameter to this function y_pred = lreg . predict ( ___ ) # Return the linear model and the prediction on the test data return lreg , y_pred In [38]: # Function to perform sampling and fit the data, with the following parameters # degree is the maximum degree of the model # num_sample is the number of samples # size is the number of random points selected from the data for each sample # x is the predictor variable # y is the response variable def gen ( degree , num_sample , size , x , y ): # Create 2 lists to store the prediction and model predicted_values , linear_models = [], [] # Loop over the number of samples for i in range ( num_sample ): # Helper code to call the make_predict_with_model function to fit on the data indexes = np . sort ( np . random . choice ( x . shape [ 0 ], size = size , replace = False )) # lreg and y_pred hold the model and predicted values for the current sample lreg , y_pred = make_predict_with_model ( polyshape ( degree , x [ indexes ]), y [ indexes ], polyshape ( degree , x )) # Append the model and predicted values to the appropriate lists predicted_values . append ( ___ ) linear_models . append ( ___ ) # Return the 2 lists, one for predicted values and one for the model return predicted_values , linear_models In [39]: # Call the function gen() twice with x and y as the # predictor and response variable respectively # Set the number of samples to 200 and the number of points as 30 # Store the return values in appropriate variables # Get results for degree 1 predicted_1 , model_1 = gen ( ___ ); # Get results for degree 100 predicted_100 , model_100 = gen ( ___ ); In [0]: # Helper code to plot the data indexes = np . sort ( np . random . choice ( x . shape [ 0 ], size = 30 , replace = False )) plt . figure ( figsize = ( 12 , 8 )) axes = make_plot () # Plot for Degree 1 axes [ 0 ] . plot ( x , f , label = \"f\" , color = 'darkblue' , linewidth = 4 ) axes [ 0 ] . plot ( x , y , '.' , label = \"Population y\" , color = '#009193' , markersize = 8 ) axes [ 0 ] . plot ( x [ indexes ], y [ indexes ], 's' , color = 'black' , label = \"Data y\" ) for i , p in enumerate ( predicted_1 [: - 1 ]): axes [ 0 ] . plot ( x , p , alpha = 0.03 , color = '#FF9300' ) axes [ 0 ] . plot ( x , predicted_1 [ - 1 ], alpha = 0.3 , color = '#FF9300' , label = \"Degree 1 from different samples\" ) # Plot for Degree 100 axes [ 1 ] . plot ( x , f , label = \"f\" , color = 'darkblue' , linewidth = 4 ) axes [ 1 ] . plot ( x , y , '.' , label = \"Population y\" , color = '#009193' , markersize = 8 ) axes [ 1 ] . plot ( x [ indexes ], y [ indexes ], 's' , color = 'black' , label = \"Data y\" ) for i , p in enumerate ( predicted_100 [: - 1 ]): axes [ 1 ] . plot ( x , p , alpha = 0.03 , color = '#FF9300' ) axes [ 1 ] . plot ( x , predicted_100 [ - 1 ], alpha = 0.2 , color = '#FF9300' , label = \"Degree 100 from different samples\" ) axes [ 0 ] . legend ( loc = 'best' ) axes [ 1 ] . legend ( loc = 'best' ) plt . show (); â¸ Does changing the degree from 100 to 10 reduce variance? Why or why not? In [0]: ### edTest(test_chow1) ### # Submit an answer choice as a string below answer1 = '___'",
        "tags": "lectures",
        "url": "lectures/lecture06/notebook-2/"
    }, {
        "title": "Lecture 6: Regularization Ridge and Lasso Regression",
        "text": "s4-ex3-challenge Title : Exercise: Hyper-parameter Tuning for Ridge Regression Description : The goal of this exercise is to perform hyper-parameter tuning and produce a plot similar to the one below: Data Description: The dataset has a total of 3 columns with names - x,y and f \"$x$\" represents the predictor variable \"$y$\" is the response variable \"$f$\" denotes the true values of the underlying function Instructions: Read the dataset polynomial50.csv as a dataframe. Assign the predictor and response variables. Visualize the dataset by making plots using the predictor and response variables along with the true function. Split the data into train and validation sets using random_state=42 . For each value of alpha from a given list: Estimate a Ridge regression on the training data with the alpha value. Calculate the MSE of training and validation data. Append to separate lists appropriately. Use the given plot_functions function to plot the value of parameters. Compute the best hyperparameter for this data based on the lowest MSE Make a plot of the MSE values for each value of hyper-parameter alpha from the list above. It should look similar to the one given above. Hints: sklearn.Ridge() Linear least squares with L2 regularization. sklearn.train_test_split() Splits the data into random train and test subsets. ax.plot() Plot y versus x as lines and/or markers. sklearn.PolynomialFeatures() Generates a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. sklearn.fit_transform() Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. sklearn.Ridge() Linear least squares with L2 regularization. sklearn.predict() Predict using the linear model. mean_squared_error() Mean squared error regression loss. Note: This exercise is auto-graded and you can try multiple attempts. In [9]: # Import necessary libraries % matplotlib inline import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from sklearn.linear_model import Ridge , Lasso from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split from sklearn.preprocessing import PolynomialFeatures plt . style . use ( 'seaborn-white' ) # These are custom functions made to help you visualise your results from helper import plot_functions from helper import plot_coefficients In [52]: # Open the file 'polynomial50.csv' as a Pandas dataframe df = pd . read_csv ( 'polynomial50.csv' ) In [0]: # Take a quick look at the data df . head () In [54]: # Assign the values of the 'x' column as the predictor x = df [[ 'x' ]] . values # Assign the values of the 'y' column as the response y = df [ 'y' ] . values # Also assign the true value of the function (column 'f') to the variable f f = df [ 'f' ] . values In [0]: # Visualise the distribution of the x, y values & also the value of the true function f fig , ax = plt . subplots () # Plot x vs y values ax . plot ( ___ , ___ , '.' , label = 'Observed values' , markersize = 10 ) # Plot x vs true function value ax . plot ( ___ , ___ , 'k-' , label = 'Function description' ) # Helper code to annotate the plot ax . legend ( loc = 'best' ) ax . set_xlabel ( 'Predictor - $X$' , fontsize = 16 ) ax . set_ylabel ( 'Response - $Y$' , fontsize = 16 ) ax . set_title ( 'Predictor vs Response plot' , fontsize = 16 ) plt . show (); In [132]: # Split the data into train and validation sets with # training size 80% and random_state = 42 x_train , x_val , y_train , y_val = train_test_split ( x , y , train_size = 0.8 , random_state = 42 ) In [0]: ### edTest(test_mse) ### fig , rows = plt . subplots ( 6 , 2 , figsize = ( 16 , 24 )) # Select the degree for polynomial features degree = ___ # List of hyper-parameter values alphas = [ 0.0 , 1e-7 , 1e-5 , 1e-3 , 0.1 , 1 ] # Create two lists for training and validation error training_error , validation_error = [],[] # Compute the polynomial features train and validation sets x_poly_train = PolynomialFeatures ( ___ ) . fit_transform ( ___ ) x_poly_val = PolynomialFeatures ( ___ ) . fit_transform ( ___ ) # Loop over all the alpha values for i , alpha in enumerate ( alphas ): # Code to get the plot grid l , r = rows [ i ] # Initialize a Ridge regression with the current alpha ridge = Ridge ( fit_intercept = False , alpha = ___ ) # Fit the model on the transformed training data ridge . fit ( ___ , ___ ) # Predict on the transformed training set y_train_pred = ridge . predict ( ___ ) # Predict on the transformed validation set y_val_pred = ridge . predict ( ___ ) # Compute the training and validation errors mse_train = mean_squared_error ( ___ , ___ ) mse_val = mean_squared_error ( ___ , ___ ) # Add the error values to the appropriate list training_error . append ( ___ ) validation_error . append ( ___ ) # Calling the helper functions plot_functions & # plot_coefficients to visualise the plots plot_functions ( degree , ridge , l , df , alpha , x_val , y_val , x_train , y_train ) plot_coefficients ( ridge , r , alpha ) sns . despine (); In [136]: ### edTest(test_hyper) ### # Find the best value of hyper parameter, which # gives the least error on the validdata best_parameter = ___ # Print the best hyper parameter print ( f 'The best hyper parameter value, alpha = { best_parameter } ' ) In [0]: # Plot the errors as a function of increasing d value # to visualise the training and validation errors fig , ax = plt . subplots ( figsize = ( 12 , 8 )) # Plot the training errors for each alpha value ax . plot ( ___ , ___ , 's--' , label = 'Training error' , color = 'Darkblue' , linewidth = 2 ) # Plot the validation errors for each alpha value ax . plot ( ___ , ___ , 's-' , label = 'validation error' , color = '#9FC131FF' , linewidth = 2 ) # Draw a vertical line at the best parameter ax . axvline ( ___ , 0 , 0.5 , color = 'r' , label = f 'Min validation error at alpha = { best_parameter } ' ) ax . set_xlabel ( 'Value of Alpha' , fontsize = 15 ) ax . set_ylabel ( 'Mean Squared Error' , fontsize = 15 ) ax . set_ylim ([ 0 , 0.010 ]) ax . legend ( loc = 'upper left' , fontsize = 16 ) ax . set_title ( 'Mean Squared Error' , fontsize = 20 ) ax . set_xscale ( 'log' ) plt . tight_layout () plt . show ();",
        "tags": "lectures",
        "url": "lectures/lecture06/notebook-3/"
    }, {
        "title": "Lecture 6: Regularization Ridge and Lasso Regression",
        "text": "s5_ex1_challenge Title : Exercise: Variation of Coefficients for Lasso and Ridge Regression Description : The goal of this exercise is to understand the variation of the coefficients of predictors with varying values of regularization parameter in Lasso and Ridge regularization. Below is a sample plot for Ridge ($L_2$ regularization) Data Description: Instructions: Read the dataset bateria_train.csv and assign the predictor and response variables. The predictor is the 'Spreading factor' and the response variable is the 'Perc_population' Use a maximum degree of 7 to make polynomial features and make a new predictor x_poly Make a list of alpha values. For each value of $\\alpha$ : Fit a multi-linear regression using $L_2$ regularization Compute the coefficient of the predictors and store to the plot later Make a plot of the coefficients along with the alpha values Make a new alpha list as per the code in the exercise Implement Lasso regularization by repeating the above steps for each value of alpha Make another plot of the coefficients along with the new alpha values Hints: np.linspace() Return evenly spaced numbers over a specified interval. np.transpose() Reverse or permute the axes of an array; returns the modified array. sklearn.PolynomialFeatures() Generates a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. sklearn.fit_transform() Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. sklearn.LinearRegression() LinearRegression fits a linear model. sklearn.fit() Fits the linear model to the training data. sklearn.predict() Predict using the linear modReturns the coefficient of the predictors in the model. mean_squared_error() Mean squared error regression loss. sklearn.coef_ Returns the coefficients of the predictors. sklearn.Lasso() Linear Model trained with L1 prior as a regularizer. sklearn.Ridge() Linear least squares with L2 regularization. Note: This exercise is auto-graded and you can try multiple attempts. In [0]: # Import necessary libraries % matplotlib inline import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.linear_model import Lasso from sklearn.linear_model import Ridge from sklearn.preprocessing import PolynomialFeatures In [0]: # Helper code to alter plot properties large = 22 ; med = 16 ; small = 10 params = { 'axes.titlesize' : large , 'legend.fontsize' : med , 'figure.figsize' : ( 16 , 10 ), 'axes.labelsize' : med , 'axes.titlesize' : med , 'axes.linewidth' : 2 , 'xtick.labelsize' : med , 'ytick.labelsize' : med , 'figure.titlesize' : large } plt . style . use ( 'seaborn-white' ) plt . rcParams . update ( params ) % matplotlib inline In [0]: # Read the file \"bacteria_train.csv\" as a dataframe df = pd . read_csv ( \"bacteria_train.csv\" ) In [0]: # Take a quick look of your dataset df . head () In [0]: # Set the values of 'Spreading_factor' as the predictor x = df [[ ___ ]] # Set the values of 'Perc_population' as the response y = df [ ___ ] In [0]: # Select the degree of the polynomial maxdeg = 4 # Compute the polynomial features on the data x_poly = PolynomialFeatures ( ___ ) . fit_transform ( ___ ) In [0]: # Get a list of 1000 alpha values ranging from 10 to 120 # np.linspace is inclusive by default unlike arange alpha_list = np . linspace ( ___ , ___ , ___ ) In [0]: ### edTest(test_ridge_fit) ### # Make an empty list called coeff_list to store the coefficients of each model coeff_list = [] # Loop over all alpha values for i in alpha_list : # Initialize a Ridge regression object with the current alpha value # and set normalize as True ridge_reg = Ridge ( alpha = ___ , normalize = ___ ) # Fit on the transformed data ridge_reg . fit ( ___ , ___ ) # Append the coeff_list with the coefficients of the trained model coeff_list . append ( ___ ) In [0]: # Take the transpose of the list to get the variation in the # coefficient values per degree trend = np . array ( coeff_list ) . T In [0]: # Helper code to plot the variation of the coefficients as per the alpha value # Just adding some nice colors. make sure to comment this cell out if you plan to use degree more than 7 colors = [ '#5059E8' , '#9FC131FF' , '#D91C1C' , '#9400D3' , '#FF2F92' , '#336600' , 'black' ] fig , ax = plt . subplots ( figsize = ( 10 , 6 )) for i in range ( maxdeg ): ax . plot ( alpha_list , np . abs ( trend [ i + 1 ]), color = colors [ i ], alpha = 0.9 , label = f 'Degree { i + 1 } ' , lw = 2.2 ) ax . legend ( loc = 'best' , fontsize = 10 ) ax . set_xlabel ( r '$\\alpha$ values' , fontsize = 20 ) ax . set_ylabel ( r '$\\beta$ values' , fontsize = 20 ) fig . suptitle ( r 'Ridge ($L_2$) Regression' ) plt . show (); Compare the results of Ridge regression with the Lasso variant In [0]: # Select a list of 1000 alpha values ranging from 1e-4 to 1e-1 alpha_list = np . linspace ( ___ , ___ , ___ ) In [0]: ### edTest(test_lasso_fit) ### # Initialize a list called to store the alpha value of each model coeff_list = [] # Loop over all the alpha values for i in alpha_list : # Initialize a Lasso regression model with the current alpha # Set normalize as True lasso_reg = Lasso ( alpha = ___ , max_iter = 250000 , normalize = ___ ) # Fit on the transformed data lasso_reg . fit ( ___ , ___ ) # Append the coeff_list with the coefficients of the model coeff_list . append ( ___ ) In [0]: # Get the transpose of the list to get the variation in the # coefficient values per degree trend = np . array ( coeff_list ) . T In [0]: # Helper code below to plot the variation of the coefficients as per the alpha value colors = [ '#5059E8' , '#9FC131FF' , '#D91C1C' , '#9400D3' , '#FF2F92' , '#336600' , 'black' ] fig , ax = plt . subplots ( figsize = ( 10 , 6 )) for i in range ( maxdeg ): ax . plot ( alpha_list , np . abs ( trend [ i + 1 ]), color = colors [ i ], alpha = 0.9 , label = f 'Degree { i + 1 } ' , lw = 2 ) ax . legend ( loc = 'best' , fontsize = 10 ) ax . set_xlabel ( r '$\\alpha$ values' , fontsize = 20 ) ax . set_ylabel ( r '$\\beta$ values' , fontsize = 20 ) fig . suptitle ( r 'Lasso ($L_1$) Regression' ) plt . show ();",
        "tags": "lectures",
        "url": "lectures/lecture06/notebook-4/"
    }, {
        "title": "Lecture 6: Regularization Ridge and Lasso Regression",
        "text": "reg_with_cv Title : Exercise: Regularization with Cross-validation Description : The aim of this exercise is to understand regularization with cross-validation. Data Description: Instructions: Initialising the required parameters for this exercise. This can be viewed in the scaffold. Read the data file polynomial50.csv and assign the predictor and response variables. Use the helper code to visualise the data. Define a function reg_with_validation that performs Ridge regularization by taking a random_state parameter. Split the data into train and validation sets by specifying the random_state. Compute the polynomial features for the train and validation sets. Run a loop for the alpha values. Within the loop: Initialise the Ridge regression model with the specified alpha. Fit the model on the training data and predict and on the train and validation set. Compute the MSE of the train and validation prediction. Store these values in lists. Run reg_with_validation for varying random states and plot a graph that depicts the best alpha value and the best MSE. The graph will be similar to the one given above. Define a function reg_with_cross_validation that performs Ridge regularization with cross-validation by taking a random_state parameter. Sample the data using the specified random state. Assign the predictor and response variables using the sampled data. Run a loop for the alpha values. Within the loop: Initialise the Ridge regression model with the specified alpha. Fit the model on the entire data and using cross-validation with 5 folds. Get the train and validation MSEs by taking their mean. Store these values in lists. Run reg_with_cross_validation for varying random states and plot a graph that depicts the best alpha value and the best MSE. Use the helper code given to print your best MSEs in the case of simple validation and cross-validation for different random states. Hints: df.sample() Returns a random sample of items from an axis of the object. sklearn.cross_validate() Evaluate metric(by cross-validation and also record fit/score times. np.mean() Compute the arithmetic mean along the specified axis. sklearn.RidgeRegression() Linear least squares with l2 regularization. sklearn.fit() Fit Ridge egression model. sklearn.predict() Predict using the linear model. sklearn.mean_squared_error() Mean squared error regression loss. sklearn.PolynomialFeatures() Generate polynomial and interaction features. sklearn.fit_transform() Fit to data, then transform it. In [0]: # Import necessary libraries import numpy as np import pandas as pd import matplotlib.pyplot as plt from prettytable import PrettyTable from sklearn.linear_model import Ridge from sklearn.metrics import mean_squared_error from sklearn.model_selection import cross_validate from sklearn.model_selection import train_test_split from sklearn.preprocessing import PolynomialFeatures % matplotlib inline In [0]: # Initialising required parameters # The list of random states ran_state = [ 0 , 10 , 21 , 42 , 66 , 109 , 310 , 1969 ] # The list of alpha for regularization alphas = [ 1e-7 , 1e-5 , 1e-3 , 0.01 , 0.1 , 1 ] # The degree of the polynomial degree = 30 In [0]: # Read the file 'polynomial50.csv' as a dataframe df = pd . read_csv ( 'polynomial50.csv' ) # Assign the values of the 'x' column as the predictor x = df [[ 'x' ]] . values # Assign the values of the 'y' column as the response y = df [ 'y' ] . values # Also assign the true value of the function (column 'f') to the variable f f = df [ 'f' ] . values In [0]: # Helper code below to visualise the distribution of the x, y values & also the value of the true function f fig , ax = plt . subplots () # Plot x vs y values ax . plot ( x , y , 'o' , label = 'Observed values' , markersize = 10 , color = 'Darkblue' ) # Plot x vs true function value ax . plot ( x , f , 'k-' , label = 'True function' , linewidth = 4 , color = '#9FC131FF' ) ax . legend ( loc = 'best' ); ax . set_xlabel ( 'Predictor - $X$' , fontsize = 16 ) ax . set_ylabel ( 'Response - $Y$' , fontsize = 16 ) ax . set_title ( 'Predictor vs Response plot' , fontsize = 16 ) plt . show (); In [0]: # Function to perform regularization with simple validation def reg_with_validation ( rs ): # Split the data into train and validation sets with train size # as 80% and random_state as the value given as the function parameter x_train , x_val , y_train , y_val = train_test_split ( x , y , train_size = 0.8 , random_state = rs ) # Create two lists for training and validation error training_error , validation_error = [],[] # Compute the polynomial features for the train and validation sets x_poly_train = ___ x_poly_val = ___ # Run a loop for all alpha values for alpha in alphas : # Initialise a Ridge regression model by specifying the current # alpha and with fit_intercept=False ridge_reg = ___ # Fit on the modified training data ___ # Predict on the training set y_train_pred = ___ # Predict on the validation set y_val_pred = ___ # Compute the training and validation mean squared errors mse_train = ___ mse_val = ___ # Append the MSEs to their respective lists training_error . append ( mse_train ) validation_error . append ( mse_val ) # Return the train and validation MSE return training_error , validation_error In [0]: ### edTest(test_validation) ### # Initialise a list to store the best alpha using simple validation for varying random states best_alpha = [] # Run a loop for different random_states for i in range ( len ( ran_state )): # Get the train and validation error by calling the # function reg_with_validation training_error , validation_error = ___ # Get the best mse from the validation_error list best_mse = ___ # Get the best alpha value based on the best mse best_parameter = ___ # Append the best alpha to the list best_alpha . append ( best_parameter ) # Use the helper code given below to plot the graphs fig , ax = plt . subplots ( figsize = ( 6 , 4 )) # Plot the training errors for each alpha value ax . plot ( alphas , training_error , 's--' , label = 'Training error' , color = 'Darkblue' , linewidth = 2 ) # Plot the validation errors for each alpha value ax . plot ( alphas , validation_error , 's-' , label = 'Validation error' , color = '#9FC131FF' , linewidth = 2 ) # Draw a vertical line at the best parameter ax . axvline ( best_parameter , 0 , 0.5 , color = 'r' , label = f 'Min validation error at alpha = { best_parameter } ' ) ax . set_xlabel ( 'Value of Alpha' , fontsize = 15 ) ax . set_ylabel ( 'Mean Squared Error' , fontsize = 15 ) ax . set_ylim ([ 0 , 0.010 ]) ax . legend ( loc = 'upper left' , fontsize = 16 ) bm = round ( best_mse , 5 ) ax . set_title ( f 'Best alpha is { best_parameter } with mse { bm } ' , fontsize = 16 ) ax . set_xscale ( 'log' ) plt . tight_layout () plt . show () In [0]: # Function to perform regularization with cross validation def reg_with_cross_validation ( rs ): # Sample the data to get different splits using the random state df_new = ___ # Assign the values of the 'x' column as the predictor from your sampled dataframe x = df_new [[ 'x' ]] . values # Assign the values of the 'y' column as the response from your sampled dataframe y = df_new [ 'y' ] . values # Create two lists for training and validation error training_error , validation_error = [],[] # Compute the polynomial features on the entire data x_poly = ___ # Run a loop for all alpha values for alpha in alphas : # Initialise a Ridge regression model by specifying the alpha value and with fit_intercept=False ridge_reg = ___ # Perform cross validation on the modified data with neg_mean_squared_error as the scoring parameter and cv=5 # Set return_train_score to True ridge_cv = ___ # Compute the training and validation errors got after cross validation mse_train = ___ mse_val = ___ # Append the MSEs to their respective lists training_error . append ( mse_train ) validation_error . append ( mse_val ) # Return the train and validation MSE return training_error , validation_error In [0]: ### edTest(test_cross_validation) ### # Initialise a list to store the best alpha using cross validation for varying random states best_cv_alpha = [] # Run a loop for different random_states for i in range ( len ( ran_state )): # Get the train and validation error by calling the function reg_with_cross_validation training_error , validation_error = ___ # Get the best mse from the validation_error list best_mse = ___ # Get the best alpha value based on the best mse best_parameter = ___ # Append the best alpha to the list best_cv_alpha . append ( ___ ) # Use the helper code given below to plot the graphs fig , ax = plt . subplots ( figsize = ( 6 , 4 )) # Plot the training errors for each alpha value ax . plot ( alphas , training_error , 's--' , label = 'Training error' , color = 'Darkblue' , linewidth = 2 ) # Plot the validation errors for each alpha value ax . plot ( alphas , validation_error , 's-' , label = 'Validation error' , color = '#9FC131FF' , linewidth = 2 ) # Draw a vertical line at the best parameter ax . axvline ( best_parameter , 0 , 0.5 , color = 'r' , label = f 'Min validation error at alpha = { best_parameter } ' ) ax . set_xlabel ( 'Value of Alpha' , fontsize = 15 ) ax . set_ylabel ( 'Mean Squared Error' , fontsize = 15 ) ax . legend ( loc = 'upper left' , fontsize = 16 ) bm = round ( best_mse , 5 ) ax . set_title ( f 'Best alpha is { best_parameter } with mse { bm } ' , fontsize = 16 ) ax . set_xscale ( 'log' ) plt . tight_layout () In [0]: # Helper code to print your findings pt = PrettyTable () pt . field_names = [ \"Random State\" , \"Best Alpha with Validation\" , \"Best Alpha with Cross-Validation\" ] for i in range ( 6 ): pt . add_row ([ ran_state [ i ], best_alpha [ i ], best_cv_alpha [ i ]]) print ( pt ) â¸ Comment on the results of regularization with simple validation and cross-validation after changing the random state and alpha values. In [0]: ### edTest(test_chow1) ### # Submit an answer choice as a string below answer1 = '___'",
        "tags": "lectures",
        "url": "lectures/lecture06/notebook-5/"
    }, {
        "title": "Lecture 5: Model Selection and Cross Validation",
        "text": "Slides Lecture 5 : Model Selection (PDF) Lecture 5 : Model Selection with Cross Validation (PDF) Exercises Lecture 5: Exercise: Exercise: Best Degree of Polynomial with Train and Validation sets [Notebook] Lecture 5: Exercise: Best Degree of Polynomial using Cross-validation [Notebook]",
        "tags": "lectures",
        "url": "lectures/lecture05/"
    }, {
        "title": "Lecture 5: Model Selection and Cross Validation",
        "text": "s2_exc1_challenge Title : Exercise: Best Degree of Polynomial with Train and Validation sets Description : The aim of this exercise is to find the best degree of polynomial based on the MSE values. Further, plot the train and validation error graphs as a function of degree of the polynomial as shown below. Data Description: Instructions: Read the dataset and split into train and validation sets. Select a max degree value for the polynomial model. Fit a polynomial regression model on the training data for each degree and predict on the validation data. Compute the train and validation error as MSE values and store in separate lists. Find out the best degree of the model. Plot the train and validation errors for each degree. Hints: pd.read_csv(filename) Returns a pandas dataframe containing the data and labels from the file data sklearn.train_test_split() Splits the data into random train and test subsets sklearn.PolynomialFeatures() Generates a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree sklearn.fit_transform() Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X sklearn.LinearRegression(fit_intercept=False) LinearRegression fits a linear model with no intercept calculation sklearn.fit() Fits the linear model to the training data sklearn.predict() Predict using the linear model plt.subplots() Create a figure and a set of subplots Note: This exercise is auto-graded and you can try multiple attempts. In [1]: # Import necessary libraries import operator import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.metrics import mean_squared_error from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.preprocessing import PolynomialFeatures % matplotlib inline Reading the dataset In [2]: # Read the file \"dataset.csv\" as a Pandas dataframe df = pd . read_csv ( \"dataset.csv\" ) # Take a quick look at the dataset df . head () Out[2]: x y 0 4.98 24.0 1 9.14 21.6 2 4.03 34.7 3 2.94 33.4 4 5.33 36.2 In [5]: # Assign the values of the x and y column values to the # predictor and response variables x = df [[ 'x' ]] . values y = df . y . values Train-validation split In [25]: ### edTest(test_random) ### # Split the dataset into train and validation sets with 75% training set # Set random_state=1 x_train , x_val , y_train , y_val = train_test_split ( x , y , train_size = 0.75 , random_state = 1 ) Computing the train and validation error in terms of MSE In [26]: ### edTest(test_regression) ### # To iterate over the range, select the maximum degree of the polynomial maxdeg = 10 # Create two empty lists to store training and validation MSEs training_error , validation_error = [],[] # Loop through the degrees of the polynomial to create different models for d in range ( maxdeg ): # Compute the polynomial features for the current degree # for the train set x_poly_train = PolynomialFeatures ( degree = d ) . fit_transform ( x_train ) # Compute the polynomial features for the validation set x_poly_val = PolynomialFeatures ( degree = d ) . fit_transform ( x_val ) # Initialize a linear regression model lreg = LinearRegression ( fit_intercept = False ) # Fit the model on the train data lreg . fit ( x_poly_train , y_train ) # Use the trained model to predict on the transformed train data y_train_pred = lreg . predict ( x_poly_train ) # Use the trained model to predict on the transformed validation data y_val_pred = lreg . predict ( x_poly_val ) # Compute the MSE on the train predictions training_error . append ( mean_squared_error ( y_train , y_train_pred )) # Compute the MSE on the validation predictions validation_error . append ( mean_squared_error ( y_val , y_val_pred )) Finding the best degree In [27]: ### edTest(test_best_degree) ### # Helper code to compute the best degree, which is the model # with the lowest validation error min_mse = min ( validation_error ) best_degree = validation_error . index ( min_mse ) # Print the degree of the best model computed above print ( \"The best degree of the model is\" , best_degree ) The best degree of the model is 2 Plotting the error graph In [28]: # Plot the errors as a function of increasing d value to visualise the training # and testing errors fig , ax = plt . subplots () # Plot the training error with labels ax . plot ( np . arange ( 0 , maxdeg ), training_error , label = \"Training Error\" ) # Plot the validation error with labels ax . plot ( np . arange ( 0 , maxdeg ), validation_error , label = \"Validation Error\" ) # Set the plot labels and legends ax . set_xlabel ( 'Degree of Polynomial' ) ax . set_ylabel ( 'Mean Squared Error' ) ax . legend ( loc = 'best' ) ax . set_yscale ( 'log' ) plt . show (); â¸ If you run the exercise with a random state of 0, do you notice any change? What would you attribute this change to? In [29]: ### edTest(test_chow1) ### # Submit an answer choice as a string below answer1 = 'No good, it change a lot' In [0]:",
        "tags": "lectures",
        "url": "lectures/lecture05/notebook-1/"
    }, {
        "title": "Lecture 5: Model Selection and Cross Validation",
        "text": "s2_exc2_challenge Title : Exercise: Best Degree of Polynomial using Cross-validation Description : The aim of this exercise is to find the best degree of polynomial based on the MSE values. Further, plot the train and cross-validation error graphs as shown below. Data Description: Instructions: Read the dataset and split into train and validation sets. Select a max degree value for the polynomial model. For each degree: Perform k-fold cross validation Fit a polynomial regression model for each degree on the training data and predict on the validation data Compute the train, validation and cross-validation error as MSE values and store them in separate lists. Print the best degree of the model for both validation and cross-validation approaches. Plot the train and cross-validation errors for each degree. Hints: pd.read_csv(filename)</a> Returns a pandas dataframe containing the data and labels from the file data. sklearn.train_test_split() Splits the data into random train and test subsets. sklearn.PolynomialFeatures() Generates a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. sklearn.cross_validate() Evaluate metric(s) by cross-validation and also record fit/score times. sklearn.fit_transform() Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. sklearn.LinearRegression(fit_intercept=False) LinearRegression fits a linear model. sklearn.fit() Fits the linear model to the training data. sklearn.predict() Predict using the linear model. plt.subplots() Create a figure and a set of subplots. operator.itemgetter() Return a callable object that fetches item from its operand. zip() Makes an iterator that aggregates elements from each of the iterables. Note: This exercise is auto-graded and you can try multiple attempts. In [39]: # Import necessary libraries % matplotlib inline import operator import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.metrics import mean_squared_error from sklearn.linear_model import LinearRegression from sklearn.model_selection import cross_validate from sklearn.model_selection import train_test_split from sklearn.preprocessing import PolynomialFeatures Reading the dataset In [40]: # Read the file \"dataset.csv\" as a Pandas dataframe df = pd . read_csv ( \"dataset.csv\" ) In [41]: # Assign the values of column x as the predictor x = df [[ 'x' ]] . values # Assign the values of column y as the response variable y = df . y . values Train-validation split In [42]: ### edTest(test_random) ### # Split the data into train and validation sets with 75% for training # and with a random_state=1 x_train , x_val , y_train , y_val = train_test_split ( ___ ) Computing the MSE In [43]: ### edTest(test_regression) ### # To iterate over the range, select the maximum degree of the polynomial maxdeg = 10 # Create three empty lists to store training, validation and cross-validation MSEs training_error , validation_error , cross_validation_error = [],[],[] # Loop through the degrees of the polynomial for d in range ( ___ ): # Compute the polynomial features for the entire data x_poly = PolynomialFeatures ( ___ ) . fit_transform ( ___ ) # Compute the polynomial features for the train data x_poly_train = PolynomialFeatures ( ___ ) . fit_transform ( ___ ) # Compute the polynomial features for the validation data x_poly_val = PolynomialFeatures ( ___ ) . fit_transform ( ___ ) # Initialize a Linear Regression object lreg = LinearRegression () # Fit model on the training set lreg . fit ( ___ ) # Predict on the training data y_train_pred = lreg . predict ( ___ ) # Predict on the validation set y_val_pred = lreg . predict ( ___ ) # Compute the mse on the train data training_error . append ( mean_squared_error ( ___ )) # Compute the mse on the validation data validation_error . append ( mean_squared_error ( ___ )) # Perform cross-validation on the entire data with 10 folds and # get the mse_scores mse_score = cross_validate ( ___ ) # Compute the mean of the cross validation error and store in list # Remember to take into account the sign of the MSE metric returned by the cross_validate function cross_validation_error . append ( ___ ) Finding the best degree In [0]: ### edTest(test_best_degree) ### # Get the best degree associated with the lowest validation error min_mse = min ( ___ ) best_degree = validation_error . index ( ___ ) # Get the best degree associated with the lowest cross-validation error min_cross_val_mse = min ( ___ ) best_cross_val_degree = cross_validation_error . index ( ___ ) # Print the values print ( \"The best degree of the model using validation is\" , best_degree ) print ( \"The best degree of the model using cross-validation is\" , best_cross_val_degree ) Plotting the error graph In [0]: # Plot the errors as a function of increasing d value to visualise the training and validation errors fig , ax = plt . subplots ( 1 , 2 , figsize = ( 16 , 8 )) # Plot the training error with labels ax [ 0 ] . plot ( range ( maxdeg ), np . log ( training_error ), label = 'Training error' , linewidth = 3 , color = '#FF7E79' , alpha = 0.4 ) # Plot the validation error with labels ax [ 0 ] . plot ( range ( maxdeg ), np . log ( validation_error ), label = 'Validation error' , linewidth = 3 , color = \"#007D66\" , alpha = 0.4 ) # Plot the training error with labels ax [ 1 ] . plot ( range ( maxdeg ), np . log ( training_error ), label = 'Training error' , linewidth = 3 , color = '#FF7E79' , alpha = 0.4 ) # Plot the cross-validation error with labels ax [ 1 ] . plot ( range ( maxdeg ), np . log ( cross_validation_error ), label = 'Cross-Validation error' , linewidth = 3 , color = \"#007D66\" , alpha = 0.4 ) # Set the plot labels and legends ax [ 0 ] . set_xlabel ( 'Degree of Polynomial' , fontsize = 12 ) ax [ 0 ] . set_ylabel ( 'Log Mean Squared Error' , fontsize = 12 ) ax [ 0 ] . set_title ( \"Log of validation error as a function of degree\" ) ax [ 1 ] . set_xlabel ( 'Degree of Polynomial' , fontsize = 12 ) ax [ 1 ] . set_ylabel ( 'Log Mean Squared Error' , fontsize = 12 ) ax [ 1 ] . set_title ( \"Log of CV error as a function of degree\" ) ax [ 0 ] . legend () ax [ 1 ] . legend () plt . show (); â¸ If you run the exercise with a random state of 0, do you notice any change? What conclusion can you draw from this experiment? In [0]: ### edTest(test_chow1) ### # Submit an answer choice as a string below answer1 = '___'",
        "tags": "lectures",
        "url": "lectures/lecture05/notebook-2/"
    }, {
        "title": "Lab 3",
        "text": "cs109a_lab_03_students CS109A Introduction to Data Science Lab 3: kNN and Linear Regression Student's version Harvard University Fall 2021 Instructors : Pavlos Protopapas and Natesh Pillai Lab Team : Marios Mattheakis, Hayden Joy, Chris Gumb, and Eleni Kaxiras Authors : Marios Mattheakis and Eleni Kaxiras In [1]: #RUN THIS CELL import requests from IPython.core.display import HTML styles = requests . get ( \"http://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\" ) . text HTML ( styles ) Out[1]: Learning Objectives For this lab, our goal is for you to try out your first models, and specifically k-Nearest Neighbors (kNN) and Linear Regression. In the course thus far, we have discussed some aspects of dealing with data, including scraping data from the web, organizing it using dictionaries and Pandas dataframes, and visualizing it using Matplotlib. Now we're moving on to data modeling! We make models to fit the training data and predict on the test data . We will see what those two types of data are. Specifically, our learning objectives are: 1 - Performing exploratory data analysis (EDA) on our dataset . 2 - Splitting this dataset into a training and test set . 3 - Training you first models (kNN and Linear Regression) on your data using the sklearn library . Using these models to understand relationships between the response variable and the predictors (also called features). Evaluating model performance on the test set using metrics such as $R&#94;2$ and MSE. In [2]: import numpy as np import pandas as pd import matplotlib import matplotlib.pyplot as plt import seaborn as sns % matplotlib inline from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsRegressor from sklearn.linear_model import LinearRegression from sklearn import metrics from pandas.api.types import CategoricalDtype from sklearn.metrics import mean_squared_error from sklearn.preprocessing import MinMaxScaler from statsmodels.api import OLS import statsmodels.api as sm # uncomment to display full dataframe # pd.set_option('display.max_rows', None) 1 - The Bikeshare dataset and preliminary EDA Our dataset was collected by the Capital Bikeshare program in Washington D.C ( source: UCI repository ). It contains over two years of data on the total number of bike rentals per day, as well as 10 attributes describing the day and its weather (see below for a description). The dataset is provided in the file bikeshare.csv . The task is to build a regression model to predict the total number of bike rentals in a given day (known as the response variable) based on attributes about the day (known as the features). Such a forecasting model would be useful in planning the number of bikes that need to be available in the system on any given day, and also in monitoring traffic in the city. Description of variables season (1:winter, 2:spring, 3:summer, 4:fall) month (1 through 12, with 1 denoting Jan) holiday (1 = the day is a holiday, 0 = otherwise, extracted from https://dchr.dc.gov/page/holiday-schedules ) day_of_week (0 through 6, with 0 denoting Sunday) workingday (1 = the day is neither a holiday or weekend, 0 = otherwise) weather 1: Clear, Few clouds, Partly cloudy, Partly cloudy 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog temp (temperature in Celsius) atemp (apparent, or relative outdoor, or real feel temperature, in Celsius) humidity (relative humidity) windspeed (wind speed) count (response variable i.e. total number of bike rentals on the day) Load and inspect the BikeShare dataset In [3]: bikeshare = pd . read_csv ( 'data/bikeshare.csv' ) print ( f 'Length of Dataset: { bikeshare . shape [ 0 ] } ' ) display ( bikeshare . head ()) Length of Dataset: 731 season month holiday day_of_week workingday weather temp atemp humidity windspeed count 0 2 5 0 2 1 2 24.0 26.0 76.58 0.12 6073 1 4 12 0 2 1 1 15.0 19.0 73.38 0.17 6606 2 2 6 0 4 1 1 26.0 28.0 56.96 0.25 7363 3 4 12 0 0 0 1 0.0 4.0 58.63 0.17 2431 4 3 9 0 3 1 3 23.0 23.0 91.71 0.10 1996 In [4]: bikeshare . columns Out[4]: Index(['season', 'month', 'holiday', 'day_of_week', 'workingday', 'weather', 'temp', 'atemp', 'humidity', 'windspeed', 'count'], dtype='object') Let's check for missing values It's always a good practice to check the types of your features and make sure they are what they are supposed to be. If not, we can cast them to the correct type using .astype() In [5]: bikeshare . isnull () . sum () Out[5]: season 0 month 0 holiday 0 day_of_week 0 workingday 0 weather 0 temp 0 atemp 0 humidity 0 windspeed 0 count 0 dtype: int64 Let's check the types of the features It's always a good practice to check the types of your features and make sure they are what they are supposed to be. If not, we can cast them to the correct type using .astype() In [6]: bikeshare . dtypes Out[6]: season int64 month int64 holiday int64 day_of_week int64 workingday int64 weather int64 temp float64 atemp float64 humidity float64 windspeed float64 count int64 dtype: object In [7]: ## here we designate the types that we want # categorical variables cat_type = CategoricalDtype ( ordered = False ) cat_vars = [ 'weather' , 'day_of_week' , 'holiday' , 'season' , 'month' , 'workingday' , 'weather' ] for var in cat_vars : bikeshare [ var ] = bikeshare [ var ] . astype ( cat_type ) # integer variables int_vars = [ 'count' ] for var in int_vars : bikeshare [ var ] = bikeshare [ var ] . astype ( 'int' ) In [8]: bikeshare . dtypes Out[8]: season category month category holiday category day_of_week category workingday category weather category temp float64 atemp float64 humidity float64 windspeed float64 count int64 dtype: object In [9]: ## OPTIONAL for good housekeeping - make a dict with the feature descriptions names_dict = { 'season' : 'Season (1 for spring, 2 for summer, 3 for fall, 4 for winter)' , 'month' : 'Month of the year (1 through 12, with 1 denoting Jan)' , 'holiday' : '1 = the day is a holiday, 0 = otherwise' , 'count' : 'Total number of bike rentals on the day' , 'temp' : 'Temperature (degrees C)' } ##... fill out the rest of the features ## useful in explaining the features in plots, tables, etc. print ( names_dict [ 'month' ]) Month of the year (1 through 12, with 1 denoting Jan) How many categories does day_of_the_week have? In [10]: # see how many categories we have bikeshare [ 'day_of_week' ] . unique () Out[10]: [2, 4, 0, 3, 6, 5, 1] Categories (7, int64): [0, 1, 2, 3, 4, 5, 6] In [11]: bikeshare [ bikeshare . season == 4 ] . shape Out[11]: (178, 11) Example of why .groupby() is powerfull You can think of .groupby() and agg (aggregate) as a way to flatten a part of the DataFrame. Is we group by month and then aggreegate the values for each month we effectively flatten all the rows that are the same month together. When you flatten the DataFrame you need to specify how to aggregate the values, e.g. sum them up, count them, etc? DataFrame.groupby(by=None, axis=0, level=None, as_index=True, sort=True, group_keys=True, squeeze=NoDefault.no_default, observed=False, dropna=True) For more see: https://pandas.pydata.org/pandas-docs/stable/reference/groupby.html In [12]: bikeshare . sort_values ([ 'season' ], ascending = False ) . groupby ( 'season' ) . head ( 2 ) Out[12]: season month holiday day_of_week workingday weather temp atemp humidity windspeed count 595 4 10 0 1 1 2 18.0 21.0 64.92 0.09 6778 211 4 12 0 1 1 2 9.0 14.0 82.71 0.06 3811 280 3 8 0 6 0 2 29.0 29.0 73.29 0.21 6299 520 3 8 0 5 1 2 29.0 30.0 61.50 0.07 7582 552 2 5 0 1 1 1 21.0 24.0 78.79 0.13 3958 652 2 4 0 4 1 2 16.0 19.0 75.67 0.18 5026 478 1 3 0 5 1 1 19.0 22.0 52.52 0.23 3239 87 1 2 1 1 0 2 4.0 7.0 60.50 0.31 1107 In [13]: ### very useful import calendar calendar . month_name [ 1 ] Out[13]: 'January' In [14]: bikeshare [[ 'season' , 'month' , 'day_of_week' , 'temp' ]] . \\ groupby ([ 'month' ]) . agg ({ 'day_of_week' : np . size , 'season' : np . unique , 'temp' : np . mean }) Out[14]: day_of_week season temp month 1 62 1 -0.080645 2 57 1 3.912281 3 62 [1, 2] 9.580645 4 60 2 14.583333 5 62 2 22.532258 6 60 [2, 3] 28.150000 7 62 3 32.580645 8 62 3 29.629032 9 60 [3, 4] 23.850000 10 62 4 15.580645 11 60 4 8.316667 12 62 [1, 4] 5.451613 Plot the count of bike rentals by month In [15]: ave_rentals_month = bikeshare . groupby ( 'month' ) . mean ()[ 'count' ] ave_rentals_month Out[15]: month 1 2498.306452 2 2824.315789 3 3822.241935 4 4348.600000 5 5268.790323 6 5547.033333 7 5563.677419 8 5584.887097 9 5653.316667 10 5199.225806 11 4454.633333 12 3781.741935 Name: count, dtype: float64 In [16]: import matplotlib.ticker as ticker fontsize = 15 fig , ax = plt . subplots ( figsize = [ 8 , 5 ]) ax . plot ( ave_rentals_month , 'bo-' , linewidth = 1 , markersize = 7 ) # labels # ax.set_ylabel('Count (x 1000)', fontsize=fontsize) # ax.set_xlabel('Month', fontsize=fontsize) ## for more meaningful names ax . set_xlabel ( names_dict [ 'month' ], fontsize = fontsize ) ax . set_ylabel ( names_dict [ 'count' ], fontsize = fontsize ) ax . set_title ( 'Bikeshare Rentals per Month' , fontsize = fontsize + 3 ) ax . yaxis . set_major_formatter ( lambda y , pos : y / 1000 ) ax . xaxis . set_major_locator ( ticker . MaxNLocator ( 12 )) ax . xaxis . set_major_formatter ( lambda x , pos : calendar . month_name [ int ( x )]) ax . tick_params ( axis = 'x' , rotation = 45 , length = 6 , width = 1 ) plt . xlim ([ 0 , 12 ]) plt . show (); /usr/lib/python3.9/site-packages/matplotlib/axes/_base.py:507: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version. Convert to a numpy array before indexing instead. x = x[:, np.newaxis] In [17]: bikeshare . temp . unique () . shape [ 0 ] Out[17]: 50 TOP 2 - Split the dataset into a training set and a test set Now that we have an idea of what the data looks like, we want to predict the number of rentals on a given day, i.e. the value of count . We will split the data randomly into a training and a testing set. We will use the train_test_split function from sklearn ( Scikit learn (sklearn) ). What is the need for training and testing data sets? The training set will be used to train the model, while the testing set will be used to quantify how well that model does on data it has never seen before. By setting random_state = 42 we ensure reproducibility of your results. Let us first call a function that will randomly split the data up into a 70-30 split, with 70% of the data going into the training set: from sklearn.model_selection import train_test_split In [18]: # set the response variable response = 'count' In [19]: # split whole data set train_data , test_data = train_test_split ( bikeshare , test_size = 0.30 , random_state = 42 ) # then create the training feature set (X_train) and the response vector (y_train) X_train , y_train = train_data . drop ( response , axis = 1 ), train_data [ response ] X_test , y_test = test_data . drop ( response , axis = 1 ), test_data [ response ] print ( X_train . shape , y_train . shape , X_test . shape , y_test . shape ) print ( f 'Training set = { train_data . shape [ 0 ] } \\ ( { ( 100 * train_data . shape [ 0 ] / bikeshare . shape [ 0 ]) : .2f } % of the total dataset)' ) print ( f 'Testing set = { test_data . shape [ 0 ] } \\ ( { ( 100 * test_data . shape [ 0 ] / bikeshare . shape [ 0 ]) : .2f } % of the total dataset)' ) (511, 10) (511,) (220, 10) (220,) Training set = 511 (69.90% of the total dataset) Testing set = 220 (30.10% of the total dataset) In [20]: # plot split results plt . figure ( figsize = [ 8 , 5 ]) sns . scatterplot ( data = train_data , x = 'temp' , y = 'count' , label = 'train' ) sns . scatterplot ( data = test_data , x = 'temp' , y = 'count' , label = 'test' ) plt . xlabel ( names_dict [ 'temp' ]) plt . ylabel ( names_dict [ 'count' ]) plt . title ( \"Data distribution for a 70/30 dataset split\" ) plt . legend () plt . show () TEAM ACTIVITY 1: Split the bikeshare data into X_train, y_train, X_test, and y_test but include only one feature (\"windspeed\") In [37]: # your code here temp_df = bikeshare [[ \"temp\" , response ]] train_data , test_data = train_test_split ( temp_df , test_size = 0.30 , random_state = 42 ) Out[37]: temp count 0 24.0 6073 1 15.0 6606 2 26.0 7363 3 0.0 2431 4 23.0 1996 In [45]: X_train , X_test , y_train , y_test = train_test_split ( bikeshare [[ \"temp\" ]], bikeshare . temp , test_size = 0.33 ) print ( X_train . shape , X_test . shape , y_train . shape , y_test . shape ) (489, 1) (242, 1) (489,) (242,) TOP 3 - Building a model with sklearn We will work with scikit-learn (sklearn) . import sklearn According to its website, Scikit-learn is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection and evaluation, and many other utilities. Why do we need to add a constant in our simple linear regression model? Let's say we a data set of two obsevations with one predictor and one response variable each. We would then have the following two equations if we run a simple linear regression model. $$y_1=\\beta_0 + \\beta_1*x_1$$ $$y_2=\\beta_0 + \\beta_1*x_2$$ For simplicity and calculation efficiency we want to \"absorb\" the constant $b_0$ into an array with $b_1$ so we have only multiplication. To do this we introduce the constant ${x}&#94;0=1$ $$y_1=\\beta_0*{x_1}&#94;0 + \\beta_1*x_1$$ $$y_2=\\beta_0 * {x_2}&#94;0 + \\beta_1*x_2$$ That becomes: $$y_1=\\beta_0*1 + \\beta_1*x_1$$ $$y_2=\\beta_0 * 1 + \\beta_1*x_2$$ In matrix notation: $$ \\left [ \\begin{array}{c} y_1 \\\\ y_2 \\\\ \\end{array} \\right] = \\left [ \\begin{array}{cc} 1& x_1 \\\\ 1 & x_2 \\\\ \\end{array} \\right] \\cdot \\left [ \\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\\\ \\end{array} \\right] $$ sklearn adds the constant for us. Steps to training a model 1 - Break the sets into one containing the predictor(s) and one with the response variable. Do separately for train and test. 2 - Define the Model model = sklearn_model_name(hyper_parameter1 = value1, hyper_parameter2 = value2) 3 - Scale the features (if needed) from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler(feature_range=(0, 1)) X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) X_train = pd.DataFrame(X_train_scaled) X_test = pd.DataFrame(X_test_scaled) 4 - Fit Model model.fit(x_train, y_train) 5 - Get the Predictions y_pred_train = model.predict(X_train) y_pred_test = model.predict(X_test) 6 - Evaluate the Model The metrics that you will use to evaluate the model depend on the task at hand, i.e. Regression or Classification. For Regression, $R&#94;2$ Score and $MSE$ are commonly used, while for Classification, Accuracy (%) is one popular metric. # Return the coefficient of determination R&#94;2 of the prediction r2_train = model.score(y_train, y_pred_train) r2_test = model.score(y_test, y_pred_test) # Calculating MSE on the test set y_pred = model.predict(X_test) MSE = mean_squared_error(y_test, y_pred) 7 - Print Results print(\"Score for Model (Training):\", r2_train) print(\"Score for Model (Testing) :\", r2_test) The scikit-learn library and the shape of things Before diving in, let's discuss more of the details of sklearn . Scikit-learn consists of estimators , Python objects, that implements the methods fit(X, y) and predict() Let's see the structure of scikit-learn needed to make these fits. fit() always takes two arguments: estimator . fit ( Xtrain , ytrain ) We will consider two estimators in this lab: LinearRegression and KNeighborsRegressor . It is very important to understand that Xtrain must be in the form of a 2x2 array with each row corresponding to one sample, and each column corresponding to the feature values for that sample. ytrain on the other hand is a simple array of responses. These are continuous for regression problems. This is the reason we write for e.g., X_train=train_data[['temp']] # two brackets and not train_data['temp'] # one bracket $k$-nearest neighbors regression Using sklearn We will use temp from our available features to predict total bike rental count . In [46]: # set the response variable response = 'count' In [47]: # from sklearn.neighbors import KNeighborsRegressor # split again train_data , test_data = train_test_split ( bikeshare , test_size = 0.30 , random_state = 42 ) # then create the training feature set (X_train) and the response vector (y_train) X_train = train_data [[ 'temp' ]] # note the two brackets y_train = train_data [ response ] # one bracket X_test = test_data [[ 'temp' ]] y_test = test_data [ response ] X_train . shape , y_train . shape , X_test . shape , y_test . shape Out[47]: ((511, 1), (511,), (220, 1), (220,)) In [48]: #from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler ( feature_range = ( 0 , 1 )) X_train_scaled = scaler . fit_transform ( X_train ) X_test_scaled = scaler . transform ( X_test ) In [49]: # from sklearn.neighbors import KNeighborsRegressor # Set kNN hyperparameter: k = 20 # First, we create the classifier object: neighbors = KNeighborsRegressor ( n_neighbors = k ) # Then, we fit the model using x_train as training data and y_train as target values: neighbors . fit ( X_train , y_train ) # Retrieve our predictions: prediction_knn = neighbors . predict ( X_test ) prediction_train_knn = neighbors . predict ( X_train ) # This returns the mean accuracy on the given test data and labels, or in other words, # the R squared value -- A constant model that always predicts the expected value of y, # disregarding the input features, would get a R&#94;2 score of 1. r2_train = neighbors . score ( X_train , y_train ) r2_test = neighbors . score ( X_test , y_test ) mse_test = mean_squared_error ( y_test , prediction_knn ) mse_train = mean_squared_error ( y_train , prediction_train_knn ) print ( f '------ kNN model score for k = { k } ' ) print ( f 'R2 training set: { r2_train : .3f } ' ) print ( f 'R2 testing set: { r2_test : .3f } ' ) print ( f 'MSE training set: { mse_train : .2f } ' ) print ( f 'MSE testing set: { mse_test : .2f } ' ) ------ kNN model score for k = 20 R2 training set: 0.348 R2 testing set: 0.248 MSE training set: 2754139.71 MSE testing set: 2825000.18 In [50]: # SubPlots fig , axes = plt . subplots ( nrows = 1 , ncols = 2 , figsize = ( 20 , 6 )) # train data axes [ 0 ] . set_ylim ([ 0 , 10000 ]) axes [ 0 ] . plot ( train_data [ 'temp' ], train_data [ 'count' ], 'bo' , alpha = 0.5 , label = 'Data' ) sorted_temp = train_data . sort_values ([ 'temp' ]) prediction_knn = neighbors . predict ( sorted_temp [[ 'temp' ]]) axes [ 0 ] . plot ( sorted_temp [ 'temp' ], prediction_knn , 'k-' , linewidth = 5 , markersize = 10 , label = 'Prediction' ) axes [ 0 ] . set_xlabel ( 'Temperature' , fontsize = 15 ) axes [ 0 ] . set_ylabel ( '# of Rentals' , fontsize = 15 ) axes [ 0 ] . set_title ( \"Train Set\" , fontsize = 18 ) axes [ 0 ] . legend ( loc = 'upper right' , fontsize = 12 ) # test data axes [ 1 ] . set_ylim ([ 0 , 10000 ]) axes [ 1 ] . plot ( test_data [ 'temp' ], test_data [ 'count' ], 'r*' , alpha = 0.5 , label = 'Data' ) sorted_temp = test_data . sort_values ([ 'temp' ]) prediction_knn = neighbors . predict ( sorted_temp [[ 'temp' ]]) axes [ 1 ] . plot ( sorted_temp [ 'temp' ], prediction_knn , 'g-' , linewidth = 5 , markersize = 10 , label = 'Prediction' ) axes [ 1 ] . set_xlabel ( 'Temperature' , fontsize = 15 ) axes [ 1 ] . set_ylabel ( '# of Rentals' , fontsize = 15 ) axes [ 1 ] . set_title ( \"Test Set\" , fontsize = 18 ) axes [ 1 ] . legend ( loc = 'upper right' , fontsize = 12 ) fig . suptitle ( \"kNN Regression (k= {} ): Temp vs Rental Count\" . format ( k ), fontsize = 20 ) plt . show () TEAM ACTIVITY 2: Practice using sklearn's kNN regression Let's now try different $k$s for our model. Load a simple dataset sim_data.csv Perform 80-20 train-test split using a random state of 42 Create a function that implements kNN regression with your choice of k (explore a few different k's) Predict on both training and test data For all kNN models generated, plot plot for the test and train set next to one other: Calculate $R&#94;2$ score Hints: dont forget to sort! you can make plot colors more transparent using \"alpha\" and lines thicker using \"linewidth\" In [53]: # read in data data = pd . read_csv ( \"data/sim_data.csv\" ) # drop Unnamed column data . drop ( columns = [ \"Unnamed: 0\" ], inplace = True ) # split into 70/30, random_state=42 sim_train_data , sim_test_data = train_test_split ( data , test_size = 0.3 , random_state = 42 ) def knn_model ( k , train_data , test_data ): # create the classifier object neighbors = KNeighborsRegressor ( n_neighbors = k ) # fit the model using x_train as training data and y_train as target values neighbors . fit ( train_data [[ 'x' ]], train_data [ 'y' ]) sorted_train = train_data . sort_values ([ 'x' ]) sorted_test = test_data . sort_values ([ 'x' ]) # Retreieve our predictions: train_preds = neighbors . predict ( sorted_train [[ 'x' ]]) test_preds = neighbors . predict ( sorted_test [[ 'x' ]]) # find r&#94;2 r2_train = neighbors . score ( train_data [[ 'x' ]], train_data [ 'y' ]) r2_test = neighbors . score ( test_data [[ 'x' ]], test_data [ 'y' ]) print ( f 'R2 training set: { r2_train : .3f } ' ) print ( f 'R2 testing set: { r2_test : .3f } ' ) return sorted_train , sorted_test , train_preds , test_preds , r2_train , r2_test def plot_predictions_same_plot ( k , train_data , test_data , train_preds , test_preds ): fig , axes = plt . subplots ( 1 , 2 , figsize = ( 20 , 6 )) # train data axes [ 0 ] . plot ( train_data [ 'x' ], train_data [ 'y' ], 'bo' , alpha = 0.5 , label = 'Train Set' ) axes [ 0 ] . plot ( train_data [ 'x' ], train_preds , 'k-' , linewidth = 2 , markersize = 10 , label = 'Train Preds' ) axes [ 1 ] . plot ( test_data [ 'x' ], test_data [ 'y' ], 'r*' , alpha = 0.5 , label = 'Test Set' ) axes [ 1 ] . plot ( test_data [ 'x' ], test_preds , 'g-' , linewidth = 2 , markersize = 10 , label = 'Test Preds' ) for i in [ 0 , 1 ]: axes [ i ] . set_xlabel ( 'x' ) axes [ i ] . set_ylabel ( 'y' ) axes [ i ] . set_title ( f 'x vs y for kNN Regression (k= { k } )' ) axes [ i ] . legend () plt . show () fig . suptitle ( 'THIS' , fontsize = 20 ) #plt.show() knn_train_preds = [] knn_test_preds = [] knn_r2_train_scores = [] knn_r2_test_scores = [] list_of_ks = [ 1 , 2 , 4 , 6 , 8 , 10 , 15 , 20 , 30 ] for k in list_of_ks : sim_sorted_train , sim_sorted_test , sim_train_preds , sim_test_preds , knn_r2_train , knn_r2_test = knn_model ( k , sim_train_data , sim_test_data ) plot_predictions_same_plot ( k , sim_sorted_train , sim_sorted_test , sim_train_preds , sim_test_preds ) knn_train_preds . append ( sim_train_preds ) knn_test_preds . append ( sim_test_preds ) knn_r2_train_scores . append ( knn_r2_train ) knn_r2_test_scores . append ( knn_r2_test ) R2 training set: 1.000 R2 testing set: -0.244 R2 training set: 0.601 R2 testing set: 0.212 R2 training set: 0.493 R2 testing set: 0.268 R2 training set: 0.463 R2 testing set: 0.415 R2 training set: 0.437 R2 testing set: 0.360 R2 training set: 0.416 R2 testing set: 0.396 R2 training set: 0.440 R2 testing set: 0.400 R2 training set: 0.425 R2 testing set: 0.394 R2 training set: 0.375 R2 testing set: 0.359 Linear Regression We just went over the kNN prediction method. Now, we will fit the same data using a linear regression model. What is the main difference between a kNN model and linear regression model? Linear regression specifies the model (whatever the data is, the model will fit a linear line) whereas kNN does not specify a model, it just assigns a value based on how close this value is to k points in the training set (based on some closeness metric). Advantages of linear regression models are that they are very fast and yield an exact optimal solution. For a more in-depth discussion on generalized linear models, please see the Advanced Section on this. We will use the same training/testing dataset as before and create linear regression objects. We can do this using sklearn (as we did for kNN) as well as with another package called statsmodels . In [54]: response = 'count' # Label data as X,Y for ease x_train , y_train = train_data [[ 'temp' ]], train_data [ response ] x_test , y_test = test_data [[ 'temp' ]], test_data [ response ] You can also split into train test by x and y using train test split X_train, X_test, y_train, y_test = train_test_split( bikeshare['temp']), bikeshare['count'] ) Linear Regression using sklearn In [55]: from sklearn.linear_model import LinearRegression lr_sklearn = LinearRegression () . fit ( x_train , y_train ) # x data must be 2D array print ( 'Coefficients:' , lr_sklearn . coef_ ) print ( 'Intercept:' , lr_sklearn . intercept_ ) Coefficients: [94.82327417] Intercept: 2996.286973239966 Note: only one coefficient here since only using one descriptor variable ( temp ) Use model to predict on training and testing data and plot prediction In [56]: # predict y_preds_train = lr_sklearn . predict ( x_train ) y_preds_test = lr_sklearn . predict ( x_test ) # plot predictions fig , axes = plt . subplots ( 1 , 2 , figsize = ( 20 , 6 )) axes = axes . ravel () axes [ 0 ] . scatter ( x_train , y_train , color = 'b' , alpha = 0.5 , label = 'Data' ) axes [ 0 ] . plot ( x_train , y_preds_train , 'k' , linewidth = 5 , label = 'Fit' ) axes [ 0 ] . set_title ( 'Train Set' , fontsize = 18 ) axes [ 1 ] . scatter ( x_test , y_test , color = 'r' , marker = '*' , alpha = 0.5 , label = 'Data' ) axes [ 1 ] . plot ( x_test , y_preds_test , 'g' , linewidth = 5 , label = 'Prediction' ) axes [ 1 ] . set_title ( 'Test Set' , fontsize = 18 ) for i , ax in enumerate ( axes ): ax . set_ylim ( 0 , 10000 ) ax . set_xlabel ( 'Temperature' , fontsize = 15 ) ax . set_ylabel ( names_dict [ response ], fontsize = 15 ) ax . legend ( loc = 'upper right' , fontsize = 12 ) fig . suptitle ( f 'Linear Regression: Temp vs { names_dict [ response ] } ' , fontsize = 20 ) plt . show () Compute performance metrics for both training and testing In [57]: # from sklearn import metrics # Mean Squared Error (MSE) print ( \"MSE Train: {:.3f} \" . format ( metrics . mean_squared_error ( y_train , y_preds_train ))) print ( \"MSE Test: {:.3f} \" . format ( metrics . mean_squared_error ( y_test , y_preds_test ))) # R&#94;2 score print ( \"R&#94;2 Train: {:.3f} \" . format ( metrics . r2_score ( y_train , y_preds_train ))) print ( \"R&#94;2 Test: {:.3f} \" . format ( metrics . r2_score ( y_test , y_preds_test ))) MSE Train: 3031302.440 MSE Test: 2915373.463 R&#94;2 Train: 0.282 R&#94;2 Test: 0.224 Residual plots Residual plots are useful for identifying non-linearity between the predictors and the response variable. After we fit our model, we calculate the predicted (of fitted ) values $\\hat{y}_{i}$. Then we calculate the residuals $e_i = y_i - \\hat{y}_{i}$ We plot the residuals $e_1$ versus: the predictor $x_i$, if we have one predictor in our model, OR the predicted values $\\hat{y}_{i}$, if we have multiple predictors. TEAM ACTIVITY 3: Plot the residuals In [59]: # plot the residuals y_pred = lr_sklearn . predict ( test_data [[ 'temp' ]]) # predicted (fitted) values y = test_data [ response ] # test values residuals = y - y_pred fig , axes = plt . subplots ( 1 , 1 , figsize = ( 8 , 5 )) plt . scatter ( y_pred , residuals , color = 'blue' ) plt . axhline () plt . title ( \"Residual Plot for Linear Regression Model\" ) plt . annotate ( 'Outlier' , xy = ( 6400 , - 5500 ), xytext = ( 6000 , - 4000 ), fontsize = 14 , arrowprops = dict ( arrowstyle = '->' , ec = 'orange' , lw = 1 )) #, bbox = dict(boxstyle=\"round\", fc=\"0.8\")) plt . ylabel ( 'Residuals' ); Out[59]: Text(0, 0.5, 'Residuals') In [60]: # find the outliers test_data [( residuals <- 5000 ) | ( residuals > 5500 )] Out[60]: season month holiday day_of_week workingday weather temp atemp humidity windspeed count 333 2 6 0 4 1 2 36.0 37.0 56.83 0.15 915 533 1 1 0 5 1 1 2.0 5.0 50.75 0.38 8961 In [61]: plt . hist ( residuals ); Out[61]: (array([ 2., 2., 11., 50., 62., 40., 35., 12., 1., 5.]), array([-5494.92484335, -4367.92571118, -3240.926579 , -2113.92744682, -986.92831464, 140.07081753, 1267.06994971, 2394.06908189, 3521.06821407, 4648.06734624, 5775.06647842]), <BarContainer object of 10 artists>) Recall that more accurate models will have higher $R&#94;2$ scores (value of 1 is perfect fitted line) and lower MSEs (meaning lower error). For more info on these, check out sklearn metrics documentation. Take a look at the end of the notebook for calculations of MSE and $R&#94;2$ metrics by hand. TEAM ACTIVITY 4: Compare kNN and linear regression for the same dataset For the sim_data dataset, split 70-30 train-test split and use random state of 42 Create a function that implements linear regression with sklearn Predict on both training and test data Create 2 subplots with the following plotted: Subplot 1: Train set Plot training data in blue Plot linear regression prediction in black Plot kNN prediction (using k = 10) in magenta ('m') Subplot 2: Test set Plot testing data in red Plot linear regression prediction in green Plot kNN prediction (using k = 10) in yellow ('y') Calculate $MSE$ scores for both train and test sets for both kNN and linear regression Hints: don't forget sort! plt.subplots(...) creates subplots In [62]: def linreg_model ( train_data , test_data ): # sort sorted_train = train_data . sort_values ([ 'x' ]) sorted_test = test_data . sort_values ([ 'x' ]) x_train , x_test , y_train , y_test = sorted_train [ 'x' ], sorted_test [ 'x' ], sorted_train [ 'y' ], sorted_test [ 'y' ] x_train_ca = x_train . values . reshape ( - 1 , 1 ) x_test_ca = x_test . values . reshape ( - 1 , 1 ) # Create Linear Regression object results = LinearRegression () . fit ( x_train_ca , y_train ) # predict train_preds = results . predict ( x_train_ca ) test_preds = results . predict ( x_test_ca ) # find r&#94;2 r2_train = metrics . r2_score ( y_train , results . predict ( x_train_ca )) r2_test = metrics . r2_score ( y_test , results . predict ( x_test_ca )) return train_preds , test_preds , r2_train , r2_test def plot_predictions2 ( k , train_data , test_data , knn_train_preds , knn_test_preds , linreg_train_preds , linreg_test_preds ): # SubPlots fig , axes = plt . subplots ( nrows = 1 , ncols = 2 , figsize = ( 20 , 6 )) axes [ 0 ] . plot ( train_data [ 'x' ], train_data [ 'y' ], 'bo' , alpha = 0.5 , label = 'Data' ) axes [ 0 ] . plot ( train_data [ 'x' ], knn_train_preds , 'm-' , linewidth = 2 , markersize = 10 , label = 'KNN Preds' ) axes [ 0 ] . plot ( train_data [ 'x' ], linreg_train_preds , 'k-' , linewidth = 2 , markersize = 10 , label = 'Linreg Preds' ) axes [ 0 ] . set_xlabel ( 'x' ) axes [ 0 ] . set_ylabel ( 'y' ) axes [ 0 ] . set_title ( \"Train Data\" ) axes [ 0 ] . legend () axes [ 1 ] . plot ( test_data [ 'x' ], test_data [ 'y' ], 'r*' , alpha = 0.5 , label = 'Data' ) axes [ 1 ] . plot ( test_data [ 'x' ], knn_test_preds , 'y-' , linewidth = 2 , markersize = 10 , label = 'KNN Preds' ) axes [ 1 ] . plot ( test_data [ 'x' ], linreg_test_preds , 'g-' , linewidth = 2 , markersize = 10 , label = 'Test Preds' ) axes [ 1 ] . set_xlabel ( 'x' ) axes [ 1 ] . set_ylabel ( 'y' ) axes [ 1 ] . set_title ( \"Test Data\" ) axes [ 1 ] . legend () fig . suptitle ( \"KNN vs Linear Regression\" ) plt . show () # get predictions linreg_train_preds , linreg_test_preds , linreg_r2_train , linreg_r2_test = linreg_model ( sim_train_data , sim_test_data ) # plot linreg predictions side by side with knn predictions k = 10 plot_predictions2 ( k , sim_sorted_train , sim_sorted_test , knn_train_preds [ 1 ], knn_test_preds [ 1 ], linreg_train_preds , linreg_test_preds ) # print r2 scores for knn with k=10 and linreg print ( \"R&#94;2 Score of kNN on training set with k= {} :\" . format ( k ), knn_r2_train_scores [ 1 ]) print ( \"R&#94;2 Score of kNN on testing set: with k= {} \" . format ( k ), knn_r2_test_scores [ 1 ]) print ( \"R&#94;2 Score of linear regression on training set\" , linreg_r2_train ) print ( \"R&#94;2 Score of linear regression on testing set\" , linreg_r2_test ) R&#94;2 Score of kNN on training set with k=10: 0.6006116858176433 R&#94;2 Score of kNN on testing set: with k=10 0.21197194463555347 R&#94;2 Score of linear regression on training set 0.3816018646727135 R&#94;2 Score of linear regression on testing set 0.46326892325303626 END of lab Bonus Material: Train-Test Split using a mask In [63]: #Function to Split data into Train and Test Set def split_data ( data ): #Calculate Length of Dataset length = len ( data ) #Define Split split = 0.7 #Set a random Seed For Shuffling np . random . seed ( 9001 ) #Generate a Mask with a X:Y Split mask = np . random . rand ( length ) < split #Separate train and test data data_train = data [ mask ] data_test = data [ ~ mask ] #Return Separately return data_train , data_test In [64]: #Split data using defined function train_data_manual , test_data_manual = split_data ( bikeshare ) print ( \"Length of Training set:\" , len ( train_data_manual )) print ( \"Length of Testing set:\" , len ( test_data_manual )) Length of Training set: 507 Length of Testing set: 224 In [65]: ## Check that the ratio between test and train sets is right test_data_manual . shape [ 0 ] / ( test_data_manual . shape [ 0 ] + train_data_manual . shape [ 0 ]) Out[65]: 0.3064295485636115 Extra: Implementing the kNN Algorithm by hand ðŸ‹ðŸ»â€â™‚ï¸ To really understand how the kNN algorithm works, it helps to go through the algorithm line by line in code. In [66]: #kNN Algorithm def knn_algorithm ( train , test , k ): #Create any empty list to store our predictions in predictions = [] #Separate the response and predictor variables from training and test set: train_x = train [ 'temp' ] train_y = train [ 'count' ] test_x = test [ 'temp' ] test_y = test [ 'count' ] for i , ele in enumerate ( test_x ): #For each test point, store the distance between all training points and test point distances = pd . DataFrame (( train_x . values - ele ) ** 2 , index = train . index ) distances . columns = [ 'dist' ] #display(distances) #Then, we sum across the columns per row to obtain the Euclidean distance squared ##distances = vec_distances.sum(axis = 1) #Sort the distances to training points (in ascending order) and take first k points nearest_k = distances . sort_values ( by = 'dist' ) . iloc [: k ] #For simplicity, we omitted the square rooting of the Euclidean distance because the #square root function preserves order. #Take the mean of the y-values of training set corresponding to the nearest k points k_mean = train_y [ nearest_k . index ] . mean () #Add on the mean to our predicted y-value list predictions . append ( k_mean ) #Create a dataframe with the x-values from test and predicted y-values predict = test . copy () predict [ 'predicted_count' ] = pd . Series ( predictions , index = test . index ) return predict Now to run the algorithm on our dataset with $k = 5$: In [67]: #Run the kNN function k = 5 predicted_knn = knn_algorithm ( train_data , test_data , k ) predicted_knn . head () Out[67]: season month holiday day_of_week workingday weather temp atemp humidity windspeed count predicted_count 703 3 7 0 2 1 1 34.0 33.0 49.21 0.13 6660 5269.2 33 4 11 0 6 0 1 6.0 9.0 51.92 0.19 3926 2958.6 300 2 6 0 1 1 2 21.0 23.0 77.79 0.17 5099 5577.4 456 4 12 0 4 1 1 2.0 5.0 58.00 0.24 7938 2377.2 633 2 5 0 0 0 2 20.0 23.0 74.00 0.15 6359 4693.4 We want to have a way to evaluate our predictions from the kNN algorithm with $k=5$. One way is to compute the $R&#94;2$ coefficient. Let's create a function for that: In [68]: #Test predictions in comparison to true value of test set def evaluate ( predicted , true ): #Find the squared error: squared_error = ( predicted [ 'predicted_count' ] - true [ 'count' ]) ** 2 #Finding the mean squared error: error_var = squared_error . sum () sample_var = (( true [ 'count' ] - true [ 'count' ] . mean ()) ** 2 ) . sum () r = ( 1 - ( error_var / sample_var )) return r Then let's apply this function to our predictions: In [69]: print ( \"Length of Test Data:\" , len ( test_data )) print ( \"R&#94;2 Score of kNN test:\" , evaluate ( predicted_knn , test_data )) Length of Test Data: 220 R&#94;2 Score of kNN test: 0.15616203314647292 In [70]: predicted_knn_train = knn_algorithm ( test_data , train_data , k ) print ( \"R&#94;2 Score of kNN train:\" , evaluate ( predicted_knn_train , train_data )) R&#94;2 Score of kNN train: 0.14054289261925446 Extra: pandas tricks In [71]: # seasons and months overlap pd . set_option ( 'display.max_rows' , None ) bikeshare . groupby ([ 'season' , 'month' ]) . size () Out[71]: season month 1 1 62 2 57 3 40 4 0 5 0 6 0 7 0 8 0 9 0 10 0 11 0 12 22 2 1 0 2 0 3 22 4 60 5 62 6 40 7 0 8 0 9 0 10 0 11 0 12 0 3 1 0 2 0 3 0 4 0 5 0 6 20 7 62 8 62 9 44 10 0 11 0 12 0 4 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 16 10 62 11 60 12 40 dtype: int64 Exercise: Look at the table above. Do you notice anything strange? answer What is the connection between seasons and months? Depends on which definition you use and if you are north or south of the equator. For more details see Seasons: Meteorological and Astronomical How to change values in specific places in the dataframe create a mask to change values to individual rows change the value by applying a function to all the rows Apply a function: .apply(lambda row: funct(row, *args)) In [72]: show_names_dict = { 1 : 'work' , 0 : 'fun' } In [73]: ## OPTIONAL EXAMPLE on how to create a new column by applying a function to the rows # def my_function(df, column1, column2, .... more columns): # s = df[column1] # b = df[column2] # ## OR some complicated function of s and b # return np.round(s*b, PRECISION) # df[new_column_name] = df.apply(lambda row: my_function(row, column1, column2), axis=1) In [74]: # we are making a copy so not to change our initial dataframe df = bikeshare . copy () mask = ( df . workingday == 1 ) # change only the rows with workingday==1 # df.loc[mask, 'workingday'] = \\ # df.loc[mask, 'workingday'].apply(lambda row: show_names_dict[row]) # change all the rows df [ 'workingday' ] = \\ df [ 'workingday' ] . apply ( lambda row : show_names_dict [ row ]) df . head () Out[74]: season month holiday day_of_week workingday weather temp atemp humidity windspeed count 0 2 5 0 2 work 2 24.0 26.0 76.58 0.12 6073 1 4 12 0 2 work 1 15.0 19.0 73.38 0.17 6606 2 2 6 0 4 work 1 26.0 28.0 56.96 0.25 7363 3 4 12 0 0 fun 1 0.0 4.0 58.63 0.17 2431 4 3 9 0 3 work 3 23.0 23.0 91.71 0.10 1996 In [0]:",
        "tags": "labs",
        "url": "labs/lab3/notebook-1/"
    }, {
        "title": "Lecture 4: Multi-linear and Polynomial Regression",
        "text": "Slides Lecture 4 : Multi-Linear Regression (PDF) Lecture 4 : Polynomial Regression (PDF) Exercises Lecture 4: Exercise: Simple Multi-linear Regression [Notebook] Lecture 4: Exercise: Linear and Polynomial Regression with Residual Analysis [Notebook] Lecture 4: Exercise: Multi-collinearity vs Model Predictions [Notebook]",
        "tags": "lectures",
        "url": "lectures/lecture04/"
    }, {
        "title": "Lecture 4: Multi-linear and Polynomial Regression",
        "text": "s2_exa1_challenge Title : Exercise: Simple Multi-linear Regression Description : The aim of this exercise is to understand how to use multi regression. Here we will observe the difference in MSE for each model as the predictors change. Data Description: Instructions: Read the file Advertisement.csv as a dataframe. For each instance of the predictor combination, form a model. For example, if you have 2 predictors, A and B, you will end up getting 3 models - one with only A, one with only B, and one with both A and B. Split the data into train and test sets. Compute the MSE of each model. Print the Predictor - MSE value pair Hints: pd.read_csv(filename) Returns a pandas dataframe containing the data and labels from the file data. sklearn.preprocessing.normalize() Scales input vectors individually to unit norm (vector length). sklearn.model_selection.train_test_split() Splits the data into random train and test subsets. sklearn.linear_model.LinearRegression LinearRegression fits a linear model. sklearn.linear_model.LinearRegression.fit() Fits the linear model to the training data. sklearn.linear_model.LinearRegression.predict() Predict using the linear model. sklearn.metrics.mean_squared_error() Computes the mean squared error regression loss Note: This exercise is auto-graded and you can try multiple attempts. In [1]: # Import necessary libraries import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn import preprocessing from prettytable import PrettyTable from sklearn.metrics import mean_squared_error from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split % matplotlib inline Reading the dataset In [2]: # Read the file \"Advertising.csv\" df = pd . read_csv ( \"Advertising.csv\" ) In [3]: # Take a quick look at the data to list all the predictors df . head () Out[3]: TV Radio Newspaper Sales 0 230.1 37.8 69.2 22.1 1 44.5 39.3 45.1 10.4 2 17.2 45.9 69.3 9.3 3 151.5 41.3 58.5 18.5 4 180.8 10.8 58.4 12.9 Create different multi predictor models In [11]: ### edTest(test_mse) ### # Initialize a list to store the MSE values mse_list = [] # List of all predictor combinations to fit the curve cols = [[ 'TV' ],[ 'Radio' ],[ 'Newspaper' ],[ 'TV' , 'Radio' ],[ 'TV' , 'Newspaper' ],[ 'Radio' , 'Newspaper' ],[ 'TV' , 'Radio' , 'Newspaper' ]] # Loop over all the predictor combinations for i in cols : # Set each of the predictors from the previous list as x x = df [ i ] # Set the \"Sales\" column as the reponse variable y = df [ \"Sales\" ] # Split the data into train-test sets with 80% training data and 20% testing data. # Set random_state as 0 x_train , x_test , y_train , y_test = train_test_split ( x , y , test_size = 0.2 , random_state = 0 ) # Initialize a Linear Regression model lreg = LinearRegression () # Fit the linear model on the train data lreg . fit ( x_train , y_train ) # Predict the response variable for the test set using the trained model y_pred = lreg . predict ( x_test ) # Compute the MSE for the test data MSE = mean_squared_error ( y_test , y_pred ) # Append the computed MSE to the list mse_list . append ( MSE ) Display the MSE with predictor combinations In [12]: # Helper code to display the MSE for each predictor combination t = PrettyTable ([ 'Predictors' , 'MSE' ]) for i in range ( len ( mse_list )): t . add_row ([ cols [ i ], mse_list [ i ]]) print ( t ) +------------------------------+-------------------+ | Predictors | MSE | +------------------------------+-------------------+ | ['TV'] | 10.18618193453022 | | ['Radio'] | 24.23723303713214 | | ['Newspaper'] | 32.13714634300907 | | ['TV', 'Radio'] | 4.391429763581883 | | ['TV', 'Newspaper'] | 8.687682675690592 | | ['Radio', 'Newspaper'] | 24.78339548293816 | | ['TV', 'Radio', 'Newspaper'] | 4.402118291449686 | +------------------------------+-------------------+ In [13]: In [0]:",
        "tags": "lectures",
        "url": "lectures/lecture04/notebook-1/"
    }, {
        "title": "Lecture 4: Multi-linear and Polynomial Regression",
        "text": "s2_exb1_challenge Title : Exercise: Linear and Polynomial Regression with Residual Analysis Description : The goal of this exercise is to fit linear regression and polynomial regression to the given data. Plot the fit curves of both the models along with the data and observe what the residuals tell us about the two fits. Data Description: Instructions: Read the poly.csv file into a dataframe. Split the data into train and test subsets. Fit a linear regression model on the entire data, using LinearRegression() object from Sklearn library. Guesstimate the degree of the polynomial which would best fit the data. Fit a polynomial regression model on the computed Polynomial Features using LinearRegression() object from sklearn library. Plot the linear and polynomial model predictions along with the test data. Compute the polynomial and linear model residuals using the formula below $\\epsilon = y_i - \\hat{y}$ Plot the histogram of the residuals and comment on your choice of the polynomial degree. Hints: pd.DataFrame.head() Returns a pandas dataframe containing the data and labels from the file data. sklearn.model_selection.train_test_split() Splits the data into random train and test subsets. plt.subplots() Create a figure and a set of subplots. sklearn.preprocessing.PolynomialFeatures() Generates a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. sklearn.preprocessing.StandardScaler.fit_transform() Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. sklearn.linear_model.LinearRegression LinearRegression fits a linear model. sklearn.linear_model.LinearRegression.fit() Fits the linear model to the training data. sklearn.linear_model.LinearRegression.predict() Predict using the linear model. plt.plot() Plots x versus y as lines and/or markers. plt.axvline() Add a vertical line across the axes. ax.hist() Plots a histogram. Note: This exercise is auto-graded and you can try multiple attempts. In [7]: # Import necessary libraries import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.preprocessing import PolynomialFeatures % matplotlib inline In [8]: # Read the data from 'poly.csv' into a Pandas dataframe df = pd . read_csv ( 'poly.csv' ) # Take a quick look at the dataframe df . head () Out[8]: x y 0 -3.292157 -46.916988 1 0.799528 -3.941553 2 -0.936214 -2.800522 3 -4.722680 -103.030914 4 -3.602674 -54.020819 In [9]: # Get the column values for x & y as numpy arrays x = df [[ 'x' ]] . values y = df [ 'y' ] . values In [10]: # Helper code to plot x & y to visually inspect the data fig , ax = plt . subplots () ax . plot ( x , y , 'x' ) ax . set_xlabel ( '$x$ values' ) ax . set_ylabel ( '$y$ values' ) ax . set_title ( '$y$ vs $x$' ) plt . show (); In [11]: # Split the data into train and test sets # Set the train size to 0.8 and random state to 22 x_train , x_test , y_train , y_test = train_test_split ( x , y , train_size = 0.8 , random_state = 22 ) In [22]: # Initialize a linear model model = LinearRegression () # Fit the model on the train data model . fit ( x_train , y_train ) # Get the predictions on the test data using the trained model y_lin_pred = model . predict ( x_test ) In [42]: ### edTest(test_deg) ### # Guess the correct polynomial degree based on the above graph guess_degree = 4 # Generate polynomial features on the train data x_poly_train = PolynomialFeatures ( degree = guess_degree ) . fit_transform ( x_train ) # Generate polynomial features on the test data x_poly_test = PolynomialFeatures ( degree = guess_degree ) . fit_transform ( x_test ) In [43]: # Initialize a model to perform polynomial regression polymodel = LinearRegression ( fit_intercept = False ) # Fit the model on the polynomial transformed train data polymodel . fit ( x_poly_train , y_train ) # Predict on the entire polynomial transformed test data y_poly_pred = polymodel . predict ( x_poly_test ) In [44]: # Helper code to visualise the results idx = np . argsort ( x_test [:, 0 ]) x_test = x_test [ idx ] # Use the above index to get the appropriate predicted values for y_test # y_test values corresponding to sorted test data y_test = y_test [ idx ] # Linear predicted values y_lin_pred = y_lin_pred [ idx ] # Non-linear predicted values y_poly_pred = y_poly_pred [ idx ] In [45]: # First plot x & y values using plt.scatter plt . scatter ( x , y , s = 10 , label = \"Test Data\" ) # Plot the linear regression fit curve plt . plot ( x_test , y_lin_pred , label = \"Linear fit\" , color = 'k' ) # Plot the polynomial regression fit curve plt . plot ( x_test , y_poly_pred , label = \"Polynomial fit\" , color = 'red' , alpha = 0.6 ) # Assigning labels to the axes plt . xlabel ( \"x values\" ) plt . ylabel ( \"y values\" ) plt . legend () plt . show (); In [46]: ### edTest(test_poly_predictions) ### # Calculate the residual values for the polynomial model poly_residuals = y_test - y_poly_pred In [47]: ### edTest(test_linear_predictions) ### # Calculate the residual values for the linear model lin_residuals = y_test - y_lin_pred In [48]: # Helper code to plot the residual values # Plot the histograms of the residuals for the two cases # Distribution of residuals fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 4 )) bins = np . linspace ( - 20 , 20 , 20 ) ax [ 0 ] . set_xlabel ( 'Residuals' ) ax [ 0 ] . set_ylabel ( 'Frequency' ) # Plot the histograms for the polynomial regression ax [ 0 ] . hist ( poly_residuals , bins , label = \"poly_residuals\" , color = '#B2D7D0' , alpha = 0.6 ) # Plot the histograms for the linear regression ax [ 0 ] . hist ( lin_residuals , bins , label = \"lin_residuals\" , color = '#EFAEA4' , alpha = 0.6 ) ax [ 0 ] . legend ( loc = 'upper left' ) # Distribution of predicted values with the residuals ax [ 1 ] . scatter ( y_poly_pred , poly_residuals , s = 10 , color = '#B2D7D0' , label = 'Polynomial predictions' ) ax [ 1 ] . scatter ( y_lin_pred , lin_residuals , s = 10 , color = '#EFAEA4' , label = 'Linear predictions' ) ax [ 1 ] . set_xlim ( - 75 , 75 ) ax [ 1 ] . set_xlabel ( 'Predicted values' ) ax [ 1 ] . set_ylabel ( 'Residuals' ) ax [ 1 ] . legend ( loc = 'upper left' ) fig . suptitle ( 'Residual Analysis (Linear vs Polynomial)' ) plt . show (); â¸ Do you think that polynomial degree is appropriate. Experiment with a degree of polynomial of 2 and comment on what you observe for the residuals? In [49]: ### edTest(test_chow1) ### # Type your answer within in the quotes given answer1 = '2 is better' In [0]:",
        "tags": "lectures",
        "url": "lectures/lecture04/notebook-2/"
    }, {
        "title": "Lecture 4: Multi-linear and Polynomial Regression",
        "text": "s2_exb2_challenge Title : Exercise: Multi-collinearity vs Model Predictions Description : The goal of this exercise is to see how multi-collinearity can affect the predictions of a model. For this, perform a multi-linear regression on the given dataset and compare the coefficients with those from simple linear regression of the individual predictors. Data Description: Instructions: Read the dataset colinearity.csv as a dataframe. For each of the predictor variable, create a linear regression model with the same response variable. Compute the coefficients for each model and store in a list. Fit all predictors using a separate multi-linear regression object. Calculate the coefficients of each model. Compare the coefficients of the multi-linear regression model with those of the simple linear regression model. DISCUSSION: Why do you think the coefficients change and what does it mean? Hints: pd.read_csv(filename) Returns a pandas dataframe containing the data and labels from the file data. pd.DataFrame.drop() Drop specified labels from rows or columns. sklearn.linear_model.LinearRegression Returns a linear regression object from the sklearn library. sklearn.linear_model.LinearRegression.coef_ This attribute returns the coefficient(s) of the linear regression object. sklearn.linear_model.LinearRegression.fit() Fit linear model to the data. pd.Series.reshape() Return a np.ndndarray with the values in the specified shape. Note: This exercise is auto-graded and you can try multiple attempts. In [0]: # Import necessary libraries import numpy as np import pandas as pd import seaborn as sns from pprint import pprint import matplotlib.pyplot as plt from sklearn.linear_model import LinearRegression % matplotlib inline In [0]: # Read the file named \"colinearity.csv\" into a Pandas dataframe df = pd . read_csv ( ___ ) In [0]: # Take a quick look at the dataset df . head () Creation of Linear Regression Objects In [0]: # Choose all the predictors as the variable 'X' (note capitalization of X for multiple features) X = df . drop ([ ___ ], axis = 1 ) # Choose the response variable 'y' y = df . ___ In [0]: ### edTest(test_coeff) ### # Initialize a list to store the beta values for each linear regression model linear_coef = [] # Loop over all the predictors # In each loop \"i\" holds the name of the predictor for i in X : # Set the current predictor as the variable x x = df [[ ___ ]] # Create a linear regression object linreg = ____ # Fit the model with training data # Remember to choose only one column at a time i.e. given by x linreg . fit ( ___ , ___ ) # Add the coefficient value of the model to the list linear_coef . append ( linreg . coef_ ) Multi-Linear Regression using all variables In [0]: # Perform multi-linear regression with all predictors multi_linear = LinearRegression () # Fit the multi-linear regression on all features of the entire data multi_linear . fit ( ___ , ___ ) # Get the coefficients (plural) of the model multi_coef = multi_linear . coef_ Printing the individual $\\beta$ values In [0]: # Helper code to see the beta values of the linear regression models print ( 'By simple(one variable) linear regression for each variable:' , sep = ' \\n ' ) for i in range ( 4 ): pprint ( f 'Value of beta { i + 1 } = { linear_coef [ i ][ 0 ] : .2f } ' ) In [0]: ### edTest(test_multi_coeff) ### # Helper code to compare with the values from the multi-linear regression print ( 'By multi-Linear regression on all variables' ) for i in range ( 4 ): pprint ( f 'Value of beta { i + 1 } = { round ( multi_coef [ i ], 2 ) } ' ) â¸ Why do you think the $\\beta$ values are different in the two cases? A. Because the random seed selected is not as random as we would imagine. B. Because of collinearity between $\\beta_1$ and $\\beta_4$ C. Because multi-linear regression is not a stable model D. Because of the measurement error in the data In [0]: ### edTest(test_chow1) ### # Submit an answer choice as a string below # (Eg. if you choose option C, put 'C') answer1 = '___' In [0]: # Helper code to visualize the heatmap of the covariance matrix corrMatrix = df [[ 'x1' , 'x2' , 'x3' , 'x4' ]] . corr () sns . heatmap ( corrMatrix , annot = True ) plt . show ()",
        "tags": "lectures",
        "url": "lectures/lecture04/notebook-3/"
    }, {
        "title": "Lecture 3: Introduction to Regression kNN and Linear Regression",
        "text": "Slides Lecture 3 : Introduction to Regression Part A - kNN (PDF) Lecture 3 : Introduction to Regression Part B - Error Evaluation and Model Comparison (PDF) Lecture 3 : Introduction to Regression Part C â€“ Linear Models (PDF) Exercises Lecture 3: Exercise: Simple Data Plotting [Notebook] Lecture 3: Exercise: Simple kNN Regression [Notebook] Lecture 3: Exercise: Finding the Best k in kNN Regression [Notebook] Lecture 3: Exercise: MSE for Varying Beta Values [Notebook] Lecture 3: Exercise: Linear Regression using Sklearn [Notebook] References Lecture 3 : List (PDF) Lecture 3 : Dictionaries (PDF) Lecture 3 : Numpy (PDF) Lecture 3 : Zip & Enumerate (PDF)",
        "tags": "lectures",
        "url": "lectures/lecture03/"
    }, {
        "title": "Lecture 3: Introduction to Regression kNN and Linear Regression",
        "text": "s1_exc3_challenge Title : Exercise: Linear Regression using Sklearn Description : The goal of this exercise is to use the sklearn package to fit a Linear Regression on the previously used Advertising.csv datafile and produce a plot like the one given below. Data Description: Instructions: Use train_test_split() function to split the dataset into training and testing sets Use the LinearRegression function to make a model Fit the model on the training set Predict on the testing set using the fit model Estimate the fit of the model using mean_squared_error function Plot the dataset along with the predictions to visualize the fit Hints: pd.read_csv(filename) Returns a pandas dataframe containing the data and labels from the file data sklearn.train_test_split() Splits the data into random train and test subsets sklearn.LinearRegression() LinearRegression fits a linear model sklearn.fit() Fits the linear model to the training data sklearn.predict() Predict using the linear model mean_squared_error() Computes the mean squared error regression loss plt.plot() Plot y versus x as lines and/or markers Note: This exercise is auto-graded, hence please remember to set all the parameters to the values mentioned in the scaffold before marking. In [1]: # Import necessary libraries import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.metrics import r2_score from sklearn.metrics import mean_squared_error from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split % matplotlib inline In [2]: # Read the data from the file \"Advertising.csv\" df = pd . read_csv ( 'Advertising.csv' ) In [0]: # Take a quick look at the data df . head () In [7]: # Assign TV advertising as predictor variable 'x' x = df [[ 'TV' ]] # Set the Sales column as the response variable 'y' y = df [ 'Sales' ] In [13]: # Split the dataset in train and test data with 80% training set x_train , x_test , y_train , y_test = train_test_split ( ___ , ___ , train_size = 0.8 ) In [14]: # Initialize a Linear Regression model using Sklearn model = LinearRegression () # Fit the linear model on the train data model . fit ( ___ , ___ ) # Peedict on the test data using the trained model y_pred_test = model . predict ( ___ ) In [0]: ### edTest(test_mse) ### # Compute the MSE of the predicted test values mse = mean_squared_error ( ___ , ___ ) # Print the computed MSE print ( f 'The test MSE is { ___ } ' ) In [0]: # Make a plot of the data along with the predicted linear regression fig , ax = plt . subplots () ax . scatter ( x , y , label = 'data points' ) # Plot the test data and the predicted output of test data ax . plot ( ___ , ___ , color = 'red' , linewidth = 2 , label = 'model predictions' ) ax . set_xlabel ( 'Advertising' ) ax . set_ylabel ( 'Sales' ) ax . legend () â¸ How does your $MSE$ change when the size of the training set is change to 60% instead of 80%? In [17]: ### edTest(test_chow1) ### # Type your answer within in the quotes given answer1 = '___'",
        "tags": "Lectures",
        "url": "lectures/lecture03/notebook/"
    }, {
        "title": "Lecture 3: Introduction to Regression kNN and Linear Regression",
        "text": "s1_ex1a_challenge Title : Description : The aim of this exercise is to plot TV Ads vs Sales based on the Advertisement dataset which should look similar to the graph given below. Data Description: Instructions: Read the Advertisement data and view the top rows of the dataframe to get an understanding of the data and the columns. Select the first 7 observations and the columns TV and Sales to make a new data frame. Create a scatter plot of the new data frame TV budget vs Sales . Hints: pd.read_csv(filename) Returns a pandas dataframe containing the data and labels from the file data df.iloc[] Returns a subset of the dataframe that is contained in the row range passed as the argument np.linspace() Returns evenly spaced numbers over a specified interval df.head() Returns the first 5 rows of the dataframe with the column names plt.scatter() A scatter plot of y vs. x with varying marker size and/or color plt.xlabel() This is used to specify the text to be displayed as the label for the x-axis plt.ylabel() This is used to specify the text to be displayed as the label for the y-axis Note: This exercise is auto-graded and you can try multiple attempts. In [1]: # Import necessary libraries import pandas as pd import matplotlib.pyplot as plt % matplotlib inline Reading the Advertisement dataset In [2]: # \"Advertising.csv\" containts the data set used in this exercise data_filename = 'Advertising.csv' # Read the file \"Advertising.csv\" file using the pandas library df = pd . read_csv ( \"Advertising.csv\" ) In [3]: # Get a quick look of the data df . describe () Out[3]: TV Radio Newspaper Sales count 200.000000 200.000000 200.000000 200.000000 mean 147.042500 23.264000 30.554000 14.022500 std 85.854236 14.846809 21.778621 5.217457 min 0.700000 0.000000 0.300000 1.600000 25% 74.375000 9.975000 12.750000 10.375000 50% 149.750000 22.900000 25.750000 12.900000 75% 218.825000 36.525000 45.100000 17.400000 max 296.400000 49.600000 114.000000 27.000000 In [4]: ### edTest(test_pandas) ### # Create a new dataframe by selecting the first 7 rows of # the current dataframe df_new = df . head ( 7 ) In [5]: # Print your new dataframe to see if you have selected 7 rows correctly print ( df_new ) TV Radio Newspaper Sales 0 230.1 37.8 69.2 22.1 1 44.5 39.3 45.1 10.4 2 17.2 45.9 69.3 9.3 3 151.5 41.3 58.5 18.5 4 180.8 10.8 58.4 12.9 5 8.7 48.9 75.0 7.2 6 57.5 32.8 23.5 11.8 Plotting the graph In [7]: # Use a scatter plot for plotting a graph of TV vs Sales plt . scatter ( df_new . TV , df_new . Sales ) # Add axis labels for clarity (x : TV budget, y : Sales) plt . xlabel ( \"TV budget\" ) plt . ylabel ( \"Sales\" ) Out[7]: Text(0, 0.5, 'Sales') Post-Exercise Question Instead of just plotting seven points, experiment to plot all points. In [8]: # Your code here plt . scatter ( df . TV , df . Sales ) # Add axis labels for clarity (x : TV budget, y : Sales) plt . xlabel ( \"TV budget\" ) plt . ylabel ( \"Sales\" ) Out[8]: Text(0, 0.5, 'Sales') In [0]:",
        "tags": "Lectures",
        "url": "lectures/lecture03/notebook-1/"
    }, {
        "title": "Lecture 3: Introduction to Regression kNN and Linear Regression",
        "text": "s1_exa2_challenge Title : Exercise: Simple kNN Regression Description : The goal of this exercise is to re-create the plots given below. You would have come across these graphs in the lecture as well. Data Description: Instructions: Part 1: KNN by hand for k=1 Read the Advertisement data. Get a subset of the data from row 5 to row 13. Apply the kNN algorithm by hand and plot the first graph as given above. Part 2: Using sklearn package Read the Advertisement dataset. Split the data into train and test sets using the train_test_split() function. Set k_list as the possible k values ranging from 1 to 70. For each value of k in k_list : Use sklearn KNearestNeighbors() to fit train data. Predict on the test data. Use the helper code to get the second plot above for k=1,10,70. Hints: np.argsort() Returns the indices that would sort an array. df.iloc[] Returns a subset of the dataframe that is contained in the column range passed as the argument. plt.plot() Plot y versus x as lines and/or markers. df.values Returns a Numpy representation of the DataFrame. pd.idxmin() Returns index of the first occurrence of minimum over requested axis. np.min() Returns the minimum along a given axis. np.max() Returns the maximum along a given axis. model.fit() Fit the k-nearest neighbors regressor from the training dataset. model.predict() Predict the target for the provided data. np.zeros() Returns a new array of given shape and type, filled with zeros. train_test_split(X,y) Split arrays or matrices into random train and test subsets. np.linspace() Returns evenly spaced numbers over a specified interval. KNeighborsRegressor(n_neighbors=k_value) Regression-based on k-nearest neighbors. Note: This exercise is auto-graded, hence please remember to set all the parameters to the values mentioned in the scaffold before marking. In [2]: # Import required libraries import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.neighbors import KNeighborsRegressor from sklearn.model_selection import train_test_split % matplotlib inline In [3]: # Read the data from the file \"Advertising.csv\" filename = 'Advertising.csv' df_adv = pd . read_csv ( filename ) In [4]: # Take a quick look of the dataset df_adv . head () Out[4]: TV Radio Newspaper Sales 0 230.1 37.8 69.2 22.1 1 44.5 39.3 45.1 10.4 2 17.2 45.9 69.3 9.3 3 151.5 41.3 58.5 18.5 4 180.8 10.8 58.4 12.9 Part 1: KNN by hand for $k=1$ In [5]: # Get a subset of the data i.e. rows 5 to 13 # Use the TV column as the predictor x_true = df_adv . TV . iloc [ 5 : 13 ] # Use the Sales column as the response y_true = df_adv . Sales . iloc [ 5 : 13 ] # Sort the data to get indices ordered from lowest to highest TV values idx = np . argsort ( x_true ) . values # Get the predictor data in the order given by idx above x_true = x_true . iloc [ idx ] . values # Get the response data in the order given by idx above y_true = y_true . iloc [ idx ] . values In [6]: ### edTest(test_findnearest) ### # Define a function that finds the index of the nearest neighbor # and returns the value of the nearest neighbor. # Note that this is just for k = 1 where the distance function is # simply the absolute value. def find_nearest ( array , value ): # Hint: To find idx, use .idxmin() function on the series idx = pd . Series ( np . abs ( array - value )) . idxmin () # Return the nearest neighbor index and value return idx , array [ idx ] In [7]: # Create some synthetic x-values (might not be in the actual dataset) x = np . linspace ( np . min ( x_true ), np . max ( x_true )) # Initialize the y-values for the length of the synthetic x-values to zero y = np . zeros (( len ( x ))) In [8]: # Apply the KNN algorithm to predict the y-value for the given x value for i , xi in enumerate ( x ): # Get the Sales values closest to the given x value y [ i ] = y [ find_nearest ( x , xi )[ 0 ]] Plotting the data In [12]: # Plot the synthetic data along with the predictions plt . plot ( x , y , '-.' ) # Plot the original data using black x's. plt . plot ( x_true , Y_true , 'kx' ) # Set the title and axis labels plt . title ( 'TV vs Sales' ) plt . xlabel ( 'TV budget in $1000' ) plt . ylabel ( 'Sales in $1000' ) --------------------------------------------------------------------------- NameError Traceback (most recent call last) /tmp/ipykernel_30/3566749308.py in <module> 3 4 # Plot the original data using black x's. ----> 5 plt . plot ( x_true , Y_true , 'kx' ) 6 7 # Set the title and axis labels NameError : name 'Y_true' is not defined Part 2: KNN for $k\\ge1$ using sklearn In [0]: # Read the data from the file \"Advertising.csv\" data_filename = 'Advertising.csv' df = pd . read_csv ( data_filename ) # Set 'TV' as the 'predictor variable' x = df [[ ___ ]] # Set 'Sales' as the response variable 'y' y = df [ ___ ] In [0]: ### edTest(test_shape) ### # Split the dataset in training and testing with 60% training set # and 40% testing set with random state = 42 x_train , x_test , y_train , y_test = train_test_split ( ___ , ___ , train_size = ___ , random_state = ___ ) In [0]: ### edTest(test_nums) ### # Choose the minimum k value based on the instructions given on the left k_value_min = ___ # Choose the maximum k value based on the instructions given on the left k_value_max = ___ # Create a list of integer k values betwwen k_value_min and k_value_max using linspace k_list = np . linspace ( k_value_min , k_value_max , 70 ) In [0]: # Set the grid to plot the values fig , ax = plt . subplots ( figsize = ( 10 , 6 )) # Variable used to alter the linewidth of each plot j = 0 # Loop over all the k values for k_value in k_list : # Creating a kNN Regression model model = KNeighborsRegressor ( n_neighbors = int ( ___ )) # Fitting the regression model on the training data model . fit ( ___ , ___ ) # Use the trained model to predict on the test data y_pred = model . predict ( ___ ) # Helper code to plot the data along with the model predictions colors = [ 'grey' , 'r' , 'b' ] if k_value in [ 1 , 10 , 70 ]: xvals = np . linspace ( x . min (), x . max (), 100 ) ypreds = model . predict ( xvals ) ax . plot ( xvals , ypreds , '-' , label = f 'k = { int ( k_value ) } ' , linewidth = j + 2 , color = colors [ j ]) j += 1 ax . legend ( loc = 'lower right' , fontsize = 20 ) ax . plot ( x_train , y_train , 'x' , label = 'train' , color = 'k' ) ax . set_xlabel ( 'TV budget in $1000' , fontsize = 20 ) ax . set_ylabel ( 'Sales in $1000' , fontsize = 20 ) plt . tight_layout () â¸ In the plotting code above, re-run ax.plot(x_train, y_train,'x',label='train',color='k') with x_test and y_test instead. According to you, which k value is the best and why? In [0]: ### edTest(test_chow1) ### # Type your answer within in the quotes given answer1 = '___'",
        "tags": "Lectures",
        "url": "lectures/lecture03/notebook-2/"
    }, {
        "title": "Lecture 3: Introduction to Regression kNN and Linear Regression",
        "text": "s1_exb1_challenge Title : Exercise: Finding the Best k in kNN Regression Description : The goal here is to find the value of k of the best performing model based on the test MSE. Data Description: Instructions: Read the data into a Pandas dataframe object. Select the sales column as the response variable and TV budget column as the predictor variable. Make a train-test split using sklearn.model_selection.train_test_split . Create a list of integer k values using numpy.linspace . For each value of k Fit a kNN regression on train set. Calculate MSE on test set and store it. Plot the test MSE values for each k. Find the k value associated with the lowest test MSE. Hints: train_test_split(X,y) Split arrays or matrices into random train and test subsets. np.linspace() Returns evenly spaced numbers over a specified interval. KNeighborsRegressor(n_neighbors=k_value) Regression-based on k-nearest neighbors. model.predict() Predict the target for the provided data. mean_squared_error() Computes the mean squared error regression loss. dict.keys() Returns a view object that displays a list of all the keys in the dictionary. dict.values() Returns a list of all the values available in a given dictionary. plt.plot() Plot y versus x as lines and/or markers. dict.items() Returns a list of dict's (key, value) tuple pairs. Note: This exercise is auto-graded and you can try multiple attempts. In [1]: # Import necessary libraries import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.utils import shuffle from sklearn.metrics import r2_score from sklearn.metrics import mean_squared_error from sklearn.neighbors import KNeighborsRegressor from sklearn.model_selection import train_test_split % matplotlib inline Reading the standard Advertising dataset In [0]: # Read the file 'Advertising.csv' into a Pandas dataset df = pd . read_csv ( 'Advertising.csv' ) In [0]: # Take a quick look at the data df . head () In [3]: # Set the 'TV' column as predictor variable x = df [[ ___ ]] # Set the 'Sales' column as response variable y = df [ ___ ] Train-Test split In [21]: ### edTest(test_shape) ### # Split the dataset in training and testing with 60% training set and # 40% testing set x_train , x_test , y_train , y_test = train_test_split ( ___ , ___ , train_size = ___ , random_state = 66 ) In [28]: ### edTest(test_nums) ### # Choosing k range from 1 to 70 k_value_min = 1 k_value_max = 70 # Create a list of integer k values between k_value_min and # k_value_max using linspace k_list = np . linspace ( k_value_min , k_value_max , num = 70 , dtype = int ) Model fit In [0]: # Setup a grid for plotting the data and predictions fig , ax = plt . subplots ( figsize = ( 10 , 6 )) # Create a dictionary to store the k value against MSE fit {k: MSE@k} knn_dict = {} # Variable used for altering the linewidth of values kNN models j = 0 # Loop over all k values for k_value in k_list : # Create a KNN Regression model for the current k model = KNeighborsRegressor ( n_neighbors = int ( ___ )) # Fit the model on the train data model . fit ( x_train , y_train ) # Use the trained model to predict on the test data y_pred = model . predict ( ___ ) # Calculate the MSE of the test data predictions MSE = ____ # Store the MSE values of each k value in the dictionary knn_dict [ k_value ] = ___ # Helper code to plot the data and various kNN model predictions colors = [ 'grey' , 'r' , 'b' ] if k_value in [ 1 , 10 , 70 ]: xvals = np . linspace ( x . min (), x . max (), 100 ) ypreds = model . predict ( xvals ) ax . plot ( xvals , ypreds , '-' , label = f 'k = { int ( k_value ) } ' , linewidth = j + 2 , color = colors [ j ]) j += 1 ax . legend ( loc = 'lower right' , fontsize = 20 ) ax . plot ( x_train , y_train , 'x' , label = 'test' , color = 'k' ) ax . set_xlabel ( 'TV budget in $1000' , fontsize = 20 ) ax . set_ylabel ( 'Sales in $1000' , fontsize = 20 ) plt . tight_layout () Graph plot In [0]: # Plot a graph which depicts the relation between the k values and MSE plt . figure ( figsize = ( 8 , 6 )) plt . plot ( ___ , ___ , 'k.-' , alpha = 0.5 , linewidth = 2 ) # Set the title and axis labels plt . xlabel ( 'k' , fontsize = 20 ) plt . ylabel ( 'MSE' , fontsize = 20 ) plt . title ( 'Test $MSE$ values for different k values - KNN regression' , fontsize = 20 ) plt . tight_layout () Find the best knn model In [0]: ### edTest(test_mse) ### # Find the lowest MSE among all the kNN models min_mse = min ( ___ ) # Use list comprehensions to find the k value associated with the lowest MSE best_model = [ key for ( key , value ) in knn_dict . items () if value == min_mse ] # Print the best k-value print ( \"The best k value is \" , best_model , \"with a MSE of \" , min_mse ) â¸ From the options below, how would you classify the \"goodness\" of your model? A. Good B. Satisfactory C. Bad In [0]: ### edTest(test_chow1) ### # Submit an answer choice as a string below (eg. if you choose option A, put 'A') answer1 = '___' In [0]: # Helper code to compute the R2_score of your best model model = KNeighborsRegressor ( n_neighbors = best_model [ 0 ]) model . fit ( x_train , y_train ) y_pred_test = model . predict ( x_test ) # Print the R2 score of the model print ( f \"The R2 score for your model is { r2_score ( y_test , y_pred_test ) } \" ) â¸ After observing the $R&#94;2$ value, how would you now classify your model? A. Good B. Satisfactory C. Bad In [1]: ### edTest(test_chow2) ### # Submit an answer choice as a string below (eg. if you choose option A, put 'A') answer2 = '___'",
        "tags": "Lectures",
        "url": "lectures/lecture03/notebook-3/"
    }, {
        "title": "Lecture 3: Introduction to Regression kNN and Linear Regression",
        "text": "s1_exc2_challenge Title : Exercise: MSE for Varying Beta Values Description : The goal of this exercise is to produce a plot like the one given below. Data Description: Instructions: Fix $\\beta_0 = 2.2$. Change $\\beta_1$ in a range $[-2, 3]$. Estimate the fit of the model by following the steps below: Create empty lists to store the MSE and $\\beta_1$. Set a range of values for $\\beta_1$ and compute MSE for each one. Hints: np.linspace(start, stop, num) Return evenly spaced numbers over a specified interval. np.arange(start, stop, increment) Return evenly spaced values within a given interval. list_name.append(item) Add an item to the end of the list. plt.xlabel() This is used to specify the text to be displayed as the label for the x-axis. plt.ylabel() This is used to specify the text to be displayed as the label for the y-axis. Note: This exercise is auto-graded, hence please remember to set all the parameters to the values mentioned in the scaffold before marking. In [13]: # Import necessary libraries import numpy as np import pandas as pd import matplotlib.pyplot as plt % matplotlib inline Reading the dataset In [14]: # Read data file 'Advertising.csv' into a Pandas Dataframe df = pd . read_csv ( 'Advertising.csv' ) In [0]: # Take a quick look at the data df . head () In [17]: # Create a new dataframe called `df_new` with the columns 'TV' and 'sales' df_new = df [[ 'TV' , 'sales' ]] Beta and MSE Computation In [6]: # Set beta0 to 2.2 beta0 = 2.2 In [12]: # Create an empty list to store the MSE mse_list = ___ # Create an empty list to store the beta1 values beta1_list = ___ In [0]: ### edTest(test_beta) ### # This loops runs from -2 to 3.0 with an increment of 0.1 # i.e a total of 51 steps for beta1 in ___ : # Calculate prediction of x using beta0 and beta1 # Recall the formula y = beta0 + beta1*x y_predict = ___ # Calculate the Mean Squared Error mean_squared_error = ___ # Append the new MSE to the list initialized above mse_list . ___ # Append the beta1 value to the appropriate list initialized above beta1_list . ___ Plotting the graph In [0]: ### edTest(test_mse) ### # Helper code to plot the MSE as a function of beta1 plt . plot ( beta1_list , mse_list ) plt . xlabel ( 'Beta1' ) plt . ylabel ( 'MSE' ) â¸ Go back and change your $\\beta_0$ value of your choice and report your new optimal $\\beta_1$ value and new lowest $MSE$. Is the MSE lower than before, or more? In [0]: ### edTest(test_chow1) ### # Submit an answer choice as a string below answer1 = '___'",
        "tags": "Lectures",
        "url": "lectures/lecture03/notebook-4/"
    }, {
        "title": "Lab 2",
        "text": "cs109a_lab02_seaborn CS109A Introduction to Data Science Lab 2: EDA with Pandas (+seaborn) Harvard University Fall 2021 Instructors : Pavlos Protopapas and Natesh Pillai Authors : Natesh Pillai In [2]: ## RUN THIS CELL TO GET THE RIGHT FORMATTING import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\" ) . text HTML ( styles ) Out[2]: In [3]: import pandas as pd import seaborn as sns import matplotlib.pyplot as plt In this lecture we will look at tools for plotting using both matplotlib and seaborn. Load data The file quartets.csv contains 4 different tiny datasets that we will use to quickly understand the value of ploting. In [4]: quartets = pd . read_csv ( 'quartets.csv' , index_col = 0 ) Exploration In [5]: quartets . info () <class 'pandas.core.frame.DataFrame'> Int64Index: 44 entries, 1 to 11 Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 x 44 non-null int64 1 y 44 non-null float64 2 quartet 44 non-null object dtypes: float64(1), int64(1), object(1) memory usage: 1.4+ KB We see there are 44 entries, two numerical columns x and y and one column to potentially identify every quartet dataset. How does this dataframe look like? In [106]: quartets . head () Out[106]: x y quartet 1 10 8.04 I 2 8 6.95 I 3 13 7.58 I 4 9 8.81 I 5 11 8.33 I How do random samples look like? In [7]: quartets . sample ( 5 ) Out[7]: x y quartet 8 19 12.50 IV 3 13 7.58 I 3 13 12.74 III 5 11 8.33 I 2 8 5.76 IV Quartet's names In [8]: quartets [ 'quartet' ] . unique () . tolist () Out[8]: ['I', 'II', 'III', 'IV'] Display the first 3 samples from every dataset In [9]: quartets . groupby ( 'quartet' ) . head ( 3 ) Out[9]: x y quartet 1 10 8.04 I 2 8 6.95 I 3 13 7.58 I 1 10 9.14 II 2 8 8.14 II 3 13 8.74 II 1 10 7.46 III 2 8 6.77 III 3 13 12.74 III 1 8 6.58 IV 2 8 5.76 IV 3 8 7.71 IV Display 2 random samples from every dataset In [10]: quartets . groupby ( 'quartet' ) . sample ( 2 ) Out[10]: x y quartet 5 11 8.33 I 10 7 4.82 I 3 13 8.74 II 7 6 6.13 II 10 7 6.42 III 2 8 6.77 III 10 8 7.91 IV 4 8 8.84 IV Display every quartet's dataset size In [11]: quartets . groupby ( 'quartet' ) . size () Out[11]: quartet I 11 II 11 III 11 IV 11 dtype: int64 Descriptive Statistics In [12]: quartets . groupby ( 'quartet' ) . agg ([ 'mean' , 'std' ]) . round ( 3 ) Out[12]: x y mean std mean std quartet I 9 3.317 7.501 2.032 II 9 3.317 7.501 2.032 III 9 3.317 7.500 2.030 IV 9 3.317 7.501 2.031 Almost same mean and standard deviation for every quartet. This looks like all quartets samples could be sampled from the same distribution. These are tiny datasets so we could read them all In [13]: quartets [ quartets [ 'quartet' ] == 'I' ] Out[13]: x y quartet 1 10 8.04 I 2 8 6.95 I 3 13 7.58 I 4 9 8.81 I 5 11 8.33 I 6 14 9.96 I 7 6 7.24 I 8 4 4.26 I 9 12 10.84 I 10 7 4.82 I 11 5 5.68 I In [14]: quartets [ quartets [ 'quartet' ] == 'II' ] Out[14]: x y quartet 1 10 9.14 II 2 8 8.14 II 3 13 8.74 II 4 9 8.77 II 5 11 9.26 II 6 14 8.10 II 7 6 6.13 II 8 4 3.10 II 9 12 9.13 II 10 7 7.26 II 11 5 4.74 II In [15]: quartets [ quartets [ 'quartet' ] == 'III' ] Out[15]: x y quartet 1 10 7.46 III 2 8 6.77 III 3 13 12.74 III 4 9 7.11 III 5 11 7.81 III 6 14 8.84 III 7 6 6.08 III 8 4 5.39 III 9 12 8.15 III 10 7 6.42 III 11 5 5.73 III In [16]: quartets [ quartets [ 'quartet' ] == 'IV' ] Out[16]: x y quartet 1 8 6.58 IV 2 8 5.76 IV 3 8 7.71 IV 4 8 8.84 IV 5 8 8.47 IV 6 8 7.04 IV 7 8 5.25 IV 8 19 12.50 IV 9 8 5.56 IV 10 8 7.91 IV 11 8 6.89 IV Plot or not to plot? Pandas by default comes with matplotlib incorporated. Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics. BoxPlot In [17]: quartets . groupby ( 'quartet' ) . boxplot ( grid = False ); Seaborn's palettes In [18]: sns . color_palette () Out[18]: In [19]: sns . color_palette ( 'pastel' ) Out[19]: In [20]: palette = 'pastel' Seaborn's boxplots Similar boxplots with Matplotlib and Seaborn In [21]: fig , axes = plt . subplots ( 2 , 2 , figsize = ( 8 , 7 )) axes = axes . flatten () . tolist () for quartet , g in quartets . groupby ( 'quartet' ): ax = axes . pop ( 0 ) sns . boxplot ( data = g , ax = ax , palette = palette ); ax . set_title ( f 'quartet { quartet } ' ) plt . suptitle ( \"Quartets' boxplots\" ); Using seaborn boxplots to compare quartes's shared features seaborn.boxplot() pandas.melt() In [22]: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 16 , 4 )) sns . boxplot ( x = 'x' , y = 'value' , hue = 'quartet' , data = pd . melt ( quartets , id_vars = 'quartet' , var_name = 'x' , value_name = 'value' ), ax = ax , palette = palette ) ax . set_title ( \"quartets' features\" ); The problem with the plot above is that we are forcing different features (like x and y ) to share the same y-axis. So, another way to acomplish the goal could be this one In [23]: fig , axes = plt . subplots ( 1 , 2 , figsize = ( 16 , 4 )) for i , col in enumerate ([ 'x' , 'y' ]): sns . boxplot ( x = 'quartet' , y = col , data = quartets , ax = axes [ i ], palette = palette ); axes [ i ] . set_title ( f 'variable { col } ' ) Histograms Pandas let us easily plot the individual quartet's feature histogram in one line of code. In [24]: quartets . groupby ( 'quartet' ) . hist (); The histograms allows us to start to see some differences Seaborn's histograms seaborn.histplot() We could do the same with seaborn with this code In [25]: for quartet , g in quartets . groupby ( 'quartet' ): fig , axes = plt . subplots ( 1 , 2 , figsize = ( 8 , 2.5 )) sns . histplot ( data = g , x = \"x\" , hue = 'quartet' , ax = axes [ 0 ], palette = palette , bins = 10 , kde = True ); sns . histplot ( data = g , x = \"y\" , hue = 'quartet' , ax = axes [ 1 ], palette = palette , bins = 10 , kde = True ); plt . suptitle ( f 'Quartet { quartet } ' ) We can plot all quartets's two features x and y in two different plots moving out the subplots creation In [26]: # some elements are 'bars' (default but too noisy when plotting so many features), 'step', 'poly' element = 'step' fig , axes = plt . subplots ( 1 , 2 , figsize = ( 12 , 5 )) legends = [] for quartet , g in quartets . groupby ( 'quartet' ): legends . append ( f 'quartet { quartet } ' ) sns . histplot ( data = g , x = \"x\" , hue = 'quartet' , ax = axes [ 0 ], palette = palette , bins = 10 , kde = False , alpha =. 2 , element = element ); sns . histplot ( data = g , x = \"y\" , hue = 'quartet' , ax = axes [ 1 ], palette = palette , bins = 10 , kde = False , alpha =. 2 , element = element ); axes [ 0 ] . legend ( legends ) axes [ 1 ] . legend ( legends ); FacetGrid This is a powerful tool that can be used in combination with ploting method from seaborn or even matplotlib to plot multiple subplots based on some conditional relationship. seaborn.FacetGrid() : Multi-plot grid for plotting conditional relationships. Grid of histograms In [27]: for feature in [ 'x' , 'y' ]: # create the grid with condition quartet g = sns . FacetGrid ( quartets , col = \"quartet\" , palette = palette , col_wrap = 4 ) # for every condition we are going to create a subplot for the grid for column \"feature\" g . map ( sns . histplot , feature , bins = 10 ); # col_wrap define the number of columns. Change the value to 3 and 2 to understand visually its behaviour We can create one FacetGrid for all. For that we need to convert the dataframe to access values based on conditions. In [28]: melted = pd . melt ( quartets , id_vars = 'quartet' , var_name = 'x' , value_name = 'value' ) . rename ( columns = { 'x' : 'variable' }) melted Out[28]: quartet variable value 0 I x 10.00 1 I x 8.00 2 I x 13.00 3 I x 9.00 4 I x 11.00 ... ... ... ... 83 IV y 5.25 84 IV y 12.50 85 IV y 5.56 86 IV y 7.91 87 IV y 6.89 88 rows Ã— 3 columns In [29]: # create the grid with quartets as columns and variable as rows g = sns . FacetGrid ( melted , row = \"variable\" , col = 'quartet' , palette = palette , sharex = False ) g . map ( sns . histplot , 'value' , bins = 10 ); # we need set sharex to False to avoid distorting shapes between rows (you can try changing it to True) Scatter plots Knowing that we have x and y features, we can think about using other kind of helpful plots. Why not a scatter plot? In [30]: quartets . groupby ( 'quartet' ) . plot . scatter ( x = 'x' , y = 'y' , s = 50 ); Scatter plots with seaborn We can combine matplotlib with seaborn to improve the aesthetic. In [31]: fig , axes = plt . subplots ( 2 , 2 , figsize = ( 7 , 7 )) axes = axes . flatten () . tolist () for quartet , g in quartets . groupby ( 'quartet' ): ax = axes . pop ( 0 ) sns . scatterplot ( data = g , x = 'x' , y = 'y' , ax = ax ) ax . set_title ( f 'quartet { quartet } ' ) plt . subplots_adjust ( hspace = 0.3 ); Scatter plots with FacetGrid FacetGrid is great to avoid writting too many lines of matplotlib code. In this case we can force the grid to share x and y domain to simplify features domains comparison. In [32]: g = sns . FacetGrid ( quartets , col = 'quartet' , palette = palette , col_wrap = 2 , sharex = True , sharey = True ) g . map ( sns . scatterplot , 'x' , 'y' ); Line plots We could also use a lineplot but to do that we need to know that dots should be ordered in the x axis. In [33]: quartets . sort_values ( by = 'x' ) . groupby ( 'quartet' ) . plot ( x = 'x' , y = 'y' , marker = 'o' , lw =. 7 ); All in one We also can use matplotlib to plot all groups in the same plot In [34]: # create one figure of 1 x 1 size. fig , ax = plt . subplots ( 1 , 1 , figsize = ( 16 , 6 )) # plot all 4 quartets in the same ax quartets . sort_values ( by = 'x' ) . groupby ( 'quartet' ) . plot ( x = 'x' , y = 'y' , marker = 'o' , ms = 10 , lw =. 7 , alpha =. 7 , ax = ax ) plt . ylabel ( 'y' ) plt . title ( 'All in one quartets' ); Lineplots with seaborn Seaborn.lineplot() simplifies the creation of the same plot. In [35]: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 16 , 6 )) sns . lineplot ( data = quartets , x = 'x' , y = 'y' , hue = 'quartet' , marker = 'o' , ms = 10 , lw =. 7 , alpha =. 7 , ax = ax ) plt . title ( 'All in one quartets' ); And we can plot all quartets together (removing the conditional hue for seaborn) In [36]: fig , axes = plt . subplots ( 1 , 2 , figsize = ( 16 , 4 )) sns . lineplot ( data = quartets , x = 'x' , y = 'y' , lw =. 7 , ax = axes [ 0 ]) axes [ 0 ] . set_title ( 'one line of seaborn' ) quartets . plot ( x = 'x' , y = 'y' , lw =. 7 , ax = axes [ 1 ]) axes [ 1 ] . set_title ( 'one line of matplotlib' ); Seaborn is built on matplotlib, so using more lines of matplotlib should let you arrive to the same seaborn plot. LetÂ´s use a different dataset We will load an already known dataset. Source: https://www.kaggle.com/spscientist/students-performance-in-exams Original source generator: http://roycekimmons.com/tools/generated_data/exams If you go to the original source you will find this is a fictitious dataset created specifically for data science training purposes. In [37]: df = pd . read_csv ( 'StudentsPerformance.csv' ) . rename ( columns = { 'race/ethnicity' : 'group' , 'parental level of education' : 'parental' , 'test preparation course' : 'course' , 'math score' : 'math' , 'reading score' : 'reading' , 'writing score' : 'writing' } ) In [38]: df . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 1000 entries, 0 to 999 Data columns (total 8 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 gender 1000 non-null object 1 group 1000 non-null object 2 parental 1000 non-null object 3 lunch 1000 non-null object 4 course 1000 non-null object 5 math 1000 non-null int64 6 reading 1000 non-null int64 7 writing 1000 non-null int64 dtypes: int64(3), object(5) memory usage: 62.6+ KB In [39]: df . head () Out[39]: gender group parental lunch course math reading writing 0 female group B bachelor's degree standard none 72 72 74 1 female group C some college standard completed 69 90 88 2 female group B master's degree standard none 90 95 93 3 male group A associate's degree free/reduced none 47 57 44 4 male group C some college standard none 76 78 75 In [40]: df [ 'group' ] . unique () . tolist () Out[40]: ['group B', 'group C', 'group A', 'group D', 'group E'] Let's simplify the dataframe We can simplify the group values to the group letter Series.str Series.str : Vectorized string functions for Series and Index. In [41]: df [ 'group' ] = df [ 'group' ] . str [ - 1 ] df [ 'group' ] . unique () . tolist () Out[41]: ['B', 'C', 'A', 'D', 'E'] In [42]: df . head () Out[42]: gender group parental lunch course math reading writing 0 female B bachelor's degree standard none 72 72 74 1 female C some college standard completed 69 90 88 2 female B master's degree standard none 90 95 93 3 male A associate's degree free/reduced none 47 57 44 4 male C some college standard none 76 78 75 In [43]: df [ 'course' ] . unique () Out[43]: array(['none', 'completed'], dtype=object) Series.apply Series.apply :Invoke function on values of Series. Series . apply ( func , convert_dtype = True , args = (), ** kwargs ) In [44]: # we verify that we have never change this column values yet if 'completed' in df [ 'course' ] . unique () . tolist (): df [ 'course' ] = df [ 'course' ] . apply ( lambda x : 1 if x == 'completed' else 0 ) # we can change the column values type to boolean df [ 'course' ] = df [ 'course' ] . astype ( bool ) df [ 'course' ] . unique () Out[44]: array([False, True]) In [45]: df . head () Out[45]: gender group parental lunch course math reading writing 0 female B bachelor's degree standard False 72 72 74 1 female C some college standard True 69 90 88 2 female B master's degree standard False 90 95 93 3 male A associate's degree free/reduced False 47 57 44 4 male C some college standard False 76 78 75 Missing values In [46]: df . isna () . sum () Out[46]: gender 0 group 0 parental 0 lunch 0 course 0 math 0 reading 0 writing 0 dtype: int64 None of the column series present missing values Some questions: Does gender affect math scores? Does math scores affect gender? Does reading and writing scores affect math scores? Do math scores affect reading and writing scores? Does a group perform better at math than the rest? Does parental level education affect math scores? In [47]: df [[ 'reading' , 'math' ]] . sample ( 5 ) Out[47]: reading math 340 61 58 370 77 84 186 76 80 687 78 77 499 71 76 In [48]: df [[ 'reading' , 'math' ]] . describe () Out[48]: reading math count 1000.000000 1000.00000 mean 69.169000 66.08900 std 14.600192 15.16308 min 17.000000 0.00000 25% 59.000000 57.00000 50% 70.000000 66.00000 75% 79.000000 77.00000 max 100.000000 100.00000 It's not common at all to see a zero on scores. Here we see a 0 found at math In [49]: df [ df [ 'math' ] == 0 ] Out[49]: gender group parental lunch course math reading writing 59 female C some high school free/reduced False 0 17 10 Does this sample look possible? Why? Histograms for our selected variables In [50]: df [[ 'reading' , 'math' ]] . hist ( bins = 50 , grid = False ); Histograms for our selected variables (seaborn) seaborn.histplot() We can plot histogram in different plots using matplotlib subplots In [51]: plt . figure ( figsize = ( 12 , 4 )) sns . histplot ( df [[ 'reading' ]], bins = 50 , ax = plt . subplot ( 121 ), palette = palette ) sns . histplot ( df [[ 'math' ]], bins = 50 , ax = plt . subplot ( 122 ), palette = palette ); But knowing that by default sns.histplot merges all features into the same plot, it could be simpler In [52]: sns . histplot ( df [[ 'reading' , 'math' ]], bins = 50 , palette = palette ); Kernel Density Estimate In [53]: df [[ 'reading' , 'math' ]] . plot . kde () plt . title ( 'KDEs' ); Seaborn comes with the method seaborn.kdeplot() to create Kernel Density Plots but we can just set the histplot params kde to True to combine them. In [54]: sns . histplot ( df [[ 'reading' , 'math' ]], bins = 50 , kde = True , palette = palette ); BoxPlot In [55]: df [[ 'reading' , 'math' ]] . boxplot (); At first glance distributions looks similar as one could expect. Math scores distribution looks a bit shifted down. Boxplots with seaborn seanborn.boxplot() In [56]: sns . boxplot ( data = df [[ 'reading' , 'math' ]], palette = palette ); Boxenplots or Letter values seaborn.boxenplot() In [57]: sns . boxenplot ( data = df [[ 'reading' , 'math' ]], palette = palette ); Violinplots seaborn.violinplot() In [58]: sns . violinplot ( data = df [[ 'reading' , 'math' ]], palette = palette ); What about the relation between the scores? Do they interact? Scatter to the rescue In [59]: df . plot . scatter ( x = 'reading' , y = 'math' , s = 10 , alpha =. 5 , figsize = ( 6 , 5 )) plt . title ( 'reading vs math' ); There is visual correlation between these variables. Correlation Pandas has implemented a method named corr() . DataFrame.corr() : Compute pairwise correlation of columns, excluding NA/null values. DataFrame . corr ( method = 'pearson' , min_periods = 1 ) In [60]: df [[ 'reading' , 'math' ]] . corr () Out[60]: reading math reading 1.00000 0.81758 math 0.81758 1.00000 Pandas corr() offers different correlation methods. In most cases pearson or/and spearman are the methods to go. In [61]: for method in [ 'pearson' , 'kendall' , 'spearman' ]: # iloc is used to access value at first row second column. corr = df [[ 'reading' , 'math' ]] . corr ( method = method ) . iloc [ 0 , 1 ] print ( f ' { method } correlation: { corr : .3f } ' ) pearson correlation: 0.818 kendall correlation: 0.617 spearman correlation: 0.804 We've confirmed there is a strong (linear) correlation between reading and math scores. Each variable could work as a proxy of the other variable. Boxplot on the whole dataframe In [62]: df . boxplot (); Correlation between all variables In [63]: df . corr () Out[63]: course math reading writing course 1.000000 0.177702 0.241780 0.312946 math 0.177702 1.000000 0.817580 0.802642 reading 0.241780 0.817580 1.000000 0.954598 writing 0.312946 0.802642 0.954598 1.000000 Reading and writing have a really strong correlation. Of course one could use plots In [64]: cols = [ 'math' , 'reading' , 'writing' ] for i , c1 in enumerate ( cols ): c2 = cols [ i + 1 ] if i < len ( cols ) - 1 else cols [ 0 ] df . plot . scatter ( x = c1 , y = c2 , s = 10 , alpha =. 5 ) plt . title ( f ' { c1 } vs { c2 } ' ) Scatter plots with seaborn Seaborn comes with seaborn.scatterplot() . In [65]: sns . pairplot ( df . select_dtypes ( 'number' ), palette = palette ); In [66]: df [[ 'gender' , 'math' , 'reading' , 'writing' ]] . sample ( 5 ) Out[66]: gender math reading writing 663 female 65 69 67 903 female 93 100 100 443 female 73 83 76 362 female 52 58 58 137 male 70 55 56 In [67]: df [[ 'gender' , 'math' , 'reading' , 'writing' ]] . describe () Out[67]: math reading writing count 1000.00000 1000.000000 1000.000000 mean 66.08900 69.169000 68.054000 std 15.16308 14.600192 15.195657 min 0.00000 17.000000 10.000000 25% 57.00000 59.000000 57.750000 50% 66.00000 70.000000 69.000000 75% 77.00000 79.000000 79.000000 max 100.00000 100.000000 100.000000 Pie plot In [68]: df [ 'gender' ] . value_counts ( normalize = True ) . plot . pie ( figsize = ( 6 , 6 )); Seaborn doesn't come with a method to plot pie plots In [69]: df . groupby ( 'gender' ) . mean () Out[69]: course math reading writing gender female 0.355212 63.633205 72.608108 72.467181 male 0.360996 68.728216 65.473029 63.311203 In [70]: df . groupby ( 'gender' ) . boxplot (); pandas.melt() is a powerful method to unpivot a dataframe. We are going to use it to simplify use of some seaborn plots. In [71]: score_cols = df . select_dtypes ( 'number' ) . columns . tolist () id_vars = [ c for c in df . columns if c not in score_cols ] score_cols , id_vars melted = pd . melt ( df , id_vars = id_vars , var_name = 'skill' , value_name = 'score' ) melted . head () Out[71]: gender group parental lunch course skill score 0 female B bachelor's degree standard False math 72 1 female C some college standard True math 69 2 female B master's degree standard False math 90 3 male A associate's degree free/reduced False math 47 4 male C some college standard False math 76 When you make things easier to read for seaborn, seaborn will make the plots easier to read for you. In [72]: for func in [ sns . boxplot , sns . boxenplot , sns . violinplot ]: g = sns . FacetGrid ( melted , col = \"skill\" ) g . map ( func , 'score' , 'gender' , order = None , palette = palette ); In [73]: sns . pairplot ( df , palette = palette , hue = 'gender' ); In [74]: df . groupby ( 'gender' ) . plot . kde (); In [75]: df [ 'is_female' ] = df [ 'gender' ] . apply ( lambda x : 1 if x == 'female' else 0 ) df [ 'is_female' ] = df [ 'is_female' ] . astype ( float ) df [ 'is_male' ] = df [ 'gender' ] . apply ( lambda x : 1 if x == 'male' else 0 ) df [ 'is_male' ] = df [ 'is_male' ] . astype ( float ) df . head () Out[75]: gender group parental lunch course math reading writing is_female is_male 0 female B bachelor's degree standard False 72 72 74 1.0 0.0 1 female C some college standard True 69 90 88 1.0 0.0 2 female B master's degree standard False 90 95 93 1.0 0.0 3 male A associate's degree free/reduced False 47 57 44 0.0 1.0 4 male C some college standard False 76 78 75 0.0 1.0 Instead of looking at correlation between all variables we want to see how this new variables is_female correlates with the scores. Pandas gives us the method DataFrame.corrwith() for this kind of cases. In [76]: df [[ 'math' , 'reading' , 'writing' ]] . corrwith ( df [ 'is_female' ]) Out[76]: math -0.167982 reading 0.244313 writing 0.301225 dtype: float64 In [77]: df [[ 'math' , 'reading' , 'writing' ]] . corrwith ( df [ 'is_male' ]) Out[77]: math 0.167982 reading -0.244313 writing -0.301225 dtype: float64 One Hot Encoding What we have just done is known as One Hot Encoding of the gender variable. Pandas has a method to simplify this kind of conversion under the name: get_dummies() pd.get_dummies() : Convert categorical variable into dummy/indicator variables. pandas . get_dummies ( data , prefix = None , prefix_sep = '_' , dummy_na = False , columns = None , sparse = False , drop_first = False , dtype = None ) In [78]: pd . get_dummies ( df [ 'gender' ]) . head () Out[78]: female male 0 1 0 1 1 0 2 1 0 3 0 1 4 0 1 In example, we could make use of this method to create a new DataFrame with the scores and the one hot encoded version of gender variable. We use get_dummies() to encode the categorical gender variable and then we use the pd.concat() method to concatenate two DataFrames on the horizonal axis. In [79]: df_encoded = pd . concat ([ df [[ 'math' , 'reading' , 'writing' ]], pd . get_dummies ( df [ 'gender' ])], axis = 1 ) df_encoded . head () Out[79]: math reading writing female male 0 72 72 74 1 0 1 69 90 88 1 0 2 90 95 93 1 0 3 47 57 44 0 1 4 76 78 75 0 1 And then using one line of code more we could arrive to the same conclusions In [80]: df_encoded . corr () Out[80]: math reading writing female male math 1.000000 0.817580 0.802642 -0.167982 0.167982 reading 0.817580 1.000000 0.954598 0.244313 -0.244313 writing 0.802642 0.954598 1.000000 0.301225 -0.301225 female -0.167982 0.244313 0.301225 1.000000 -1.000000 male 0.167982 -0.244313 -0.301225 -1.000000 1.000000 Extra Most of ML models don't work with categorial variables. You will become familiar with method like get_dummies() from pandas or similar ones from other libraries to prepare the data that will feed your models. Sometimes, it is convenient to normalize or standardize the data. We already known that the new dataset ranges, so we could normalize it using one line of code. In [81]: df_normalized = df_encoded . div ( df_encoded . max () - df_encoded . min ()) df_normalized . head () Out[81]: math reading writing female male 0 0.72 0.867470 0.822222 1.0 0.0 1 0.69 1.084337 0.977778 1.0 0.0 2 0.90 1.144578 1.033333 1.0 0.0 3 0.47 0.686747 0.488889 0.0 1.0 4 0.76 0.939759 0.833333 0.0 1.0 The new dataset df_normalized looks like a generic dataset for any kind of ML algorithm And we can check that we don't change correlations after normalizing them. In [82]: df_normalized . corr () . round ( 14 ) == df_encoded . corr () . round ( 14 ) Out[82]: math reading writing female male math True True True True True reading True True True True True writing True True True True True female True True True True True male True True True True True Heatmap seaborn.heatmap() : Plot rectangular data as a color-encoded matrix. Heatmap is a great tool for plotting features' correlations In [83]: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 6 )) sns . heatmap ( df_normalized . corr (), annot = True , fmt = '.2f' , cmap = 'Blues' , ax = ax ); Who will approve? In [84]: approval_threshold = 40 In [85]: df [ 'approved' ] = df [ 'math' ] >= approval_threshold df [ 'approved' ] = df [ 'approved' ] . astype ( int ) df . head () Out[85]: gender group parental lunch course math reading writing is_female is_male approved 0 female B bachelor's degree standard False 72 72 74 1.0 0.0 1 1 female C some college standard True 69 90 88 1.0 0.0 1 2 female B master's degree standard False 90 95 93 1.0 0.0 1 3 male A associate's degree free/reduced False 47 57 44 0.0 1.0 1 4 male C some college standard False 76 78 75 0.0 1.0 1 In [86]: df [ 'approved' ] . value_counts ( normalize = True ) . plot . bar (); Seaborn has a method for plotting counts of feature's values. In [87]: sns . countplot ( data = df , x = 'group' , palette = palette ); The problem is that the countplot method doesn't count with a normalize parameter. So trying to plot a normalized version is to so simple as when using pandas ( Series.value_counts(normalize=True).plot.bar() ) In [88]: df [ 'approved' ] . value_counts ( normalize = True ) . to_frame () Out[88]: approved 1 0.96 0 0.04 In [89]: sns . barplot ( data = (( df [ 'approved' ] . value_counts ( normalize = True ) * 100 ) . to_frame () . reset_index () . rename ( columns = { 'approved' : '%' , 'index' : 'approved' })), x = 'approved' , y = '%' , palette = palette ); In [90]: df [[ 'gender' , 'course' , 'reading' , 'writing' , 'math' ]] . groupby ( 'gender' ) . corrwith ( df [ 'approved' ]) Out[90]: course reading writing math gender female 0.102233 0.513812 0.549594 0.548292 male 0.071767 0.317447 0.331337 0.339371 In [91]: df . groupby ( 'approved' )[ 'gender' ] . value_counts ( normalize = True ) . plot . bar (); We will try to do the same plot with seaborn In [92]: tmp = ( df . groupby ( 'approved' )[ 'gender' ] . value_counts ( normalize = True ) . to_frame () . rename ( columns = { 'gender' : '%' }) * 100 ) . reset_index () sns . barplot ( data = tmp , x = 'approved' , y = '%' , hue = 'gender' , palette = palette ); Years in a cell When plotting you can think of using one of these four approaches: Pandas Pandas + Matplotlib Pandas + Seaborn Pandas + Seaborn + Matplotlib Pandas Learning: easy Default Visual: bad Custom Visual: regular TIP: just knowing what are the plotting methods implemented in pandas is enough to start plotting many things to extract information for you (but maybe not for a presentation). Pandas + Matplotlib Learning: difficult Default Visual: regular Custom Visual: excellent but tricky (it's all about learning matplotlib, not easy to start from scratch) TIP: Think the plot you want and then using DataFrame.groupby or some condition applied to the dataframe will be enough to feed your plots. Pandas + Seaborn Learning: good Default Visual: good Custom Visual: very good TIP: Seaborn is almost about preparing a DataFrame to feed the seaborn plot you are looking for. So you need to learn about Seaborn's available plots and probably expend some time learning pandas methods like melt and pivot to transform the dataframe in an input kind of the ones seaborn likes. Pandas + Matplotlib + Seaborn Learning: difficult Default Visual: good Custom Visual: excellent TIP: Sky is the limit. Remember that seaborn was built on matplotlib. In [93]: sns . countplot ( data = df , x = 'approved' , hue = 'gender' , palette = palette ); In [94]: ax = plt . subplot () for group , g in df . groupby ([ 'approved' , 'gender' ]): g [[ 'math' ]] . hist ( bins = 50 , ax = ax , alpha =. 3 , label = f ' { group [ 0 ] } { group [ 1 ] } ' ); plt . legend (); To do the same plot with seaborn we will need to convert the dataframe like the melted one and add some new column that represents a combination of gender and approved. Sometimes it's better to look for alternatives that let us do the same analysis without too much coding. In [95]: g = sns . FacetGrid ( df , col = 'approved' , row = 'gender' ) g . map ( sns . histplot , 'math' , palette = palette ); In [96]: g = sns . FacetGrid ( df , col = 'approved' , row = 'gender' ) g . map ( sns . histplot , 'reading' , palette = palette ); In [97]: g = sns . FacetGrid ( df , col = 'approved' , row = 'gender' ) g . map ( sns . histplot , 'writing' , palette = palette ); In [98]: # let's repeat the three features with violinplots for feature in [ 'reading' , 'writing' , 'math' ]: g = sns . FacetGrid ( df , col = 'approved' , row = 'gender' , sharex = True , sharey = True ) g . map ( sns . violinplot , feature , order = None , palette = palette ); If we prepare data for seaborn, seaborn will give what we want. For instance, seaborn.violinplot() permits to split the violin distribution using a secondary binary hue feature. But this just can be done when using parameters x and y . In this case we can use a dummy feature to plot what we want. Knowing this will help us to improve our previous plot. In [99]: df [ 'dummy' ] = '' # let's repeat the three features with violinplots for feature in [ 'reading' , 'writing' , 'math' ]: g = sns . FacetGrid ( df , col = 'approved' , sharey = True ) g . map ( sns . violinplot , data = df , x = 'dummy' , y = feature , hue = 'gender' , split = True , order = None , palette = palette ); g . add_legend () # we want to display the gender legend g . set_ylabels ( 'score' ) g . fig . subplots_adjust ( top = 0.8 ) g . fig . suptitle ( f 'feature: { feature } ' , fontsize = 12 , font = 'verdana' ) del df [ 'dummy' ] PairGrid seaborn.PairGrid() is a great tool that let us extend seaborn plots easily. In [100]: # this should do something similar to pairplot() but without setting the histogram in the diagonal g = sns . PairGrid ( df ) g . map ( sns . scatterplot ); In [101]: del df [ 'is_female' ] del df [ 'is_male' ] Maybe you didn't see the power of PairGrids. Let's try again with a new custom PairGrid plot with multivariate KDE subplots In [102]: # Create a cubehelix colormap to use with kdeplot cmap = sns . cubehelix_palette ( start = 0 , light =. 95 , as_cmap = True ) g = sns . PairGrid ( df , diag_sharey = False ) g . map_upper ( sns . kdeplot , cmap = cmap , fill = True ) g . map_lower ( sns . kdeplot , cmap = cmap , fill = True ) g . map_diag ( sns . kdeplot , color = '#aa0000' , fill = True ); Summary In this lecture you've learnt: Some important things about using SEABORN with PANDAS! Importance of plots Using pandas for EDA Notion of One Hot Encoding Pandas pandas.read_csv() pandas.concat() pandas.get_dummies() DataFrame.info() DataFrame.head() DataFrame.sample() DataFrame.describe() DataFrame.unique() DataFrame.str DataFrame.grouby() DataFrame.sourt_values() DataFrame.corr() DataFrame.corrwith() DataFrameGroupBy.size() Pandas (plotting) DataFrame.boxplot() DataFrame.hist() DataFrame.plot() DataFrame.plot.kde() DataFrame.plot.pie() DataFrame.plot.scatter() matplotlib matplotlib.pyplot.subplots() matplotlib.pyplot.title() matplotlib.pyplot.plot() matplotlib.pyplot.suptitle() matplotlib.pyplot.subplot() matplotlib.pyplot.subplots_adjust() matplotlib.pyplot.ylabel() matplotlib.pyplot.legend() matplotlib.pyplot.figure() seaborn seaborn.boxplot() seaborn.boxenplot() seaborn.histplot() seaborn.barplot() seaborn.countplot() seaborn.scatterplot() seaborn.violinplot() seaborn.lineplot() seaborn.pairplot() seaborn.heatmap() seaborn.kdeplot() seaborn.FacetGrid() seaborn.PairGrid() In [ ]:",
        "tags": "labs",
        "url": "labs/lab2/notebook-1/"
    }, {
        "title": "Lab 2",
        "text": "Ex1_lab2 Tracking Covid-19 at U.S. Colleges and Universities The New York Times is releasing counts of Covid-19 cases reported on college and university campuses in the United States. Since late July, they have been conducting a rolling survey of American colleges and universities â€” including every four-year public institution and every private college that competes in N.C.A.A. sports â€” to track the number of coronavirus cases reported among students and employees. The survey now includes more than 1,900 colleges. Starting in 2021 the number of cases in 2021 is also included. Colleges and universities that have reported zero cases will be listed with a zero for cases. We have excluded missing values. This is an exercise meant to make you think of ways of grouping, aggregating, plotting, and presenting this data in a way that an audience would appreciate. Data Data can be found in the colleges.csv file. date,state,county,city,ipeds_id,college,cases,cases_2021,notes 2021-02-26,Alabama,Madison,Huntsville,100654,Alabama A&M University,41,, â€¦ 2021-02-26,Alabama,Jefferson,Birmingham,100663,University of Alabama at Birmingham,2856,570,\"Total is known to include one or more cases from a medical school, medical center, teaching hospital, clinical setting or other academic program in health sciences.\" The fields have the following definitions: date : The date of the last update. state : The state where the college is located. county : The county where the college is located. city : The city where the college is located. ipeds_id : The Integrated Postsecondary Education Data System (IPEDS) ID number for the college. college : The name of the college or university. cases : The total number of reported Covid-19 cases among university students and employees in all fields, including those whose roles as doctors, nurses, pharmacists or medical students put them at higher risk of contracting the virus, since the beginning of the pandemic. In [2]: % matplotlib inline import pandas as pd ; pd . set_option ( 'max_columns' , 6 ) import seaborn as sns import matplotlib.pyplot as plt In [3]: colleges = pd . read_csv ( 'colleges.csv' ) In [4]: colleges . shape Out[4]: (1946, 7) In [5]: from IPython.core.display import display , HTML display ( HTML ( colleges . head ( 10 ) . to_html ())) date state county city ipeds_id college cases 0 2021-05-26 Alabama Madison Huntsville 100654 Alabama A&M University 41 1 2021-05-26 Alabama Montgomery Montgomery 100724 Alabama State University 2 2 2021-05-26 Alabama Limestone Athens 100812 Athens State University 45 3 2021-05-26 Alabama Lee Auburn 100858 Auburn University 2742 4 2021-05-26 Alabama Montgomery Montgomery 100830 Auburn University at Montgomery 220 5 2021-05-26 Alabama Walker Jasper 102429 Bevill State Community College 4 6 2021-05-26 Alabama Jefferson Birmingham 100937 Birmingham-Southern College 263 7 2021-05-26 Alabama Limestone Tanner 101514 Calhoun Community College 137 8 2021-05-26 Alabama Tallapoosa Alexander City 100760 Central Alabama Community College 49 9 2021-05-26 Alabama Coffee Enterprise 101143 Enterprise State Community College 76 Some ways of grouping the data would be by state or by county . When presenting, a plot is usually better that just arrays of numbers. In [6]: colleges . groupby ( '_____' ) . agg ([ 'mean' , 'std' ]) . shape Out[6]: (55, 2) In [53]: colleges . groupby ( '_____' ) . agg ({ 'cases' : sum }) Out[53]: cases county Abbeville 0 Acadia 149 Ada 1642 Adair 749 Adams 948 ... ... Yellow Medicine 93 Yellowstone 246 Yolo 678 York 804 Yuma 41 736 rows Ã— 1 columns You are encouraged to make your own story with plots. Optional: add geospatial data and maps to your plots People have written libraries to display geospatial data and nice visualizations of maps. To run the following cells in your environment you will need to install the libraries geoplot and geopandas using conda or pip . Feel free to try this example, although, for this class it's entirely optional . The example below is from the geoplot website. In [2]: #!conda install geoplot -c conda-forge #!conda install geopandas In [1]: import geopandas as gpd import geoplot as gplt In [2]: usa_cities = gpd . read_file ( gplt . datasets . get_path ( 'usa_cities' )) usa_cities . head () Out[2]: id POP_2010 ELEV_IN_FT STATE geometry 0 53 40888.0 1611.0 ND POINT (-101.29627 48.23251) 1 101 52838.0 830.0 ND POINT (-97.03285 47.92526) 2 153 15427.0 1407.0 ND POINT (-98.70844 46.91054) 3 177 105549.0 902.0 ND POINT (-96.78980 46.87719) 4 192 17787.0 2411.0 ND POINT (-102.78962 46.87918) In [3]: continental_usa_cities = usa_cities . query ( 'STATE not in [\"HI\", \"AK\", \"PR\"]' ) gplt . pointplot ( continental_usa_cities ); Out[3]: <AxesSubplot:> In [4]: contiguous_usa = gpd . read_file ( gplt . datasets . get_path ( 'contiguous_usa' )) ax = gplt . polyplot ( contiguous_usa ) gplt . pointplot ( continental_usa_cities , ax = ax ); Out[4]: <AxesSubplot:> In [5]: import geoplot.crs as gcrs ax = gplt . webmap ( contiguous_usa , projection = gcrs . WebMercator ()) gplt . pointplot ( continental_usa_cities , ax = ax ) Out[5]: <GeoAxesSubplot:> In [0]:",
        "tags": "labs",
        "url": "labs/lab2/notebook-2/"
    }, {
        "title": "Lab 2",
        "text": "Lec2_Ex2 CS109A Introduction to PANDAS Lecture 2, Exercise 2: PANDAS Intro 2 Harvard University Fall 2021 Instructors : Pavlos Protopapas and Natesh Pillai Exercise 2: PANDAS Intro 2 Let's get some more practice with a few of the core PANDAS functions. In [2]: import pandas as pd We'll continue working with StudentsPerformance dataset from Exercise 1. In [3]: # import the CSV file df = pd . read_csv ( \"StudentsPerformance.csv\" ) df . head () Indexing - iloc and loc It's very important to understand the differences between loc and iloc. Looking at the next cell code one could think that they do the same thing. (When you query just for one row you obtain an object whose name is the index of the selected row.) In [133]: df . iloc [ 10 ] == df . loc [ 10 ] In [134]: all ( df . iloc [ 10 ] == df . loc [ 10 ]) The first time we loaded the CSV into a DataFrame we didn't tell pandas to interpret a specific column as an index, so pandas created an index for us. Whether the ordering imposed on our data by this index should be respected is a matter on which iloc and loc disagree. To really learn the difference of iloc and loc we need to shuffle the rows To do that we can use the sample method to take a new sample of the dataframe original size (frac=1 means that) _Bonus: Stop and consider: what is the purpose of setting a random_state when we call them sample method?_ In [135]: df = df . sample ( frac = 1 , random_state = 109 ) df Now let's repeat our code from ealier. In [136]: df . iloc [ 10 ] == df . loc [ 10 ] In [137]: all ( df . iloc [ 10 ] == df . loc [ 10 ]) It turns out that loc filters by index value (something like where df.index == value ). That is, loc 's results are depend only on the indices (which are now scrambled after sampling). The actual positions of elements in the DataFrame are ignored. In [147]: df . loc [ 10 ] By contrast, iloc filters by row position (something like where df.index == df.index[value] ) So iloc 's results depend on the actual positions of elements in a pandas data structure. The indices of these elements are ignored. It's this difference that explains counterintuitive results like this: In [150]: df . index [ 10 ] Consider a single row index with iloc In [146]: df . iloc [ 10 ] And take note of where you can find the index in output formatted like this. Enough chat. Time for... Exercise In the cell below, fill in the blank so that the row5 variable stores the 5th row of df. To be clear, imagine our DataFrame looked as follows: Index Words 0 this 1 is 2 not 3 easy We'd say the 1st row is the one with this word, the 2nd row is the one with is word, the 3rd row is the one with not word, etc. In [0]: ### edTest(test_a) ### row5 = ________ row5 You can display the first rows to have a better understanding of what you did. Can you find the row you've just selected? In [0]: df . iloc [: 5 ] Notice how we can use familar Python slice notation with iloc and loc ! Sorting We scrambled out df earlier with sample . We should also know how to tidy things up. Exercise In the cell below, fill in the blank so that sorted_df now stores df after sorting it by the math score column in decreasing order ( HINT ) In [11]: ### edTest(test_b) ### sorted_df = ________ sorted_df Exercise In the cell below, fill in the blank so that sorted_row5 stores the 5th row of sorted_df . To be clear, imagine our sorted DataFrame looked as follows: Index Words 3 easy 1 is 2 not 0 this We'd say the 1st row is the one with easy , the 2nd row is the one with is , the 3rd row is the one with not , etc. In [12]: ### edTest(test_c) ### sorted_row5 = ________ sorted_row5 Can you find the row you've just selected? In [149]: # len('head()') < len('iloc[:5]') :) sorted_df . head () Column Naming Conventions How you've named your df columns can affect the amount of typing required to manipulate your data, the readability of your code, and even the syntax optons you have. Let's take a look at some best practices for naming columns. In [10]: df We can see that there are columns names whose lengths are not confortable for coding. What we can do to make our life easier: Try to work with short columns names Try to avoid characters like spaces that will allow us to use column access without brackets Try working in only lower or upper case (prefably lower case; there's no need to shout) Our df already conforms to this last suggestion. We'll find that, after some renaming, things should become easier for us, making: expressions like these ones: condition = ( df [ 'test preparation course' ] != 'completed' ) & ( df [ 'writing score' ] > df [ 'writing score' ] . median ()) to become: condition = ( df . course != 'completed' ) & ( df . writing > df . writing . median ()) Exercise In the cell below fill in the blank with these goals in mind: rename column race/ethnicity to race rename column parental level of education to peduc raname column test preparation course to course remove score (included the left space char) from the rest of the columns HINT1 : Don't by shy, check the documentation if you need some help HINT2 : in many cases, it's faster to access docstrings using help help ( df . rename ) HINT3 : TAB autocomplete can be ued to you explore an object's available methods and attributes.\\ HINT4 : Still more exciting, place your cursor after the opening paranthesis of a function call and press SHIFT+TAB once or twice. Instantly, you're presented with a docstring. It's a whole new world! ðŸŒˆ In [0]: ### edTest(test_d) ### df = df . rename ( columns = { ____ }) df Look for missing values Missing values in our DataFrames can cause many issues. They can cause certain operations and function calls to fail and throw an error. Perhaps worse, this problems can happen 'silently,' affecting our results without us realizing there is a problem. Unless we take precautions of course! The first step is locating missingness. This dataset doesn't have any missing values. So we'll make some ourselves and 'poke a holes' in the Dataframe. In [19]: # 'poking holes' in the data df . iloc [ 0 , 5 ] = None df . iloc [ 2 , 2 ] = None Exercise Fill in the blank to display whether or not entries in the first 5 rows are missing ( HINT ) Solution In [0]: df . ___ . ___ Exercise Fill in the blanks to sum the total number of missing values in each of the dataset's columns. In [0]: ### edTest(test_e) ### resultE = df . ___ . ___ display ( resultE ) Now let's deal with these 'holes' we've just made Exercise Fill the missing math entry with that column's mean . Hint: Select subsets of the data frame using a column name or names Note: The blanks here represent just one way of doing this. Don't feel constrain by the blanks here. Solution In [0]: df___ = df___ . ___ ( df___ . ___ ) df . head () Exercise Drop the row with the missing peduc from the DataFrame. Hint: to make it easier, consider that this is now the only remaining missing value Solution In [0]: df = df___ df . head () Categorical Columns - nunique() and unique() If some of your data is categorical (e.g., taking on a descredte set of values with an inherent ordering) you'll often what to know exactly how many unique values you're dealing with. nunique and unique are here to help and can be used on a single column or across multiple columns. Please consider, unique will return all unique values. What if you ask for the uniques of a column with 1 million different values? ðŸ¤” This particular method should be used wisely. In [46]: df . nunique () In [63]: df [ 'gender' ] . unique () . tolist () In [66]: # the line below represents the usage that should be avoided # df['math'].unique() Exercise Fill in the blanks using unique() and nunique() to complete the method print_uniques to help us to learn more about the categorical variables in our data. In [0]: ### edTest(test_f) ### def print_uniques ( df , col , limit = 10 ): \"\"\"Print column's uniques values when the number of column's uniques is lower or equal than limit \"\"\" n = df [ col ] . ______ if n <= limit : print ( f ' { col } :' , df [ col ] . ______ . tolist ()) else : print ( f ' { col } :' , f 'more than { limit } uniques' ) for col in df . columns : print_uniques ( df , col ) Descriptive statistics In Pandas, DataFrames have a simple method for displaying summary statistics. In [0]: df . describe () Exercise Sometimes we don't want to access all these statistics. In the cell below fill in the blanks to get the mean and the standard deviation of the writing and reading columns ( HINT ). In [0]: ### edTest(test_g) ### resultG = df [[ ____ ]] . aggregate ([ ____ ]) display ( resultG ) Now we can group our dataframe by a specific column(s) value's and aggregate the other columns. Hint: Try using agg() as an alternative to spare some typing Exercise Group the dataframe by peduc and gender while aggregating math , reading , and writing with the mean and course with the mode. Tip: Again, don't feel constained by the blanks. This command may span multiple lines. In [33]: df . ___ ( _____ ) . ___ ( _________ ) Out[33]: math reading writing peduc gender associate's degree female 65.250000 74.120690 74.000000 male 70.764151 67.433962 65.405660 bachelor's degree female 68.349206 77.285714 78.380952 male 70.581818 68.090909 67.654545 high school female 59.322581 68.268817 66.731183 male 64.705882 61.480392 58.539216 master's degree female 66.500000 76.805556 77.638889 male 74.826087 73.130435 72.608696 some college female 65.406780 73.550847 74.050847 male 69.009259 64.990741 63.148148 some high school female 59.296703 69.109890 68.285714 male 67.955672 64.693182 61.375000 We would likely want to treat peduc as an 'ordinal' rather than a categorical variable as it does have an inherent ordering. Feel free to try using indexing to sort the rows. But after the grouping we now have multiple indices for each row! There's still more Pandas to discover ðŸ¼",
        "tags": "labs",
        "url": "labs/lab2/notebook-3/"
    }, {
        "title": "Lecture 2: Introduction to PANDAS and EDA",
        "text": "Slides Lecture 2: Introduction to PANDAS and EDA (Notebook) Exercises Lecture 2: Introduction to Data Science (Notebook) Lecture 2: Introduction to PANDAS 2 (Notebook)",
        "tags": "lectures",
        "url": "lectures/lecture02/"
    }, {
        "title": "Lecture 2: Introduction to Data Science",
        "text": "Lec2_Ex1 CS109a Introduction to Data Science Lecture 2, Exercise 1: PANDAS Intro Harvard University Fall 2021 Instructors : Pavlos Protopapas and Natesh Pillai Exercise 1: PANDAS Intro As discussed in class, PANDAS is Python library that contains highly useful data structures, including DataFrames, which makes Exploratory Data Analysis (EDA) easy. Here, we get practice with some of the elementary functions. In [1]: import pandas as pd For this exercise, we will be working with StudentsPerformance dataset made available through Kaggle . It contains information about the exame score of ( fictional ) high school students. In [2]: # import the CSV file df = pd . read_csv ( \"StudentsPerformance.csv\" ) PANDAS Basics ðŸ¼ Let's get started with basic functionality of PANDAS! Exercise In the cell below fill in the blank to display general dataframe info rmation. _Tip: The Pandas documention will be your best friend. But in many cases, a simple tab autocomplete can find what your looking for._ In [5]: ### edTest(test_a) ### df . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 1000 entries, 0 to 999 Data columns (total 8 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 gender 1000 non-null object 1 race/ethnicity 1000 non-null object 2 parental level of education 1000 non-null object 3 lunch 1000 non-null object 4 test preparation course 1000 non-null object 5 math score 1000 non-null int64 6 reading score 1000 non-null int64 7 writing score 1000 non-null int64 dtypes: int64(3), object(5) memory usage: 62.6+ KB Examine the output carefully. There's a lot in there. Can you interpret each column? What about the details in header footer? Exercise In the cell below, fill in the blank so that the variable cols stores the df 's column names. NOTE: Please keep the type of the data structure as a <class 'pandas.core.indexes.base.Index'> . Do not have to convert this to a list.\\ Tip: Reviewing the DataFrame object itself might help In [6]: ### edTest(test_b) ### cols = df . columns # Check at least the type is the right one assert type ( cols ) == pd . core . indexes . base . Index Exercise In the cell below, fill in the blank so that: num_cols stores the number of columns in df \\ ( HINT ) In [7]: ### edTest(test_c) ### num_rows = df . shape [ 0 ] num_cols = df . shape [ 1 ] Exercise In the cell below, fill in the blank so that first_seven is equal to the first 7 rows. ( HINT ) In [9]: ### edTest(test_d) ### first_seven = df . head ( 7 ) Exercise In the cell below, fill in the blank so that last_four is equal to the last 4 rows. ( HINT ) In [8]: ### edTest(test_e) ### last_four = df . tail ( 4 ) Exercise In the cell below, fill in the blank so that the unique_parental_education_levels variable stores a list of the 6 distinct values found within the parental level of education column of df .\\ Tip: Again, try searching the documentation In [26]: ### edTest(test_f) ### unique_parental_education_levels = df [ 'parental level of education' ] . unique () unique_parental_education_levels Out[26]: array([\"bachelor's degree\", 'some college', \"master's degree\", \"associate's degree\", 'high school', 'some high school'], dtype=object) In [27]: # we can check if they are really 6 print ( 'Are there 6 unique values?:' , len ( unique_parental_education_levels ) == 6 ) # we can display them here print () print ( unique_parental_education_levels ) Are there 6 unique values?: True [\"bachelor's degree\" 'some college' \"master's degree\" \"associate's degree\" 'high school' 'some high school'] Exercise In the cell below, fill in the blank so that the scored_100_at_math variable stores the DataFrame row(s) that correspond to everyone who scored 100 at math.\\ Hint: Think 'indexing.' Specifically, boolean indexing In [13]: ### edTest(test_g) ### scored_100_at_math = df [ df [ \"math score\" ] == 100 ] scored_100_at_math Out[13]: gender race/ethnicity parental level of education lunch test preparation course math score reading score writing score 149 male group E associate's degree free/reduced completed 100 100 93 451 female group E some college standard none 100 92 97 458 female group E bachelor's degree standard none 100 100 100 623 male group A some college standard completed 100 96 86 625 male group D some college standard completed 100 97 99 916 male group E bachelor's degree standard completed 100 100 100 962 female group E associate's degree standard none 100 100 100 Some observations about conditions In [14]: # this shows that using the condition with 'loc' is the same as not using 'loc' condition = df [ 'math score' ] == 100 all ( df [ condition ] == df . loc [ condition ]) Out[14]: True In [15]: # the condition is a boolean series to be used with the whole dataframe # the condition will be used to filter the dataframe to those rows where condition value is True # we can see that condition has as many rows as the original dataframe condition Out[15]: 0 False 1 False 2 False 3 False 4 False ... 995 False 996 False 997 False 998 False 999 False Name: math score, Length: 1000, dtype: bool Exercise In the cell below, fill in the blank to display scores' descriptive statistics ( HINT ). In [16]: ### edTest(test_h) ### df . describe () Out[16]: math score reading score writing score count 1000.00000 1000.000000 1000.000000 mean 66.08900 69.169000 68.054000 std 15.16308 14.600192 15.195657 min 0.00000 17.000000 10.000000 25% 57.00000 59.000000 57.750000 50% 66.00000 70.000000 69.000000 75% 77.00000 79.000000 79.000000 max 100.00000 100.000000 100.000000 Exercise In the cell below, fill in the blanks so that the uncompleted_with_good_writing_score variable stores the DataFrame rows that correspond to everyone who hasn't completed the preparation course and there writing score is above the median. In [25]: ### edTest(test_i) ### # not completed preparation course condition cond1 = df [ 'test preparation course' ] != 'completed' # writing score above the median cond2 = df [ 'writing score' ] > df [ 'writing score' ] . median () uncompleted_with_good_writing_score = df . loc [ cond1 & cond2 ] uncompleted_with_good_writing_score Out[25]: gender race/ethnicity parental level of education lunch test preparation course math score reading score writing score 0 female group B bachelor's degree standard none 72 72 74 2 female group B master's degree standard none 90 95 93 4 male group C some college standard none 76 78 75 5 female group B associate's degree standard none 71 83 78 12 female group B high school standard none 65 81 73 ... ... ... ... ... ... ... ... ... 981 male group D some high school standard none 81 78 78 984 female group C some high school standard none 74 75 82 992 female group D associate's degree free/reduced none 55 76 76 993 female group D bachelor's degree free/reduced none 62 72 74 999 female group D some college free/reduced none 77 86 86 251 rows Ã— 8 columns Obvervation: the '&' operator differs from the 'and' operator In [0]:",
        "tags": "Lectures",
        "url": "lectures/lecture02/notebook-1/"
    }, {
        "title": "Lecture 2: Introduction to PANDAS 2",
        "text": "Lec2_Ex2 CS109A Introduction to PANDAS Lecture 2, Exercise 2: PANDAS Intro 2 Harvard University Fall 2021 Instructors : Pavlos Protopapas and Natesh Pillai Exercise 2: PANDAS Intro 2 Let's get some more practice with a few of the core PANDAS functions. In [1]: import pandas as pd We'll continue working with StudentsPerformance dataset from Exercise 1. In [2]: # import the CSV file df = pd . read_csv ( \"StudentsPerformance.csv\" ) df . head () Out[2]: gender race/ethnicity parental level of education lunch test preparation course math score reading score writing score 0 female group B bachelor's degree standard none 72 72 74 1 female group C some college standard completed 69 90 88 2 female group B master's degree standard none 90 95 93 3 male group A associate's degree free/reduced none 47 57 44 4 male group C some college standard none 76 78 75 Indexing - iloc and loc It's very important to understand the differences between loc and iloc. Looking at the next cell code one could think that they do the same thing. (When you query just for one row you obtain an object whose name is the index of the selected row.) In [3]: df . iloc [ 10 ] == df . loc [ 10 ] Out[3]: gender True race/ethnicity True parental level of education True lunch True test preparation course True math score True reading score True writing score True Name: 10, dtype: bool In [4]: all ( df . iloc [ 10 ] == df . loc [ 10 ]) Out[4]: True The first time we loaded the CSV into a DataFrame we didn't tell pandas to interpret a specific column as an index, so pandas created an index for us. Whether the ordering imposed on our data by this index should be respected is a matter on which iloc and loc disagree. To really learn the difference of iloc and loc we need to shuffle the rows To do that we can use the sample method to take a new sample of the dataframe original size (frac=1 means that) _Bonus: Stop and consider: what is the purpose of setting a random_state when we call them sample method?_ In [5]: df = df . sample ( frac = 1 , random_state = 109 ) df Out[5]: gender race/ethnicity parental level of education lunch test preparation course math score reading score writing score 301 male group D some high school free/reduced none 56 54 52 895 female group E some high school free/reduced none 32 34 38 763 female group B high school standard none 62 62 63 854 male group C some high school standard none 62 64 55 49 male group C high school standard completed 82 84 82 ... ... ... ... ... ... ... ... ... 399 male group D some high school standard none 60 59 54 141 female group C some college free/reduced none 59 62 64 757 male group E bachelor's degree free/reduced completed 70 68 72 245 male group C associate's degree standard none 85 76 71 262 female group C some high school free/reduced none 44 50 51 1000 rows Ã— 8 columns Now let's repeat our code from ealier. In [6]: df . iloc [ 10 ] == df . loc [ 10 ] Out[6]: gender False race/ethnicity False parental level of education False lunch True test preparation course False math score False reading score False writing score False dtype: bool In [7]: all ( df . iloc [ 10 ] == df . loc [ 10 ]) Out[7]: False It turns out that loc filters by index value (something like where df.index == value ). That is, loc 's results are depend only on the indices (which are now scrambled after sampling). The actual positions of elements in the DataFrame are ignored. In [8]: df . loc [ 10 ] Out[8]: gender male race/ethnicity group C parental level of education associate's degree lunch standard test preparation course none math score 58 reading score 54 writing score 52 Name: 10, dtype: object By contrast, iloc filters by row position (something like where df.index == df.index[value] ) So iloc 's results depend on the actual positions of elements in a pandas data structure. The indices of these elements are ignored. It's this difference that explains counterintuitive results like this: In [9]: df . index [ 10 ] Out[9]: 342 Consider a single row index with iloc In [10]: df . iloc [ 10 ] Out[10]: gender female race/ethnicity group B parental level of education high school lunch standard test preparation course completed math score 69 reading score 76 writing score 74 Name: 342, dtype: object And take note of where you can find the index in output formatted like this. Enough chat. Time for... Exercise In the cell below, fill in the blank so that the row5 variable stores the 5th row of df. To be clear, imagine our DataFrame looked as follows: Index Words 0 this 1 is 2 not 3 easy We'd say the 1st row is the one with this word, the 2nd row is the one with is word, the 3rd row is the one with not word, etc. In [16]: ### edTest(test_a) ### row5 = df . iloc [ 4 ] row5 Out[16]: gender male race/ethnicity group C parental level of education high school lunch standard test preparation course completed math score 82 reading score 84 writing score 82 Name: 49, dtype: object You can display the first rows to have a better understanding of what you did. Can you find the row you've just selected? In [17]: df . iloc [: 5 ] Out[17]: gender race/ethnicity parental level of education lunch test preparation course math score reading score writing score 301 male group D some high school free/reduced none 56 54 52 895 female group E some high school free/reduced none 32 34 38 763 female group B high school standard none 62 62 63 854 male group C some high school standard none 62 64 55 49 male group C high school standard completed 82 84 82 Notice how we can use familar Python slice notation with iloc and loc ! Sorting We scrambled out df earlier with sample . We should also know how to tidy things up. Exercise In the cell below, fill in the blank so that sorted_df now stores df after sorting it by the math score column in decreasing order ( HINT ) In [18]: ### edTest(test_b) ### sorted_df = df . sort_values ( by = 'math score' , ascending = False ) sorted_df Out[18]: gender race/ethnicity parental level of education lunch test preparation course math score reading score writing score 623 male group A some college standard completed 100 96 86 625 male group D some college standard completed 100 97 99 149 male group E associate's degree free/reduced completed 100 100 93 458 female group E bachelor's degree standard none 100 100 100 962 female group E associate's degree standard none 100 100 100 ... ... ... ... ... ... ... ... ... 145 female group C some college free/reduced none 22 39 33 787 female group B some college standard none 19 38 32 17 female group B some high school free/reduced none 18 32 28 980 female group B high school free/reduced none 8 24 23 59 female group C some high school free/reduced none 0 17 10 1000 rows Ã— 8 columns Exercise In the cell below, fill in the blank so that sorted_row5 stores the 5th row of sorted_df . To be clear, imagine our sorted DataFrame looked as follows: Index Words 3 easy 1 is 2 not 0 this We'd say the 1st row is the one with easy , the 2nd row is the one with is , the 3rd row is the one with not , etc. In [21]: ### edTest(test_c) ### sorted_row5 = sorted_df . iloc [ 4 ] sorted_row5 Out[21]: gender female race/ethnicity group E parental level of education associate's degree lunch standard test preparation course none math score 100 reading score 100 writing score 100 Name: 962, dtype: object Can you find the row you've just selected? In [22]: # len('head()') < len('iloc[:5]') :) sorted_df . head () Out[22]: gender race/ethnicity parental level of education lunch test preparation course math score reading score writing score 623 male group A some college standard completed 100 96 86 625 male group D some college standard completed 100 97 99 149 male group E associate's degree free/reduced completed 100 100 93 458 female group E bachelor's degree standard none 100 100 100 962 female group E associate's degree standard none 100 100 100 Column Naming Conventions How you've named your df columns can affect the amount of typing required to manipulate your data, the readability of your code, and even the syntax optons you have. Let's take a look at some best practices for naming columns. In [23]: df Out[23]: gender race/ethnicity parental level of education lunch test preparation course math score reading score writing score 301 male group D some high school free/reduced none 56 54 52 895 female group E some high school free/reduced none 32 34 38 763 female group B high school standard none 62 62 63 854 male group C some high school standard none 62 64 55 49 male group C high school standard completed 82 84 82 ... ... ... ... ... ... ... ... ... 399 male group D some high school standard none 60 59 54 141 female group C some college free/reduced none 59 62 64 757 male group E bachelor's degree free/reduced completed 70 68 72 245 male group C associate's degree standard none 85 76 71 262 female group C some high school free/reduced none 44 50 51 1000 rows Ã— 8 columns We can see that there are columns names whose lengths are not confortable for coding. What we can do to make our life easier: Try to work with short columns names Try to avoid characters like spaces that will allow us to use column access without brackets Try working in only lower or upper case (prefably lower case; there's no need to shout) Our df already conforms to this last suggestion. We'll find that, after some renaming, things should become easier for us, making: expressions like these ones: condition = ( df [ 'test preparation course' ] != 'completed' ) & ( df [ 'writing score' ] > df [ 'writing score' ] . median ()) to become: condition = ( df . course != 'completed' ) & ( df . writing > df . writing . median ()) Exercise In the cell below fill in the blank with these goals in mind: rename column race/ethnicity to race rename column parental level of education to peduc raname column test preparation course to course remove score (included the left space char) from the rest of the columns HINT1 : Don't by shy, check the documentation if you need some help HINT2 : in many cases, it's faster to access docstrings using help help ( df . rename ) HINT3 : TAB autocomplete can be ued to you explore an object's available methods and attributes.\\ HINT4 : Still more exciting, place your cursor after the opening paranthesis of a function call and press SHIFT+TAB once or twice. Instantly, you're presented with a docstring. It's a whole new world! ðŸŒˆ In [24]: df . columns Out[24]: Index(['gender', 'race/ethnicity', 'parental level of education', 'lunch', 'test preparation course', 'math score', 'reading score', 'writing score'], dtype='object') In [27]: ### edTest(test_d) ### df = df . rename ( columns = { 'race/ethnicity' : 'race' , 'parental level of education' : 'peduc' , 'test preparation course' : 'course' , 'math score' : 'math' , 'reading score' : 'reading' , 'writing score' : 'writing' , }) df Out[27]: gender race peduc lunch course math reading writing 301 male group D some high school free/reduced none 56 54 52 895 female group E some high school free/reduced none 32 34 38 763 female group B high school standard none 62 62 63 854 male group C some high school standard none 62 64 55 49 male group C high school standard completed 82 84 82 ... ... ... ... ... ... ... ... ... 399 male group D some high school standard none 60 59 54 141 female group C some college free/reduced none 59 62 64 757 male group E bachelor's degree free/reduced completed 70 68 72 245 male group C associate's degree standard none 85 76 71 262 female group C some high school free/reduced none 44 50 51 1000 rows Ã— 8 columns Look for missing values Missing values in our DataFrames can cause many issues. They can cause certain operations and function calls to fail and throw an error. Perhaps worse, this problems can happen 'silently,' affecting our results without us realizing there is a problem. Unless we take precautions of course! The first step is locating missingness. This dataset doesn't have any missing values. So we'll make some ourselves and 'poke a holes' in the Dataframe. In [28]: # 'poking holes' in the data df . iloc [ 0 , 5 ] = None df . iloc [ 2 , 2 ] = None Exercise Fill in the blank to display whether or not entries in the first 5 rows are missing ( HINT ) Solution In [30]: df . isna () . head () Out[30]: gender race peduc lunch course math reading writing 301 False False False False False True False False 895 False False False False False False False False 763 False False True False False False False False 854 False False False False False False False False 49 False False False False False False False False Exercise Fill in the blanks to sum the total number of missing values in each of the dataset's columns. In [31]: ### edTest(test_e) ### resultE = df . isna () . sum () display ( resultE ) gender 0 race 0 peduc 1 lunch 0 course 0 math 1 reading 0 writing 0 dtype: int64 Now let's deal with these 'holes' we've just made Exercise Fill the missing math entry with that column's mean . Hint: Select subsets of the data frame using a column name or names Note: The blanks here represent just one way of doing this. Don't feel constrain by the blanks here. Solution In [32]: df [ 'math' ] = df [ 'math' ] . fillna ( df [ 'math' ] . mean ()) df . head () Out[32]: gender race peduc lunch course math reading writing 301 male group D some high school free/reduced none 66.099099 54 52 895 female group E some high school free/reduced none 32.000000 34 38 763 female group B None standard none 62.000000 62 63 854 male group C some high school standard none 62.000000 64 55 49 male group C high school standard completed 82.000000 84 82 Exercise Drop the row with the missing peduc from the DataFrame. Hint: to make it easier, consider that this is now the only remaining missing value Solution In [33]: df = df . dropna () df . head () Out[33]: gender race peduc lunch course math reading writing 301 male group D some high school free/reduced none 66.099099 54 52 895 female group E some high school free/reduced none 32.000000 34 38 854 male group C some high school standard none 62.000000 64 55 49 male group C high school standard completed 82.000000 84 82 790 female group B high school standard none 48.000000 62 60 Categorical Columns - nunique() and unique() If some of your data is categorical (e.g., taking on a descredte set of values with an inherent ordering) you'll often what to know exactly how many unique values you're dealing with. nunique and unique are here to help and can be used on a single column or across multiple columns. Please consider, unique will return all unique values. What if you ask for the uniques of a column with 1 million different values? ðŸ¤” This particular method should be used wisely. In [34]: df . nunique () Out[34]: gender 2 race 5 peduc 6 lunch 2 course 2 math 82 reading 72 writing 77 dtype: int64 In [35]: df [ 'gender' ] . unique () . tolist () Out[35]: ['male', 'female'] In [36]: # the line below represents the usage that should be avoided # df['math'].unique() Exercise Fill in the blanks using unique() and nunique() to complete the method print_uniques to help us to learn more about the categorical variables in our data. In [37]: ### edTest(test_f) ### def print_uniques ( df , col , limit = 10 ): \"\"\"Print column's uniques values when the number of column's uniques is lower or equal than limit \"\"\" n = df [ col ] . nunique () if n <= limit : print ( f ' { col } :' , df [ col ] . unique () . tolist ()) else : print ( f ' { col } :' , f 'more than { limit } uniques' ) for col in df . columns : print_uniques ( df , col ) gender: ['male', 'female'] race: ['group D', 'group E', 'group C', 'group B', 'group A'] peduc: ['some high school', 'high school', \"bachelor's degree\", 'some college', \"associate's degree\", \"master's degree\"] lunch: ['free/reduced', 'standard'] course: ['none', 'completed'] math: more than 10 uniques reading: more than 10 uniques writing: more than 10 uniques Descriptive statistics In Pandas, DataFrames have a simple method for displaying summary statistics. In [38]: df . describe () Out[38]: math reading writing count 999.000000 999.000000 999.000000 mean 66.103202 69.176176 68.059059 std 15.166754 14.605740 15.202426 min 0.000000 17.000000 10.000000 25% 57.000000 59.000000 57.500000 50% 66.000000 70.000000 69.000000 75% 77.000000 79.000000 79.000000 max 100.000000 100.000000 100.000000 Exercise Sometimes we don't want to access all these statistics. In the cell below fill in the blanks to get the mean and the standard deviation of the writing and reading columns ( HINT ). In [39]: ### edTest(test_g) ### resultG = df [[ 'writing' , 'reading' ]] . aggregate ([ 'mean' , 'std' ]) display ( resultG ) writing reading mean 68.059059 69.176176 std 15.202426 14.605740 Now we can group our dataframe by a specific column(s) value's and aggregate the other columns. Hint: Try using agg() as an alternative to spare some typing Exercise Group the dataframe by peduc and gender while aggregating math , reading , and writing with the mean and course with the mode. Tip: Again, don't feel constained by the blanks. This command may span multiple lines. In [42]: df . groupby ([ 'peduc' , 'gender' ]) . agg ({ 'math' : 'mean' , 'reading' : 'mean' , 'writing' : 'mean' }) Out[42]: math reading writing peduc gender associate's degree female 65.250000 74.120690 74.000000 male 70.764151 67.433962 65.405660 bachelor's degree female 68.349206 77.285714 78.380952 male 70.581818 68.090909 67.654545 high school female 59.322581 68.268817 66.731183 male 64.705882 61.480392 58.539216 master's degree female 66.500000 76.805556 77.638889 male 74.826087 73.130435 72.608696 some college female 65.406780 73.550847 74.050847 male 69.009259 64.990741 63.148148 some high school female 59.296703 69.109890 68.285714 male 67.955672 64.693182 61.375000 We would likely want to treat peduc as an 'ordinal' rather than a categorical variable as it does have an inherent ordering. Feel free to try using indexing to sort the rows. But after the grouping we now have multiple indices for each row! There's still more Pandas to discover ðŸ¼",
        "tags": "Lectures",
        "url": "lectures/lecture02/notebook-2/"
    }, {
        "title": "Lecture 2: Introduction to PANDAS",
        "text": "Lec2_notebook CS109a Introduction to PANDAS Lecture 1, Pandas Intro Harvard University Fall 2021 Instructors : Pavlos Protopapas and Natesh Pillai Pandas PANDAS is Python library that contains highly useful data structures, including DataFrames, which makes Exploratory Data Analysis (EDA) easy. Here we will see some of the elementary functions in practice. Installing Using conda conda install pandas Using pip pip install pandas TIP: You can try installing a library from a jupyter notebook cell adding \"!\" # using conda !conda install pandas # or using pip !pip install pandas PANDAS Basics Let's get started with basic functionality of PANDAS! Importing pandas importing pandas is as simple as next line import pandas But because of lazyness for convenience we usually import it as pd In [4]: import pandas as pd You can always check for the version of almost any library using __version__ In [5]: pd . __version__ Out[5]: '1.3.2' Pandas data structures The main data structures in pandas are the Series (useful for time series) and the DataFrame . Series Formal: One-dimensional ndarray with axis labels (including time series). Roughly: You can think of it as kind of spreadsheet column or a relational database table of one column DataFrame Formal: Two-dimensional, size-mutable, potentially heterogeneous tabular data. Roughly: to a relational database table. Where every DataFrame's column is a Series . Both DataFrames and Series always have an index . pd.Series pd.Series(data=None, index=None, dtype=None, name=None, copy=False) When not using an index pandas will add an index for us: >>> s1 = pd . Series ( range ( 0 , 50 , 10 )) 0 0 1 10 2 20 3 30 4 40 dtype : int64 The data can be strings not just numbers The index can be anything, but the data and index should have the same length. In [6]: s = pd . Series ( data = [ 'A' , 'B' , 'C' , 'D' , 'E' ], index = range ( 10 , 5 , - 1 )) s Out[6]: 10 A 9 B 8 C 7 D 6 E dtype: object We can independently access the series' values or its index In [7]: s . values Out[7]: array(['A', 'B', 'C', 'D', 'E'], dtype=object) In [8]: s . index Out[8]: RangeIndex(start=10, stop=5, step=-1) pd.DataFrame pd.DataFrame(data=None, index=None, columns=None, dtype=None, copy=None) This data structure also contains labeled axes (rows and columns). index First Name Last Name 0 Ann Gatton 1 John Fosa 2 Zack Kaufman DataFrame class offers powerful ways to create them. For instance the two code lines belows generate the same DataFrame object. # using rows pd . DataFrame ( data = [[ 1 , 2 ], [ 3 , 4 ], [ 5 , 6 ]], columns = [ 'A' , 'B' ]) # using columns pd . DataFrame ( data = { 'A' :[ 1 , 3 , 5 ], 'B' : [ 2 , 4 , 6 ]}) A B 0 1 2 1 3 4 2 5 6 Loading data It's common to create DataFrames, but usually we read data from external sources. This is easy to do in Pandas. In [9]: tpl = 'https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas. {} .html' for m in [ 'clipboard' , 'csv' , 'excel' , 'feather' , 'fwf' , 'gbq' , 'hdf' , 'html' , 'json' , 'parquet' , 'pickle' , 'spss' , 'sql' , 'sql_query' , 'sql_table' , 'stata' , 'table' , 'xml' ]: method = f 'read_ { m } ' url = tpl . format ( method ) print ( f ' { method } \\t { url } ' ) read_clipboard https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_clipboard.html read_csv https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html read_excel https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html read_feather https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_feather.html read_fwf https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_fwf.html read_gbq https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_gbq.html read_hdf https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_hdf.html read_html https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_html.html read_json https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_json.html read_parquet https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_parquet.html read_pickle https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_pickle.html read_spss https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_spss.html read_sql https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql.html read_sql_query https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql_query.html read_sql_table https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql_table.html read_stata https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_stata.html read_table https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_table.html read_xml https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_xml.html Example The method read_html is powerful and requires a bit of expirience. The first line processes the url and extracts html that match the criteria into a DataFrames The header will come as first row of the DataFrame, so in line 2 we use the first row values as columns names for the dataframe and finally we remove the first row. In [10]: df = pd . read_html ( 'https://en.wikipedia.org/wiki/Harvard_University' , match = 'School' )[ 0 ] In [11]: df Out[11]: 0 1 0 School Founded 1 Harvard College 1636 2 Medicine 1782 3 Divinity 1816 4 Law 1817 5 Dental Medicine 1867 6 Arts and Sciences 1872 7 Business 1908 8 Extension 1910 9 Design 1914 10 Education 1920 11 Public Health 1922 12 Government 1936 13 Engineering and Applied Sciences 2007 In [15]: df = pd . read_html ( 'https://en.wikipedia.org/wiki/Harvard_University' , match = 'School' )[ 0 ] df = df . rename ( columns = df . iloc [ 0 ])[ 1 :] df Out[15]: School Founded 1 Harvard College 1636 2 Medicine 1782 3 Divinity 1816 4 Law 1817 5 Dental Medicine 1867 6 Arts and Sciences 1872 7 Business 1908 8 Extension 1910 9 Design 1914 10 Education 1920 11 Public Health 1922 12 Government 1936 13 Engineering and Applied Sciences 2007 pd.read_csv read_csv is the recommended starting point for anyone learning pandas. You can read its docs here . Let's use it to load Avocado prices It is a well known fact that Millenials LOVE Avocado Toast. It's also a well known fact that all Millenials live in their parents basements. Clearly, they aren't buying home because they are buying too much Avocado Toast! But maybe there's hopeâ€¦ if a Millenial could find a city with cheap avocados, they could live out the Millenial American Dream. The table below represents weekly 2018 retail scan data for National retail volume (units) and price. Retail scan data comes directly from retailers' cash registers based on actual retail sales of Hass avocados. Starting in 2013, the table below reflects an expanded, multi-outlet retail data set. Multi-outlet reporting includes an aggregation of the following channels: grocery, mass, club, drug, dollar and military. The Average Price (of avocados) in the table reflects a per unit (per avocado) cost, even when multiple units (avocados) are sold in bags. The Product Lookup codes (PLU's) in the table are only for Hass avocados. Other varieties of avocados (e.g. greenskins) are not included in this table. Some relevant columns in the dataset: Date : The date of the observation AveragePrice : the average price of a single avocado type : conventional or organic year : the year Region : the city or region of the observation Total Volume : Total number of avocados sold 4046 : Total number of avocados with PLU 4046 sold 4225 : Total number of avocados with PLU 4225 sold 4770 : Total number of avocados with PLU 4770 sold Load dataset Read a compressed csv file. We ask pandas to use first csv column as index to avoid creating a new one by default. TIP : when you are blind about what you are loading or you already know it is a big dataset you can fix the number of rows to be loaded using the parameter nrows ( nrows=None to load all and it's the default value) In [12]: df = pd . read_csv ( 'avocado.csv.zip' , index_col = 0 , compression = 'zip' , nrows = None ) Roughly exploring the data We can quickly see the dataframe's dimension In [13]: df . shape Out[13]: (18249, 13) The shape is a tuple with the number of rows and the number of columns In [14]: len ( df . index ), len ( df . columns ) Out[14]: (18249, 13) Show only the columns' names In [15]: df . columns Out[15]: Index(['Date', 'AveragePrice', 'Total Volume', '4046', '4225', '4770', 'Total Bags', 'Small Bags', 'Large Bags', 'XLarge Bags', 'type', 'year', 'region'], dtype='object') The columns attribute is not a python list. In [16]: type ( df . columns ) == pd . Index Out[16]: True Show only the index In [17]: df . index Out[17]: Int64Index([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, ... 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], dtype='int64', length=18249) Sometimes some column type is incorrect and possible ways to detect it is using df.info() or df.dtypes . In [20]: df . dtypes Out[20]: Date object AveragePrice float64 Total Volume float64 4046 float64 4225 float64 4770 float64 Total Bags float64 Small Bags float64 Large Bags float64 XLarge Bags float64 type object year int64 region object dtype: object In example, here Date is an object (the way pandas save strings). We can use a better column type for that. In [21]: df [ 'Date' ] = pd . to_datetime ( df [ 'Date' ]) df . dtypes Out[21]: Date datetime64[ns] AveragePrice float64 Total Volume float64 4046 float64 4225 float64 4770 float64 Total Bags float64 Small Bags float64 Large Bags float64 XLarge Bags float64 type object year int64 region object dtype: object show first (by default: 5) rows In [22]: df . head () Out[22]: Date AveragePrice Total Volume 4046 4225 4770 Total Bags Small Bags Large Bags XLarge Bags type year region 0 2015-12-27 1.33 64236.62 1036.74 54454.85 48.16 8696.87 8603.62 93.25 0.0 conventional 2015 Albany 1 2015-12-20 1.35 54876.98 674.28 44638.81 58.33 9505.56 9408.07 97.49 0.0 conventional 2015 Albany 2 2015-12-13 0.93 118220.22 794.70 109149.67 130.50 8145.35 8042.21 103.14 0.0 conventional 2015 Albany 3 2015-12-06 1.08 78992.15 1132.00 71976.41 72.58 5811.16 5677.40 133.76 0.0 conventional 2015 Albany 4 2015-11-29 1.28 51039.60 941.48 43838.39 75.78 6183.95 5986.26 197.69 0.0 conventional 2015 Albany show last 2 rows In [24]: df . tail ( 2 ) Out[24]: Date AveragePrice Total Volume 4046 4225 4770 Total Bags Small Bags Large Bags XLarge Bags type year region 10 2018-01-14 1.93 16205.22 1527.63 2981.04 727.01 10969.54 10919.54 50.00 0.0 organic 2018 WestTexNewMexico 11 2018-01-07 1.62 17489.58 2894.77 2356.13 224.53 12014.15 11988.14 26.01 0.0 organic 2018 WestTexNewMexico display some data info Sometimes the Dataframe method info() is a great way to take a first data snapshot with few columns datasets. It displays: columns names number of rows (as entries) number of non null values data type per column (per Series) memory usage TIP : if you know that the number of columns is high (maybe when printing df.shape[1] ), then you can pass a False flag to the method info() to reduce the information just to global information. In [25]: few_columns = True df . info ( verbose = few_columns ) <class 'pandas.core.frame.DataFrame'> Int64Index: 18249 entries, 0 to 11 Data columns (total 13 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Date 18249 non-null datetime64[ns] 1 AveragePrice 18249 non-null float64 2 Total Volume 18249 non-null float64 3 4046 18249 non-null float64 4 4225 18249 non-null float64 5 4770 18249 non-null float64 6 Total Bags 18249 non-null float64 7 Small Bags 18249 non-null float64 8 Large Bags 18249 non-null float64 9 XLarge Bags 18249 non-null float64 10 type 18249 non-null object 11 year 18249 non-null int64 12 region 18249 non-null object dtypes: datetime64[ns](1), float64(9), int64(1), object(2) memory usage: 1.9+ MB Descriptive statistics We can take a fast look at some data statistics with one line of code In [26]: df . describe () Out[26]: AveragePrice Total Volume 4046 4225 4770 Total Bags Small Bags Large Bags XLarge Bags year count 18249.000000 1.824900e+04 1.824900e+04 1.824900e+04 1.824900e+04 1.824900e+04 1.824900e+04 1.824900e+04 18249.000000 18249.000000 mean 1.405978 8.506440e+05 2.930084e+05 2.951546e+05 2.283974e+04 2.396392e+05 1.821947e+05 5.433809e+04 3106.426507 2016.147899 std 0.402677 3.453545e+06 1.264989e+06 1.204120e+06 1.074641e+05 9.862424e+05 7.461785e+05 2.439660e+05 17692.894652 0.939938 min 0.440000 8.456000e+01 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000 2015.000000 25% 1.100000 1.083858e+04 8.540700e+02 3.008780e+03 0.000000e+00 5.088640e+03 2.849420e+03 1.274700e+02 0.000000 2015.000000 50% 1.370000 1.073768e+05 8.645300e+03 2.906102e+04 1.849900e+02 3.974383e+04 2.636282e+04 2.647710e+03 0.000000 2016.000000 75% 1.660000 4.329623e+05 1.110202e+05 1.502069e+05 6.243420e+03 1.107834e+05 8.333767e+04 2.202925e+04 132.500000 2017.000000 max 3.250000 6.250565e+07 2.274362e+07 2.047057e+07 2.546439e+06 1.937313e+07 1.338459e+07 5.719097e+06 551693.650000 2018.000000 Data Selection Column names They represent a powerful tool to access subgroups or individual values (in combination with other methods) in a DataFrame. $[]$ vs $[[]]$ Using column name as key will return the column values as type Series # returns a Series with dataframe values for column 'my_col' df [ 'my_col' ] # this gives the same access but is not recommended. Can't work when there is a space or a not allowed char in the name. df . my_col Using a python list of column names as key will return a sub dataframe with that columns # returns a DataFrame with the two columns df [[ 'my_col_A' , 'my_col_B' ]] # returns a Series with my_col_A values df [[ 'my_col_A' ]] In [27]: # this should be False because we just say that column name inside brackets returns a Series type ( df [ 'AveragePrice' ]) == pd . DataFrame Out[27]: False In [28]: type ( df [ 'AveragePrice' ]) == pd . Series Out[28]: True In [29]: # this should be True because we say that a list of column names inside brackets returns a sub dataframe type ( df [[ 'AveragePrice' ]]) == pd . DataFrame Out[29]: True Accessing column Series In [30]: df [ 'AveragePrice' ] . head () Out[30]: 0 1.33 1 1.35 2 0.93 3 1.08 4 1.28 Name: AveragePrice, dtype: float64 Accessing subdataframe of one column In [31]: df [[ 'AveragePrice' ]] . head () Out[31]: AveragePrice 0 1.33 1 1.35 2 0.93 3 1.08 4 1.28 Let's try to visualize the difference once more using the method values that return the data as numpy array. In [32]: df [ 'AveragePrice' ] . values Out[32]: array([1.33, 1.35, 0.93, ..., 1.87, 1.93, 1.62]) In [33]: df [[ 'AveragePrice' ]] . values Out[33]: array([[1.33], [1.35], [0.93], ..., [1.87], [1.93], [1.62]]) This is because Series.values returns a one dimensional array with the column values and DataFrame.values returns a two dimensional array that could be thought as an array of rows. In [34]: df [ 'AveragePrice' ] . values . shape , df [[ 'AveragePrice' ]] . values . shape Out[34]: ((18249,), (18249, 1)) Exercise In the cell below fill in the blanks to display the first 10 rows of a sub-dataframe with columns Date and AveragePrice . Remember that DataFrame is a class that allows chaining composition. In [35]: df [[ 'Date' , 'AveragePrice' ]] . head ( 10 ) Out[35]: Date AveragePrice 0 2015-12-27 1.33 1 2015-12-20 1.35 2 2015-12-13 0.93 3 2015-12-06 1.08 4 2015-11-29 1.28 5 2015-11-22 1.26 6 2015-11-15 0.99 7 2015-11-08 0.98 8 2015-11-01 1.02 9 2015-10-25 1.07 Filtering An expresion like the one show below represents a condition that will return a boolean list with many boolean values as values in the Series df['Date'] . And this number, its length is the same size of the number of rows in the DataFrame df. df [ 'Date' ] == '2015-10-25' The boolean list will be True for rows where the condition is True and False otherwise. A list of boolean values let as filter a DataFrame based on the condition. In [36]: condition = df [ 'Date' ] == '2015-10-25' condition Out[36]: 0 False 1 False 2 False 3 False 4 False ... 7 False 8 False 9 False 10 False 11 False Name: Date, Length: 18249, dtype: bool In [38]: df [ condition ] . head () Out[38]: Date AveragePrice Total Volume 4046 4225 4770 Total Bags Small Bags Large Bags XLarge Bags type year region 9 2015-10-25 1.07 74338.76 842.40 64757.44 113.00 8625.92 8061.47 564.45 0.0 conventional 2015 Albany 9 2015-10-25 1.09 358478.08 236814.29 64607.97 304.36 56751.46 31826.88 24924.58 0.0 conventional 2015 Atlanta 9 2015-10-25 1.19 656892.03 53766.25 397911.35 49085.74 156128.69 149987.55 6141.14 0.0 conventional 2015 BaltimoreWashington 9 2015-10-25 1.11 59874.45 29521.58 10089.82 6551.57 13711.48 13660.98 0.00 50.5 conventional 2015 Boise 9 2015-10-25 1.02 534249.47 4005.39 430725.78 191.31 99326.99 94581.94 4745.05 0.0 conventional 2015 Boston It's common to find this kind of expressions directly df [ df [ 'Date' ] == '2015-10-25' ] Logical expressions Example of conditions condition = df [ col ] > value condition = df [ col ] <= value condition = df [ col ] == value condition = df [ col ] != value # in list condition = df [ col ] . isin ([ value1 , value2 ]) # not in list condition = ~ df [ col ] . isin ([ value1 , value2 ]) # between (inclusive) condition = df [ col ] . between ( value1 , value2 ) Then we can combine different conditions with logical operators like \"&\" or \"|\". df . loc [ cond1 & cond2 ] df . loc [ cond1 | cond2 ] These above expressions can be executed without the loc operator df [ cond1 & cond2 ] df [ cond1 | cond2 ] TIP: many problems can be avoided using parenthesis for each simple condition in situations where we need to combine two or more conditions. In [39]: df [( df [ 'Date' ] == '2015-10-25' ) & ( df [ 'AveragePrice' ] < . 90 )] . head () Out[39]: Date AveragePrice Total Volume 4046 4225 4770 Total Bags Small Bags Large Bags XLarge Bags type year region 9 2015-10-25 0.86 1010394.81 557469.46 301143.50 49959.10 101822.75 96417.63 5279.41 125.71 conventional 2015 DallasFtWorth 9 2015-10-25 0.88 933623.58 437329.85 313129.29 81274.85 101889.59 57577.21 44260.60 51.78 conventional 2015 Houston 9 2015-10-25 0.83 761261.71 435986.90 240689.98 19968.66 64616.17 64585.35 30.82 0.00 conventional 2015 PhoenixTucson 9 2015-10-25 0.86 4912068.04 2542914.87 1537781.45 247539.31 583832.41 475267.20 108231.39 333.82 conventional 2015 SouthCentral 9 2015-10-25 0.82 635873.60 363487.08 166607.85 31960.04 73818.63 72717.86 1100.77 0.00 conventional 2015 WestTexNewMexico # be careful with expressions like this that will fail when doing the bit operation df [ df [ 'Date' ] == '2015-10-25' & df [ 'AveragePrice' ] < . 90 ] .loc[] vs .iloc[] Accessing rows .loc[] This operator allows us to access information by index label, but by definition it could be used with a boolean array as we saw with conditions: Access a group of rows and columns by label(s) or a boolean array. .loc[] is primarily label based, but may also be used with a boolean array. When using df.info() we discover that the number of unique values for index (index domain values) is between 0 and 11 included. So, we can use .loc to filter the rows where the index value is 9. In [43]: df . loc [ 9 ] Out[43]: Date AveragePrice Total Volume 4046 4225 4770 Total Bags Small Bags Large Bags XLarge Bags type year region 9 2015-10-25 1.07 74338.76 842.40 64757.44 113.00 8625.92 8061.47 564.45 0.00 conventional 2015 Albany 9 2015-10-25 1.09 358478.08 236814.29 64607.97 304.36 56751.46 31826.88 24924.58 0.00 conventional 2015 Atlanta 9 2015-10-25 1.19 656892.03 53766.25 397911.35 49085.74 156128.69 149987.55 6141.14 0.00 conventional 2015 BaltimoreWashington 9 2015-10-25 1.11 59874.45 29521.58 10089.82 6551.57 13711.48 13660.98 0.00 50.50 conventional 2015 Boise 9 2015-10-25 1.02 534249.47 4005.39 430725.78 191.31 99326.99 94581.94 4745.05 0.00 conventional 2015 Boston ... ... ... ... ... ... ... ... ... ... ... ... ... ... 9 2018-01-21 1.27 3159.80 92.12 73.17 0.00 2994.51 2117.69 876.82 0.00 organic 2018 Syracuse 9 2018-01-21 1.52 6871.05 76.66 407.09 0.00 6387.30 6375.55 11.75 0.00 organic 2018 Tampa 9 2018-01-21 1.63 1283987.65 108705.28 259172.13 1490.02 914409.26 710654.40 203526.59 228.27 organic 2018 TotalUS 9 2018-01-21 1.83 189317.99 27049.44 33561.32 439.47 128267.76 76091.99 51947.50 228.27 organic 2018 West 9 2018-01-21 1.87 13766.76 1191.92 2452.79 727.94 9394.11 9351.80 42.31 0.00 organic 2018 WestTexNewMexico 432 rows Ã— 13 columns .iloc[] This operator allows us to access information by index position in the way we usually do with other programing languages like C. Purely integer-location based indexing for selection by position. .iloc[] is primarily integer position based (from 0 to length-1 of the axis), but may also be used with a boolean array. When using df.iloc[9] we are going to access to the 10th row in the DataFrame df. The returned value should be of type Series with the row values ( df.iloc[9].values ) as values and the columns names as the Series index. In [47]: df . iloc [ 9 ] Out[47]: Date 2015-10-25 00:00:00 AveragePrice 1.07 Total Volume 74338.76 4046 842.4 4225 64757.44 4770 113.0 Total Bags 8625.92 Small Bags 8061.47 Large Bags 564.45 XLarge Bags 0.0 type conventional year 2015 region Albany Name: 9, dtype: object In [36]: type ( df . iloc [ 9 ]) Out[36]: pandas.core.series.Series The name of the series is the index label value of the original dataframe. TIP: practice to really learn how and when to use .loc vs i.loc Mathematical and other methods on a DataFrame Pandas Series and DataFrame offers access to hundred of methods to operate on them like: sum(), mul(), mean(), std(), max(), min(), etc . All of these methods usually operate by default over columns but they can operate over rows. Look at the next cell results and try to think about what happened (take a look at fields like type or region). In [48]: df . sum () /tmp/ipykernel_32/1703867807.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. df.sum() Out[48]: AveragePrice 25657.7 Total Volume 15523402593.400002 4046 5347110739.26 4225 5386275717.93 4770 416802342.13 Total Bags 4373175798.389999 Small Bags 3324870837.51 Large Bags 991615770.55 XLarge Bags 56689177.33 type conventionalconventionalconventionalconvention... year 36792683 region AlbanyAlbanyAlbanyAlbanyAlbanyAlbanyAlbanyAlba... dtype: object In [49]: df [ 'AveragePrice' ] . mean () Out[49]: 1.4059784097758825 Missing Data This is a critical problem for any Data Scientist and deserves its own Lecture. What to do when some of the data are missing? Pandas offers some options to explore a dataframe looking for missing data. # returns a boolean dataframe of the same size with True values for cells where values are NaN df . isna () # returns a boolean dataframe of the same size with True values for cells where values aren't NaN df . notna () # alias of the above methods df . isnull () df . notnull () Count the number of NaN values for every column In [50]: df . isna () . sum () Out[50]: Date 0 AveragePrice 0 Total Volume 0 4046 0 4225 0 4770 0 Total Bags 0 Small Bags 0 Large Bags 0 XLarge Bags 0 type 0 year 0 region 0 dtype: int64 Count the number of NaN values per row In [51]: df . isna () . sum ( axis = 1 ) Out[51]: 0 0 1 0 2 0 3 0 4 0 .. 7 0 8 0 9 0 10 0 11 0 Length: 18249, dtype: int64 Count the total number of NaN values in the dataframe In [52]: df . isna () . sum () . sum () Out[52]: 0 Select the rows with at least one NaN value In [53]: df [ df . isna () . any ( axis = 1 )] Out[53]: Date AveragePrice Total Volume 4046 4225 4770 Total Bags Small Bags Large Bags XLarge Bags type year region There are specific methods related to face this problem like: fillna() bfill() ffill() dropna() It's important to learn to handle missing data Dropping Sometimes you'll want to discard information. Here is an example of how to use the drop method to do that. df . drop ( labels = None , axis = 0 , index = None , columns = None , level = None , inplace = False , errors = 'raise' ) In [54]: drop_columns = [ '4046' , '4225' , '4770' , 'Total Bags' , 'Small Bags' , 'Large Bags' , 'XLarge Bags' ] df = df . drop ( columns = drop_columns ) df Out[54]: Date AveragePrice Total Volume type year region 0 2015-12-27 1.33 64236.62 conventional 2015 Albany 1 2015-12-20 1.35 54876.98 conventional 2015 Albany 2 2015-12-13 0.93 118220.22 conventional 2015 Albany 3 2015-12-06 1.08 78992.15 conventional 2015 Albany 4 2015-11-29 1.28 51039.60 conventional 2015 Albany ... ... ... ... ... ... ... 7 2018-02-04 1.63 17074.83 organic 2018 WestTexNewMexico 8 2018-01-28 1.71 13888.04 organic 2018 WestTexNewMexico 9 2018-01-21 1.87 13766.76 organic 2018 WestTexNewMexico 10 2018-01-14 1.93 16205.22 organic 2018 WestTexNewMexico 11 2018-01-07 1.62 17489.58 organic 2018 WestTexNewMexico 18249 rows Ã— 6 columns Sorting sort_values and sort_index are common methods when using pandas. sort_values : Sort by the values along either axis. df . sort_values ( by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' , ignore_index = False , key = None ) Next cell filter data for a particular Date and type. Then sort values by region in ascending order and finally display only the first 10 rows. In [55]: condition = ( df [ 'Date' ] == '2018-01-07' ) & ( df [ 'type' ] == 'organic' ) df [ condition ] . sort_values ( by = 'region' ) . head ( 10 ) Out[55]: Date AveragePrice Total Volume type year region 11 2018-01-07 1.54 4816.90 organic 2018 Albany 11 2018-01-07 1.53 15714.11 organic 2018 Atlanta 11 2018-01-07 1.15 82282.71 organic 2018 BaltimoreWashington 11 2018-01-07 1.77 2553.90 organic 2018 Boise 11 2018-01-07 1.91 30096.00 organic 2018 Boston 11 2018-01-07 1.17 9115.92 organic 2018 BuffaloRochester 11 2018-01-07 1.95 156341.57 organic 2018 California 11 2018-01-07 1.08 28741.11 organic 2018 Charlotte 11 2018-01-07 1.83 41573.25 organic 2018 Chicago 11 2018-01-07 1.71 13141.82 organic 2018 CincinnatiDayton Sorting can use more columns using a python list with some parameters. In [56]: df [ condition ] . sort_values ( by = [ 'region' , 'AveragePrice' ], ascending = [ True , False ]) . head () Out[56]: Date AveragePrice Total Volume type year region 11 2018-01-07 1.54 4816.90 organic 2018 Albany 11 2018-01-07 1.53 15714.11 organic 2018 Atlanta 11 2018-01-07 1.15 82282.71 organic 2018 BaltimoreWashington 11 2018-01-07 1.77 2553.90 organic 2018 Boise 11 2018-01-07 1.91 30096.00 organic 2018 Boston sort_index : Sort object by labels (along an axis) df . sort_index ( axis = 0 , level = None , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' , sort_remaining = True , ignore_index = False , key = None ) Next cell sort rows based on the index values (in ascending order) In [57]: df . sort_index () Out[57]: Date AveragePrice Total Volume type year region 0 2015-12-27 1.33 64236.62 conventional 2015 Albany 0 2016-12-25 1.85 8657.87 organic 2016 PhoenixTucson 0 2015-12-27 1.25 73109.90 conventional 2015 Pittsburgh 0 2016-12-25 1.90 11376.97 organic 2016 Philadelphia 0 2016-12-25 1.27 5601.65 organic 2016 Orlando ... ... ... ... ... ... ... 52 2017-01-01 2.06 39260.55 organic 2017 NewYork 52 2017-01-01 1.11 476239.03 conventional 2017 NorthernNewEngland 52 2017-01-01 2.00 115256.09 organic 2017 Northeast 52 2017-01-01 0.93 547565.88 conventional 2017 Atlanta 52 2017-01-01 0.97 142347.90 conventional 2017 Roanoke 18249 rows Ã— 6 columns Renaming It's very common to rename things for convenience. In example, when column names are too long or use special characters it could come in handy to short them. The method rename() is a great tool to help us in many situations. Let's see a simple example In [58]: df . rename ( columns = { 'AveragePrice' : 'price' , 'Total Volume' : 'volume' }, inplace = True ) df Out[58]: Date price volume type year region 0 2015-12-27 1.33 64236.62 conventional 2015 Albany 1 2015-12-20 1.35 54876.98 conventional 2015 Albany 2 2015-12-13 0.93 118220.22 conventional 2015 Albany 3 2015-12-06 1.08 78992.15 conventional 2015 Albany 4 2015-11-29 1.28 51039.60 conventional 2015 Albany ... ... ... ... ... ... ... 7 2018-02-04 1.63 17074.83 organic 2018 WestTexNewMexico 8 2018-01-28 1.71 13888.04 organic 2018 WestTexNewMexico 9 2018-01-21 1.87 13766.76 organic 2018 WestTexNewMexico 10 2018-01-14 1.93 16205.22 organic 2018 WestTexNewMexico 11 2018-01-07 1.62 17489.58 organic 2018 WestTexNewMexico 18249 rows Ã— 6 columns Counting Counting number of values without NaNs We already saw different ways to access to the number of rows. But what if you want to count the number of rows with not NaN values? The count() method counts non-NA cells for each column or row In [59]: df . count () Out[59]: Date 18249 price 18249 volume 18249 type 18249 year 18249 region 18249 dtype: int64 Counting number of unique values per column (Series) in the DataFrame Number of unique for one Series In [60]: df . region . nunique () Out[60]: 54 Number of unique values for every Series in the DataFrame In [61]: df . nunique () Out[61]: Date 169 price 259 volume 18237 type 2 year 4 region 54 dtype: int64 Unique values In [62]: df [ 'type' ] . unique () Out[62]: array(['conventional', 'organic'], dtype=object) Remember that we can easily change a DataFrame or Series output into a python list In [63]: df [ 'type' ] . unique () . tolist () Out[63]: ['conventional', 'organic'] Counting rows based on unique values value_counts() return a Series containing counts of unique rows in the DataFrame DataFrame . value_counts ( subset = None , normalize = False , sort = True , ascending = False , dropna = True ) This method is simple but powerful for simple exploration. Let's look some examples In [64]: df . value_counts ( subset = 'type' ) Out[64]: type conventional 9126 organic 9123 dtype: int64 In [65]: df . value_counts ( subset = 'year' , sort = False ) Out[65]: year 2015 5615 2016 5616 2017 5722 2018 1296 dtype: int64 We can use a bigger subset for a more detailed view In [66]: df . value_counts ( subset = [ 'year' , 'type' ], sort = False ) Out[66]: year type 2015 conventional 2808 organic 2807 2016 conventional 2808 organic 2808 2017 conventional 2862 organic 2860 2018 conventional 648 organic 648 dtype: int64 And we just need add one more flag to access to the same but normalized values In [67]: df . value_counts ( subset = [ 'year' , 'type' ], sort = False , normalize = True ) * 100 Out[67]: year type 2015 conventional 15.387145 organic 15.381665 2016 conventional 15.387145 organic 15.387145 2017 conventional 15.683051 organic 15.672092 2018 conventional 3.550880 organic 3.550880 dtype: float64 Grouping Have you been looking for power: meet groupby() A groupby operation involves some combination of splitting the object, applying a function, and combining the results. This can be used to group large amounts of data and compute operations on these groups. DataFrame . groupby ( by = None , axis = 0 , level = None , as_index = True , sort = True , group_keys = True , squeeze =< no_default > , observed = False , dropna = True ) Next cell mimics in some way the value_counts() behaviour. Split dataset into subdaframes where each subdataframe.year is unique Count the number of rows without NaNs for every column In [68]: df . groupby ( 'year' ) . count () Out[68]: Date price volume type region year 2015 5615 5615 5615 5615 5615 2016 5616 5616 5616 5616 5616 2017 5722 5722 5722 5722 5722 2018 1296 1296 1296 1296 1296 The above rows present the same values because the original dataset is free of NaN values. It's a great dataset: No NaNs and Avocados everywhere. Functions What about max()? In [69]: df . groupby ( 'year' ) . max () Out[69]: Date price volume type region year 2015 2015-12-27 2.79 44655461.51 organic WestTexNewMexico 2016 2016-12-25 3.25 52288697.89 organic WestTexNewMexico 2017 2017-12-31 3.17 61034457.10 organic WestTexNewMexico 2018 2018-03-25 2.30 62505646.52 organic WestTexNewMexico And mean()? In [70]: df . groupby ( 'year' ) . mean () Out[70]: price volume year 2015 1.375590 7.810274e+05 2016 1.338640 8.584206e+05 2017 1.515128 8.623393e+05 2018 1.347531 1.066928e+06 Do you find something different between using max and mean? What are your thoughts? In [74]: df . groupby ( 'year' ) . describe () Out[74]: price volume count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max year 2015 5615.0 1.375590 0.375595 0.49 1.07 1.300 1.67 2.79 5615.0 7.810274e+05 3.171256e+06 84.56 6931.6300 76146.82 400176.6800 44655461.51 2016 5616.0 1.338640 0.393708 0.51 1.04 1.300 1.56 3.25 5616.0 8.584206e+05 3.478732e+06 385.55 10643.6850 109597.29 451107.2925 52288697.89 2017 5722.0 1.515128 0.432906 0.44 1.22 1.490 1.77 3.17 5722.0 8.623393e+05 3.481957e+06 515.01 13790.6975 122915.75 426454.5125 61034457.10 2018 1296.0 1.347531 0.305858 0.56 1.13 1.345 1.56 2.30 1296.0 1.066928e+06 4.285501e+06 2064.90 17690.9825 157175.09 529462.2450 62505646.52 There exist other methods that can be chain to gropuby(). In example first and last will return the first and the last row of each group respectivily. In [78]: df . groupby ( 'year' ) . first () Out[78]: Date price volume type region year 2015 2015-12-27 1.33 64236.62 conventional Albany 2016 2016-12-25 1.52 73341.73 conventional Albany 2017 2017-12-31 1.47 113514.42 conventional Albany 2018 2018-03-25 1.57 149396.50 conventional Albany Previous call is similar to using head, but head() keeps the group index where first set a new index: the year In [85]: df . groupby ( 'year' ) . head ( 1 ) Out[85]: Date price volume type year region 0 2015-12-27 1.33 64236.62 conventional 2015 Albany 0 2016-12-25 1.52 73341.73 conventional 2016 Albany 0 2017-12-31 1.47 113514.42 conventional 2017 Albany 0 2018-03-25 1.57 149396.50 conventional 2018 Albany Aggregate aggregate : Aggregate using one or more operations over the specified axis ( agg is an alias). Next cell shows the aggregated avocado price and volume values from year 2018 In [81]: condition = ( df [ 'year' ] == 2018 ) df [ condition ][[ 'price' , 'volume' ]] . agg ([ 'min' , 'mean' , 'std' , 'max' ]) Out[81]: price volume min 0.560000 2.064900e+03 mean 1.347531 1.066928e+06 std 0.305858 4.285501e+06 max 2.300000 6.250565e+07 aggregate can be applied to dataframes though it can be chained with groupby() In [86]: df . groupby ( 'year' )[[ 'price' , 'volume' ]] . agg ([ 'min' , 'mean' , 'std' , 'max' ]) Out[86]: price volume min mean std max min mean std max year 2015 0.49 1.375590 0.375595 2.79 84.56 7.810274e+05 3.171256e+06 44655461.51 2016 0.51 1.338640 0.393708 3.25 385.55 8.584206e+05 3.478732e+06 52288697.89 2017 0.44 1.515128 0.432906 3.17 515.01 8.623393e+05 3.481957e+06 61034457.10 2018 0.56 1.347531 0.305858 2.30 2064.90 1.066928e+06 4.285501e+06 62505646.52 Suppose you need a way to extract percentiles. The quantile() method can be applied directly on a dataframe to extract the thing you want. In [87]: df [ condition ][[ 'price' , 'volume' ]] . quantile ( . 10 ) Out[87]: price 0.970 volume 8174.655 Name: 0.1, dtype: float64 But there are cases where you need to extract more than that. For those cases it can be convenient to create custom methods to be used with aggregation. In [88]: def percentil_10 ( x ): return x . quantile ( . 10 ) def percentil_90 ( x ): return x . quantile ( . 90 ) df [ condition ][[ 'price' , 'volume' ]] . agg ([ percentil_10 , 'median' , percentil_90 ]) Out[88]: price volume percentil_10 0.970 8174.655 median 1.345 157175.090 percentil_90 1.750 1810981.615 In [89]: df . groupby ( 'year' )[[ 'price' , 'volume' ]] . agg ([ percentil_10 , 'median' , percentil_90 ]) Out[89]: price volume percentil_10 median percentil_90 percentil_10 median percentil_90 year 2015 0.96 1.300 1.90 2431.434 76146.82 1285267.958 2016 0.88 1.300 1.86 4146.935 109597.29 1351865.735 2017 0.98 1.490 2.07 5889.687 122915.75 1398304.817 2018 0.97 1.345 1.75 8174.655 157175.09 1810981.615 Summary In this lecture you've learnt: How to install pandas About pandas What are Series and DataFrame data structures How to create a simple DataFrame How to load a DataFrame with an external data source How to access column Series How to access row Series (index) The differences between loc[] and iloc[] Different ways to start exploring the data general structure Different ways to access to description statistics How to look for missing data How to do data filtering using conditions How to do sorting How to do counting How to group information How to do aggregation Facts: We've only imported pandas! Almost everything was about accessing and processing the data, and not creating it. Topics left out maybe for other lectures: DataFrame operations: How to add a column to a DataFrame DataFrame inter columns operations apply() applymap() pipe() Merging DataFrames (merge) Concatenating DataFrames (concat) Appending DataFrames, Series or a simple row (append) Using loops with: iterrows() itertuples() groupby() Next lecture Plotting with PANDAS (showing the importance of plots) EDA with PANDAS (using Seaborn if possible) In [0]:",
        "tags": "Lectures",
        "url": "lectures/lecture02/notebook-3/"
    }, {
        "title": "Lab 1",
        "text": "cs109a_section_scaffold_1 CS109A Introduction to Data Science Lab 01: Introduction to Web Scraping Harvard University Fall 2021 Instructors : Pavlos Protopapas and Natesh Pillai Lab Team : Marios Mattheakis, Hayden Joy, Chris Gumb, and Eleni Kaxiras Authors : Varshini Reddy, Marios Mattheakis and Pavlos Protopapas In [21]: ## RUN THIS CELL TO GET THE RIGHT FORMATTING import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\" ) . text HTML ( styles ) Out[21]: Lab Learning Objectives When we're done today, you will approach messy real-world data with confidence that you can get it into a format that you can manipulate. Specifically, our learning objectives are: Understand the tree-like structure of an HTML document and use that structure to extract desired information. Use Python data structures such as lists, dictionaries to store and manipulate information. Practice using Python packages such as BeautifulSoup , including how to navigate their documentation to find functionality. Identify other (semi-)structured formats commonly used for storing and transferring data, such as CSV. Pre-Requisites Before you start working on the lab, we expect you to be familiar with Python programming. Following is the list of topics you need to brush up on before attending the lab session. We have provided some quick start references as well. Python Data Structures Lists Dictionaries Functions in python Python classes Files and strings In [16]: # Importing necessary libraries import numpy as np import matplotlib.pyplot as plt import pandas as pd from bs4 import BeautifulSoup import requests import json from IPython.display import HTML % matplotlib inline In [2]: # Setting up 'requests' to make HTTPS requests properly takes some # extra steps. requests . packages . urllib3 . disable_warnings () import warnings warnings . filterwarnings ( \"ignore\" ) Lab Data Analysis Questions Is science becoming more collaborative over time? How about literature? Are there a few \"geniuses\" or lots of hard workers? One way we might answer those questions is by looking at Nobel Prizes winners. We could ask questions like: 1) Has anyone won a prize more than once? 2) How has the total number of recipients changed over time? 3) How has the number of recipients per award changed over time? To answer these questions, we will need data: who received what award and when . When possible: find a structured dataset (.csv, .json, .xls) After a google search we stumble upon this dataset on github . It is also in the lab folder named github-nobel-prize-winners.csv . We use Pandas to read it. Pandas will be covered next week in more details. In [5]: df = pd . read_csv ( \"data/github-nobel-prize-winners.csv\" ) df . head () Out[5]: year discipline winner desc 0 1901 chemistry Jacobus H. van 't Hoff in recognition of the extraordinary services h... 1 1901 literature Sully Prudhomme in special recognition of his poetic compositi... 2 1901 medicine Emil von Behring for his work on serum therapy, especially its ... 3 1901 peace Henry Dunant NaN 4 1901 peace Fr&eacute;d&eacute;ric Passy NaN Research Question 1: Did anyone recieve the Nobel Prize more than once? How would you check if anyone recieved more than one nobel prize? We will be using Python lists for this, which is a pre-requisite for this lab as mentioned earlier. If you have any questions with regards to lists or list comprehensions, refer to the slides from us here . In [6]: # Initialize the list storing all the names name_winners = [] for name in df . winner : # Check if we already encountered this name: if name in name_winners : # (TODO) If so, print the name print ( ___ ) else : # (TODO) Otherwise append the name to the list name_winners . append ( ___ ) We don't want to print \"No Prize was Awarded\" all the time. In [7]: # List storing all the names name_winners = [] for name in df . winner : # (TODO) Check if we already encountered this name and the name is not \"No Prize was Awarded\": if name in name_winners and name != ___ : # (TODO) If so, print the name print ( ___ ) else : # (TODO) Otherwise append the name to the list name_winners . append ( ___ ) we can use .split() on a string to separate the words into individual strings and store them in a list. Experiment with the .split() below before using it. In [9]: UN_string = \"Office of the United Nations\" print ( UN_string . split ()) n_words = len ( UN_string . split ()) print ( \"Number of words: \" + str ( n_words )); ['Office', 'of', 'the', 'United', 'Nations'] Number of words: 5 Let us only print winners with only two words in their name: In [8]: name_winners = [] for name in df . winner : # (TODO) Check if we already encountered this name and the name consists of no more than 2 words: if name in name_winners and len ( ___ ) <= 2 : # (TODO) If so, print the name print ( ___ ) else : # (TODO) Otherwise append the name to the list name_winners . append ( ___ ) Marie Curie recieved the nobel prize in physics in 1903 and chemistry in 1911. She is one of only four people to recieve two Nobel Prizes. All questions, such as \"did anyone receive the Noble Price more than once?\", are easy to answer when the data is present in such a clean tabular form. However, many times (if not most) we do not find the data we need in such a format. In such cases, we need to perform web scraping and cleaning to get the data we desire. The end result of this lab is to create a pandas dataframe after web scraping and cleaning. WEB SCRAPING The first step in web scraping is to understand the HTML structure of the webpage. But, what is HTML? HTML stands for Hyper Text Markup Language. It is the standard markup language for documents designed to be displayed in a web browser. It can be assisted by technologies such as Cascading Style Sheets and scripting languages such as JavaScript. Standard HTML documents HTML documents generally have the following structure: **\\ ** **\\ ** **\\ ** **\\ Page Title\\ ** **\\ ** **\\ ** **\\ Page Heading\\ ** **\\ The first paragraph of page\\ ** **.** **.** **.** **.** **\\ ** **\\ ** What does each of these tags indicate? The \\<!DOCTYPE html> declaration defines that this document is an HTML5 document The \\ element is the root element of an HTML page The \\ element contains meta information about the HTML page The \\ element specifies a title for the HTML page (which is shown in the browser's title bar or in the page's tab) The \\ element defines the document's body, and is a container for all the visible contents, such as headings, paragraphs, images, hyperlinks, tables, lists, etc. The \\ element defines a large heading. There are other heading tags in html, \\ , \\ , \\ , \\ , \\ The \\ element defines a paragraph What is an HTML Element? An HTML element is defined by a start tag, some content, and an end tag: \\ Tag content \\ An example of an HTML element is as follows: \\ The Page Heading \\ WEB SCRAPING The official Nobel website has the data we want, but in 2018 and 2019 the physics prize was awarded to multiple groups so we will use an archived version of the web-page for an easier introduction to web scraping. The Internet Archive periodically crawls most of the Internet and saves what it finds. (That's a lot of data!) So let's grab the data from the Archive's \"Wayback Machine\" (great name!). We've just given you the direct URL, but at the very end you'll see how we can get it out of a JSON response from the Wayback Machine API. Let's take a look at the 2018 version of the Nobel website and to look at the underhood HTML: right-click and click on inspect .You should see something like this. Mapping the HTML tags to the webpage When you inspect, try to map each element on the webpage to its HTML. In [12]: # here is what we will get after selecting using the class by year tag. # we use the HTML parser module to render the html einstein = HTML ( ' \\ <div class =\"Class: by year\"> \\ <h3> \\ <a href=\"http://web.archive.org/web/20180820111639/https://www.nobelprize.org/nobel_prizes/physics/laureates/1921/\"> \\ The Nobel Prize in Physics 1921 \\ </a> \\ </h3> \\ <h6> \\ <a href=\"http://web.archive.org/web/20180820111639/https://www.nobelprize.org/nobel_prizes/physics/laureates/1921/einstein-facts.html\"> \\ Albert Einstein</a> \\ </h6> \\ <p> \\ \"for his services to Theoretical Physics, and especially for his discovery of the law of the photoelectric effect\" \\ </p> \\ ' ) display ( einstein ) The Nobel Prize in Physics 1921 Albert Einstein \"for his services to Theoretical Physics, and especially for his discovery of the law of the photoelectric effect\" In [23]: snapshot_url = 'http://web.archive.org/web/20180820111639/https://www.nobelprize.org/prizes/lists/all-nobel-prizes/' In [0]: # (TODO) make a GET request to snapshot_url snapshot = requests . get ( ___ ) snapshot Response [200] is a success status code. Let's google: response 200 meaning . All possible codes here . In [14]: type ( snapshot ) Try to request \"www.xoogle.be\". What happens? In [15]: snapshot_url2 = 'http://web.archive.org/web/20180820111639/https://www.xoogle.be' # (TODO) make a GET request to snapshot_url2 snapshot = requests . get ( ___ ) snapshot Always remember to \"not to be evil\" when scraping with requests! If downloading multiple pages (like you will be doing on HW1), always put a delay between requests (e.g., time.sleep(1) , with the time library), so you do not unwittingly hammer someone's webserver and/or get blocked. Let's look at the content we just scraped! In [0]: snapshot = requests . get ( snapshot_url ) raw_html = snapshot . text print ( raw_html [: 5000 ]) What makes Python special ? In [15]: import this The Zen of Python, by Tim Peters Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Readability counts. Special cases aren't special enough to break the rules. Although practicality beats purity. Errors should never pass silently. Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess. There should be one-- and preferably only one --obvious way to do it. Although that way may not be obvious at first unless you're Dutch. Now is better than never. Although never is often better than *right* now. If the implementation is hard to explain, it's a bad idea. If the implementation is easy to explain, it may be a good idea. Namespaces are one honking great idea -- let's do more of those! Regular Expressions You can find specific patterns or strings in text by using Regular Expressions (or re, regex, regexp): This is a pattern matching mechanism used throughout Computer Science and programming (it's not just specific to Python). A short summary of regular expressions from us can be found here . Some great resources that we recommend, if you are interested in them (could be very useful for a homework problem): https://docs.python.org/3.3/library/re.html https://regexone.com https://docs.python.org/3/howto/regex.html . Specify a specific sequence with the help of regex special characters. Some examples: \\S : Matches any character which is not a Unicode whitespace character: spaces, tabs, newlines \\d : Matches any Unicode decimal digit, 0 , 1 , ..., 9 * : Causes the resulting RE to match 0 or more repetitions of the preceding RE, as many repetitions as are possible. Let's find all the occurances of 'Marie' in our raw_html: In [17]: import re In [18]: re . findall ( r 'Marie' , raw_html ) Note we use an r before the string to get the raw text. Using \\S to match 'Marie' + ' ' + 'any character which is not a Unicode whitespace character': In [19]: re . findall ( r 'Marie \\S' , raw_html ) How would we find the lastnames that come after Marie? In [20]: # Your code here Hint: The \\w character represents any alpha-numeric character. \\w* is greedy and gets a repeat of the characters until the next bit of whitespace. Now, we have all our data in the notebook. Unfortunately, it is the form of one really long string, which is hard to work with directly. This is where BeautifulSoup comes in. This is an example of code that grabs the first title. Regex can quickly become complex, which motivates beautiful soup. In [21]: first_title = re . findall ( r '<h3><a.*>.*<\\/a><\\/h3>' , raw_html )[ 0 ] print ( first_title ) #you can do this via regex, but it gets complicated fast! This motivates Beautiful Soup. Parse the HTML with BeautifulSoup BeautifulSoup works by parsing the raw html text into a tree. Every tag in the raw html becomes a node in the tree. We can then navigate the tree by selecting a node and querying its parent, children, siblings, etc. In [0]: soup = BeautifulSoup ( raw_html , 'html.parser' ) Key BeautifulSoup functions we'll be using in this lab: tag.prettify() : Returns cleaned-up version of raw HTML, useful for printing tag.select(selector) : Return a list of nodes matching a CSS selector tag.select_one(selector) : Return the first node matching a CSS selector tag.text/soup.get_text() : Returns visible text of a node (e.g.,\" <p>Some text</p> \" -> \"Some text\") tag.contents : A list of the immediate children of this node You can also use these functions to find nodes. tag.find_all(tag_name, attrs=attributes_dict) : Returns a list of matching nodes tag.find(tag_name, attrs=attributes_dict) : Returns first matching node BeautifulSoup is a very powerful library -- much more info here: https://www.crummy.com/software/BeautifulSoup/bs4/doc/ Let's practice some BeautifulSoup commands, Print a cleaned-up version of the raw HTML Which function should we use from above? In [0]: pretty_soup = soup . prettify () print ( pretty_soup [: 500 ]) #what about negative indices? Find the first \"title\" object In [0]: soup . select ( \"title\" )[: 50 ] Extract the text of first \"heading\" object given by $<h3>$ In [0]: soup . select_one ( 'a h3' ) Extracting award data Let's use the structure of the HTML document to extract the data we want. From inspecting the page in DevTools, we found that each award is in a div with a by_year class. Let's get all of them. In [26]: award_nodes = soup . select ( '.by_year' ) #<div class =\"by year\" len ( award_nodes ) Let's pull out an example. In [27]: award_node = award_nodes [ 200 ] In [0]: award_node . prettify () We use the HTML library to render the HTML below In [0]: HTML ( award_node . prettify ()) Let's practice getting data out of a BS node (award_node) The prize title Check the html from above and note that the prize title is in the h3 tag. In [29]: award_node . select_one ( 'h3' ) . text How do we separate the year from the selected prize title? In [30]: award_node . select_one ( 'h3' ) . text [ - 4 :] How do we drop the year from the title? In [31]: award_node . select_one ( 'h3' ) . text [: - 4 ] . strip () Let's put them into functions: In [32]: # wrap the above code inside a function def get_award_title ( award_node ): return award_node . select_one ( 'h3' ) . text [ ___ ] . strip () def get_award_year ( award_node ): return int ( award_node . select_one ( 'h3' ) . text [ ___ ]) Make a list of titles for all awards In [54]: #original code: list_awards = [] for award_node in award_nodes : list_awards . append ( get_award_title ( ___ )) list_awards [: 50 ] How can we make this into a oneliner? We can use list comprehension l = [ f ( x ) for x in some_list ] which is equivalent to l = [] for x in some_list : element = f ( x ) l . append ( element ) List comprehensions are explained in the slides from us linked above. In [34]: # (TODO) use list comprehension to get a list of titles [ get_award_title ( ___ ) for award_node in award_nodes ] The recipients Check the html from above and note that the prize title is in the h6 a selector. In [35]: award_node . select ( 'h6 a' ) How do we handle there being more than one? In [36]: [ node . text for node in award_node . select ( 'h6 a' )] Let's encapsulate this process into a function and make it into a function. In [37]: def get_recipients ( award_node ): return [ node . text for node in award_node . select ( 'h6 a' )] We'll leave them as a list for now, to return to this later. This is how you would get the links: (Relevant for the homework) In [38]: [ state_node . get ( \"href\" ) for state_node in award_node . select ( 'h6 a' )] The prize \"motivation\" How would you get the 'motivation'/reason of the prize from the following award_node ? In [39]: award_node = award_nodes [ 200 ] award_node In [40]: print ( award_node . select ( 'p' )[ 0 ] . text ); Putting everything into functions: In [41]: def get_award_motivation ( award_node ): award_node = award_node . select_one ( 'p' ) if not award_node : #0, [], None, and {} all default to False in a python conditional statement. return None return award_node . text Let's create a Pandas dataframe Next, we parse the collected data and create a pandas.DataFrame . A DataFrame is like a table, where each row corresponds to a data entry and each column corresponds to a feature. Once we have a DataFrame, we can easily export it to our disk in CSV, JSON, or other formats. The easiest way to create a DataFrame is to build a list of dictionaries. Dictionaries are a pre-requisite for this lab. Refer to the slides from us here for a better understanding. Each entry (dict) in the list is a data point, where keys are column names in the table. Let's see it in action. In [1]: awards = [] for award_node in soup . select ( '.by_year' ): recipients = get_recipients ( award_node ) # Initialize the dictionary award = {} #{key: value} # Call `get_award_title` to get the title of award_node award [ 'title' ] = get_award_title ( award_node ) # Call `get_award_title` to get the year of award_node award [ 'year' ] = get_award_year ( award_node ) # Call `get_recipients` to get the list of recipients of award_node award [ 'recipients' ] = recipients # Count number of recipients using the built-in `len()` function award [ 'num_recipients' ] = len ( recipients ) # (TODO) call `get_award_motivation` to get the motivation of award_node award [ 'motivation' ] = get_award_motivation ( award_node ) awards . append ( award ) awards [ 0 : 2 ] In [2]: # (TODO) convert the list of dictionaries to a pandas DataFrame df_awards_raw = pd . DataFrame ( awards ) In [55]: df_awards_raw To export the data to a local CSV file, let's used the .to_csv() method. After you run the follwing code, you can find a scraped_awards.csv in the same directory with this notebook. You can open the notebook using Microsoft Excel or Numbers, but make sure you are using the UTF-8 codec. In [45]: df_awards_raw . to_csv ( 'scraped_awards.csv' ) Some quick EDA. In [46]: df_awards_raw . info () In [47]: df_awards_raw . year . min () What is going on with the recipients column? In [48]: df_awards_raw . head () Visualizing Number of Recipients by Year Finally, we visualize the number of recipients for each Nobel Prize by year. Don't worry about the syntax for the moment, you'll get used to it in future exercise. In [49]: titles = set ( df_awards_raw . title ) fig = plt . figure ( figsize = ( 20 , 44 ), dpi = 100 ) axes = fig . subplots ( len ( titles ), 1 ) for title , ax in zip ( titles , axes ): # (TODO) select entries whose titles match `title` plot_df = df_awards_raw [ df_awards_raw . title == title ] # (TODO) plot the selected entries using bar-plot, where x-axis is year and y-axis is number of recipeints ax . bar ( ___ , ___ , color = \"#97CFC4\" ) ax . set_title ( ___ ) ax . set_xlabel ( ___ ) ax . set_ylabel ( ___ ) In [50]: # `counter` is used to save the number of nobel prize winners every year counter = {} for year in range ( min ( df_awards_raw . year ), max ( df_awards_raw . year ) + 1 ): # (TODO) compute total number of recipients that year count = df_awards_raw [ df_awards_raw . year == year ] . num_recipients . sum () counter [ year ] = count fig = plt . figure ( figsize = ( 20 , 6 ), dpi = 100 ) ax = fig . add_subplot ( 1 , 1 , 1 ) # (TODO) make another bar-plot, where x-axis is year and y-axis is total number of recipeints ax . bar ( ___ , ___ , color = \"#97CFC4\" ) ax . set_title ( 'Total Amount of Nobel Prize' ) ax . set_xlabel ( 'year' ) ax . set_ylabel ( '#Recipients' ); End of Normal Lab Optional Further Readings Here are a couple resources that he referenced early in his course that helped solidify my understanding of data science. 50 Years of Data Science by Dave Donoho (2017) Tidy data by Hadley Wickam (2014)",
        "tags": "labs",
        "url": "labs/lab1/notebook-1/"
    }, {
        "title": "Lecture 1: Introduction to CS109A",
        "text": "Slides Lecture 1 : Introduction to CS109A (PDF) Exercises Lecture 1: Exercise: The Data Science Process [Notebook]",
        "tags": "lectures",
        "url": "lectures/lecture01/"
    }, {
        "title": "Lecture 1: Introduction to CS109A",
        "text": "session1_scaffold CS109A Introduction to Data Science Lecture 1, Exercise 1: The Data Science Process Harvard University Fall 2021 Instructors : Pavlos Protopapas and Natesh Pillai Title : Exercise: The Data Science Process Description : The aim of this exercise is to understand all the steps involved in a Data Science setting. Data Description: Hubway was metro-Boston's public bike share program, with more than 1600 bikes at 160+ stations across the Greater Boston area. Hubway was owned by four municipalities in the area. By 2016, Hubway operated 185 stations and 1750 bicycles, with 5 million rides since launching in 2011. In April 2017, Hubway held a Data Visualization Challenge at the Microsoft NERD Center in Cambridge, releasing 5 years of trip data. Instructions: Read the data files hubway_stations.csv and hubway_trips.csv into separate pandas dataframes. Get a quick understanding of the columns present in the data and their types. Remove all the data points with null values in any one (or more) of the columns. Create a new column age that gives the age of the rider using their birth date. Perform relevant EDA to answer the questions asked on the scaffold. Create a simple linear model to predict the number of checkouts based on the distance of the bikes from the centre of the city. Visualize the prediction against the data. Hints: pd.read_csv(filename) Returns a pandas dataframe containing the data and labels from the file data pd.describe() Generates descriptive statistics of the dataframe. pd.dropna() Removes missing values from the dataframe. It removes either the columns or rows based on the axis parameter. In [1]: # Import necessary libraries import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from helper import get_distance from math import radians , cos , sin , asin , sqrt from sklearn.linear_model import LinearRegression % matplotlib inline In [3]: # Read the data from the file \"hubway_stations.csv\" stations = pd . read_csv ( \"hubway_stations.csv\" ) # Read the data from the file \"hubway_trips.csv\" trips = pd . read_csv ( \"hubway_trips.csv\" ) In [4]: # Take a quick look at the stations data stations . head () Out[4]: id terminal station municipal lat lng status 0 3 B32006 Colleges of the Fenway Boston 42.340021 -71.100812 Existing 1 4 C32000 Tremont St. at Berkeley St. Boston 42.345392 -71.069616 Existing 2 5 B32012 Northeastern U / North Parking Lot Boston 42.341814 -71.090179 Existing 3 6 D32000 Cambridge St. at Joy St. Boston 42.361285 -71.065140 Existing 4 7 A32000 Fan Pier Boston 42.353412 -71.044624 Existing In [5]: # Take a quick look at the trips data trips . head () Out[5]: Unnamed: 0 hubway_id duration start_date strt_statn end_date end_statn zip_code birth_date gender 0 426015 482077 675 8/18/2012 19:48:00 8.0 8/18/2012 20:00:00 8.0 '02134 1983.0 Male 1 193080 220612 204 4/26/2012 18:14:00 31.0 4/26/2012 18:17:00 64.0 '02210 1953.0 Male 2 530051 598721 888 9/23/2012 09:26:00 39.0 9/23/2012 09:41:00 39.0 '02118 1985.0 Male 3 484594 547645 526 9/8/2012 12:55:00 88.0 9/8/2012 13:04:00 72.0 '02139 1985.0 Male 4 291265 332163 554 6/21/2012 18:53:00 47.0 6/21/2012 19:02:00 54.0 '02113 1986.0 Female UNDERSTANDING THE DATA It is important to completely understand all the information provided in the data. The first step for this is to take a closer look at all the columns and understand their types. In [6]: # Getting the data type of each column in the stations data stations . dtypes Out[6]: id int64 terminal object station object municipal object lat float64 lng float64 status object dtype: object In [7]: # Getting the data type of each column in the trips data trips . dtypes Out[7]: Unnamed: 0 int64 hubway_id int64 duration int64 start_date object strt_statn float64 end_date object end_statn float64 zip_code object birth_date float64 gender object dtype: object â¸ Based on the datatypes, do you see any possible issues? In [11]: ### edTest(test_chow0) ### # Submit the questions as a string below. Separate each question by an eroteme (question mark) answer0 = 'Yes' In [15]: # Getting some statistical information from the stations data stations . describe () Out[15]: id lat lng count 142.000000 142.000000 142.000000 mean 74.323944 42.354820 -71.089087 std 41.389098 0.020089 0.027111 min 3.000000 42.309467 -71.146452 25% 39.250000 42.341652 -71.113183 50% 74.500000 42.353373 -71.089191 75% 109.750000 42.366265 -71.065210 max 145.000000 42.404490 -71.035705 In [14]: # Getting some statistical information from the trips data trips . describe () Out[14]: Unnamed: 0 hubway_id duration strt_statn end_statn birth_date count 210239.000000 210239.000000 2.102390e+05 210239.000000 210239.000000 210239.000000 mean 283491.142771 321401.542806 7.794459e+02 36.727567 36.662261 1976.285594 std 153204.497985 173059.875974 1.349006e+04 18.592716 18.551934 11.002281 min 0.000000 8.000000 0.000000e+00 3.000000 3.000000 1932.000000 25% 153899.000000 174103.000000 3.460000e+02 22.000000 22.000000 1969.000000 50% 280081.000000 319856.000000 5.320000e+02 38.000000 38.000000 1979.000000 75% 414740.000000 469290.000000 8.280000e+02 50.000000 50.000000 1985.000000 max 549285.000000 620312.000000 5.351083e+06 98.000000 98.000000 1995.000000 â¸ Based on your understanding of the data, what questions would you like to have answered? In [16]: ### edTest(test_chow1) ### # Submit the questions as a string below. Separate each question by an eroteme (question mark) answer1 = 'Most common station, Who take more trips (male or female)' DATA PRE-PROCESSING Let us clean the data before breaking it down further. There are many pre-processing techqniues which will be covered later in the course. In [17]: # Delete all the rows of the stations dataframe with null values # axis=0 indicates that the rows with null values are to be deleted stations . dropna ( axis = 0 , inplace = True ) In [18]: # Delete all the rows of the trips dataframe with null values trips . dropna ( axis = 0 , inplace = True ) In [19]: # Create a new column that gives the age of each rider age_col = 2021.0 - trips [ 'birth_date' ] . values # Add the age column to the trips dataframe trips [ 'age' ] = age_col # Drop the 'birth_date' column trips . drop ( 'birth_date' , axis = 1 , inplace = True ) EXPLORATORY DATA ANALYSIS (EDA) As you would have noticed, the information extracted above is not sufficient to answer most of the questions and is definitely not sufficient to ask relevant questions. Hence, we will need to perform additional data analysis. In [20]: # Find out if there is any relation between the predictors of the stations data sns . pairplot ( stations ); Out[20]: <seaborn.axisgrid.PairGrid at 0x7f7d665c8850> â¸ Based on the plot above, do you notice any recognizable relationship between any of the columns? A. The latitude and longitude are directly proportional to each other. B. The latitude and longitude are inversely proportional to each other. C. It is random. There seems to be no relation between the latitude and longitude. In [25]: ### edTest(test_chow2) ### # Submit an answer choice as a string below (eg. if you choose option A, put 'A') answer2 = 'C' In [26]: # Get the unique number of male and female bike riders gender_counts = np . unique ( trips [ 'gender' ] . values , return_counts = True ) # Plotting the genders of riders as a histogram fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 6 )) ax . bar ( range ( 2 ), width = 0.5 , height = gender_counts [ 1 ], color = [ '#e4a199' , 'green' ], alpha = 0.5 ) ax . set_xticks ([ 0 , 1 ]) ax . set_xticklabels ( gender_counts [ 0 ]) ax . set_title ( 'Users by Gender' ); Out[26]: Text(0.5, 1.0, 'Users by Gender') â¸ Based on the plot above, who uses the bikes more, men or women? A. Women B. Men C. Can't say In [27]: ### edTest(test_chow3) ### # Submit an answer choice as a string below (eg. if you choose option A, put 'A') answer3 = 'B' In [28]: # Plotting the usage of bikes based on the ages of riders fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 6 )) age_counts = np . unique ( trips [ 'age' ], return_counts = True ) ax . bar ( age_counts [ 0 ], age_counts [ 1 ], align = 'center' , width = 0.4 , alpha = 0.6 ) ax . axvline ( x = np . mean ( age_col ), color = 'red' , label = 'average age' ) ax . axvline ( x = np . percentile ( age_col , 25 ), color = 'red' , linestyle = '--' , label = 'lower quartile' ) ax . axvline ( x = np . percentile ( age_col , 75 ), color = 'red' , linestyle = '--' , label = 'upper quartile' ) ax . set_xlim ([ 1 , 90 ]) ax . set_xlabel ( 'Age' ) ax . set_ylabel ( 'Number of Checkouts' ) ax . legend () ax . set_title ( 'Users by Age' ) plt . tight_layout () â¸ Based on the plot above, who uses the bikes more, older or younger people? A. Older B. Younger In [36]: ### edTest(test_chow4) ### # Submit an answer choice as a string below (eg. if you choose option A, put 'A') answer4 = 'B' In [30]: # Get the hourwise bike checkouts check_out_hours = trips [ 'start_date' ] . apply ( lambda s : int ( s [ - 8 : - 6 ])) # Plotting the bike checkouts hourwise fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 5 )) check_out_counts = np . unique ( check_out_hours , return_counts = True ) ax . bar ( check_out_counts [ 0 ], check_out_counts [ 1 ], align = 'center' , width = 0.4 , alpha = 0.6 ) ax . set_xlim ([ - 1 , 24 ]) ax . set_xticks ( range ( 24 )) ax . set_xlabel ( 'Hour of Day' ) ax . set_ylabel ( 'Number of Checkouts' ) ax . set_title ( 'Time of Day vs Checkouts' ) plt . show () â¸ Based on the plot above, when is the biggest rush hour? In [31]: ### edTest(test_chow5) ### # Submit the integer value below within the quotes answer5 = '17' MORE QUESTIONS? There are many questions that haven't been covered here. For what reasons are the bikes being used? Recreation, traffic or for health benfits? Is the usage more during the weekdays or weekends? Are people using bikes more in Boston or Cambridge? Feel free to add new code cells and find the answers. DATA MODELLING There are some questions that cannot be answered with simple graphing techniques. It requires combining different variables. How does user demographics impact the duration the bikes are being used? Or where they are being checked out? How does weather or traffic conditions impact bike usage? How do the characteristics of the station location affect the number of bikes being checked out? Let us try to answer the question: How does the distance from the center of the city affect the bike usage? In [32]: # Helper function within helper.py to compute the distance of the bike from the city center # It returns a dataframe that has the column of the checkout distance from the center counts_df = get_distance () # Take a quick look at the dataframe counts_df . head () Out[32]: id checkouts terminal station municipal lat lng status dist_to_center 0 3.0 1878 B32006 Colleges of the Fenway Boston 42.340021 -71.100812 Existing 2.335706 1 4.0 3376 C32000 Tremont St. at Berkeley St. Boston 42.345392 -71.069616 Existing 0.853095 2 5.0 1913 B32012 Northeastern U / North Parking Lot Boston 42.341814 -71.090179 Existing 1.802423 3 6.0 3616 D32000 Cambridge St. at Joy St. Boston 42.361285 -71.065140 Existing 0.467803 4 7.0 1384 A32000 Fan Pier Boston 42.353412 -71.044624 Existing 0.807582 In [33]: # Let us use a straight line y = ax + b to model the relation # between the number of checkouts and distance to the city center beta0 = 4394 beta1 = - 1175 y_pred = beta0 + beta1 * counts_df [ 'dist_to_center' ] . values In [34]: # Plotting the true data and the prediction fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 5 )) ax . scatter ( counts_df [ 'dist_to_center' ] . values , counts_df [ 'checkouts' ] . values , label = 'Data' , s = 70 , c = '#e4a199' ) ax . plot ( counts_df [ 'dist_to_center' ] . values , y_pred , c = 'blue' , alpha = 0.5 , linewidth = 2 , label = 'Prediction' ) ax . set_xlabel ( 'Distance to City Center (Miles)' ) ax . set_ylabel ( 'Number of Checkouts' ) ax . set_title ( 'Distance to City Center vs Checkouts' ); ax . legend (); Out[34]: <matplotlib.legend.Legend at 0x7f7d41a5c820> â¸ Based on our \"linear\" model, what would most likely be the number of checkouts for a distance of 2.5 miles from the city center? A. 45000 B. 12530 C. 1450 D. 650 In [35]: ### edTest(test_chow6) ### # Submit an answer choice as a string below (eg. if you choose option A, put 'A') answer6 = 'C' In [ ]:",
        "tags": "Lectures",
        "url": "lectures/lecture01/notebook/"
    }, {
        "title": "CS109a: Introduction to Data Science",
        "text": "Fall 2021 Pavlos Protopapas and Natesh Pillai Welcome to CS109a/STAT121a/AC209a, also offered by the DCE as CSCI E-109a, Introduction to Data Science. This course is the first half of a oneâ€year course to data science. We will focus on the analysis of data to perform predictions using statistical and machine learning methods. Topics include data scraping, data management, data visualization, regression and classification methods, and deep neural networks. You will get ample practice through weekly homework assignments. The class material integrates the five key facets of an investigation using data: 1. data collection â€ data wrangling, cleaning, and sampling to get a suitable data set 2. data management â€ accessing data quickly and reliably 3. exploratory data analysis â€“ generating hypotheses and building intuition 4. prediction or statistical learning 5. communication â€“ summarizing results through visualization, stories, and interpretable summaries Only one of CS 109a, AC 209a, or Stat 121a can be taken for credit. Students who have previously taken CS 109, AC 209, or Stat 121 cannot take CS 109a, AC 209a, or Stat 121a for credit. Important Dates: Tuesday 9/8 - HW1 released Wednesday 9/8 - HW0 due at 11:59pm EST Helpline: cs109a2021@gmail.com Lectures: Mon & Wed 9:45-11am - SEC Room 1.321 Lab: Fri 9:45-11am - l114 Western Ave., Allston Room 2.111 Advanced Sections: Wed 12:45-2pm [starting 9/29] - SEC Room LL2.229 (See course schedule for dates) Office Hours: Current Schedule Here With More To Come Course material can be viewed in the public GitHub repository . Previous Material 2020 2019 2018 2017 2015 2014 . 2013",
        "tags": "pages",
        "url": "pages/cs109a-introduction-to-data-science/"
    }, {"title": "Lab 05:", "text": "Slides", "tags": "labs", "url": "labs/lab05/"}, {
        "title": "Lab 07:",
        "text": "Slides",
        "tags": "labs",
        "url": "labs/lab07/"
    }, {"title": "Lab 08:", "text": "Slides", "tags": "labs", "url": "labs/lab08/"}, {
        "title": "Lab 09:",
        "text": "Slides",
        "tags": "labs",
        "url": "labs/lab09/"
    }, {"title": "Lab 10:", "text": "Slides", "tags": "labs", "url": "labs/lab10/"}, {
        "title": "Lab 11:",
        "text": "Slides",
        "tags": "labs",
        "url": "labs/lab11/"
    }, {
        "title": "S-Section 04: Regularization and Model Selection",
        "text": "Jupyter Notebooks S-Section 4: Regularization and Model Selection",
        "tags": "sections",
        "url": "sections/section4/"
    }]
}